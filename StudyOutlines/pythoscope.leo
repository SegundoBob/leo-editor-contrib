<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet ekr_test?>
<leo_file>
<leo_header file_format="2" tnodes="0" max_tnode_index="0" clone_windows="0"/>
<globals body_outline_ratio="0.5" body_secondary_ratio="0.5">
	<global_window_position top="50" left="50" height="500" width="700"/>
	<global_log_window_position top="0" left="0" height="0" width="0"/>
</globals>
<preferences/>
<find_panel_settings/>
<vnodes>
<v t="ekr.20090302123851.3"><vh>Startup</vh>
<v t="ekr.20090302123851.1"><vh>@chapters</vh></v>
</v>
<v t="ekr.20100315100121.2449"><vh>lib2to3</vh>
<v t="ekr.20090302123851.372"><vh>@path c:\leo.repo\pythoscope\lib2to3</vh>
<v t="ekr.20090302123851.373"><vh>test of path directives</vh></v>
<v t="ekr.20090302123851.374" a="E"><vh>@@auto patcomp.py</vh>
<v t="ekr.20090302123851.375"><vh>patcomp declarations</vh></v>
<v t="ekr.20090302123851.376"><vh>tokenize_wrapper</vh></v>
<v t="ekr.20090302123851.377"><vh>class PatternCompiler</vh>
<v t="ekr.20090302123851.378"><vh>__init__</vh></v>
<v t="ekr.20090302123851.379"><vh>compile_pattern</vh></v>
<v t="ekr.20090302123851.380"><vh>compile_node</vh></v>
<v t="ekr.20090302123851.381"><vh>compile_basic</vh></v>
<v t="ekr.20090302123851.382"><vh>get_int</vh></v>
</v>
<v t="ekr.20090302123851.383"><vh>pattern_convert</vh></v>
<v t="ekr.20090302123851.384"><vh>compile_pattern</vh></v>
</v>
<v t="ekr.20090302123851.385"><vh>@@auto pygram.py</vh>
<v t="ekr.20090302123851.386"><vh>pygram declarations</vh></v>
<v t="ekr.20090302123851.387"><vh>class Symbols</vh>
<v t="ekr.20090302123851.388"><vh>__init__</vh></v>
</v>
<v t="ekr.20090302123851.389"><vh>parenthesize</vh></v>
</v>
<v t="ekr.20090302123851.390"><vh>@@auto pytree.py</vh>
<v t="ekr.20090302123851.391"><vh>pytree declarations</vh></v>
<v t="ekr.20090302123851.392"><vh>type_repr</vh></v>
<v t="ekr.20090302123851.393"><vh>class Base</vh>
<v t="ekr.20090302123851.394"><vh>__new__</vh></v>
<v t="ekr.20090302123851.395"><vh>__eq__</vh></v>
<v t="ekr.20090302123851.396"><vh>__ne__</vh></v>
<v t="ekr.20090302123851.397"><vh>_eq</vh></v>
<v t="ekr.20090302123851.398"><vh>clone</vh></v>
<v t="ekr.20090302123851.399"><vh>post_order</vh></v>
<v t="ekr.20090302123851.400"><vh>pre_order</vh></v>
<v t="ekr.20090302123851.401"><vh>set_prefix</vh></v>
<v t="ekr.20090302123851.402"><vh>get_prefix</vh></v>
<v t="ekr.20090302123851.403"><vh>replace</vh></v>
<v t="ekr.20090302123851.404"><vh>get_lineno</vh></v>
<v t="ekr.20090302123851.405"><vh>changed</vh></v>
<v t="ekr.20090302123851.406"><vh>remove</vh></v>
<v t="ekr.20090302123851.407"><vh>get_next_sibling</vh></v>
<v t="ekr.20090302123851.408"><vh>get_prev_sibling</vh></v>
<v t="ekr.20090302123851.409"><vh>get_suffix</vh></v>
</v>
<v t="ekr.20090302123851.410"><vh>class Node</vh>
<v t="ekr.20090302123851.411"><vh>__init__</vh></v>
<v t="ekr.20090302123851.412"><vh>__repr__</vh></v>
<v t="ekr.20090302123851.413"><vh>__str__</vh></v>
<v t="ekr.20090302123851.414"><vh>_eq</vh></v>
<v t="ekr.20090302123851.415"><vh>clone</vh></v>
<v t="ekr.20090302123851.416"><vh>post_order</vh></v>
<v t="ekr.20090302123851.417"><vh>pre_order</vh></v>
<v t="ekr.20090302123851.418"><vh>set_prefix</vh></v>
<v t="ekr.20090302123851.419"><vh>get_prefix</vh></v>
<v t="ekr.20090302123851.420"><vh>set_child</vh></v>
<v t="ekr.20090302123851.421"><vh>insert_child</vh></v>
<v t="ekr.20090302123851.422"><vh>append_child</vh></v>
</v>
<v t="ekr.20090302123851.423"><vh>class Leaf</vh>
<v t="ekr.20090302123851.424"><vh>__init__</vh></v>
<v t="ekr.20090302123851.425"><vh>__repr__</vh></v>
<v t="ekr.20090302123851.426"><vh>__str__</vh></v>
<v t="ekr.20090302123851.427"><vh>_eq</vh></v>
<v t="ekr.20090302123851.428"><vh>clone</vh></v>
<v t="ekr.20090302123851.429"><vh>post_order</vh></v>
<v t="ekr.20090302123851.430"><vh>pre_order</vh></v>
<v t="ekr.20090302123851.431"><vh>set_prefix</vh></v>
<v t="ekr.20090302123851.432"><vh>get_prefix</vh></v>
</v>
<v t="ekr.20090302123851.433"><vh>convert</vh></v>
<v t="ekr.20090302123851.434"><vh>class BasePattern</vh>
<v t="ekr.20090302123851.435"><vh>__new__</vh></v>
<v t="ekr.20090302123851.436"><vh>__repr__</vh></v>
<v t="ekr.20090302123851.437"><vh>optimize</vh></v>
<v t="ekr.20090302123851.438"><vh>match</vh></v>
<v t="ekr.20090302123851.439"><vh>match_seq</vh></v>
<v t="ekr.20090302123851.440"><vh>generate_matches</vh></v>
</v>
<v t="ekr.20090302123851.441"><vh>class LeafPattern</vh>
<v t="ekr.20090302123851.442"><vh>__init__</vh></v>
<v t="ekr.20090302123851.443"><vh>match</vh></v>
<v t="ekr.20090302123851.444"><vh>_submatch</vh></v>
</v>
<v t="ekr.20090302123851.445"><vh>class NodePattern</vh>
<v t="ekr.20090302123851.446"><vh>__init__</vh></v>
<v t="ekr.20090302123851.447"><vh>_submatch</vh></v>
</v>
<v t="ekr.20090302123851.448"><vh>class WildcardPattern</vh>
<v t="ekr.20090302123851.449"><vh>__init__</vh></v>
<v t="ekr.20090302123851.450"><vh>optimize</vh></v>
<v t="ekr.20090302123851.451"><vh>match</vh></v>
<v t="ekr.20090302123851.452"><vh>match_seq</vh></v>
<v t="ekr.20090302123851.453"><vh>generate_matches</vh></v>
<v t="ekr.20090302123851.454"><vh>_bare_name_matches</vh></v>
<v t="ekr.20090302123851.455"><vh>_recursive_matches</vh></v>
</v>
<v t="ekr.20090302123851.456"><vh>class NegatedPattern</vh>
<v t="ekr.20090302123851.457"><vh>__init__</vh></v>
<v t="ekr.20090302123851.458"><vh>match</vh></v>
<v t="ekr.20090302123851.459"><vh>match_seq</vh></v>
<v t="ekr.20090302123851.460"><vh>generate_matches</vh></v>
</v>
<v t="ekr.20090302123851.461"><vh>generate_matches</vh></v>
</v>
<v t="ekr.20090302123851.462"><vh>@@auto __init__.py</vh></v>
</v>
<v t="ekr.20090302123851.463"><vh>@path c:\leo.repo\pythoscope\lib2to3\pgen2</vh>
<v t="ekr.20090302123851.464"><vh>Comments from parse.c</vh></v>
<v t="ekr.20090302123851.465"><vh>@@auto conv.py</vh>
<v t="ekr.20090302123851.466"><vh>conv declarations</vh></v>
<v t="ekr.20090302123851.467"><vh>class Converter</vh>
<v t="ekr.20090302123851.468"><vh>run</vh></v>
<v t="ekr.20090302123851.469"><vh>parse_graminit_h</vh></v>
<v t="ekr.20090302123851.470"><vh>parse_graminit_c</vh></v>
<v t="ekr.20090302123851.471"><vh>finish_off</vh></v>
</v>
</v>
<v t="ekr.20090302123851.472"><vh>@@auto driver.py</vh>
<v t="ekr.20090302123851.473"><vh>driver declarations</vh></v>
<v t="ekr.20090302123851.474"><vh>class Driver</vh>
<v t="ekr.20090302123851.475"><vh>__init__</vh></v>
<v t="ekr.20090302123851.476"><vh>parse_tokens</vh></v>
<v t="ekr.20090302123851.477"><vh>parse_stream_raw</vh></v>
<v t="ekr.20090302123851.478"><vh>parse_stream</vh></v>
<v t="ekr.20090302123851.479"><vh>parse_file</vh></v>
<v t="ekr.20090302123851.480"><vh>parse_string</vh></v>
</v>
<v t="ekr.20090302123851.481"><vh>generate_lines</vh></v>
<v t="ekr.20090302123851.482"><vh>load_grammar</vh></v>
<v t="ekr.20090302123851.483"><vh>_newer</vh></v>
</v>
<v t="ekr.20090302123851.484"><vh>@@auto grammar.py </vh>
<v t="ekr.20090302123851.485"><vh>grammar declarations</vh></v>
<v t="ekr.20090302123851.486"><vh>class Grammar</vh>
<v t="ekr.20090302123851.487"><vh>__init__</vh></v>
<v t="ekr.20090302123851.488"><vh>dump</vh></v>
<v t="ekr.20090302123851.489"><vh>load</vh></v>
<v t="ekr.20090302123851.490"><vh>report</vh></v>
</v>
</v>
<v t="ekr.20090302123851.491"><vh>@@auto literals.py</vh>
<v t="ekr.20090302123851.492"><vh>literals declarations</vh></v>
<v t="ekr.20090302123851.493"><vh>escape</vh></v>
<v t="ekr.20090302123851.494"><vh>evalString</vh></v>
<v t="ekr.20090302123851.495"><vh>test</vh></v>
</v>
<v t="ekr.20090302123851.496"><vh>@@auto parse.py</vh>
<v t="ekr.20090302123851.497"><vh>parse declarations</vh></v>
<v t="ekr.20090302123851.498"><vh>class ParseError</vh>
<v t="ekr.20090302123851.499"><vh>__init__</vh></v>
<v t="ekr.20090302123851.500"><vh>__reduce__</vh></v>
</v>
<v t="ekr.20090302123851.501"><vh>class Parser</vh>
<v t="ekr.20090302123851.502"><vh>__init__</vh></v>
<v t="ekr.20090302123851.503"><vh>setup</vh></v>
<v t="ekr.20090302123851.504"><vh>addtoken</vh></v>
<v t="ekr.20090302123851.505"><vh>classify</vh></v>
<v t="ekr.20090302123851.506"><vh>shift</vh></v>
<v t="ekr.20090302123851.507"><vh>push</vh></v>
<v t="ekr.20090302123851.508"><vh>pop</vh></v>
</v>
</v>
<v t="ekr.20090302123851.509"><vh>@@auto pgen.py</vh>
<v t="ekr.20090302123851.510"><vh>pgen declarations</vh></v>
<v t="ekr.20090302123851.511"><vh>class PgenGrammar</vh></v>
<v t="ekr.20090302123851.512"><vh>class ParserGenerator</vh>
<v t="ekr.20090302123851.513"><vh>__init__</vh></v>
<v t="ekr.20090302123851.514"><vh>make_grammar</vh></v>
<v t="ekr.20090302123851.515"><vh>make_first</vh></v>
<v t="ekr.20090302123851.516"><vh>make_label</vh></v>
<v t="ekr.20090302123851.517"><vh>addfirstsets</vh></v>
<v t="ekr.20090302123851.518"><vh>calcfirst</vh></v>
<v t="ekr.20090302123851.519"><vh>parse</vh></v>
<v t="ekr.20090302123851.520"><vh>make_dfa</vh></v>
<v t="ekr.20090302123851.521"><vh>dump_nfa</vh></v>
<v t="ekr.20090302123851.522"><vh>dump_dfa</vh></v>
<v t="ekr.20090302123851.523"><vh>simplify_dfa</vh></v>
<v t="ekr.20090302123851.524"><vh>parse_rhs</vh></v>
<v t="ekr.20090302123851.525"><vh>parse_alt</vh></v>
<v t="ekr.20090302123851.526"><vh>parse_item</vh></v>
<v t="ekr.20090302123851.527"><vh>parse_atom</vh></v>
<v t="ekr.20090302123851.528"><vh>expect</vh></v>
<v t="ekr.20090302123851.529"><vh>gettoken</vh></v>
<v t="ekr.20090302123851.530"><vh>raise_error</vh></v>
</v>
<v t="ekr.20090302123851.531"><vh>class NFAState</vh>
<v t="ekr.20090302123851.532"><vh>__init__</vh></v>
<v t="ekr.20090302123851.533"><vh>addarc</vh></v>
</v>
<v t="ekr.20090302123851.534"><vh>class DFAState</vh>
<v t="ekr.20090302123851.535"><vh>__init__</vh></v>
<v t="ekr.20090302123851.536"><vh>addarc</vh></v>
<v t="ekr.20090302123851.537"><vh>unifystate</vh></v>
<v t="ekr.20090302123851.538"><vh>__eq__</vh></v>
</v>
<v t="ekr.20090302123851.539"><vh>generate_grammar</vh></v>
</v>
<v t="ekr.20090302123851.540"><vh>@@auto token.py</vh>
<v t="ekr.20090302123851.541"><vh>token declarations</vh></v>
<v t="ekr.20090302123851.542"><vh>ISTERMINAL</vh></v>
<v t="ekr.20090302123851.543"><vh>ISNONTERMINAL</vh></v>
<v t="ekr.20090302123851.544"><vh>ISEOF</vh></v>
</v>
<v t="ekr.20090302123851.545"><vh>@@auto tokenize.py</vh>
<v t="ekr.20090302123851.546"><vh>tokenize declarations</vh></v>
<v t="ekr.20090302123851.547"><vh>group</vh></v>
<v t="ekr.20090302123851.548"><vh>any</vh></v>
<v t="ekr.20090302123851.549"><vh>maybe</vh></v>
<v t="ekr.20090302123851.550"><vh>class TokenError</vh></v>
<v t="ekr.20090302123851.551"><vh>class StopTokenizing</vh></v>
<v t="ekr.20090302123851.552"><vh>printtoken</vh></v>
<v t="ekr.20090302123851.553"><vh>tokenize</vh></v>
<v t="ekr.20090302123851.554"><vh>tokenize_loop</vh></v>
<v t="ekr.20090302123851.555"><vh>class Untokenizer</vh>
<v t="ekr.20090302123851.556"><vh>__init__</vh></v>
<v t="ekr.20090302123851.557"><vh>add_whitespace</vh></v>
<v t="ekr.20090302123851.558"><vh>untokenize</vh></v>
<v t="ekr.20090302123851.559"><vh>compat</vh></v>
</v>
<v t="ekr.20090302123851.560"><vh>untokenize</vh></v>
<v t="ekr.20090302123851.561"><vh>generate_tokens</vh></v>
</v>
<v t="ekr.20090302123851.562"><vh>@@auto __init__.py</vh>
<v t="ekr.20090302123851.563"><vh>__init__ declarations</vh></v>
</v>
</v>
</v>
<v t="ekr.20090302123851.4" a="E"><vh>pythoscope</vh>
<v t="ekr.20090302123851.5"><vh>@path c:\leo.repo\pythoscope\pythoscope</vh>
<v t="ekr.20090302123851.6"><vh>@@auto\pythoscope\astvisitor.py</vh>
<v t="ekr.20090302123851.7"><vh>astvisitor declarations</vh></v>
<v t="ekr.20090302123851.8"><vh>clone</vh></v>
<v t="ekr.20090302123851.9"><vh>create_import</vh></v>
<v t="ekr.20090302123851.10"><vh>descend</vh></v>
<v t="ekr.20090302123851.11"><vh>find_last_leaf</vh></v>
<v t="ekr.20090302123851.12"><vh>get_starting_whitespace</vh></v>
<v t="ekr.20090302123851.13"><vh>remove_trailing_whitespace</vh></v>
<v t="ekr.20090302123851.14"><vh>parse</vh></v>
<v t="ekr.20090302123851.15"><vh>parse_fragment</vh></v>
<v t="ekr.20090302123851.16"><vh>regenerate</vh></v>
<v t="ekr.20090302123851.17"><vh>class ASTError</vh></v>
<v t="ekr.20090302123851.18"><vh>is_leaf_of_type</vh></v>
<v t="ekr.20090302123851.19"><vh>is_node_of_type</vh></v>
<v t="ekr.20090302123851.20"><vh>leaf_value</vh></v>
<v t="ekr.20090302123851.21"><vh>remove_commas</vh></v>
<v t="ekr.20090302123851.22"><vh>remove_defaults</vh></v>
<v t="ekr.20090302123851.23"><vh>derive_class_name</vh></v>
<v t="ekr.20090302123851.24"><vh>derive_class_names</vh></v>
<v t="ekr.20090302123851.25"><vh>derive_argument</vh></v>
<v t="ekr.20090302123851.26"><vh>derive_arguments</vh></v>
<v t="ekr.20090302123851.27"><vh>derive_import_name</vh></v>
<v t="ekr.20090302123851.28"><vh>derive_import_names</vh></v>
<v t="ekr.20090302123851.29"><vh>class ASTVisitor</vh>
<v t="ekr.20090302123851.30"><vh>__init__</vh></v>
<v t="ekr.20090302123851.31"><vh>register_pattern</vh></v>
<v t="ekr.20090302123851.32"><vh>visit</vh></v>
<v t="ekr.20090302123851.33"><vh>visit_leaf</vh></v>
<v t="ekr.20090302123851.34"><vh>visit_node</vh></v>
<v t="ekr.20090302123851.35"><vh>visit_class</vh></v>
<v t="ekr.20090302123851.36"><vh>visit_function</vh></v>
<v t="ekr.20090302123851.37"><vh>visit_import</vh></v>
<v t="ekr.20090302123851.38"><vh>visit_lambda_assign</vh></v>
<v t="ekr.20090302123851.39"><vh>visit_main_snippet</vh></v>
<v t="ekr.20090302123851.40"><vh>_visit_all</vh></v>
<v t="ekr.20090302123851.41"><vh>_visit_class</vh></v>
<v t="ekr.20090302123851.42"><vh>_visit_function</vh></v>
<v t="ekr.20090302123851.43"><vh>_visit_import</vh></v>
<v t="ekr.20090302123851.44"><vh>_visit_lambda_assign</vh></v>
<v t="ekr.20090302123851.45"><vh>_visit_main_snippet</vh></v>
</v>
</v>
<v t="ekr.20090302123851.46"><vh>@@auto logger.py</vh>
<v t="ekr.20090302123851.47"><vh>logger declarations</vh></v>
<v t="ekr.20090302123851.48"><vh>path2modname</vh></v>
<v t="ekr.20090302123851.49"><vh>class LogFormatter</vh>
<v t="ekr.20090302123851.50"><vh>format</vh></v>
</v>
<v t="ekr.20090302123851.51"><vh>setup_logger</vh></v>
<v t="ekr.20090302123851.52"><vh>get_output</vh></v>
<v t="ekr.20090302123851.53"><vh>set_output</vh></v>
</v>
<v t="ekr.20090302123851.54" a="E"><vh>@@auto serializer.py</vh>
<v t="ekr.20090302123851.55"><vh>serializer declarations</vh></v>
<v t="ekr.20090302123851.56"><vh>can_be_constructed</vh></v>
<v t="ekr.20090302123851.57"><vh>string2id</vh></v>
<v t="ekr.20090302123851.58"><vh>get_type_name</vh></v>
<v t="ekr.20090302123851.59"><vh>get_module_name</vh></v>
<v t="ekr.20090302123851.60"><vh>get_partial_reconstructor</vh></v>
<v t="ekr.20090302123851.61"><vh>get_human_readable_id</vh></v>
<v t="ekr.20090302123851.62"><vh>is_parsable</vh></v>
<v t="ekr.20090302123851.63"><vh>get_reconstructor_with_imports</vh></v>
<v t="ekr.20090302123851.64"><vh>class SerializedObject</vh>
<v t="ekr.20090302123851.65"><vh>__init__</vh></v>
<v t="ekr.20090302123851.66"><vh>__eq__</vh></v>
<v t="ekr.20090302123851.67"><vh>__hash__</vh></v>
<v t="ekr.20090302123851.68"><vh>__repr__</vh></v>
</v>
<v t="ekr.20090302123851.69"><vh>serialize</vh></v>
<v t="ekr.20090302123851.70"><vh>serialize_call_arguments</vh></v>
</v>
<v t="ekr.20090302123851.71"><vh>@@auto store.py</vh>
<v t="ekr.20090302123851.72"><vh>store declarations</vh></v>
<v t="ekr.20090302123851.73" a="E"><vh>class ModuleNeedsAnalysis</vh>
<v t="ekr.20090302123851.74"><vh>__init__</vh></v>
</v>
<v t="ekr.20090302123851.75" a="E"><vh>class ModuleNotFound</vh>
<v t="ekr.20090302123851.76"><vh>__init__</vh></v>
</v>
<v t="ekr.20090302123851.77"><vh>class ModuleSaveError</vh>
<v t="ekr.20090302123851.78"><vh>__init__</vh></v>
</v>
<v t="ekr.20090302123851.79"><vh>get_pythoscope_path</vh></v>
<v t="ekr.20090302123851.80"><vh>get_pickle_path</vh></v>
<v t="ekr.20090302123851.81"><vh>get_points_of_entry_path</vh></v>
<v t="ekr.20090302123851.82"><vh>get_test_objects</vh></v>
<v t="ekr.20090302123851.83"><vh>class Project</vh>
<v t="ekr.20090302123851.84"><vh>from_directory</vh></v>
<v t="ekr.20090302123851.85"><vh>__init__</vh></v>
<v t="ekr.20090302123851.86"><vh>_get_pickle_path</vh></v>
<v t="ekr.20090302123851.87"><vh>_get_points_of_entry_path</vh></v>
<v t="ekr.20090302123851.88"><vh>_find_new_tests_directory</vh></v>
<v t="ekr.20090302123851.89"><vh>save</vh></v>
<v t="ekr.20090302123851.90"><vh>find_module_by_full_path</vh></v>
<v t="ekr.20090302123851.91"><vh>ensure_point_of_entry</vh></v>
<v t="ekr.20090302123851.92"><vh>remove_point_of_entry</vh></v>
<v t="ekr.20090302123851.93"><vh>create_module</vh></v>
<v t="ekr.20090302123851.94"><vh>create_test_module_from_name</vh></v>
<v t="ekr.20090302123851.95"><vh>remove_module</vh></v>
<v t="ekr.20090302123851.96"><vh>_replace_references_to_module</vh></v>
<v t="ekr.20090302123851.97"><vh>_extract_point_of_entry_subpath</vh></v>
<v t="ekr.20090302123851.98"><vh>_extract_subpath</vh></v>
<v t="ekr.20090302123851.99"><vh>iter_test_cases</vh></v>
<v t="ekr.20090302123851.100"><vh>_path_for_test</vh></v>
<v t="ekr.20090302123851.101"><vh>__getitem__</vh></v>
<v t="ekr.20090302123851.102"><vh>get_modules</vh></v>
<v t="ekr.20090302123851.103"><vh>iter_modules</vh></v>
<v t="ekr.20090302123851.104"><vh>iter_classes</vh></v>
<v t="ekr.20090302123851.105"><vh>iter_functions</vh></v>
<v t="ekr.20090302123851.106"><vh>iter_generator_objects</vh></v>
<v t="ekr.20090302123851.107"><vh>find_object</vh></v>
</v>
<v t="ekr.20090302123851.108"><vh>class Call</vh>
<v t="ekr.20090302123851.109"><vh>__init__</vh></v>
<v t="ekr.20090302123851.110"><vh>add_subcall</vh></v>
<v t="ekr.20090302123851.111"><vh>raised_exception</vh></v>
<v t="ekr.20090302123851.112"><vh>set_output</vh></v>
<v t="ekr.20090302123851.113"><vh>set_exception</vh></v>
<v t="ekr.20090302123851.114"><vh>clear_exception</vh></v>
<v t="ekr.20090302123851.115"><vh>is_testable</vh></v>
<v t="ekr.20090302123851.116"><vh>__eq__</vh></v>
<v t="ekr.20090302123851.117"><vh>__hash__</vh></v>
<v t="ekr.20090302123851.118"><vh>__repr__</vh></v>
</v>
<v t="ekr.20090302123851.119"><vh>class FunctionCall</vh>
<v t="ekr.20090302123851.120"><vh>__init__</vh></v>
</v>
<v t="ekr.20090302123851.121"><vh>class MethodCall</vh></v>
<v t="ekr.20090302123851.122"><vh>class Definition</vh>
<v t="ekr.20090302123851.123"><vh>__init__</vh></v>
</v>
<v t="ekr.20090302123851.124"><vh>class Callable</vh>
<v t="ekr.20090302123851.125"><vh>__init__</vh></v>
<v t="ekr.20090302123851.126"><vh>add_call</vh></v>
<v t="ekr.20090302123851.127"><vh>get_generator_object</vh></v>
<v t="ekr.20090302123851.128"><vh>remove_calls_from</vh></v>
</v>
<v t="ekr.20090302123851.129"><vh>class Function</vh>
<v t="ekr.20090302123851.130"><vh>__init__</vh></v>
<v t="ekr.20090302123851.131"><vh>is_testable</vh></v>
<v t="ekr.20090302123851.132"><vh>get_unique_calls</vh></v>
<v t="ekr.20090302123851.133"><vh>__repr__</vh></v>
</v>
<v t="ekr.20090302123851.134"><vh>class Method</vh></v>
<v t="ekr.20090302123851.135"><vh>class GeneratorObject</vh>
<v t="ekr.20090302123851.136"><vh>__init__</vh></v>
<v t="ekr.20090302123851.137"><vh>set_output</vh></v>
<v t="ekr.20090302123851.138"><vh>is_testable</vh></v>
<v t="ekr.20090302123851.139"><vh>__hash__</vh></v>
<v t="ekr.20090302123851.140"><vh>__repr__</vh></v>
</v>
<v t="ekr.20090302123851.141"><vh>class LiveObject</vh>
<v t="ekr.20090302123851.142"><vh>__init__</vh></v>
<v t="ekr.20090302123851.143"><vh>get_init_call</vh></v>
<v t="ekr.20090302123851.144"><vh>get_external_calls</vh></v>
<v t="ekr.20090302123851.145"><vh>__repr__</vh></v>
</v>
<v t="ekr.20090302123851.146"><vh>class Class</vh>
<v t="ekr.20090302123851.147"><vh>__init__</vh></v>
<v t="ekr.20090302123851.148"><vh>is_testable</vh></v>
<v t="ekr.20090302123851.149"><vh>add_live_object</vh></v>
<v t="ekr.20090302123851.150"><vh>remove_live_objects_from</vh></v>
<v t="ekr.20090302123851.151"><vh>get_traced_method_names</vh></v>
<v t="ekr.20090302123851.152"><vh>get_untraced_methods</vh></v>
<v t="ekr.20090302123851.153"><vh>find_method_by_name</vh></v>
</v>
<v t="ekr.20090302123851.154"><vh>class TestCase</vh>
<v t="ekr.20090302123851.155"><vh>__init__</vh></v>
<v t="ekr.20090302123851.156"><vh>replace_itself_with</vh></v>
</v>
<v t="ekr.20090302123851.157"><vh>class TestSuite</vh>
<v t="ekr.20090302123851.158"><vh>__init__</vh></v>
<v t="ekr.20090302123851.159"><vh>add_test_cases</vh></v>
<v t="ekr.20090302123851.160"><vh>add_test_case</vh></v>
<v t="ekr.20090302123851.161"><vh>replace_test_case</vh></v>
<v t="ekr.20090302123851.162"><vh>mark_as_changed</vh></v>
<v t="ekr.20090302123851.163"><vh>ensure_imports</vh></v>
<v t="ekr.20090302123851.164"><vh>_ensure_import</vh></v>
<v t="ekr.20090302123851.165"><vh>_contains_import</vh></v>
<v t="ekr.20090302123851.166"><vh>_check_test_case_type</vh></v>
</v>
<v t="ekr.20090302123851.167"><vh>class TestMethod</vh></v>
<v t="ekr.20090302123851.168"><vh>class TestClass</vh>
<v t="ekr.20090302123851.169"><vh>__init__</vh></v>
<v t="ekr.20090302123851.170"><vh>_append_test_case_code</vh></v>
<v t="ekr.20090302123851.171"><vh>find_method_by_name</vh></v>
<v t="ekr.20090302123851.172"><vh>is_testable</vh></v>
</v>
<v t="ekr.20090302123851.173"><vh>class Localizable</vh>
<v t="ekr.20090302123851.174"><vh>__init__</vh></v>
<v t="ekr.20090302123851.175"><vh>_get_locator</vh></v>
<v t="ekr.20090302123851.176"><vh>is_out_of_sync</vh></v>
<v t="ekr.20090302123851.177"><vh>is_up_to_date</vh></v>
<v t="ekr.20090302123851.178"><vh>get_path</vh></v>
<v t="ekr.20090302123851.179"><vh>write</vh></v>
<v t="ekr.20090302123851.180"><vh>exists</vh></v>
</v>
<v t="ekr.20090302123851.181"><vh>class Module</vh>
<v t="ekr.20090302123851.182"><vh>__init__</vh></v>
<v t="ekr.20090302123851.183"><vh>_get_testable_objects</vh></v>
<v t="ekr.20090302123851.184"><vh>_get_classes</vh></v>
<v t="ekr.20090302123851.185"><vh>_get_functions</vh></v>
<v t="ekr.20090302123851.186"><vh>_get_test_classes</vh></v>
<v t="ekr.20090302123851.187"><vh>add_test_case</vh></v>
<v t="ekr.20090302123851.188"><vh>get_content</vh></v>
<v t="ekr.20090302123851.189"><vh>get_test_cases_for_module</vh></v>
<v t="ekr.20090302123851.190"><vh>_ensure_main_snippet</vh></v>
<v t="ekr.20090302123851.191"><vh>_ensure_import</vh></v>
<v t="ekr.20090302123851.192"><vh>_add_import</vh></v>
<v t="ekr.20090302123851.193"><vh>_append_test_case_code</vh></v>
<v t="ekr.20090302123851.194"><vh>_insert_before_main_snippet</vh></v>
<v t="ekr.20090302123851.195"><vh>save</vh></v>
</v>
<v t="ekr.20090302123851.196"><vh>class PointOfEntry</vh>
<v t="ekr.20090302123851.197"><vh>__init__</vh></v>
<v t="ekr.20090302123851.198"><vh>get_path</vh></v>
<v t="ekr.20090302123851.199"><vh>get_content</vh></v>
<v t="ekr.20090302123851.200"><vh>clear_previous_run</vh></v>
<v t="ekr.20090302123851.201"><vh>create_method_call</vh></v>
<v t="ekr.20090302123851.202"><vh>create_function_call</vh></v>
<v t="ekr.20090302123851.203"><vh>finalize_inspection</vh></v>
<v t="ekr.20090302123851.204"><vh>_retrieve_live_object_for_method</vh></v>
<v t="ekr.20090302123851.205"><vh>_retrieve_generator_object</vh></v>
<v t="ekr.20090302123851.206"><vh>_retrieve_live_object</vh></v>
<v t="ekr.20090302123851.207"><vh>_preserve</vh></v>
<v t="ekr.20090302123851.208"><vh>_fix_generator_objects</vh></v>
</v>
</v>
<v t="ekr.20090302123851.209"><vh>@@auto util.py</vh>
<v t="ekr.20090302123851.210"><vh>util declarations</vh></v>
<v t="ekr.20090302123851.211"><vh>camelize</vh></v>
<v t="ekr.20090302123851.212"><vh>underscore</vh></v>
<v t="ekr.20090302123851.213"><vh>read_file_contents</vh></v>
<v t="ekr.20090302123851.214"><vh>write_string_to_file</vh></v>
<v t="ekr.20090302123851.215"><vh>all_of_type</vh></v>
<v t="ekr.20090302123851.216"><vh>max_by_not_zero</vh></v>
<v t="ekr.20090302123851.217"><vh>python_modules_below</vh></v>
<v t="ekr.20090302123851.218"><vh>rlistdir</vh></v>
<v t="ekr.20090302123851.219"><vh>get_names</vh></v>
<v t="ekr.20090302123851.220"><vh>class DirectoryException</vh></v>
<v t="ekr.20090302123851.221"><vh>ensure_directory</vh></v>
<v t="ekr.20090302123851.222"><vh>get_last_modification_time</vh></v>
<v t="ekr.20090302123851.223"><vh>extract_subpath</vh></v>
<v t="ekr.20090302123851.224"><vh>directories_under</vh></v>
<v t="ekr.20090302123851.225"><vh>findfirst</vh></v>
<v t="ekr.20090302123851.226"><vh>contains_active_generator</vh></v>
<v t="ekr.20090302123851.227"><vh>is_generator_code</vh></v>
<v t="ekr.20090302123851.228"><vh>compile_without_warnings</vh></v>
<v t="ekr.20090302123851.229"><vh>quoted_block</vh></v>
<v t="ekr.20090302123851.230"><vh>cname</vh></v>
<v t="ekr.20090302123851.231"><vh>module_path_to_name</vh></v>
<v t="ekr.20090302123851.232"><vh>regexp_flags_as_string</vh></v>
</v>
<v t="ekr.20090302123851.233"><vh>@@auto __init__.py</vh>
<v t="ekr.20090302123851.234"><vh>__init__ declarations</vh></v>
<v t="ekr.20090302123851.235"><vh>class PythoscopeDirectoryMissing</vh></v>
<v t="ekr.20090302123851.236"><vh>find_project_directory</vh></v>
<v t="ekr.20090302123851.237"><vh>init_project</vh></v>
<v t="ekr.20090302123851.238"><vh>generate_tests</vh></v>
<v t="ekr.20090302123851.239"><vh>main</vh></v>
</v>
<v t="ekr.20090302123851.240"><vh>@@auto generator\adder.py</vh>
<v t="ekr.20090302123851.241"><vh>adder declarations</vh></v>
<v t="ekr.20090302123851.242"><vh>add_test_case_to_project</vh></v>
<v t="ekr.20090302123851.243"><vh>find_test_class_by_name</vh></v>
<v t="ekr.20090302123851.244"><vh>merge_test_classes</vh></v>
<v t="ekr.20090302123851.245"><vh>find_place_for_test_class</vh></v>
<v t="ekr.20090302123851.246"><vh>find_test_module</vh></v>
<v t="ekr.20090302123851.247"><vh>find_associate_test_module_by_name</vh></v>
<v t="ekr.20090302123851.248"><vh>find_associate_test_module_by_test_classs</vh></v>
<v t="ekr.20090302123851.249"><vh>test_module_name_for_test_case</vh></v>
<v t="ekr.20090302123851.250"><vh>create_test_module</vh></v>
<v t="ekr.20090302123851.251"><vh>module_path_to_test_path</vh></v>
<v t="ekr.20090302123851.252"><vh>possible_test_module_names</vh></v>
<v t="ekr.20090302123851.253"><vh>possible_test_module_paths</vh></v>
</v>
<v t="ekr.20090302123851.254"><vh>@@auto generator\__init__.py</vh>
<v t="ekr.20090302123851.255"><vh>__init__ declarations</vh></v>
<v t="ekr.20090302123851.256"><vh>list_of</vh></v>
<v t="ekr.20090302123851.257"><vh>type_as_string</vh></v>
<v t="ekr.20090302123851.258"><vh>class CallString</vh>
<v t="ekr.20090302123851.259"><vh>__new__</vh></v>
<v t="ekr.20090302123851.260"><vh>extend</vh></v>
</v>
<v t="ekr.20090302123851.261"><vh>constructor_as_string</vh></v>
<v t="ekr.20090302123851.262"><vh>call_as_string</vh></v>
<v t="ekr.20090302123851.263"><vh>object2id</vh></v>
<v t="ekr.20090302123851.264"><vh>objects_list_to_id</vh></v>
<v t="ekr.20090302123851.265"><vh>input_as_string</vh></v>
<v t="ekr.20090302123851.266"><vh>objcall2testname</vh></v>
<v t="ekr.20090302123851.267"><vh>exccall2testname</vh></v>
<v t="ekr.20090302123851.268"><vh>gencall2testname</vh></v>
<v t="ekr.20090302123851.269"><vh>call2testname</vh></v>
<v t="ekr.20090302123851.270"><vh>sorted_test_method_descriptions</vh></v>
<v t="ekr.20090302123851.271"><vh>name2testname</vh></v>
<v t="ekr.20090302123851.272"><vh>in_lambda</vh></v>
<v t="ekr.20090302123851.273"><vh>in_list</vh></v>
<v t="ekr.20090302123851.274"><vh>type_of</vh></v>
<v t="ekr.20090302123851.275"><vh>map_types</vh></v>
<v t="ekr.20090302123851.276"><vh>decorate_call</vh></v>
<v t="ekr.20090302123851.277"><vh>should_ignore_method</vh></v>
<v t="ekr.20090302123851.278"><vh>testable_calls</vh></v>
<v t="ekr.20090302123851.279"><vh>is_builtin_exception</vh></v>
<v t="ekr.20090302123851.280"><vh>class UnknownTemplate</vh>
<v t="ekr.20090302123851.281"><vh>__init__</vh></v>
</v>
<v t="ekr.20090302123851.282"><vh>find_method_code</vh></v>
<v t="ekr.20090302123851.283"><vh>class TestMethodDescription</vh>
<v t="ekr.20090302123851.284"><vh>__init__</vh></v>
<v t="ekr.20090302123851.285"><vh>contains_code</vh></v>
<v t="ekr.20090302123851.286"><vh>_get_code_assertions</vh></v>
<v t="ekr.20090302123851.287"><vh>_has_complete_setup</vh></v>
</v>
<v t="ekr.20090302123851.288"><vh>class TestGenerator</vh>
<v t="ekr.20090302123851.289"><vh>from_template</vh></v>
<v t="ekr.20090302123851.290"><vh>__init__</vh></v>
<v t="ekr.20090302123851.291"><vh>ensure_import</vh></v>
<v t="ekr.20090302123851.292"><vh>ensure_imports</vh></v>
<v t="ekr.20090302123851.293"><vh>add_tests_to_project</vh></v>
<v t="ekr.20090302123851.294"><vh>create_test_class</vh></v>
<v t="ekr.20090302123851.295"><vh>comment_assertion</vh></v>
<v t="ekr.20090302123851.296"><vh>equal_stub_assertion</vh></v>
<v t="ekr.20090302123851.297"><vh>raises_stub_assertion</vh></v>
<v t="ekr.20090302123851.298"><vh>_add_tests_for_module</vh></v>
<v t="ekr.20090302123851.299"><vh>_generate_test_cases</vh></v>
<v t="ekr.20090302123851.300"><vh>_generate_test_case</vh></v>
<v t="ekr.20090302123851.301"><vh>_generate_test_method_descriptions</vh></v>
<v t="ekr.20090302123851.302"><vh>_generate_test_method_descriptions_for_function</vh></v>
<v t="ekr.20090302123851.303"><vh>_generate_test_method_descriptions_for_class</vh></v>
<v t="ekr.20090302123851.304"><vh>_generate_test_method_description_for_method</vh></v>
<v t="ekr.20090302123851.305"><vh>_method_descriptions_from_function</vh></v>
<v t="ekr.20090302123851.306"><vh>_method_description_from_live_object</vh></v>
<v t="ekr.20090302123851.307"><vh>_create_assertion</vh></v>
</v>
<v t="ekr.20090302123851.308"><vh>class UnittestTestGenerator</vh>
<v t="ekr.20090302123851.309"><vh>test_class_header</vh></v>
<v t="ekr.20090302123851.310"><vh>equal_assertion</vh></v>
<v t="ekr.20090302123851.311"><vh>raises_assertion</vh></v>
<v t="ekr.20090302123851.312"><vh>missing_assertion</vh></v>
</v>
<v t="ekr.20090302123851.313"><vh>class NoseTestGenerator</vh>
<v t="ekr.20090302123851.314"><vh>test_class_header</vh></v>
<v t="ekr.20090302123851.315"><vh>equal_assertion</vh></v>
<v t="ekr.20090302123851.316"><vh>raises_assertion</vh></v>
<v t="ekr.20090302123851.317"><vh>missing_assertion</vh></v>
</v>
<v t="ekr.20090302123851.318"><vh>add_tests_to_project</vh></v>
</v>
<v t="ekr.20090302123851.319"><vh>@@auto inspector\dynamic.py</vh>
<v t="ekr.20090302123851.320"><vh>dynamic declarations</vh></v>
<v t="ekr.20090302123851.321"><vh>class CallStack</vh>
<v t="ekr.20090302123851.322"><vh>__init__</vh></v>
<v t="ekr.20090302123851.323"><vh>called</vh></v>
<v t="ekr.20090302123851.324"><vh>returned</vh></v>
<v t="ekr.20090302123851.325"><vh>raised</vh></v>
</v>
<v t="ekr.20090302123851.326"><vh>compact</vh></v>
<v t="ekr.20090302123851.327"><vh>find_variable</vh></v>
<v t="ekr.20090302123851.328"><vh>callable_type</vh></v>
<v t="ekr.20090302123851.329"><vh>is_class_definition</vh></v>
<v t="ekr.20090302123851.330"><vh>class NotMethodFrame</vh></v>
<v t="ekr.20090302123851.331"><vh>get_method_information</vh></v>
<v t="ekr.20090302123851.332"><vh>resolve_args</vh></v>
<v t="ekr.20090302123851.333"><vh>input_from_argvalues</vh></v>
<v t="ekr.20090302123851.334"><vh>is_ignored_code</vh></v>
<v t="ekr.20090302123851.335"><vh>create_call</vh></v>
<v t="ekr.20090302123851.336"><vh>tracer</vh></v>
<v t="ekr.20090302123851.337"><vh>start_tracing</vh></v>
<v t="ekr.20090302123851.338"><vh>stop_tracing</vh></v>
<v t="ekr.20090302123851.339"><vh>trace_function</vh></v>
<v t="ekr.20090302123851.340"><vh>trace_exec</vh></v>
<v t="ekr.20090302123851.341"><vh>setup_tracing</vh></v>
<v t="ekr.20090302123851.342"><vh>teardown_tracing</vh></v>
<v t="ekr.20090302123851.343"><vh>inspect_point_of_entry</vh></v>
</v>
<v t="ekr.20090302123851.344"><vh>@@auto inspector\static.py</vh>
<v t="ekr.20090302123851.345"><vh>static declarations</vh></v>
<v t="ekr.20090302123851.346"><vh>is_test_class</vh></v>
<v t="ekr.20090302123851.347"><vh>unindent</vh></v>
<v t="ekr.20090302123851.348"><vh>function_code_from_definition</vh></v>
<v t="ekr.20090302123851.349"><vh>is_generator_definition</vh></v>
<v t="ekr.20090302123851.350"><vh>create_definition</vh></v>
<v t="ekr.20090302123851.351"><vh>class ModuleVisitor</vh>
<v t="ekr.20090302123851.352"><vh>__init__</vh></v>
<v t="ekr.20090302123851.353"><vh>visit_class</vh></v>
<v t="ekr.20090302123851.354"><vh>visit_function</vh></v>
<v t="ekr.20090302123851.355"><vh>visit_lambda_assign</vh></v>
<v t="ekr.20090302123851.356"><vh>visit_import</vh></v>
<v t="ekr.20090302123851.357"><vh>visit_main_snippet</vh></v>
</v>
<v t="ekr.20090302123851.358"><vh>class ClassVisitor</vh>
<v t="ekr.20090302123851.359"><vh>__init__</vh></v>
<v t="ekr.20090302123851.360"><vh>visit_class</vh></v>
<v t="ekr.20090302123851.361"><vh>visit_function</vh></v>
</v>
<v t="ekr.20090302123851.362"><vh>inspect_module</vh></v>
<v t="ekr.20090302123851.363"><vh>inspect_code</vh></v>
</v>
<v t="ekr.20090302123851.364"><vh>@@auto inspector\__init__.py</vh>
<v t="ekr.20090302123851.365"><vh>__init__ declarations</vh></v>
<v t="ekr.20090302123851.366"><vh>inspect_project</vh></v>
<v t="ekr.20090302123851.367"><vh>remove_deleted_modules</vh></v>
<v t="ekr.20090302123851.368"><vh>add_and_update_modules</vh></v>
<v t="ekr.20090302123851.369"><vh>remove_deleted_points_of_entry</vh></v>
<v t="ekr.20090302123851.370"><vh>add_and_update_points_of_entry</vh></v>
<v t="ekr.20090302123851.371"><vh>inspect_project_dynamically</vh></v>
</v>
</v>
</v>
</vnodes>
<tnodes>
<t tx="ekr.20090302123851.1"></t>
<t tx="ekr.20090302123851.10">def descend(tree, visitor_type):
    """Walk over the AST using a visitor of a given type and return the visitor
    object once done.
    """
    visitor = visitor_type()
    visitor.visit(tree)
    return visitor

</t>
<t tx="ekr.20090302123851.100">def _path_for_test(self, test_module_name):
    """Return a full path to test module with given name.
    """
    return os.path.join(self.path, self.new_tests_directory, test_module_name)

</t>
<t tx="ekr.20090302123851.101">def __getitem__(self, module):
    for mod in self.iter_modules():
        if module in [mod.subpath, mod.locator]:
            return mod
    raise ModuleNotFound(module)

</t>
<t tx="ekr.20090302123851.102">def get_modules(self):
    return self._modules.values()

</t>
<t tx="ekr.20090302123851.103">def iter_modules(self):
    return self._modules.values()

</t>
<t tx="ekr.20090302123851.104">def iter_classes(self):
    for module in self.iter_modules():
        for klass in module.classes:
            yield klass

</t>
<t tx="ekr.20090302123851.105">def iter_functions(self):
    for module in self.iter_modules():
        for function in module.functions:
            yield function

</t>
<t tx="ekr.20090302123851.106">def iter_generator_objects(self):
    for module in self.iter_modules():
        for generator in module.generators:
            for gobject in generator.calls:
                yield gobject

</t>
<t tx="ekr.20090302123851.107">def find_object(self, type, name, modulepath):
    modulename = self._extract_subpath(modulepath)
    try:
        for obj in all_of_type(self[modulename].objects, type):
            if obj.name == name:
                return obj
    except ModuleNotFound:
        pass

</t>
<t tx="ekr.20090302123851.108">class Call(object):
    """Stores information about a single function or method call.

    Includes reference to the caller, all call arguments, references to
    other calls made inside this one and finally an output value.

    There's more to function/method call than arguments and outputs.
    They're the only attributes for now, but information on side effects
    will be added later.

    __eq__ and __hash__ definitions provided for Function.get_unique_calls()
    and LiveObject.get_external_calls().
    """
    @others
</t>
<t tx="ekr.20090302123851.109">def __init__(self, definition, input, output=None, exception=None):
    if [value for value in input.values() if not isinstance(value, SerializedObject)]:
        raise ValueError("All input values should be instances of SerializedObject class.")
    if output and exception:
        raise ValueError("Call should have a single point of return.")
    if not isinstance(definition, Definition):
        raise ValueError("Call definition object should be an instance of Definition.")

    self.definition = definition
    self.input = input
    self.output = output
    self.exception = exception

    self.caller = None
    self.subcalls = []

</t>
<t tx="ekr.20090302123851.11">def find_last_leaf(node):
    if isinstance(node, Leaf):
        return node
    else:
        return find_last_leaf(node.children[-1])

</t>
<t tx="ekr.20090302123851.110">def add_subcall(self, call):
    # Don't add the same GeneratorObject more than once.
    if isinstance(call, GeneratorObject) and call.caller is self:
        return
    if call.caller is not None:
        raise TypeError("This %s of %s already has a caller." % \
                            (cname(call), call.definition.name))
    call.caller = self
    self.subcalls.append(call)

</t>
<t tx="ekr.20090302123851.111">def raised_exception(self):
    return self.exception is not None

</t>
<t tx="ekr.20090302123851.112">def set_output(self, output):
    self.output = serialize(output)

</t>
<t tx="ekr.20090302123851.113">def set_exception(self, exception):
    self.exception = serialize(exception)

</t>
<t tx="ekr.20090302123851.114">def clear_exception(self):
    self.exception = None

</t>
<t tx="ekr.20090302123851.115">def is_testable(self):
    return True

</t>
<t tx="ekr.20090302123851.116">def __eq__(self, other):
    return self.definition == other.definition and \
           self.input == other.input and \
           self.output == other.output and \
           self.exception == other.exception

</t>
<t tx="ekr.20090302123851.117">def __hash__(self):
    return hash((self.definition.name,
                 tuple(self.input.iteritems()),
                 self.output,
                 self.exception))

</t>
<t tx="ekr.20090302123851.118">def __repr__(self):
    return "%s(definition=%s, input=%r, output=%r, exception=%r)" % \
        (cname(self), self.definition.name, self.input, self.output,
         self.exception)

</t>
<t tx="ekr.20090302123851.119">class FunctionCall(Call):
    @others
</t>
<t tx="ekr.20090302123851.12">def get_starting_whitespace(code):
    whitespace = ""
    for child in code.children:
        if is_leaf_of_type(child, token.NEWLINE, token.INDENT):
            whitespace += child.value
        else:
            break
    return whitespace

</t>
<t tx="ekr.20090302123851.120">def __init__(self, point_of_entry, function, input, output=None, exception=None):
    Call.__init__(self, function, input, output, exception)
    self.point_of_entry = point_of_entry

</t>
<t tx="ekr.20090302123851.121">class MethodCall(Call):
    pass

</t>
<t tx="ekr.20090302123851.122">class Definition(object):
    @others
</t>
<t tx="ekr.20090302123851.123">def __init__(self, name, code=None, is_generator=False):
    if code is None:
        code = EmptyCode()
    self.name = name
    self.code = code
    self.is_generator = is_generator

</t>
<t tx="ekr.20090302123851.124">class Callable(object):
    @others
</t>
<t tx="ekr.20090302123851.125">def __init__(self, calls=None):
    if calls is None:
        calls = []
    self.calls = calls

</t>
<t tx="ekr.20090302123851.126">def add_call(self, call):
    # Don't add the same GeneratorObject more than once.
    if isinstance(call, GeneratorObject) and call in self.calls:
        return
    self.calls.append(call)

</t>
<t tx="ekr.20090302123851.127">def get_generator_object(self, unique_id):
    def is_matching_gobject(call):
        return isinstance(call, GeneratorObject) and call.unique_id == unique_id
    return findfirst(is_matching_gobject, self.calls)

</t>
<t tx="ekr.20090302123851.128">def remove_calls_from(self, point_of_entry):
    self.calls = [call for call in self.calls if call.point_of_entry is not point_of_entry]

</t>
<t tx="ekr.20090302123851.129">class Function(Definition, Callable):
    @others
</t>
<t tx="ekr.20090302123851.13">def remove_trailing_whitespace(code):
    leaf = find_last_leaf(code)
    leaf.prefix = leaf.prefix.replace(' ', '').replace('\t', '')

</t>
<t tx="ekr.20090302123851.130">def __init__(self, name, code=None, calls=None, is_generator=False):
    Definition.__init__(self, name, code, is_generator)
    Callable.__init__(self, calls)

</t>
<t tx="ekr.20090302123851.131">def is_testable(self):
    return not self.name.startswith('_')

</t>
<t tx="ekr.20090302123851.132">def get_unique_calls(self):
    return set(self.calls)

</t>
<t tx="ekr.20090302123851.133">def __repr__(self):
    return "Function(name=%r, calls=%r)" % (self.name, self.calls)

</t>
<t tx="ekr.20090302123851.134"># Methods are not Callable, because they cannot be called by itself - they
# need a bound object. We represent this object by LiveObject class, which
# gathers all MethodCalls for given instance.
class Method(Definition):
    pass

</t>
<t tx="ekr.20090302123851.135">class GeneratorObject(Call):
    """Representation of a generator object - a callable with on input and many
    outputs (here called "yields").

    Although a generator object execution is not a single call, but consists of
    a series of suspensions and resumes, we make it conform to the Call interface
    for simplicity.
    """
    @others
</t>
<t tx="ekr.20090302123851.136">def __init__(self, id, generator, point_of_entry, input, yields=None, exception=None):
    if yields is None:
        yields = []
    Call.__init__(self, generator, input, yields, exception)

    self.id = id
    self.point_of_entry = point_of_entry
    self.unique_id = (point_of_entry.name, id)

</t>
<t tx="ekr.20090302123851.137">def set_output(self, output):
    self.output.append(serialize(output))

</t>
<t tx="ekr.20090302123851.138">def is_testable(self):
    return self.raised_exception() or self.output

</t>
<t tx="ekr.20090302123851.139">def __hash__(self):
    return hash((self.definition.name,
                 tuple(self.input.iteritems()),
                 tuple(self.output),
                 self.exception))

</t>
<t tx="ekr.20090302123851.14">def parse(code):
    """String -&gt; AST

    Parse the string and return its AST representation. May raise
    a ParseError exception.
    """
    added_newline = False
    if not code.endswith("\n"):
        code += "\n"
        added_newline = True

    try:
        drv = driver.Driver(pygram.python_grammar, pytree.convert)
        result = drv.parse_string(code, True)
    except ParseError:
        log.debug("Had problems parsing:\n%s\n" % quoted_block(code))
        raise

    # Always return a Node, not a Leaf.
    if isinstance(result, Leaf):
        result = Node(syms.file_input, [result])

    result.added_newline = added_newline

    return result

</t>
<t tx="ekr.20090302123851.140">def __repr__(self):
    return "GeneratorObject(id=%d, generator=%r, yields=%r)" % \
           (self.id, self.definition.name, self.output)

</t>
<t tx="ekr.20090302123851.141">class LiveObject(Callable):
    """Representation of an object which creation and usage was traced
    during dynamic inspection.

    Note that the LiveObject.id is only unique to a given point of entry.
    In other words, it is possible to have two points of entry holding
    separate live objects with the same id. Use LiveObject.unique_id for
    identification purposes.
    """
    @others
</t>
<t tx="ekr.20090302123851.142">def __init__(self, id, klass, point_of_entry):
    Callable.__init__(self)

    self.id = id
    self.klass = klass
    self.point_of_entry = point_of_entry

    self.unique_id = (point_of_entry.name, id)

</t>
<t tx="ekr.20090302123851.143">def get_init_call(self):
    """Return a call to __init__ or None if it wasn't called.
    """
    return findfirst(lambda call: call.definition.name == '__init__', self.calls)

</t>
<t tx="ekr.20090302123851.144">def get_external_calls(self):
    """Return all calls to this object made from the outside.

    Note: __init__ is considered an internal call.
    """
    def is_not_init_call(call):
        return call.definition.name != '__init__'
    def is_external_call(call):
        return (not call.caller) or (call.caller not in self.calls)
    return filter(is_not_init_call, filter(is_external_call, self.calls))

</t>
<t tx="ekr.20090302123851.145">def __repr__(self):
    return "LiveObject(id=%d, klass=%r, calls=%r)" % (self.id, self.klass.name, self.calls)

</t>
<t tx="ekr.20090302123851.146">class Class(object):
    @others
</t>
<t tx="ekr.20090302123851.147">def __init__(self, name, methods=[], bases=[]):
    self.name = name
    self.methods = methods
    self.bases = bases
    self.live_objects = {}

</t>
<t tx="ekr.20090302123851.148">def is_testable(self):
    ignored_superclasses = ['Exception', 'unittest.TestCase']
    for klass in ignored_superclasses:
        if klass in self.bases:
            return False
    return True

</t>
<t tx="ekr.20090302123851.149">def add_live_object(self, live_object):
    self.live_objects[live_object.unique_id] = live_object

</t>
<t tx="ekr.20090302123851.15">def parse_fragment(code):
    """Works like parse() but returns an object stripped of the file_input
    wrapper. This eases merging this piece of code into other ones.
    """
    parsed_code = parse(code)

    if is_node_of_type(parsed_code, 'file_input') and \
           len(parsed_code.children) == 2 and \
           is_leaf_of_type(parsed_code.children[-1], token.ENDMARKER):
        return parsed_code.children[0]
    return parsed_code

</t>
<t tx="ekr.20090302123851.150">def remove_live_objects_from(self, point_of_entry):
    # We're removing elements, so iterate over a shallow copy.
    for id, live_object in self.live_objects.copy().iteritems():
        if live_object.point_of_entry is point_of_entry:
            del self.live_objects[id]

</t>
<t tx="ekr.20090302123851.151">def get_traced_method_names(self):
    traced_method_names = set()
    for live_object in self.live_objects.values():
        for call in live_object.calls:
            traced_method_names.add(call.definition.name)
    return traced_method_names

</t>
<t tx="ekr.20090302123851.152">def get_untraced_methods(self):
    traced_method_names = self.get_traced_method_names()
    def is_untraced(method):
        return method.name not in traced_method_names
    return filter(is_untraced, self.methods)

</t>
<t tx="ekr.20090302123851.153">def find_method_by_name(self, name):
    for method in self.methods:
        if method.name == name:
            return method

</t>
<t tx="ekr.20090302123851.154">class TestCase(object):
    """A single test object, possibly contained within a test suite (denoted
    as parent attribute).
    """
    @others
</t>
<t tx="ekr.20090302123851.155">def __init__(self, name, code=None, parent=None):
    if code is None:
        code = EmptyCode()
    self.name = name
    self.code = code
    self.parent = parent

</t>
<t tx="ekr.20090302123851.156">def replace_itself_with(self, new_test_case):
    self.parent.replace_test_case(self, new_test_case)

</t>
<t tx="ekr.20090302123851.157">class TestSuite(TestCase):
    """A test objects container.

    Keeps both test cases and other test suites in test_cases attribute.
    """
    allowed_test_case_classes = []

    @others
</t>
<t tx="ekr.20090302123851.158">def __init__(self, name, code=None, parent=None, test_cases=[], imports=None):
    TestCase.__init__(self, name, code, parent)

    if imports is None:
        imports = []

    self.changed = False
    self.test_cases = []
    self.imports = imports

</t>
<t tx="ekr.20090302123851.159">def add_test_cases(self, test_cases, append_code=True):
    for test_case in test_cases:
        self.add_test_case(test_case, append_code)

</t>
<t tx="ekr.20090302123851.16">def regenerate(tree):
    """AST -&gt; String

    Regenerate the source code from the AST tree.
    """
    if hasattr(tree, 'added_newline') and tree.added_newline:
        return str(tree)[:-1]
    else:
        return str(tree)

</t>
<t tx="ekr.20090302123851.160">def add_test_case(self, test_case, append_code=True):
    self._check_test_case_type(test_case)

    test_case.parent = self
    self.test_cases.append(test_case)

    if append_code:
        self._append_test_case_code(test_case.code)
        self.mark_as_changed()

</t>
<t tx="ekr.20090302123851.161">def replace_test_case(self, old_test_case, new_test_case):
    self._check_test_case_type(new_test_case)
    if old_test_case not in self.test_cases:
        raise ValueError("Given test case is not part of this test suite.")

    self.test_cases.remove(old_test_case)

    # The easiest way to get the new code inside the AST is to call
    # replace() on the old test case code.
    # It is destructive, but since we're discarding the old test case
    # anyway, it doesn't matter.
    old_test_case.code.replace(new_test_case.code)

    self.add_test_case(new_test_case, False)
    self.mark_as_changed()

</t>
<t tx="ekr.20090302123851.162">def mark_as_changed(self):
    self.changed = True
    if self.parent:
        self.parent.mark_as_changed()

</t>
<t tx="ekr.20090302123851.163">def ensure_imports(self, imports):
    "Make sure that all required imports are present."
    for imp in imports:
        self._ensure_import(imp)
    if self.parent:
        self.parent.ensure_imports(imports)

</t>
<t tx="ekr.20090302123851.164">def _ensure_import(self, import_desc):
    if not self._contains_import(import_desc):
        self.imports.append(import_desc)

</t>
<t tx="ekr.20090302123851.165">def _contains_import(self, import_desc):
    return import_desc in self.imports

</t>
<t tx="ekr.20090302123851.166">def _check_test_case_type(self, test_case):
    if not isinstance(test_case, tuple(self.allowed_test_case_classes)):
        raise TypeError("Given test case isn't allowed to be added to this test suite.")

</t>
<t tx="ekr.20090302123851.167">class TestMethod(TestCase):
    pass

</t>
<t tx="ekr.20090302123851.168">class TestClass(TestSuite):
    """Testing class, either generated by Pythoscope or hand-writen by the user.

    Each test class contains a set of requirements its surrounding must meet,
    like the list of imports it needs, contents of the "if __name__ == '__main__'"
    snippet or specific setup and teardown instructions.

    associated_modules is a list of Modules which this test class exercises.
    """
    allowed_test_case_classes = [TestMethod]

    @others
</t>
<t tx="ekr.20090302123851.169">def __init__(self, name, code=None, parent=None, test_cases=[],
             imports=None, main_snippet=None, associated_modules=None):
    TestSuite.__init__(self, name, code, parent, test_cases, imports)

    if associated_modules is None:
        associated_modules = []

    self.main_snippet = main_snippet
    self.associated_modules = associated_modules

    # Code of test cases passed to the constructor is already contained
    # within the class code.
    self.add_test_cases(test_cases, False)

</t>
<t tx="ekr.20090302123851.17">class ASTError(Exception):
    pass

</t>
<t tx="ekr.20090302123851.170">def _append_test_case_code(self, code):
    """Append to the right node, so that indentation level of the
    new method is good.
    """
    if self.code.children and is_node_of_type(self.code.children[-1], 'suite'):
        remove_trailing_whitespace(code)
        suite = self.code.children[-1]
        # Prefix the definition with the right amount of whitespace.
        node = find_last_leaf(suite.children[-2])
        ident = get_starting_whitespace(suite)
        # There's no need to have extra newlines.
        if node.prefix.endswith("\n"):
            node.prefix += ident.lstrip("\n")
        else:
            node.prefix += ident
        # Insert before the class contents dedent.
        suite.insert_child(-1, code)
    else:
        self.code.append_child(code)
    self.mark_as_changed()

</t>
<t tx="ekr.20090302123851.171">def find_method_by_name(self, name):
    for method in self.test_cases:
        if method.name == name:
            return method

</t>
<t tx="ekr.20090302123851.172">def is_testable(self):
    return False

</t>
<t tx="ekr.20090302123851.173">class Localizable(object):
    """An object which has a corresponding file belonging to some Project.

    Each Localizable has a 'path' attribute and an information when it was
    created, to be in sync with its file system counterpart. Path is always
    relative to the project this localizable belongs to.
    """
    @others
</t>
<t tx="ekr.20090302123851.174">def __init__(self, project, subpath, created=None):
    self.project = project
    self.subpath = subpath
    if created is None:
        created = time.time()
    self.created = created

</t>
<t tx="ekr.20090302123851.175">def _get_locator(self):
    return module_path_to_name(self.subpath, newsep=".")
</t>
<t tx="ekr.20090302123851.176">locator = property(_get_locator)

def is_out_of_sync(self):
    """Is the object out of sync with its file.
    """
    return get_last_modification_time(self.get_path()) &gt; self.created

</t>
<t tx="ekr.20090302123851.177">def is_up_to_date(self):
    return not self.is_out_of_sync()

</t>
<t tx="ekr.20090302123851.178">def get_path(self):
    """Return the full path to the file.
    """
    return os.path.join(self.project.path, self.subpath)

</t>
<t tx="ekr.20090302123851.179">def write(self, new_content):
    """Overwrite the file with new contents and update its created time.

    Creates the containing directories if needed.
    """
    ensure_directory(os.path.dirname(self.get_path()))
    write_string_to_file(new_content, self.get_path())
    self.created = time.time()

</t>
<t tx="ekr.20090302123851.18">def is_leaf_of_type(leaf, *types):
    return isinstance(leaf, Leaf) and leaf.type in types

</t>
<t tx="ekr.20090302123851.180">def exists(self):
    return os.path.isfile(self.get_path())

</t>
<t tx="ekr.20090302123851.181">class Module(Localizable, TestSuite):
    allowed_test_case_classes = [TestClass]

    @others
</t>
<t tx="ekr.20090302123851.182">def __init__(self, project, subpath, code=None, objects=None, imports=None,
             main_snippet=None, errors=[]):
    if objects is None:
        objects = []
    test_cases = get_test_objects(objects)

    Localizable.__init__(self, project, subpath)
    TestSuite.__init__(self, self.locator, code, None, test_cases, imports)

    self.objects = objects
    self.main_snippet = main_snippet
    self.errors = errors

    # Code of test cases passed to the constructor is already contained
    # within the module code.
    self.add_test_cases(test_cases, False)

</t>
<t tx="ekr.20090302123851.183">def _get_testable_objects(self):
    return [o for o in self.objects if o.is_testable()]
</t>
<t tx="ekr.20090302123851.184">testable_objects = property(_get_testable_objects)

def _get_classes(self):
    return all_of_type(self.objects, Class)
</t>
<t tx="ekr.20090302123851.185">classes = property(_get_classes)

def _get_functions(self):
    return all_of_type(self.objects, Function)
</t>
<t tx="ekr.20090302123851.186">functions = property(_get_functions)

def _get_test_classes(self):
    return all_of_type(self.objects, TestClass)
</t>
<t tx="ekr.20090302123851.187">test_classes = property(_get_test_classes)

def add_test_case(self, test_case, append_code=True):
    TestSuite.add_test_case(self, test_case, append_code)

    self.ensure_imports(test_case.imports)
    self._ensure_main_snippet(test_case.main_snippet)

</t>
<t tx="ekr.20090302123851.188"># def replace_test_case:
#   Using the default definition. We don't remove imports or main_snippet,
#   because we may unintentionally break something.

def get_content(self):
    return regenerate(self.code)

</t>
<t tx="ekr.20090302123851.189">def get_test_cases_for_module(self, module):
    """Return all test cases that are associated with given module.
    """
    return [tc for tc in self.test_cases if module in tc.associated_modules]

</t>
<t tx="ekr.20090302123851.19">def is_node_of_type(node, *types):
    return isinstance(node, Node) and pytree.type_repr(node.type) in types

</t>
<t tx="ekr.20090302123851.190">def _ensure_main_snippet(self, main_snippet, force=False):
    """Make sure the main_snippet is present. Won't overwrite the snippet
    unless force flag is set.
    """
    if not main_snippet:
        return

    if not self.main_snippet:
        self.main_snippet = main_snippet
        self.code.append_child(main_snippet)
        self.mark_as_changed()
    elif force:
        self.main_snippet.replace(main_snippet)
        self.main_snippet = main_snippet
        self.mark_as_changed()

</t>
<t tx="ekr.20090302123851.191">def _ensure_import(self, import_desc):
    # Add an extra newline separating imports from the code.
    if not self.imports:
        self.code.insert_child(0, Newline())
        self.mark_as_changed()
    if not self._contains_import(import_desc):
        self._add_import(import_desc)

</t>
<t tx="ekr.20090302123851.192">def _add_import(self, import_desc):
    self.imports.append(import_desc)
    self.code.insert_child(0, create_import(import_desc))
    self.mark_as_changed()

</t>
<t tx="ekr.20090302123851.193">def _append_test_case_code(self, code):
    # If the main_snippet exists we have to put the new test case
    # before it. If it doesn't we put the test case at the end.
    if self.main_snippet:
        self._insert_before_main_snippet(code)
    else:
        self.code.append_child(code)
    self.mark_as_changed()

</t>
<t tx="ekr.20090302123851.194">def _insert_before_main_snippet(self, code):
    for i, child in enumerate(self.code.children):
        if child == self.main_snippet:
            self.code.insert_child(i, code)
            break

</t>
<t tx="ekr.20090302123851.195">def save(self):
    # Don't save the test file unless it has been changed.
    if self.changed:
        if self.is_out_of_sync():
            raise ModuleNeedsAnalysis(self.subpath, out_of_sync=True)
        try:
            self.write(self.get_content())
        except DirectoryException, err:
            raise ModuleSaveError(self.subpath, err.message)
        self.changed = False

</t>
<t tx="ekr.20090302123851.196">class PointOfEntry(Localizable):
    """Piece of code provided by the user that allows dynamic analysis.

    In add_method_call/add_function_call if we can't find a class or function
    in Project, we don't care about it. This way we don't record any information
    about thid-party and dynamically created code.
    """
    @others
</t>
<t tx="ekr.20090302123851.197">def __init__(self, project, name):
    poes_subpath = project._extract_subpath(project._get_points_of_entry_path())
    # Points of entry start with created attribute equal to 0, as they are
    # not up-to-date until they're run. See finalize_inspection().
    Localizable.__init__(self, project, os.path.join(poes_subpath, name), created=0)

    self.name = name
    # After an inspection run, this will be a reference to the top level call.
    self.call_graph = None

    self._preserved_objects = []
    self._gobjects = []

</t>
<t tx="ekr.20090302123851.198">def get_path(self):
    return os.path.join(self.project._get_points_of_entry_path(), self.name)

</t>
<t tx="ekr.20090302123851.199">def get_content(self):
    return read_file_contents(self.get_path())

</t>
<t tx="ekr.20090302123851.20">def leaf_value(leaf):
    return leaf.value

</t>
<t tx="ekr.20090302123851.200">def clear_previous_run(self):
    for klass in self.project.iter_classes():
        klass.remove_live_objects_from(self)
    for function in self.project.iter_functions():
        function.remove_calls_from(self)
    self.call_graph = None

</t>
<t tx="ekr.20090302123851.201">def create_method_call(self, name, classname, modulepath, object, input, code, frame):
    try:
        live_object, method = self._retrieve_live_object_for_method(object, name, classname, modulepath)
        if is_generator_code(code):
            call = self._retrieve_generator_object(live_object, method, input, code, frame)
        else:
            call = MethodCall(method, serialize_call_arguments(input))
        live_object.add_call(call)
        return call
    except LookupError:
        pass

</t>
<t tx="ekr.20090302123851.202">def create_function_call(self, name, modulepath, input, code, frame):
    function = self.project.find_object(Function, name, modulepath)
    if function:
        if is_generator_code(code):
            return self._retrieve_generator_object(function, function, input, code, frame)
        else:
            call = FunctionCall(self, function, serialize_call_arguments(input))
            function.add_call(call)
            return call

</t>
<t tx="ekr.20090302123851.203">def finalize_inspection(self):
    # We can release preserved objects now.
    self._preserved_objects = []
    # Mark the point of entry as up-to-date.
    self.created = time.time()
    # Fix output of generator objects.
    self._fix_generator_objects()

</t>
<t tx="ekr.20090302123851.204">def _retrieve_live_object_for_method(self, object, name, classname, modulepath):
    klass = self.project.find_object(Class, classname, modulepath)
    if not klass:
        raise LookupError("Couldn't find class %s in %s." %\
                              (classname, modulepath))

    method = klass.find_method_by_name(name)
    if not method:
        raise LookupError("Couldn't find method %s in %s:%s." %\
                              (name, modulepath, classname))

    live_object = self._retrieve_live_object(object, klass)

    return live_object, method

</t>
<t tx="ekr.20090302123851.205">def _retrieve_generator_object(self, callable, generator, input, code, frame):
    gobject = callable.get_generator_object((self.name, id(code)))

    if not gobject:
        gobject = GeneratorObject(id(code), generator, self, serialize_call_arguments(input))
        callable.add_call(gobject)
        self._gobjects.append(gobject)
        self._preserve(code)

        # Generator objects return None to the tracer when stopped. That
        # extra None we have to filter out manually (see
        # _fix_generator_objects method). The only way to distinguish
        # between active and stopped generators is to ask garbage collector
        # about them. So we temporarily save the generator frame inside the
        # GeneratorObject, so it can be inspected later.
        gobject._frame = frame

    return gobject

</t>
<t tx="ekr.20090302123851.206">def _retrieve_live_object(self, object, klass):
    try:
        live_object = klass.live_objects[(self.name, id(object))]
    except KeyError:
        live_object = LiveObject(id(object), klass, self)
        klass.add_live_object(live_object)
        self._preserve(object)
    return live_object

</t>
<t tx="ekr.20090302123851.207">def _preserve(self, object):
    """Preserve an object from garbage collection, so its id won't get
    occupied by any other object.
    """
    self._preserved_objects.append(object)

</t>
<t tx="ekr.20090302123851.208">def _fix_generator_objects(self):
    """Remove last yielded values of generator objects, as those are
    just bogus Nones placed on generator stop.
    """
    for gobject in self._gobjects:
        if not contains_active_generator(gobject._frame) \
               and gobject.output \
               and gobject.output[-1] == serialize(None):
            gobject.output.pop()
        # Once we know if the generator is active or not, we can discard
        # the frame.
        del gobject._frame

    self._gobjects = []
</t>
<t tx="ekr.20090302123851.209">@language python
@tabwidth -4
@others
</t>
<t tx="ekr.20090302123851.21">def remove_commas(nodes):
    def isnt_comma(node):
        return not is_leaf_of_type(node, token.COMMA)
    return filter(isnt_comma, nodes)

</t>
<t tx="ekr.20090302123851.210">import gc
import os
import re
import types
import warnings

# Portability code.
try:
    set = set
except NameError:
    from sets import Set as set

try:
    frozenset = frozenset
except NameError:
    from sets import ImmutableSet as frozenset

try:
    sorted = sorted
except NameError:
    def sorted(iterable, cmp=cmp, key=None):
        if key:
            cmp = lambda x,y: cmp(key(x), key(y))
        alist = list(iterable)
        alist.sort(cmp)
        return alist

try:
    all = all
except NameError:
    def all(iterable):
        for element in iterable:
            if not element:
                return False
        return True

try:
    from itertools import groupby
except ImportError:
    # Code taken from http://docs.python.org/lib/itertools-functions.html .
    class groupby(object):
        def __init__(self, iterable, key=None):
            if key is None:
                key = lambda x: x
            self.keyfunc = key
            self.it = iter(iterable)
            self.tgtkey = self.currkey = self.currvalue = xrange(0)
        def __iter__(self):
            return self
        def next(self):
            while self.currkey == self.tgtkey:
                self.currvalue = self.it.next() # Exit on StopIteration
                self.currkey = self.keyfunc(self.currvalue)
            self.tgtkey = self.currkey
            return (self.currkey, self._grouper(self.tgtkey))
        def _grouper(self, tgtkey):
            while self.currkey == tgtkey:
                yield self.currvalue
                self.currvalue = self.it.next() # Exit on StopIteration
                self.currkey = self.keyfunc(self.currvalue)

try:
    from os.path import samefile
except ImportError:
    def samefile(file1, file2):
        return os.path.realpath(file1) == os.path.realpath(file2)

</t>
<t tx="ekr.20090302123851.211">def camelize(name):
    """Covert name into CamelCase.

    &gt;&gt;&gt; camelize('underscore_name')
    'UnderscoreName'
    &gt;&gt;&gt; camelize('AlreadyCamelCase')
    'AlreadyCamelCase'
    &gt;&gt;&gt; camelize('')
    ''
    """
    def upcase(match):
        return match.group(1).upper()
    return re.sub(r'(?:^|_)(.)', upcase, name)


</t>
<t tx="ekr.20090302123851.212">def underscore(name):
    """Convert name into underscore_name.

    &gt;&gt;&gt; underscore('CamelCase')
    'camel_case'
    &gt;&gt;&gt; underscore('already_underscore_name')
    'already_underscore_name'
    &gt;&gt;&gt; underscore('BigHTMLClass')
    'big_html_class'
    &gt;&gt;&gt; underscore('')
    ''
    """
    if name and name[0].isupper():
        name = name[0].lower() + name[1:]

    def capitalize(match):
        string = match.group(1).capitalize()
        return string[:-1] + string[-1].upper()

    def underscore(match):
        return '_' + match.group(1).lower()

    name = re.sub(r'([A-Z]+)', capitalize, name)
    return re.sub(r'([A-Z])', underscore, name)

</t>
<t tx="ekr.20090302123851.213">def read_file_contents(filename):
    fd = file(filename)
    contents = fd.read()
    fd.close()
    return contents

</t>
<t tx="ekr.20090302123851.214">def write_string_to_file(string, filename):
    fd = file(filename, 'w')
    fd.write(string)
    fd.close()

</t>
<t tx="ekr.20090302123851.215">def all_of_type(objects, type):
    """Return all objects that are instances of a given type.
    """
    return [o for o in objects if isinstance(o, type)]

</t>
<t tx="ekr.20090302123851.216">def max_by_not_zero(func, collection):
    """Return the element of a collection for which func returns the highest
    value, greater than 0.

    Return None if there is no such value.

    &gt;&gt;&gt; max_by_not_zero(len, ["abc", "d", "ef"])
    'abc'
    &gt;&gt;&gt; max_by_not_zero(lambda x: x, [0, 0, 0, 0]) is None
    True
    &gt;&gt;&gt; max_by_not_zero(None, []) is None
    True
    """
    if not collection:
        return None

    def annotate(element):
        return (func(element), element)

    highest = max(map(annotate, collection))
    if highest and highest[0] &gt; 0:
        return highest[1]
    else:
        return None

</t>
<t tx="ekr.20090302123851.217">def python_modules_below(path):
    def is_python_module(path):
        return path.endswith(".py")
    return filter(is_python_module, rlistdir(path))

</t>
<t tx="ekr.20090302123851.218">def rlistdir(path):
    """Resursive directory listing. Yield all files below given path,
    ignoring those which names begin with a dot.
    """
    if os.path.basename(path).startswith('.'):
        return

    if os.path.isdir(path):
        for entry in os.listdir(path):
            for subpath in rlistdir(os.path.join(path, entry)):
                yield subpath
    else:
        yield path

</t>
<t tx="ekr.20090302123851.219">def get_names(objects):
    return map(lambda c: c.name, objects)

</t>
<t tx="ekr.20090302123851.22">def remove_defaults(nodes):
    ignore_next = False
    for node in nodes:
        if ignore_next is True:
            ignore_next = False
            continue
        if is_leaf_of_type(node, token.EQUAL):
            ignore_next = True
            continue
        yield node

</t>
<t tx="ekr.20090302123851.220">class DirectoryException(Exception):
    pass

</t>
<t tx="ekr.20090302123851.221">def ensure_directory(directory):
    """Make sure given directory exists, creating it if necessary.
    """
    if os.path.exists(directory):
        if not os.path.isdir(directory):
            raise DirectoryException("Destination is not a directory.")
    else:
        os.makedirs(directory)

</t>
<t tx="ekr.20090302123851.222">def get_last_modification_time(path):
    try:
        # Casting to int, because we don't need better resolution anyway and it
        # eases testing on different OSes.
        return int(os.path.getmtime(path))
    except OSError:
        # File may not exist, in which case it was never modified.
        return 0

</t>
<t tx="ekr.20090302123851.223">def extract_subpath(path, prefix):
    """Remove prefix from given path to generate subpath, so the following
    correspondence is preserved:

      path &lt;=&gt; os.path.join(prefix, subpath)

    in terms of physical path (i.e. not necessarily strict string
    equality).
    """
    prefix_length = len(prefix)
    if not prefix.endswith(os.path.sep):
        prefix_length += 1
    return os.path.realpath(path)[prefix_length:]

</t>
<t tx="ekr.20090302123851.224">def directories_under(path):
    """Return names of directories under given path (not recursive).
    """
    for entry in os.listdir(path):
        if os.path.isdir(os.path.join(path, entry)):
            yield entry

</t>
<t tx="ekr.20090302123851.225">def findfirst(pred, seq):
    """Return the first element of given sequence that matches predicate.
    """
    for item in seq:
        if pred(item):
            return item

</t>
<t tx="ekr.20090302123851.226">def contains_active_generator(frame):
    return bool(all_of_type(gc.get_referrers(frame), types.GeneratorType))

</t>
<t tx="ekr.20090302123851.227">def is_generator_code(code):
    return code.co_flags &amp; 0x20 != 0

</t>
<t tx="ekr.20090302123851.228">def compile_without_warnings(stmt):
    """Compile single interactive statement with Python interpreter warnings
    disabled.
    """
    warnings.simplefilter('ignore')
    code = compile(stmt, '', 'single')
    warnings.resetwarnings()
    return code

</t>
<t tx="ekr.20090302123851.229">def quoted_block(text):
    return ''.join(["&gt; %s" % line for line in text.splitlines(True)])

</t>
<t tx="ekr.20090302123851.23">def derive_class_name(node):
    if is_leaf_of_type(node, token.NAME, token.DOT):
        return node.value
    elif is_node_of_type(node, 'power', 'trailer'):
        return "".join(map(derive_class_name, node.children))
    else:
        raise ASTError("Unknown node type: %r." % node)

</t>
<t tx="ekr.20090302123851.230">def cname(obj):
    """Return name of this object's class."""
    return obj.__class__.__name__

</t>
<t tx="ekr.20090302123851.231">def module_path_to_name(module_path, newsep="_"):
    return re.sub(r'(%s__init__)?\.py$' % re.escape(os.path.sep), '', module_path).\
        replace(os.path.sep, newsep)

</t>
<t tx="ekr.20090302123851.232"># Regular expressions helpers.

RePatternType = type(re.compile(''))

def regexp_flags_as_string(flags):
    """Return an expression in string form that corresponds to given set of
    regexp flags.
    """
    strings = []
    if flags &amp; re.IGNORECASE:
        strings.append('re.IGNORECASE')
    if flags &amp; re.LOCALE:
        strings.append('re.LOCALE')
    if flags &amp; re.MULTILINE:
        strings.append('re.MULTILINE')
    if flags &amp; re.DOTALL:
        strings.append('re.DOTALL')
    if flags &amp; re.VERBOSE:
        strings.append('re.VERBOSE')
    if flags &amp; re.UNICODE:
        strings.append('re.UNICODE')
    return " | ".join(strings)
</t>
<t tx="ekr.20090302123851.233">@language python
@tabwidth -4
@others
</t>
<t tx="ekr.20090302123851.234">import getopt
import os
import sys

import logger

from inspector import inspect_project
from generator import add_tests_to_project, UnknownTemplate
from logger import log
from store import Project, ModuleNotFound, ModuleNeedsAnalysis, \
     ModuleSaveError, get_pythoscope_path, get_points_of_entry_path
from util import samefile


__version__ = '0.3.2'

BUGTRACKER_URL = "https://bugs.launchpad.net/pythoscope"
USAGE = """Pythoscope usage:

    %s [options] [module names...]

By default, this command generates test suites for the listed modules.
It will automatically check for any source code changes and rerun all
points of entry if necessary.

As a module name, you can use both direct path or a locator in dot-style
notation. For example, both of the following are acceptable:

  package/sub/module.py
  package.sub.module

All test files will be written to a single directory.

Options:
  -f, --force    Go ahead and overwrite any existing test files. Default
                 is to skip generation of tests for files that would
                 otherwise get overwriten.
  -h, --help     Show this help message and exit.
  -i. --init     This option will initialize given project directory for
                 further Pythoscope usage. This is required for each new
                 project.
                 Initialization creates .pythoscope/ directory in the
                 project directory, which will store all information
                 related to test generation.
                 It will also perform a static (thus perfectly safe)
                 inspection of the project source code.
                 You may provide an argument after this option, which
                 should be a path pointing to a directory of a project
                 you want to initialize. If you don't provide one,
                 current directory will be used.
  -t TEMPLATE_NAME, --template=TEMPLATE_NAME
                 Name of a template to use (see below for a list of
                 available templates). Default is "unittest".
  -q, --quiet    Don't print anything unless it's an error.
  -v, --verbose  Be very verbose (basically enable debug output).
  -V, --version  Print Pythoscope version and exit.

Available templates:
  * unittest     All tests are placed into classes which derive from
                 unittest.TestCase. Each test module ends with an
                 import-safe call to unittest.main().
  * nose         Nose-style tests, which don't import unittest and use
                 SkipTest as a default test body.
"""

</t>
<t tx="ekr.20090302123851.235">class PythoscopeDirectoryMissing(Exception):
    pass

</t>
<t tx="ekr.20090302123851.236">def find_project_directory(path):
    """Try to find a pythoscope project directory for a given path,
    i.e. the closest directory that contains .pythoscope/ subdirectory.

    Will go up the directory tree and return the first matching path.
    """
    path = os.path.realpath(path)

    if not os.path.isdir(path):
        return find_project_directory(os.path.dirname(path))

    pythoscope_path = get_pythoscope_path(path)
    parent_path = os.path.join(path, os.path.pardir)

    # We reached the root.
    if samefile(path, parent_path):
        raise PythoscopeDirectoryMissing()
    elif os.path.isdir(pythoscope_path):
        return path
    else:
        return find_project_directory(os.path.join(path, os.path.pardir))

</t>
<t tx="ekr.20090302123851.237">def init_project(path):
    pythoscope_path = get_pythoscope_path(path)

    try:
        log.debug("Initializing .pythoscope directory: %s" % (os.path.abspath(pythoscope_path)))
        os.makedirs(pythoscope_path)
        os.makedirs(get_points_of_entry_path(path))
    except OSError, err:
        log.error("Couldn't initialize Pythoscope directory: %s." % err.strerror)

</t>
<t tx="ekr.20090302123851.238">def generate_tests(modules, force, template):
    try:
        project = Project.from_directory(find_project_directory(modules[0]))
        inspect_project(project)
        add_tests_to_project(project, modules, template, force)
        project.save()
    except PythoscopeDirectoryMissing:
        log.error("Can't find .pythoscope/ directory for this project. "
                  "Initialize the project with the '--init' option first.")
    except ModuleNeedsAnalysis, err:
        if err.out_of_sync:
            log.error("Tried to generate tests for test module located at %r, "
                      "but it has been modified during this run. Please try "
                      "running pythoscope again." % err.path)
        else:
            log.error("Tried to generate tests for test module located at %r, "
                      "but it was created during this run. Please try running "
                      "pythoscope again." % err.path)
    except ModuleNotFound, err:
        if os.path.exists(err.module):
            log.error("Couldn't find information on module %r. This shouldn't "
                      "happen, please file a bug report at %s." % (err.module, BUGTRACKER_URL))
        else:
            log.error("File doesn't exist: %s." % err.module)
    except ModuleSaveError, err:
        log.error("Couldn't save module %r: %s." % (err.module, err.reason))
    except UnknownTemplate, err:
        log.error("Couldn't find template named %r. Available templates are "
                  "'nose' and 'unittest'." % err.template)

</t>
<t tx="ekr.20090302123851.239">def main():
    appname = os.path.basename(sys.argv[0])

    try:
        options, args = getopt.getopt(sys.argv[1:], "fhit:qvV",
                        ["force", "help", "init", "template=", "quiet", "verbose", "version"])
    except getopt.GetoptError, err:
        log.error("Error: %s\n" % err)
        print USAGE % appname
        sys.exit(1)

    force = False
    init = False
    template = "unittest"

    for opt, value in options:
        if opt in ("-f", "--force"):
            force = True
        elif opt in ("-h", "--help"):
            print USAGE % appname
            sys.exit()
        elif opt in ("-i", "--init"):
            init = True
        elif opt in ("-t", "--template"):
            template = value
        elif opt in ("-q", "--quiet"):
            log.level = logger.ERROR
        elif opt in ("-v", "--verbose"):
            log.level = logger.DEBUG
        elif opt in ("-V", "--version"):
            print "%s %s" % (appname, __version__)
            sys.exit()

    try:
        if init:
            if args:
                project_path = args[0]
            else:
                project_path = "."
            init_project(project_path)
        else:
            if not args:
                log.error("You didn't specify any modules for test generation.\n")
                print USAGE % appname
            else:
                generate_tests(args, force, template)
    except:
        log.error("Oops, it seems internal Pythoscope error occured. Please file a bug report at %s\n" % BUGTRACKER_URL)
        raise
</t>
<t tx="ekr.20090302123851.24">def derive_class_names(node):
    if node is None:
        return []
    elif is_node_of_type(node, 'arglist'):
        return map(derive_class_name, remove_commas(node.children))
    else:
        return [derive_class_name(node)]

</t>
<t tx="ekr.20090302123851.240">@language python
@tabwidth -4
@others
</t>
<t tx="ekr.20090302123851.241">"""Module responsible for adding generated test cases to a project.

Client of this module should use it through add_test_case_to_project() function.
"""

import os.path

from pythoscope.logger import log
from pythoscope.util import max_by_not_zero, module_path_to_name


</t>
<t tx="ekr.20090302123851.242">def add_test_case_to_project(project, test_class, force=False):
    existing_test_class = find_test_class_by_name(project, test_class.name)
    if not existing_test_class:
        place = find_place_for_test_class(project, test_class)
        log.info("Adding generated %s to %s." % (test_class.name, place.subpath))
        place.add_test_case(test_class)
    else:
        merge_test_classes(existing_test_class, test_class, force)

</t>
<t tx="ekr.20090302123851.243">def find_test_class_by_name(project, name):
    for tcase in project.iter_test_cases():
        if tcase.name == name:
            return tcase

</t>
<t tx="ekr.20090302123851.244">def merge_test_classes(test_class, other_test_class, force):
    """Merge other_test_case into test_case.
    """
    for method in other_test_class.test_cases:
        existing_test_method = test_class.find_method_by_name(method.name)
        if not existing_test_method:
            log.info("Adding generated %s to %s in %s." % \
                         (method.name, test_class.name, test_class.parent.subpath))
            test_class.add_test_case(method)
        elif force:
            log.info("Replacing %s.%s from %s with generated version." % \
                         (test_class.name, existing_test_method.name, test_class.parent.subpath))
            test_class.replace_test_case(existing_test_method, method)
        else:
            log.info("Test case %s.%s already exists in %s, skipping." % \
                         (test_class.name, existing_test_method.name, test_class.parent.subpath))
    test_class.ensure_imports(other_test_class.imports)

</t>
<t tx="ekr.20090302123851.245">def find_place_for_test_class(project, test_class):
    """Find the best place for the new test case to be added. If there is
    no such place in existing test modules, a new one will be created.
    """
    return find_test_module(project, test_class) or \
        create_test_module(project, test_class)

</t>
<t tx="ekr.20090302123851.246">def find_test_module(project, test_class):
    """Find test module that will be good for the given test case.
    """
    for module in test_class.associated_modules:
        test_module = find_associate_test_module_by_name(project, module) or \
                      find_associate_test_module_by_test_classs(project, module)
        if test_module:
            return test_module

</t>
<t tx="ekr.20090302123851.247">def find_associate_test_module_by_name(project, module):
    """Try to find a test module with name corresponding to the name of
    the application module.
    """
    possible_paths = possible_test_module_paths(module, project.new_tests_directory)
    for module in project.get_modules():
        if module.subpath in possible_paths:
            return module

</t>
<t tx="ekr.20090302123851.248">def find_associate_test_module_by_test_classs(project, module):
    """Try to find a test module with most test cases for the given
    application module.
    """
    def test_classs_number(mod):
        return len(mod.get_test_cases_for_module(module))
    test_module = max_by_not_zero(test_classs_number, project.get_modules())
    if test_module:
        return test_module

</t>
<t tx="ekr.20090302123851.249">def test_module_name_for_test_case(test_case):
    """Come up with a name for a test module which will contain given test case.
    """
    # Assuming the test case has at least one associated module, which indeed
    # is a case in current implementation of generator.
    return module_path_to_test_path(test_case.associated_modules[0].subpath)

</t>
<t tx="ekr.20090302123851.25">def derive_argument(node):
    if is_leaf_of_type(node, token.NAME):
        return node.value
    elif is_node_of_type(node, 'tfpdef'):
        return tuple(map(derive_argument,
                         remove_commas(node.children[1].children)))

</t>
<t tx="ekr.20090302123851.250">def create_test_module(project, test_case):
    """Create a new test module for a given test case.
    """
    test_name = test_module_name_for_test_case(test_case)
    return project.create_test_module_from_name(test_name)

</t>
<t tx="ekr.20090302123851.251">def module_path_to_test_path(module):
    """Convert a module locator to a proper test filename.
    """
    return "test_%s.py" % module_path_to_name(module)

</t>
<t tx="ekr.20090302123851.252">def possible_test_module_names(module):
    module_name = module_path_to_name(module.subpath)

    for name in ["test_%s", "%s_test", "%sTest", "tests_%s", "%s_tests", "%sTests"]:
        yield (name % module_name) + ".py"
    for name in ["test%s", "Test%s", "%sTest", "tests%s", "Tests%s", "%sTests"]:
        yield (name % module_name.capitalize()) + ".py"

</t>
<t tx="ekr.20090302123851.253">def possible_test_module_paths(module, new_tests_directory):
    """Return possible locations of a test module corresponding to given
    application module.
    """
    test_directories = ["", "test", "tests"]
    if new_tests_directory not in test_directories:
        test_directories.append(new_tests_directory)
    def generate():
        for name in possible_test_module_names(module):
            for test_directory in test_directories:
                yield os.path.join(test_directory, name)
    return list(generate())
</t>
<t tx="ekr.20090302123851.254">@language python
@tabwidth -4
@others
</t>
<t tx="ekr.20090302123851.255">import exceptions

from pythoscope.astvisitor import descend, parse_fragment, ASTVisitor, \
    EmptyCode
from pythoscope.logger import log
from pythoscope.generator.adder import add_test_case_to_project
from pythoscope.serializer import SerializedObject, can_be_constructed, \
    serialize, serialize_call_arguments
from pythoscope.store import Class, Function, TestClass, TestMethod, ModuleNotFound, \
    LiveObject, MethodCall, Method, Project, PointOfEntry, GeneratorObject
from pythoscope.util import camelize, underscore, sorted, groupby, set


</t>
<t tx="ekr.20090302123851.256"># :: [string] -&gt; string
def list_of(strings):
    return "[%s]" % ', '.join(strings)

</t>
<t tx="ekr.20090302123851.257"># :: SerializedObject | [SerializedObject] -&gt; string
def type_as_string(object):
    """Return a most common representation of the wrapped object type.

    &gt;&gt;&gt; type_as_string([serialize(()), serialize({})])
    '[tuple, dict]'
    """
    if isinstance(object, list):
        return list_of(map(type_as_string, object))

    return object.type_name

</t>
<t tx="ekr.20090302123851.258">class CallString(str):
    """A string that holds information on the function/method call it
    represents.

    `uncomplete` attribute denotes whether it is a complete call
    or just a template.

    `imports` is a list of imports that this call requires.
    """
    @others
</t>
<t tx="ekr.20090302123851.259">def __new__(cls, string, uncomplete=False, imports=None):
    if imports is None:
        imports = set()
    call_string = str.__new__(cls, string)
    call_string.uncomplete = uncomplete
    call_string.imports = imports
    return call_string

</t>
<t tx="ekr.20090302123851.26">def derive_arguments(node):
    if node == []:
        return []
    elif is_node_of_type(node, 'typedargslist'):
        return map(derive_argument,
                   remove_defaults(remove_commas(node.children)))
    else:
        return [derive_argument(node)]

</t>
<t tx="ekr.20090302123851.260">def extend(self, value, uncomplete=False, imports=set()):
    return CallString(value, self.uncomplete or uncomplete,
                      self.imports.union(imports))

</t>
<t tx="ekr.20090302123851.261"># :: SerializedObject | LiveObject | list -&gt; CallString
def constructor_as_string(object):
    """For a given object (either SerializedObject, a LiveObject instance or list
    of any combination of those) return a string representing a code that will
    construct it.

    &gt;&gt;&gt; constructor_as_string(serialize(123))
    '123'
    &gt;&gt;&gt; constructor_as_string(serialize('string'))
    "'string'"
    &gt;&gt;&gt; constructor_as_string([serialize(1), serialize('two')])
    "[1, 'two']"
    &gt;&gt;&gt; obj = LiveObject(None, Class('SomeClass'), PointOfEntry(Project('.'), 'poe'))
    &gt;&gt;&gt; constructor_as_string(obj)
    'SomeClass()'
    &gt;&gt;&gt; obj.add_call(MethodCall(Method('__init__'), serialize_call_arguments({'arg': 'whatever'}), None))
    &gt;&gt;&gt; constructor_as_string(obj)
    "SomeClass(arg='whatever')"
    """
    if isinstance(object, LiveObject):
        args = {}
        # Look for __init__ call and base the constructor on that.
        init_call = object.get_init_call()
        if init_call:
            args = init_call.input
        return call_as_string(object.klass.name, args)
    elif isinstance(object, list):
        return list_of(map(constructor_as_string, object))
    elif isinstance(object, SerializedObject):
        try:
            reconstructor, imports = object.reconstructor_with_imports
            return CallString(reconstructor, imports=imports)
        except TypeError:
            return CallString("&lt;TODO: %s&gt;" % object.partial_reconstructor, uncomplete=True)
    else:
        raise TypeError("constructor_as_string expected SerializedObject or LiveObject object at input, not %s" % object)

</t>
<t tx="ekr.20090302123851.262"># :: (string, dict) -&gt; CallString
def call_as_string(object_name, input):
    """Generate code for calling an object with given input.

    &gt;&gt;&gt; call_as_string('fun', serialize_call_arguments({'a': 1, 'b': 2}))
    'fun(a=1, b=2)'
    &gt;&gt;&gt; call_as_string('capitalize', serialize_call_arguments({'str': 'string'}))
    "capitalize(str='string')"

    &gt;&gt;&gt; result = call_as_string('call', serialize_call_arguments({'f': call_as_string}))
    &gt;&gt;&gt; result
    'call(f=call_as_string)'
    &gt;&gt;&gt; result.uncomplete
    False

    &gt;&gt;&gt; result = call_as_string('map', serialize_call_arguments({'f': lambda x: 42, 'L': [1,2,3]}))
    &gt;&gt;&gt; result
    'map(L=[1, 2, 3], f=&lt;TODO: function&gt;)'
    &gt;&gt;&gt; result.uncomplete
    True
    """
    arguments = []
    uncomplete = False
    imports = set()
    for arg, value in input.iteritems():
        constructor = constructor_as_string(value)
        uncomplete = uncomplete or constructor.uncomplete
        imports.update(constructor.imports)
        arguments.append("%s=%s" % (arg, constructor))
    return CallString("%s(%s)" % (object_name, ', '.join(arguments)),
                      uncomplete=uncomplete, imports=imports)

</t>
<t tx="ekr.20090302123851.263"># :: SerializedObject -&gt; string
def object2id(object):
    """Convert object to string that can be used as an identifier.
    """
    if not isinstance(object, SerializedObject):
        raise TypeError("object2id() should be called with a SerializedObject argument, not %s" % object)
    return object.human_readable_id

</t>
<t tx="ekr.20090302123851.264">def objects_list_to_id(objects):
    """Convert given list of objects into string that can be used as an
    identifier.
    """
    if not objects:
        return 'nothing'
    return '_then_'.join(map(object2id, objects))

</t>
<t tx="ekr.20090302123851.265">def input_as_string(input):
    """Generate an underscored description of given input arguments.

    &gt;&gt;&gt; input_as_string({})
    ''
    &gt;&gt;&gt; input_as_string(serialize_call_arguments({'x': 7, 'y': 13}))
    'x_equal_7_and_y_equal_13'
    """
    if len(input) == 1:
        return object2id(input.values()[0])
    return "_and_".join(["%s_equal_%s" % (arg, object2id(value))
                         for arg, value in sorted(input.iteritems())])

</t>
<t tx="ekr.20090302123851.266">def objcall2testname(object_name, input, output):
    """Generate a test method name that describes given object call.

    &gt;&gt;&gt; objcall2testname('do_this', {}, serialize(True))
    'test_do_this_returns_true'
    &gt;&gt;&gt; objcall2testname('compute', {}, serialize('whatever you say'))
    'test_compute_returns_whatever_you_say'
    &gt;&gt;&gt; objcall2testname('square', serialize_call_arguments({'x': 7}), serialize(49))
    'test_square_returns_49_for_7'
    &gt;&gt;&gt; objcall2testname('capitalize', serialize_call_arguments({'str': 'a word.'}), serialize('A word.'))
    'test_capitalize_returns_A_word_for_a_word'

    Two or more arguments are mentioned by name.
        &gt;&gt;&gt; objcall2testname('ackermann', serialize_call_arguments({'m': 3, 'n': 2}), serialize(29))
        'test_ackermann_returns_29_for_m_equal_3_and_n_equal_2'

    Will sort arguments alphabetically.
        &gt;&gt;&gt; objcall2testname('concat', serialize_call_arguments({'s1': 'Hello ', 's2': 'world!'}), serialize('Hello world!'))
        'test_concat_returns_Hello_world_for_s1_equal_Hello_and_s2_equal_world'

    Always starts and ends a word with a letter or number.
        &gt;&gt;&gt; objcall2testname('strip', serialize_call_arguments({'n': 1, 's': '  A bit of whitespace  '}), serialize(' A bit of whitespace '))
        'test_strip_returns_A_bit_of_whitespace_for_n_equal_1_and_s_equal_A_bit_of_whitespace'
    """
    if input:
        call_description = "%s_for_%s" % (object2id(output), input_as_string(input))
    else:
        call_description = object2id(output)
    return "test_%s_returns_%s" % (underscore(object_name), call_description)

</t>
<t tx="ekr.20090302123851.267">def exccall2testname(object_name, input, exception):
    """Generate a test method name that describes given object call raising
    an exception.

    &gt;&gt;&gt; exccall2testname('do_this', {}, serialize(Exception()))
    'test_do_this_raises_exception'
    &gt;&gt;&gt; exccall2testname('square', serialize_call_arguments({'x': 'a string'}), serialize(TypeError()))
    'test_square_raises_type_error_for_a_string'
    """
    if input:
        call_description = "%s_for_%s" % (object2id(exception), input_as_string(input))
    else:
        call_description = object2id(exception)
    return "test_%s_raises_%s" % (underscore(object_name), call_description)

</t>
<t tx="ekr.20090302123851.268">def gencall2testname(object_name, input, yields):
    """Generate a test method name that describes given generator object call
    yielding some values.

    &gt;&gt;&gt; gencall2testname('generate', {}, [])
    'test_generate_yields_nothing'
    &gt;&gt;&gt; gencall2testname('generate', {}, [serialize(1), serialize(2), serialize(3)])
    'test_generate_yields_1_then_2_then_3'
    &gt;&gt;&gt; gencall2testname('backwards', serialize_call_arguments({'x': 321}), [serialize('one'), serialize('two'), serialize('three')])
    'test_backwards_yields_one_then_two_then_three_for_321'
    """
    if input:
        call_description = "%s_for_%s" % (objects_list_to_id(yields), input_as_string(input))
    else:
        call_description = objects_list_to_id(yields)
    return "test_%s_yields_%s" % (underscore(object_name), call_description)

</t>
<t tx="ekr.20090302123851.269">def call2testname(call, object_name):
    # Note: order is significant. We may have a GeneratorObject that raised
    # an exception, and we care about exceptions more.
    if call.raised_exception():
        return exccall2testname(object_name, call.input, call.exception)
    elif isinstance(call, GeneratorObject):
        return gencall2testname(object_name, call.input, call.output)
    else:
        return objcall2testname(object_name, call.input, call.output)

</t>
<t tx="ekr.20090302123851.27">def derive_import_name(node):
    if is_leaf_of_type(node, token.NAME):
        return node.value
    elif is_node_of_type(node, 'dotted_as_name'):
        return (derive_import_name(node.children[0]),
                derive_import_name(node.children[2]))
    elif is_node_of_type(node, 'dotted_name'):
        return "".join(map(leaf_value, node.children))

</t>
<t tx="ekr.20090302123851.270">def sorted_test_method_descriptions(descriptions):
    return sorted(descriptions, key=lambda md: md.name)

</t>
<t tx="ekr.20090302123851.271">def name2testname(name):
    if name[0].isupper():
        return "Test%s" % name
    return "test_%s" % name

</t>
<t tx="ekr.20090302123851.272">def in_lambda(string):
    return "lambda: %s" % string

</t>
<t tx="ekr.20090302123851.273">def in_list(string):
    return "list(%s)" % string

</t>
<t tx="ekr.20090302123851.274">def type_of(string):
    return "type(%s)" % string

</t>
<t tx="ekr.20090302123851.275">def map_types(string):
    return "map(type, %s)" % string

</t>
<t tx="ekr.20090302123851.276"># :: (Call, CallString) -&gt; CallString
def decorate_call(call, string):
    if isinstance(call, GeneratorObject):
        invocations = len(call.output)
        if call.raised_exception():
            invocations += 1
        # TODO: generators were added to Python 2.2, while itertools appeared in
        # release  2.3, so we may generate incompatible tests here.
        return string.extend("list(islice(%s, %d))" % (string, invocations),
                             imports=[("itertools", "islice")])
    return string

</t>
<t tx="ekr.20090302123851.277">def should_ignore_method(method):
    return method.name.startswith('_') and method.name != "__init__"

</t>
<t tx="ekr.20090302123851.278">def testable_calls(calls):
    return [c for c in calls if c.is_testable()]

</t>
<t tx="ekr.20090302123851.279">def is_builtin_exception(exception):
    return exception in dir(exceptions)

</t>
<t tx="ekr.20090302123851.28">def derive_import_names(node):
    if node is None:
        return None
    elif is_node_of_type(node, 'dotted_as_names', 'import_as_names'):
        return map(derive_import_name,
                   remove_commas(node.children))
    else:
        return [derive_import_name(node)]


</t>
<t tx="ekr.20090302123851.280">class UnknownTemplate(Exception):
    @others
</t>
<t tx="ekr.20090302123851.281">def __init__(self, template):
    Exception.__init__(self, "Couldn't find template %r." % template)
    self.template = template

</t>
<t tx="ekr.20090302123851.282">def find_method_code(code, method_name):
    """Return part of the code tree that corresponds to the given method
    definition.
    """
    class LocalizeMethodVisitor(ASTVisitor):
        def __init__(self):
            ASTVisitor.__init__(self)
            self.method_body = None
        def visit_function(self, name, args, body):
            if name == method_name:
                self.method_body = body

    return descend(code.children, LocalizeMethodVisitor).method_body

</t>
<t tx="ekr.20090302123851.283">class TestMethodDescription(object):
    @others
</t>
<t tx="ekr.20090302123851.284"># Assertions should be tuples (type, attributes...), where type is a string
# denoting a type of an assertion, e.g. 'equal' is an equality assertion.
#
# During test generation assertion attributes are passed to the corresponding
# TestGenerator method as arguments. E.g. assertion of type 'equal' invokes
# 'equal_assertion' method of the TestGenerator.
def __init__(self, name, assertions=[], setup=""):
    self.name = name
    self.assertions = assertions
    self.setup = setup

</t>
<t tx="ekr.20090302123851.285">def contains_code(self):
    return self._has_complete_setup() or self._get_code_assertions()

</t>
<t tx="ekr.20090302123851.286">def _get_code_assertions(self):
    return [a for a in self.assertions if a[0] in ['equal', 'raises']]

</t>
<t tx="ekr.20090302123851.287">def _has_complete_setup(self):
    return self.setup and not self.setup.startswith("#")

</t>
<t tx="ekr.20090302123851.288">class TestGenerator(object):
    main_snippet = EmptyCode()

    @others
</t>
<t tx="ekr.20090302123851.289">def from_template(cls, template):
    if template == 'unittest':
        return UnittestTestGenerator()
    elif template == 'nose':
        return NoseTestGenerator()
    else:
        raise UnknownTemplate(template)
</t>
<t tx="ekr.20090302123851.29">class ASTVisitor(object):
    DEFAULT_PATTERNS = [
        ('_visit_all', "file_input&lt; nodes=any* &gt;"),
        ('_visit_all', "suite&lt; nodes=any* &gt;"),
        ('_visit_class', "body=classdef&lt; 'class' name=NAME ['(' bases=any ')'] ':' any &gt;"),
        ('_visit_function', "body=funcdef&lt; 'def' name=NAME parameters&lt; '(' [args=any] ')' &gt; ':' any &gt;"),
        ('_visit_import', "import_name&lt; 'import' names=any &gt; | import_from&lt; 'from' import_from=any 'import' names=any &gt;"),
        ('_visit_lambda_assign', "expr_stmt&lt; name=NAME '=' lambdef&lt; 'lambda' any ':' any &gt; &gt;"),
        ('_visit_main_snippet', "body=if_stmt&lt; 'if' comparison&lt; '__name__' '==' (\"'__main__'\" | '\"__main__\"' ) &gt; ':' any &gt;"),
    ]

    @others
</t>
<t tx="ekr.20090302123851.290">from_template = classmethod(from_template)

def __init__(self):
    self.imports = []

</t>
<t tx="ekr.20090302123851.291">def ensure_import(self, import_):
    if import_ not in self.imports:
        self.imports.append(import_)

</t>
<t tx="ekr.20090302123851.292">def ensure_imports(self, imports):
    for import_ in imports:
        self.ensure_import(import_)

</t>
<t tx="ekr.20090302123851.293">def add_tests_to_project(self, project, modnames, force=False):
    for modname in modnames:
        module = project.find_module_by_full_path(modname)
        self._add_tests_for_module(module, project, force)

</t>
<t tx="ekr.20090302123851.294">def create_test_class(self, class_name, method_descriptions):
    result = "%s\n" % (self.test_class_header(class_name))
    for method_description in method_descriptions:
        if method_description.assertions:
            result += "    def %s(self):\n" % method_description.name
            if method_description.setup:
                result += "        " + method_description.setup
            for assertion in method_description.assertions:
                apply_template = getattr(self, "%s_assertion" % assertion[0])
                result += "        %s\n" % apply_template(*assertion[1:])
            # We need at least one statement in a method to be syntatically correct.
            if not method_description.contains_code():
                result += "        pass\n"
            result += "\n"
        else:
            result += "    def %s(self):\n" % method_description.name
            result += "        %s\n\n" % self.missing_assertion()
    return result

</t>
<t tx="ekr.20090302123851.295">def comment_assertion(self, comment):
    return comment

</t>
<t tx="ekr.20090302123851.296">def equal_stub_assertion(self, expected, actual):
    return "# %s" % self.equal_assertion(expected, actual)

</t>
<t tx="ekr.20090302123851.297">def raises_stub_assertion(self, exception, code):
    return "# %s" % self.raises_assertion(exception, code)

</t>
<t tx="ekr.20090302123851.298">def _add_tests_for_module(self, module, project, force):
    log.info("Generating tests for module %s." % module.subpath)
    for test_case in self._generate_test_cases(module):
        add_test_case_to_project(project, test_case, force)

</t>
<t tx="ekr.20090302123851.299">def _generate_test_cases(self, module):
    for object in module.testable_objects:
        test_case = self._generate_test_case(object, module)
        if test_case:
            yield test_case

</t>
<t tx="ekr.20090302123851.3"></t>
<t tx="ekr.20090302123851.30">def __init__(self):
    self.patterns = []
    for method, pattern in self.DEFAULT_PATTERNS:
        self.register_pattern(method, pattern)

</t>
<t tx="ekr.20090302123851.300">def _generate_test_case(self, object, module):
    class_name = name2testname(camelize(object.name))
    method_descriptions = sorted_test_method_descriptions(self._generate_test_method_descriptions(object, module))

    # Don't generate empty test classes.
    if method_descriptions:
        test_body = self.create_test_class(class_name, method_descriptions)
        test_code = parse_fragment(test_body)
        def methoddesc2testmethod(method_description):
            name = method_description.name
            return TestMethod(name=name, code=find_method_code(test_code, name))
        return TestClass(name=class_name,
                         code=test_code,
                         test_cases=map(methoddesc2testmethod, method_descriptions),
                         imports=self.imports,
                         main_snippet=self.main_snippet,
                         associated_modules=[module])

</t>
<t tx="ekr.20090302123851.301">def _generate_test_method_descriptions(self, object, module):
    if isinstance(object, Function):
        return self._generate_test_method_descriptions_for_function(object, module)
    elif isinstance(object, Class):
        return self._generate_test_method_descriptions_for_class(object, module)
    else:
        raise TypeError("Don't know how to generate test method descriptions for %s" % object)

</t>
<t tx="ekr.20090302123851.302">def _generate_test_method_descriptions_for_function(self, function, module):
    if testable_calls(function.calls):
        log.debug("Detected %d testable calls in function %s." % \
                      (len(testable_calls(function.calls)), function.name))

        # We're calling the function, so we have to make sure it will
        # be imported in the test
        self.ensure_import((module.locator, function.name))

        # We have at least one call registered, so use it.
        return self._method_descriptions_from_function(function)
    else:
        # No calls were traced, so we're go for a single test stub.
        log.debug("Detected _no_ testable calls in function %s." % function.name)
        return [TestMethodDescription(name2testname(underscore(function.name)))]

</t>
<t tx="ekr.20090302123851.303">def _generate_test_method_descriptions_for_class(self, klass, module):
    if klass.live_objects:
        # We're calling the method, so we have to make sure its class
        # will be imported in the test.
        self.ensure_import((module.locator, klass.name))

    for live_object in klass.live_objects.values():
        yield self._method_description_from_live_object(live_object)

    # No calls were traced for those methods, so we'll go for simple test stubs.
    for method in klass.get_untraced_methods():
        if not should_ignore_method(method):
            yield self._generate_test_method_description_for_method(method)

</t>
<t tx="ekr.20090302123851.304">def _generate_test_method_description_for_method(self, method):
    if method.name == '__init__':
        name = "object_initialization"
    else:
        name = method.name
    return TestMethodDescription(name2testname(name))

</t>
<t tx="ekr.20090302123851.305">def _method_descriptions_from_function(self, function):
    for call in testable_calls(function.get_unique_calls()):
        name = call2testname(call, function.name)
        assertions = [self._create_assertion(function.name, call)]

        yield TestMethodDescription(name, assertions)

</t>
<t tx="ekr.20090302123851.306">def _method_description_from_live_object(self, live_object):
    init_call = live_object.get_init_call()
    external_calls = testable_calls(live_object.get_external_calls())
    local_name = underscore(live_object.klass.name)
    constructor = constructor_as_string(live_object)
    stub_all = constructor.uncomplete

    self.ensure_imports(constructor.imports)

    def test_name():
        if len(external_calls) == 0 and init_call:
            test_name = "test_creation"
            if init_call.input:
                test_name += "_with_%s" % input_as_string(init_call.input)
            if init_call.raised_exception():
                test_name += "_raises_%s" % object2id(init_call.exception)
        else:
            if len(external_calls) == 1:
                call = external_calls[0]
                test_name = call2testname(call, call.definition.name)
            # Methods with more than one external call use more brief
            # descriptions that don't include inputs and outputs.
            else:
                methods = []
                for method, icalls in groupby(sorted([call.definition.name for call in external_calls])):
                    calls = list(icalls)
                    if len(calls) == 1:
                        methods.append(method)
                    else:
                        methods.append("%s_%d_times" % (method, len(calls)))
                test_name = "test_%s" % '_and_'.join(methods)
            if init_call and init_call.input:
                test_name += "_after_creation_with_%s" % input_as_string(init_call.input)
        return test_name

    def assertions():
        if init_call and len(external_calls) == 0:
            # If the constructor raised an exception, object creation should be an assertion.
            if init_call.raised_exception():
                yield self._create_assertion(live_object.klass.name, init_call, stub_all)
            else:
                yield(('comment', "# Make sure it doesn't raise any exceptions."))

        for call in external_calls:
            name = "%s.%s" % (local_name, call.definition.name)
            yield(self._create_assertion(name, call, stub_all))

    def setup():
        if init_call and init_call.raised_exception():
            return ""
        else:
            setup = "%s = %s\n" % (local_name, constructor)
            # Comment out the constructor if it isn't complete.
            if stub_all:
                setup = "# %s" % setup
            return setup

    return TestMethodDescription(test_name(), list(assertions()), setup())

</t>
<t tx="ekr.20090302123851.307">def _create_assertion(self, name, call, stub=False):
    """Create a new assertion based on a given call and a name provided
    for it.

    Generated assertion will be a stub if input of a call cannot be
    constructed or if stub argument is True.
    """
    callstring = decorate_call(call, call_as_string(name, call.input))

    self.ensure_imports(callstring.imports)

    if call.raised_exception():
        if callstring.uncomplete or stub:
            assertion_type = 'raises_stub'
        else:
            assertion_type = 'raises'
        if not is_builtin_exception(call.exception):
            self.ensure_import(call.exception.type_import)
        return (assertion_type,
                call.exception.type_name,
                in_lambda(callstring))
    else:
        if callstring.uncomplete or stub:
            assertion_type = 'equal_stub'
        else:
            assertion_type = 'equal'

        if can_be_constructed(call.output):
            return (assertion_type, constructor_as_string(call.output),
                    callstring)
        else:
            # If we can't test for real values, let's at least test for the right type.
            output_type = type_as_string(call.output)
            if isinstance(call, GeneratorObject):
                callstring_type = map_types(callstring)
            else:
                callstring_type = type_of(callstring)
            self.ensure_import('types')
            return (assertion_type, output_type, callstring_type)

</t>
<t tx="ekr.20090302123851.308">class UnittestTestGenerator(TestGenerator):
    main_snippet = parse_fragment("if __name__ == '__main__':\n    unittest.main()\n")

    @others
</t>
<t tx="ekr.20090302123851.309">def test_class_header(self, name):
    self.ensure_import('unittest')
    return "class %s(unittest.TestCase):" % name

</t>
<t tx="ekr.20090302123851.31">def register_pattern(self, method, pattern):
    """Register method to handle given pattern.
    """
    self.patterns.append((method, compile_pattern(pattern)))

</t>
<t tx="ekr.20090302123851.310">def equal_assertion(self, expected, actual):
    return "self.assertEqual(%s, %s)" % (expected, actual)

</t>
<t tx="ekr.20090302123851.311">def raises_assertion(self, exception, code):
    return "self.assertRaises(%s, %s)" % (exception, code)

</t>
<t tx="ekr.20090302123851.312">def missing_assertion(self):
    return "assert False # TODO: implement your test here"

</t>
<t tx="ekr.20090302123851.313">class NoseTestGenerator(TestGenerator):
    @others
</t>
<t tx="ekr.20090302123851.314">def test_class_header(self, name):
    return "class %s:" % name

</t>
<t tx="ekr.20090302123851.315">def equal_assertion(self, expected, actual):
    self.ensure_import(('nose.tools', 'assert_equal'))
    return "assert_equal(%s, %s)" % (expected, actual)

</t>
<t tx="ekr.20090302123851.316">def raises_assertion(self, exception, code):
    self.ensure_import(('nose.tools', 'assert_raises'))
    return "assert_raises(%s, %s)" % (exception, code)

</t>
<t tx="ekr.20090302123851.317">def missing_assertion(self):
    self.ensure_import(('nose', 'SkipTest'))
    return "raise SkipTest # TODO: implement your test here"

</t>
<t tx="ekr.20090302123851.318">def add_tests_to_project(project, modnames, template, force=False):
    generator = TestGenerator.from_template(template)
    generator.add_tests_to_project(project, modnames, force)
</t>
<t tx="ekr.20090302123851.319">@language python
@tabwidth -4
@others
</t>
<t tx="ekr.20090302123851.32">def visit(self, tree):
    """Main entry point of the ASTVisitor class.
    """
    if isinstance(tree, Leaf):
        self.visit_leaf(tree)
    elif isinstance(tree, Node):
        self.visit_node(tree)
    elif isinstance(tree, list):
        for subtree in tree:
            self.visit(subtree)
    else:
        raise ASTError("Unknown tree type: %r." % tree)

</t>
<t tx="ekr.20090302123851.320">import inspect
import sys
import types


IGNORED_NAMES = ["&lt;module&gt;", "&lt;genexpr&gt;"]

_traced_callables = None
_top_level_function = None
_sys_modules = None
_point_of_entry = None
_call_stack = None

</t>
<t tx="ekr.20090302123851.321">class CallStack(object):
    @others
</t>
<t tx="ekr.20090302123851.322">def __init__(self):
    self.last_traceback = None
    self.stack = []
    self.top_level_calls = []

</t>
<t tx="ekr.20090302123851.323">def called(self, call):
    if self.stack:
        self.stack[-1].add_subcall(call)
    else:
        self.top_level_calls.append(call)
    self.stack.append(call)

</t>
<t tx="ekr.20090302123851.324">def returned(self, output):
    if self.stack:
        caller = self.stack.pop()
        caller.set_output(output)

        # If the last exception is reported by sys.exc_info() it means
        # it was handled inside the returning call.
        handled_traceback = sys.exc_info()[2]
        if handled_traceback is self.last_traceback:
            caller.clear_exception()

</t>
<t tx="ekr.20090302123851.325">def raised(self, exception, traceback):
    if self.stack:
        caller = self.stack[-1]
        caller.set_exception(exception)
        self.last_traceback = traceback

</t>
<t tx="ekr.20090302123851.326">def compact(lst):
    "Remove all occurences of None from the given list."
    return [x for x in lst if x is not None]

</t>
<t tx="ekr.20090302123851.327">def find_variable(frame, varname):
    """Find variable named varname in the scope of a frame.

    Raise a KeyError when the varname cannot be found.
    """
    try:
        return frame.f_locals[varname]
    except KeyError:
        return frame.f_globals[varname]

</t>
<t tx="ekr.20090302123851.328">def callable_type(frame):
    """Return a type of a called frame or raise a KeyError if it can't be
    retrieved.

    The latter is the case for class definitions and method calls, which are
    not refrenced neither in local nor global scope.
    """
    return type(find_variable(frame.f_back, frame.f_code.co_name))

</t>
<t tx="ekr.20090302123851.329">def is_class_definition(frame):
    "Return True if given frame represents a class definition."
    try:
        # Old-style classes are of type "ClassType", while new-style
        # classes or of type "type".
        return callable_type(frame) in [types.ClassType, type]
    except KeyError:
        return frame.f_code.co_names[:2] == ('__name__', '__module__')

</t>
<t tx="ekr.20090302123851.33">def visit_leaf(self, leaf):
    pass

</t>
<t tx="ekr.20090302123851.330">class NotMethodFrame(Exception):
    pass

</t>
<t tx="ekr.20090302123851.331">def get_method_information(frame):
    """Analyze the frame and return relevant information about the method
    call it presumably represents.

    Returns a tuple: (self_object, input_dictionary).

    If the frame doesn't represent a method call, raises NotMethodFrame
    exception.
    """
    try:
        args, varargs, varkw, locals = inspect.getargvalues(frame)
        if args:
            # Will raise TypeError if args[0] is a list.
            self = locals[args[0]]
        else:
            # Will raise an IndexError if no arguments were passed.
            self = locals[varargs][0]

        methodname = frame.f_code.co_name
        # Will raise AttributeError when the self is None or doesn't
        # have method with given name.
        method = getattr(self, methodname)

        # This isn't a call on the first argument's method.
        if not method.im_func.func_code == frame.f_code:
            raise NotMethodFrame

        # Remove the "self" argument.
        if args:
            args.pop(0)
        elif varargs and locals[varargs]:
            # No pop(), because locals[varargs] is a tuple.
            locals[varargs] = locals[varargs][1:]
        else:
            raise NotMethodFrame

        return (self, input_from_argvalues(args, varargs, varkw, locals))
    except (AttributeError, KeyError, TypeError, IndexError):
        raise NotMethodFrame

</t>
<t tx="ekr.20090302123851.332">def resolve_args(names, locals):
    """Returns a list of tuples representing argument names and values.

    Handles nested arguments lists well.
        &gt;&gt;&gt; resolve_args([['a', 'b'], 'c'], {'.0': (1, 2), 'c': 3})
        [('a', 1), ('b', 2), ('c', 3)]

        &gt;&gt;&gt; resolve_args(['a', ['b', 'c']], {'.1': (8, 7), 'a': 9})
        [('a', 9), ('b', 8), ('c', 7)]
    """
    result = []
    for i, name in enumerate(names):
        if isinstance(name, list):
            result.extend(zip(name, locals['.%d' % i]))
        else:
            result.append((name, locals[name]))
    return result

</t>
<t tx="ekr.20090302123851.333">def input_from_argvalues(args, varargs, varkw, locals):
    return dict(resolve_args(args + compact([varargs, varkw]), locals))

</t>
<t tx="ekr.20090302123851.334">def is_ignored_code(code):
    if code.co_name in IGNORED_NAMES:
        return True
    if code in [_top_level_function.func_code, stop_tracing.func_code]:
        return True
    return False

</t>
<t tx="ekr.20090302123851.335">def create_call(frame):
    code = frame.f_code
    name = code.co_name
    modulepath = code.co_filename

    if not is_ignored_code(code):
        try:
            self, input = get_method_information(frame)
            classname = self.__class__.__name__
            return _point_of_entry.create_method_call(name, classname, modulepath, self, input, code, frame)
        except NotMethodFrame:
            input = input_from_argvalues(*inspect.getargvalues(frame))
            return _point_of_entry.create_function_call(name, modulepath, input, code, frame)

</t>
<t tx="ekr.20090302123851.336">def tracer(frame, event, arg):
    if event == 'call':
        if not is_class_definition(frame):
            call = create_call(frame)
            if call:
                _call_stack.called(call)
                return tracer
    elif event == 'return':
        _call_stack.returned(arg)
    elif event == 'exception':
        if arg[0] is not GeneratorExit:
            _call_stack.raised(arg[1], arg[2])

</t>
<t tx="ekr.20090302123851.337">def start_tracing():
    sys.settrace(tracer)

</t>
<t tx="ekr.20090302123851.338">def stop_tracing():
    sys.settrace(None)

</t>
<t tx="ekr.20090302123851.339">def trace_function(fun):
    """Trace given function and add Calls to given PointOfEntry instance.
    """
    global _top_level_function
    _top_level_function = fun

    start_tracing()
    try:
        fun()
    finally:
        stop_tracing()

</t>
<t tx="ekr.20090302123851.34">def visit_node(self, node):
    for method, pattern in self.patterns:
        results = {}
        if pattern.match(node, results):
            getattr(self, method)(results)
            break
    else:
        # For unknown nodes simply descend to their list of children.
        self.visit(node.children)

</t>
<t tx="ekr.20090302123851.340">def trace_exec(exec_string, scope={}):
    def fun():
        exec exec_string in scope
    return trace_function(fun)

</t>
<t tx="ekr.20090302123851.341">def setup_tracing(point_of_entry):
    global _sys_modules, _point_of_entry, _call_stack

    # Put project's path into PYTHONPATH, so point of entry's imports work.
    sys.path.insert(0, point_of_entry.project.path)
    point_of_entry.clear_previous_run()

    _call_stack = CallStack()
    _point_of_entry = point_of_entry
    _sys_modules = sys.modules.keys()

</t>
<t tx="ekr.20090302123851.342">def teardown_tracing(point_of_entry):
    global _sys_modules, _point_of_entry, _call_stack

    # Revert any changes to sys.modules.
    # This unfortunatelly doesn't include changes to the modules' state itself.
    # Replaced module instances in sys.modules are also not reverted.
    modnames = [m for m in sys.modules.keys() if m not in _sys_modules]
    for modname in modnames:
        del sys.modules[modname]

    # Copy the call graph structure to the point of entry.
    _point_of_entry.call_graph = _call_stack.top_level_calls
    _point_of_entry.finalize_inspection()

    _sys_modules = None
    _point_of_entry = None
    _call_stack = None

    sys.path.remove(point_of_entry.project.path)

</t>
<t tx="ekr.20090302123851.343">def inspect_point_of_entry(point_of_entry):
    setup_tracing(point_of_entry)
    try:
        trace_exec(point_of_entry.get_content())
    finally:
        teardown_tracing(point_of_entry)
</t>
<t tx="ekr.20090302123851.344">@language python
@tabwidth -4
@others
</t>
<t tx="ekr.20090302123851.345">import re
import types

from pythoscope.astvisitor import descend, parse, ParseError, ASTVisitor
from pythoscope.store import Class, Function, Method, TestClass,TestMethod
from pythoscope.util import all_of_type, is_generator_code, \
    read_file_contents, compile_without_warnings


</t>
<t tx="ekr.20090302123851.346">def is_test_class(name, bases):
    """Look at the name and bases of a class to determine whether it's a test
    class or not.

    &gt;&gt;&gt; is_test_class("TestSomething", [])
    True
    &gt;&gt;&gt; is_test_class("SomethingElse", [])
    False
    &gt;&gt;&gt; is_test_class("ItDoesntLookLikeOne", ["unittest.TestCase"])
    True
    """
    return name.startswith("Test") or name.endswith("Test") \
           or "unittest.TestCase" in bases

</t>
<t tx="ekr.20090302123851.347">def unindent(string):
    """Remove the initial part of whitespace from string.

    &gt;&gt;&gt; unindent("1 + 2 + 3\\n")
    '1 + 2 + 3\\n'
    &gt;&gt;&gt; unindent("  def fun():\\n    return 42\\n")
    'def fun():\\n  return 42\\n'
    """
    match = re.match(r'^([\t ]+)', string)
    if not match:
        return string
    whitespace = match.group(1)

    lines = []
    for line in string.splitlines(True):
        if line.startswith(whitespace):
            lines.append(line[len(whitespace):])
        else:
            return string
    return ''.join(lines)

</t>
<t tx="ekr.20090302123851.348">def function_code_from_definition(definition):
    """Return a code object of a given function definition.

    Can raise SyntaxError if the definition is not valid.
    """
    consts = compile_without_warnings(unindent(str(definition))).co_consts
    return all_of_type(consts, types.CodeType)[0]

</t>
<t tx="ekr.20090302123851.349">def is_generator_definition(definition):
    """Return True if given piece of code is a generator definition.

    &gt;&gt;&gt; is_generator_definition("def f():\\n  return 1\\n")
    False
    &gt;&gt;&gt; is_generator_definition("def g():\\n  yield 2\\n")
    True
    &gt;&gt;&gt; is_generator_definition("  def indented_gen():\\n    yield 3\\n")
    True
    """
    try:
        return is_generator_code(function_code_from_definition(definition))
    except SyntaxError:
        # This most likely means given code used "return" with argument
        # inside generator.
        return False

</t>
<t tx="ekr.20090302123851.35">def visit_class(self, name, bases, body):
    self.visit(body.children)

</t>
<t tx="ekr.20090302123851.350">def create_definition(name, body, definition_type):
    return definition_type(name, body, is_generator=is_generator_definition(body))

</t>
<t tx="ekr.20090302123851.351">class ModuleVisitor(ASTVisitor):
    @others
</t>
<t tx="ekr.20090302123851.352">def __init__(self):
    ASTVisitor.__init__(self)
    self.imports = []
    self.objects = []
    self.main_snippet = None

</t>
<t tx="ekr.20090302123851.353">def visit_class(self, name, bases, body):
    visitor = descend(body.children, ClassVisitor)
    if is_test_class(name, bases):
        methods = [TestMethod(n, c) for (n, c) in visitor.methods]
        klass = TestClass(name=name, test_cases=methods, code=body)
    else:
        methods = [create_definition(n, b, Method) for (n, b) in visitor.methods]
        klass = Class(name=name, methods=methods, bases=bases)
    self.objects.append(klass)

</t>
<t tx="ekr.20090302123851.354">def visit_function(self, name, args, body):
    self.objects.append(create_definition(name, body, Function))

</t>
<t tx="ekr.20090302123851.355">def visit_lambda_assign(self, name):
    self.objects.append(Function(name))

</t>
<t tx="ekr.20090302123851.356">def visit_import(self, names, import_from):
    if import_from:
        for name in names:
            self.imports.append((import_from, name))
    else:
        self.imports.extend(names)

</t>
<t tx="ekr.20090302123851.357">def visit_main_snippet(self, body):
    self.main_snippet = body

</t>
<t tx="ekr.20090302123851.358">class ClassVisitor(ASTVisitor):
    @others
</t>
<t tx="ekr.20090302123851.359">def __init__(self):
    ASTVisitor.__init__(self)
    self.methods = []

</t>
<t tx="ekr.20090302123851.36">def visit_function(self, name, args, body):
    self.visit(body.children)

</t>
<t tx="ekr.20090302123851.360">def visit_class(self, name, bases, body):
    # Ignore definitions of subclasses.
    pass

</t>
<t tx="ekr.20090302123851.361">def visit_function(self, name, args, body):
    self.methods.append((name, body))

</t>
<t tx="ekr.20090302123851.362">def inspect_module(project, path):
    return inspect_code(project, path, read_file_contents(path))

</t>
<t tx="ekr.20090302123851.363">def inspect_code(project, path, code):
    try:
        tree = parse(code)
    except ParseError, e:
        return project.create_module(path, errors=[e])
    visitor = descend(tree, ModuleVisitor)

    # We assume that all test classes in this module has dependencies on
    # imports and a main snippet the module contains.
    for test_class in [o for o in visitor.objects if isinstance(o, TestClass)]:
        # We gathered all imports in a single list, but import lists of those
        # classes may diverge in time, so we don't want to share their
        # structure.
        test_class.imports = visitor.imports[:]
        test_class.main_snippet = visitor.main_snippet

    return project.create_module(path, code=tree, objects=visitor.objects,
                                 imports=visitor.imports,
                                 main_snippet=visitor.main_snippet)
</t>
<t tx="ekr.20090302123851.364">@language python
@tabwidth -4
@others
</t>
<t tx="ekr.20090302123851.365">from pythoscope.inspector import static, dynamic
from pythoscope.logger import log
from pythoscope.store import ModuleNotFound
from pythoscope.util import python_modules_below


</t>
<t tx="ekr.20090302123851.366">def inspect_project(project):
    remove_deleted_modules(project)
    remove_deleted_points_of_entry(project)

    updates = add_and_update_modules(project) + add_and_update_points_of_entry(project)

    # If nothing new was discovered statically and there are no new points of
    # entry, don't run dynamic inspection.
    if updates:
        inspect_project_dynamically(project)
    else:
        log.info("No changes discovered in the source code, skipping dynamic inspection.")

</t>
<t tx="ekr.20090302123851.367">def remove_deleted_modules(project):
    subpaths = [mod.subpath for mod in project.iter_modules() if not mod.exists()]
    for subpath in subpaths:
        project.remove_module(subpath)

</t>
<t tx="ekr.20090302123851.368">def add_and_update_modules(project):
    count = 0
    for modpath in python_modules_below(project.path):
        try:
            module = project.find_module_by_full_path(modpath)
            if module.is_up_to_date():
                log.info("%s hasn't changed since last inspection, skipping." % module.subpath)
                continue
        except ModuleNotFound:
            pass
        log.info("Inspecting module %s." % project._extract_subpath(modpath))
        static.inspect_module(project, modpath)
        count += 1
    return count

</t>
<t tx="ekr.20090302123851.369">def remove_deleted_points_of_entry(project):
    names = [poe.name for poe in project.points_of_entry.values() if not poe.exists()]
    for name in names:
        project.remove_point_of_entry(name)

</t>
<t tx="ekr.20090302123851.37">def visit_import(self, names, import_from):
    pass

</t>
<t tx="ekr.20090302123851.370">def add_and_update_points_of_entry(project):
    count = 0
    for path in python_modules_below(project._get_points_of_entry_path()):
        poe = project.ensure_point_of_entry(path)
        if poe.is_out_of_sync():
            count += 1
    return count

</t>
<t tx="ekr.20090302123851.371">def inspect_project_dynamically(project):
    for poe in project.points_of_entry.values():
        try:
            log.info("Inspecting point of entry %s." % poe.name)
            dynamic.inspect_point_of_entry(poe)
        except SyntaxError, err:
            log.warning("Point of entry contains a syntax error: %s" % err)
        except (Exception, KeyboardInterrupt, SystemExit), err:
            log.warning("Point of entry exited with error: %s" % repr(err))
</t>
<t tx="ekr.20090302123851.372"></t>
<t tx="ekr.20090302123851.373"># d = c.scanAllDirectives(p)
# g.es(d.get('path'))

aList = g.get_directives_dict_list(p)
g.es(c.scanAtPathDirectives(aList))</t>
<t tx="ekr.20090302123851.374">@language python
@tabwidth -4
@others
</t>
<t tx="ekr.20090302123851.375"># Copyright 2006 Google, Inc. All Rights Reserved.
# Licensed to PSF under a Contributor Agreement.

"""Pattern compiler.

The grammer is taken from PatternGrammar.txt.

The compiler compiles a pattern to a pytree.*Pattern instance.
"""

__author__ = "Guido van Rossum &lt;guido@python.org&gt;"

# Python imports
import os

# Fairly local imports
from pgen2 import driver
from pgen2 import literals
from pgen2 import token
from pgen2 import tokenize

# Really local imports
import pytree
import pygram

# The pattern grammar file
_PATTERN_GRAMMAR_FILE = os.path.join(os.path.dirname(__file__),
                                     "PatternGrammar.txt")


</t>
<t tx="ekr.20090302123851.376">def tokenize_wrapper(input):
    """Tokenizes a string suppressing significant whitespace."""
    skip = (token.NEWLINE, token.INDENT, token.DEDENT)
    tokens = tokenize.generate_tokens(driver.generate_lines(input).next)
    for quintuple in tokens:
        type, value, start, end, line_text = quintuple
        if type not in skip:
            yield quintuple


</t>
<t tx="ekr.20090302123851.377">class PatternCompiler(object):
    @others
</t>
<t tx="ekr.20090302123851.378">
def __init__(self, grammar_file=_PATTERN_GRAMMAR_FILE):
    """Initializer.

    Takes an optional alternative filename for the pattern grammar.
    """
    self.grammar = driver.load_grammar(grammar_file)
    self.syms = pygram.Symbols(self.grammar)
    self.pygrammar = pygram.python_grammar
    self.pysyms = pygram.python_symbols
    self.driver = driver.Driver(self.grammar, convert=pattern_convert)

</t>
<t tx="ekr.20090302123851.379">def compile_pattern(self, input, debug=False):
    """Compiles a pattern string to a nested pytree.*Pattern object."""
    tokens = tokenize_wrapper(input)
    root = self.driver.parse_tokens(tokens, debug=debug)
    return self.compile_node(root)

</t>
<t tx="ekr.20090302123851.38">def visit_lambda_assign(self, name):
    pass

</t>
<t tx="ekr.20090302123851.380">def compile_node(self, node):
    """Compiles a node, recursively.

    This is one big switch on the node type.
    """
    # XXX Optimize certain Wildcard-containing-Wildcard patterns
    # that can be merged
    if node.type == self.syms.Matcher:
        node = node.children[0] # Avoid unneeded recursion

    if node.type == self.syms.Alternatives:
        # Skip the odd children since they are just '|' tokens
        alts = [self.compile_node(ch) for ch in node.children[::2]]
        if len(alts) == 1:
            return alts[0]
        p = pytree.WildcardPattern([[a] for a in alts], min=1, max=1)
        return p.optimize()

    if node.type == self.syms.Alternative:
        units = [self.compile_node(ch) for ch in node.children]
        if len(units) == 1:
            return units[0]
        p = pytree.WildcardPattern([units], min=1, max=1)
        return p.optimize()

    if node.type == self.syms.NegatedUnit:
        pattern = self.compile_basic(node.children[1:])
        p = pytree.NegatedPattern(pattern)
        return p.optimize()

    assert node.type == self.syms.Unit

    name = None
    nodes = node.children
    if len(nodes) &gt;= 3 and nodes[1].type == token.EQUAL:
        name = nodes[0].value
        nodes = nodes[2:]
    repeat = None
    if len(nodes) &gt;= 2 and nodes[-1].type == self.syms.Repeater:
        repeat = nodes[-1]
        nodes = nodes[:-1]

    # Now we've reduced it to: STRING | NAME [Details] | (...) | [...]
    pattern = self.compile_basic(nodes, repeat)

    if repeat is not None:
        assert repeat.type == self.syms.Repeater
        children = repeat.children
        child = children[0]
        if child.type == token.STAR:
            min = 0
            max = pytree.HUGE
        elif child.type == token.PLUS:
            min = 1
            max = pytree.HUGE
        elif child.type == token.LBRACE:
            assert children[-1].type == token.RBRACE
            assert  len(children) in (3, 5)
            min = max = self.get_int(children[1])
            if len(children) == 5:
                max = self.get_int(children[3])
        else:
            assert False
        if min != 1 or max != 1:
            pattern = pattern.optimize()
            pattern = pytree.WildcardPattern([[pattern]], min=min, max=max)

    if name is not None:
        pattern.name = name
    return pattern.optimize()

</t>
<t tx="ekr.20090302123851.381">def compile_basic(self, nodes, repeat=None):
    # Compile STRING | NAME [Details] | (...) | [...]
    assert len(nodes) &gt;= 1
    node = nodes[0]
    if node.type == token.STRING:
        value = literals.evalString(node.value)
        return pytree.LeafPattern(content=value)
    elif node.type == token.NAME:
        value = node.value
        if value.isupper():
            if value not in TOKEN_MAP:
                raise SyntaxError("Invalid token: %r" % value)
            return pytree.LeafPattern(TOKEN_MAP[value])
        else:
            if value == "any":
                type = None
            elif not value.startswith("_"):
                type = getattr(self.pysyms, value, None)
                if type is None:
                    raise SyntaxError("Invalid symbol: %r" % value)
            if nodes[1:]: # Details present
                content = [self.compile_node(nodes[1].children[1])]
            else:
                content = None
            return pytree.NodePattern(type, content)
    elif node.value == "(":
        return self.compile_node(nodes[1])
    elif node.value == "[":
        assert repeat is None
        subpattern = self.compile_node(nodes[1])
        return pytree.WildcardPattern([[subpattern]], min=0, max=1)
    assert False, node

</t>
<t tx="ekr.20090302123851.382">def get_int(self, node):
    assert node.type == token.NUMBER
    return int(node.value)


</t>
<t tx="ekr.20090302123851.383"># Map named tokens to the type value for a LeafPattern
TOKEN_MAP = {"NAME": token.NAME,
             "STRING": token.STRING,
             "NUMBER": token.NUMBER,
             "TOKEN": None}


def pattern_convert(grammar, raw_node_info):
    """Converts raw node information to a Node or Leaf instance."""
    type, value, context, children = raw_node_info
    if children or type in grammar.number2symbol:
        return pytree.Node(type, children, context=context)
    else:
        return pytree.Leaf(type, value, context=context)


</t>
<t tx="ekr.20090302123851.384">def compile_pattern(pattern):
    return PatternCompiler().compile_pattern(pattern)
</t>
<t tx="ekr.20090302123851.385">@language python
@tabwidth -4
@others
</t>
<t tx="ekr.20090302123851.386"># Copyright 2006 Google, Inc. All Rights Reserved.
# Licensed to PSF under a Contributor Agreement.

"""Export the Python grammar and symbols."""

# Python imports
import os

# Local imports
from pgen2 import token
from pgen2 import driver
import pytree

# The grammar file
_GRAMMAR_FILE = os.path.join(os.path.dirname(__file__), "Grammar.txt")


</t>
<t tx="ekr.20090302123851.387">class Symbols(object):
    @others
</t>
<t tx="ekr.20090302123851.388">
def __init__(self, grammar):
    """Initializer.

    Creates an attribute for each grammar symbol (nonterminal),
    whose value is the symbol's type (an int &gt;= 256).
    """
    for name, symbol in grammar.symbol2number.iteritems():
        setattr(self, name, symbol)


</t>
<t tx="ekr.20090302123851.389">python_grammar = driver.load_grammar(_GRAMMAR_FILE)
python_symbols = Symbols(python_grammar)


def parenthesize(node):
    return pytree.Node(python_symbols.atom,
                       (pytree.Leaf(token.LPAR, "("),
                        node,
                        pytree.Leaf(token.RPAR, ")")))
</t>
<t tx="ekr.20090302123851.39">def visit_main_snippet(self, body):
    pass

</t>
<t tx="ekr.20090302123851.390">@language python
@tabwidth -4
@others
</t>
<t tx="ekr.20090302123851.391"># Copyright 2006 Google, Inc. All Rights Reserved.
# Licensed to PSF under a Contributor Agreement.

"""Python parse tree definitions.

This is a very concrete parse tree; we need to keep every token and
even the comments and whitespace between tokens.

There's also a pattern matching implementation here.
"""

__author__ = "Guido van Rossum &lt;guido@python.org&gt;"


HUGE = 0x7FFFFFFF  # maximum repeat count, default max

_type_reprs = {}
</t>
<t tx="ekr.20090302123851.392">def type_repr(type_num):
    global _type_reprs
    if not _type_reprs:
        from pygram import python_symbols
        # printing tokens is possible but not as useful
        # from .pgen2 import token // token.__dict__.items():
        for name, val in python_symbols.__dict__.items():
            if type(val) == int: _type_reprs[val] = name
    return _type_reprs.setdefault(type_num, type_num)


</t>
<t tx="ekr.20090302123851.393">class Base(object):

    """Abstract base class for Node and Leaf.

    This provides some default functionality and boilerplate using the
    template pattern.

    A node may be a subnode of at most one parent.
    """

    # Default values for instance variables
    type = None    # int: token number (&lt; 256) or symbol number (&gt;= 256)
    parent = None  # Parent node pointer, or None
    children = ()  # Tuple of subnodes
    was_changed = False

    @others
</t>
<t tx="ekr.20090302123851.394">def __new__(cls, *args, **kwds):
    """Constructor that prevents Base from being instantiated."""
    assert cls is not Base, "Cannot instantiate Base"
    return object.__new__(cls)

</t>
<t tx="ekr.20090302123851.395">def __eq__(self, other):
    """Compares two nodes for equality.

    This calls the method _eq().
    """
    if self.__class__ is not other.__class__:
        return NotImplemented
    return self._eq(other)

</t>
<t tx="ekr.20090302123851.396">def __ne__(self, other):
    """Compares two nodes for inequality.

    This calls the method _eq().
    """
    if self.__class__ is not other.__class__:
        return NotImplemented
    return not self._eq(other)

</t>
<t tx="ekr.20090302123851.397">def _eq(self, other):
    """Compares two nodes for equality.

    This is called by __eq__ and __ne__.  It is only called if the
    two nodes have the same type.  This must be implemented by the
    concrete subclass.  Nodes should be considered equal if they
    have the same structure, ignoring the prefix string and other
    context information.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20090302123851.398">def clone(self):
    """Returns a cloned (deep) copy of self.

    This must be implemented by the concrete subclass.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20090302123851.399">def post_order(self):
    """Returns a post-order iterator for the tree.

    This must be implemented by the concrete subclass.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20090302123851.4"></t>
<t tx="ekr.20090302123851.40">def _visit_all(self, results):
    self.visit(results['nodes'])

</t>
<t tx="ekr.20090302123851.400">def pre_order(self):
    """Returns a pre-order iterator for the tree.

    This must be implemented by the concrete subclass.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20090302123851.401">def set_prefix(self, prefix):
    """Sets the prefix for the node (see Leaf class).

    This must be implemented by the concrete subclass.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20090302123851.402">def get_prefix(self):
    """Returns the prefix for the node (see Leaf class).

    This must be implemented by the concrete subclass.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20090302123851.403">def replace(self, new):
    """Replaces this node with a new one in the parent."""
    assert self.parent is not None, str(self)
    assert new is not None
    if not isinstance(new, list):
        new = [new]
    l_children = []
    found = False
    for ch in self.parent.children:
        if ch is self:
            assert not found, (self.parent.children, self, new)
            if new is not None:
                l_children.extend(new)
            found = True
        else:
            l_children.append(ch)
    assert found, (self.children, self, new)
    self.parent.changed()
    self.parent.children = l_children
    for x in new:
        x.parent = self.parent
    self.parent = None

</t>
<t tx="ekr.20090302123851.404">def get_lineno(self):
    """Returns the line number which generated the invocant node."""
    node = self
    while not isinstance(node, Leaf):
        if not node.children:
            return
        node = node.children[0]
    return node.lineno

</t>
<t tx="ekr.20090302123851.405">def changed(self):
    if self.parent:
        self.parent.changed()
    self.was_changed = True

</t>
<t tx="ekr.20090302123851.406">def remove(self):
    """Remove the node from the tree. Returns the position of the node
    in its parent's children before it was removed."""
    if self.parent:
        for i, node in enumerate(self.parent.children):
            if node is self:
                self.parent.changed()
                del self.parent.children[i]
                self.parent = None
                return i

</t>
<t tx="ekr.20090302123851.407">def get_next_sibling(self):
    """Return the node immediately following the invocant in their
    parent's children list. If the invocant does not have a next
    sibling, return None."""
    if self.parent is None:
        return None

    # Can't use index(); we need to test by identity
    for i, child in enumerate(self.parent.children):
        if child is self:
            try:
                return self.parent.children[i+1]
            except IndexError:
                return None

</t>
<t tx="ekr.20090302123851.408">def get_prev_sibling(self):
    """Return the node immediately preceding the invocant in their
    parent's children list. If the invocant does not have a previous
    sibling, return None."""
    if self.parent is None:
        return None

    # Can't use index(); we need to test by identity
    for i, child in enumerate(self.parent.children):
        if child is self:
            if i == 0:
                return None
            return self.parent.children[i-1]

</t>
<t tx="ekr.20090302123851.409">def get_suffix(self):
    """Return the string immediately following the invocant node. This
    is effectively equivalent to node.get_next_sibling().get_prefix()"""
    next_sib = self.get_next_sibling()
    if next_sib is None:
        return ""
    return next_sib.get_prefix()


</t>
<t tx="ekr.20090302123851.41">def _visit_class(self, results):
    self.visit_class(name=results['name'].value,
                     bases=derive_class_names(results.get('bases')),
                     body=results['body'])

</t>
<t tx="ekr.20090302123851.410">class Node(Base):

    """Concrete implementation for interior nodes."""
    @others
</t>
<t tx="ekr.20090302123851.411">
def __init__(self, type, children, context=None, prefix=None):
    """Initializer.

    Takes a type constant (a symbol number &gt;= 256), a sequence of
    child nodes, and an optional context keyword argument.

    As a side effect, the parent pointers of the children are updated.
    """
    assert type &gt;= 256, type
    self.type = type
    self.children = list(children)
    for ch in self.children:
        assert ch.parent is None, repr(ch)
        ch.parent = self
    if prefix is not None:
        self.set_prefix(prefix)

</t>
<t tx="ekr.20090302123851.412">def __repr__(self):
    """Returns a canonical string representation."""
    return "%s(%s, %r)" % (self.__class__.__name__,
                           type_repr(self.type),
                           self.children)

</t>
<t tx="ekr.20090302123851.413">def __str__(self):
    """Returns a pretty string representation.

    This reproduces the input source exactly.
    """
    return "".join(map(str, self.children))

</t>
<t tx="ekr.20090302123851.414">def _eq(self, other):
    """Compares two nodes for equality."""
    return (self.type, self.children) == (other.type, other.children)

</t>
<t tx="ekr.20090302123851.415">def clone(self):
    """Returns a cloned (deep) copy of self."""
    return Node(self.type, [ch.clone() for ch in self.children])

</t>
<t tx="ekr.20090302123851.416">def post_order(self):
    """Returns a post-order iterator for the tree."""
    for child in self.children:
        for node in child.post_order():
            yield node
    yield self

</t>
<t tx="ekr.20090302123851.417">def pre_order(self):
    """Returns a pre-order iterator for the tree."""
    yield self
    for child in self.children:
        for node in child.post_order():
            yield node

</t>
<t tx="ekr.20090302123851.418">def set_prefix(self, prefix):
    """Sets the prefix for the node.

    This passes the responsibility on to the first child.
    """
    if self.children:
        self.children[0].set_prefix(prefix)

</t>
<t tx="ekr.20090302123851.419">def get_prefix(self):
    """Returns the prefix for the node.

    This passes the call on to the first child.
    """
    if not self.children:
        return ""
    return self.children[0].get_prefix()

</t>
<t tx="ekr.20090302123851.42">def _visit_function(self, results):
    self.visit_function(name=results['name'].value,
                        args=derive_arguments(results.get('args', [])),
                        body=results['body'])

</t>
<t tx="ekr.20090302123851.420">def set_child(self, i, child):
    """Equivalent to 'node.children[i] = child'. This method also sets the
    child's parent attribute appropriately."""
    child.parent = self
    self.children[i].parent = None
    self.children[i] = child

</t>
<t tx="ekr.20090302123851.421">def insert_child(self, i, child):
    """Equivalent to 'node.children.insert(i, child)'. This method also
    sets the child's parent attribute appropriately."""
    child.parent = self
    self.children.insert(i, child)

</t>
<t tx="ekr.20090302123851.422">def append_child(self, child):
    """Equivalent to 'node.children.append(child)'. This method also
    sets the child's parent attribute appropriately."""
    child.parent = self
    self.children.append(child)


</t>
<t tx="ekr.20090302123851.423">class Leaf(Base):

    """Concrete implementation for leaf nodes."""

    # Default values for instance variables
    prefix = ""  # Whitespace and comments preceding this token in the input
    lineno = 0   # Line where this token starts in the input
    column = 0   # Column where this token tarts in the input

    @others
</t>
<t tx="ekr.20090302123851.424">def __init__(self, type, value, context=None, prefix=None):
    """Initializer.

    Takes a type constant (a token number &lt; 256), a string value,
    and an optional context keyword argument.
    """
    assert 0 &lt;= type &lt; 256, type
    if context is not None:
        self.prefix, (self.lineno, self.column) = context
    self.type = type
    self.value = value
    if prefix is not None:
        self.prefix = prefix

</t>
<t tx="ekr.20090302123851.425">def __repr__(self):
    """Returns a canonical string representation."""
    return "%s(%r, %r)" % (self.__class__.__name__,
                           self.type,
                           self.value)

</t>
<t tx="ekr.20090302123851.426">def __str__(self):
    """Returns a pretty string representation.

    This reproduces the input source exactly.
    """
    return self.prefix + str(self.value)

</t>
<t tx="ekr.20090302123851.427">def _eq(self, other):
    """Compares two nodes for equality."""
    return (self.type, self.value) == (other.type, other.value)

</t>
<t tx="ekr.20090302123851.428">def clone(self):
    """Returns a cloned (deep) copy of self."""
    return Leaf(self.type, self.value,
                (self.prefix, (self.lineno, self.column)))

</t>
<t tx="ekr.20090302123851.429">def post_order(self):
    """Returns a post-order iterator for the tree."""
    yield self

</t>
<t tx="ekr.20090302123851.43">def _visit_import(self, results):
    self.visit_import(names=derive_import_names(results['names']),
                      import_from=derive_import_name(results.get('import_from')))

</t>
<t tx="ekr.20090302123851.430">def pre_order(self):
    """Returns a pre-order iterator for the tree."""
    yield self

</t>
<t tx="ekr.20090302123851.431">def set_prefix(self, prefix):
    """Sets the prefix for the node."""
    self.changed()
    self.prefix = prefix

</t>
<t tx="ekr.20090302123851.432">def get_prefix(self):
    """Returns the prefix for the node."""
    return self.prefix


</t>
<t tx="ekr.20090302123851.433">def convert(gr, raw_node):
    """Converts raw node information to a Node or Leaf instance.

    This is passed to the parser driver which calls it whenever a
    reduction of a grammar rule produces a new complete node, so that
    the tree is build strictly bottom-up.
    """
    type, value, context, children = raw_node
    if children or type in gr.number2symbol:
        # If there's exactly one child, return that child instead of
        # creating a new node.
        if len(children) == 1:
            return children[0]
        return Node(type, children, context=context)
    else:
        return Leaf(type, value, context=context)


</t>
<t tx="ekr.20090302123851.434">class BasePattern(object):

    """A pattern is a tree matching pattern.

    It looks for a specific node type (token or symbol), and
    optionally for a specific content.

    This is an abstract base class.  There are three concrete
    subclasses:

    - LeafPattern matches a single leaf node;
    - NodePattern matches a single node (usually non-leaf);
    - WildcardPattern matches a sequence of nodes of variable length.
    """

    # Defaults for instance variables
    type = None     # Node type (token if &lt; 256, symbol if &gt;= 256)
    content = None  # Optional content matching pattern
    name = None     # Optional name used to store match in results dict

    @others
</t>
<t tx="ekr.20090302123851.435">def __new__(cls, *args, **kwds):
    """Constructor that prevents BasePattern from being instantiated."""
    assert cls is not BasePattern, "Cannot instantiate BasePattern"
    return object.__new__(cls)

</t>
<t tx="ekr.20090302123851.436">def __repr__(self):
    args = [type_repr(self.type), self.content, self.name]
    while args and args[-1] is None:
        del args[-1]
    return "%s(%s)" % (self.__class__.__name__, ", ".join(map(repr, args)))

</t>
<t tx="ekr.20090302123851.437">def optimize(self):
    """A subclass can define this as a hook for optimizations.

    Returns either self or another node with the same effect.
    """
    return self

</t>
<t tx="ekr.20090302123851.438">def match(self, node, results=None):
    """Does this pattern exactly match a node?

    Returns True if it matches, False if not.

    If results is not None, it must be a dict which will be
    updated with the nodes matching named subpatterns.

    Default implementation for non-wildcard patterns.
    """
    if self.type is not None and node.type != self.type:
        return False
    if self.content is not None:
        r = None
        if results is not None:
            r = {}
        if not self._submatch(node, r):
            return False
        if r:
            results.update(r)
    if results is not None and self.name:
        results[self.name] = node
    return True

</t>
<t tx="ekr.20090302123851.439">def match_seq(self, nodes, results=None):
    """Does this pattern exactly match a sequence of nodes?

    Default implementation for non-wildcard patterns.
    """
    if len(nodes) != 1:
        return False
    return self.match(nodes[0], results)

</t>
<t tx="ekr.20090302123851.44">def _visit_lambda_assign(self, results):
    self.visit_lambda_assign(name=results['name'].value)

</t>
<t tx="ekr.20090302123851.440">def generate_matches(self, nodes):
    """Generator yielding all matches for this pattern.

    Default implementation for non-wildcard patterns.
    """
    r = {}
    if nodes and self.match(nodes[0], r):
        yield 1, r


</t>
<t tx="ekr.20090302123851.441">class LeafPattern(BasePattern):
    @others
</t>
<t tx="ekr.20090302123851.442">
def __init__(self, type=None, content=None, name=None):
    """Initializer.  Takes optional type, content, and name.

    The type, if given must be a token type (&lt; 256).  If not given,
    this matches any *leaf* node; the content may still be required.

    The content, if given, must be a string.

    If a name is given, the matching node is stored in the results
    dict under that key.
    """
    if type is not None:
        assert 0 &lt;= type &lt; 256, type
    if content is not None:
        assert isinstance(content, basestring), repr(content)
    self.type = type
    self.content = content
    self.name = name

</t>
<t tx="ekr.20090302123851.443">def match(self, node, results=None):
    """Override match() to insist on a leaf node."""
    if not isinstance(node, Leaf):
        return False
    return BasePattern.match(self, node, results)

</t>
<t tx="ekr.20090302123851.444">def _submatch(self, node, results=None):
    """Match the pattern's content to the node's children.

    This assumes the node type matches and self.content is not None.

    Returns True if it matches, False if not.

    If results is not None, it must be a dict which will be
    updated with the nodes matching named subpatterns.

    When returning False, the results dict may still be updated.
    """
    return self.content == node.value


</t>
<t tx="ekr.20090302123851.445">class NodePattern(BasePattern):

    wildcards = False

    @others
</t>
<t tx="ekr.20090302123851.446">def __init__(self, type=None, content=None, name=None):
    """Initializer.  Takes optional type, content, and name.

    The type, if given, must be a symbol type (&gt;= 256).  If the
    type is None this matches *any* single node (leaf or not),
    except if content is not None, in which it only matches
    non-leaf nodes that also match the content pattern.

    The content, if not None, must be a sequence of Patterns that
    must match the node's children exactly.  If the content is
    given, the type must not be None.

    If a name is given, the matching node is stored in the results
    dict under that key.
    """
    if type is not None:
        assert type &gt;= 256, type
    if content is not None:
        assert not isinstance(content, basestring), repr(content)
        content = list(content)
        for i, item in enumerate(content):
            assert isinstance(item, BasePattern), (i, item)
            if isinstance(item, WildcardPattern):
                self.wildcards = True
    self.type = type
    self.content = content
    self.name = name

</t>
<t tx="ekr.20090302123851.447">def _submatch(self, node, results=None):
    """Match the pattern's content to the node's children.

    This assumes the node type matches and self.content is not None.

    Returns True if it matches, False if not.

    If results is not None, it must be a dict which will be
    updated with the nodes matching named subpatterns.

    When returning False, the results dict may still be updated.
    """
    if self.wildcards:
        for c, r in generate_matches(self.content, node.children):
            if c == len(node.children):
                if results is not None:
                    results.update(r)
                return True
        return False
    if len(self.content) != len(node.children):
        return False
    for subpattern, child in zip(self.content, node.children):
        if not subpattern.match(child, results):
            return False
    return True


</t>
<t tx="ekr.20090302123851.448">class WildcardPattern(BasePattern):

    """A wildcard pattern can match zero or more nodes.

    This has all the flexibility needed to implement patterns like:

    .*      .+      .?      .{m,n}
    (a b c | d e | f)
    (...)*  (...)+  (...)?  (...){m,n}

    except it always uses non-greedy matching.
    """
    @others
</t>
<t tx="ekr.20090302123851.449">
def __init__(self, content=None, min=0, max=HUGE, name=None):
    """Initializer.

    Args:
        content: optional sequence of subsequences of patterns;
                 if absent, matches one node;
                 if present, each subsequence is an alternative [*]
        min: optinal minumum number of times to match, default 0
        max: optional maximum number of times tro match, default HUGE
        name: optional name assigned to this match

    [*] Thus, if content is [[a, b, c], [d, e], [f, g, h]] this is
        equivalent to (a b c | d e | f g h); if content is None,
        this is equivalent to '.' in regular expression terms.
        The min and max parameters work as follows:
            min=0, max=maxint: .*
            min=1, max=maxint: .+
            min=0, max=1: .?
            min=1, max=1: .
        If content is not None, replace the dot with the parenthesized
        list of alternatives, e.g. (a b c | d e | f g h)*
    """
    assert 0 &lt;= min &lt;= max &lt;= HUGE, (min, max)
    if content is not None:
        content = tuple(map(tuple, content))  # Protect against alterations
        # Check sanity of alternatives
        assert len(content), repr(content)  # Can't have zero alternatives
        for alt in content:
            assert len(alt), repr(alt) # Can have empty alternatives
    self.content = content
    self.min = min
    self.max = max
    self.name = name

</t>
<t tx="ekr.20090302123851.45">def _visit_main_snippet(self, results):
    self.visit_main_snippet(body=results['body'])
</t>
<t tx="ekr.20090302123851.450">def optimize(self):
    """Optimize certain stacked wildcard patterns."""
    subpattern = None
    if (self.content is not None and
        len(self.content) == 1 and len(self.content[0]) == 1):
        subpattern = self.content[0][0]
    if self.min == 1 and self.max == 1:
        if self.content is None:
            return NodePattern(name=self.name)
        if subpattern is not None and  self.name == subpattern.name:
            return subpattern.optimize()
    if (self.min &lt;= 1 and isinstance(subpattern, WildcardPattern) and
        subpattern.min &lt;= 1 and self.name == subpattern.name):
        return WildcardPattern(subpattern.content,
                               self.min*subpattern.min,
                               self.max*subpattern.max,
                               subpattern.name)
    return self

</t>
<t tx="ekr.20090302123851.451">def match(self, node, results=None):
    """Does this pattern exactly match a node?"""
    return self.match_seq([node], results)

</t>
<t tx="ekr.20090302123851.452">def match_seq(self, nodes, results=None):
    """Does this pattern exactly match a sequence of nodes?"""
    for c, r in self.generate_matches(nodes):
        if c == len(nodes):
            if results is not None:
                results.update(r)
                if self.name:
                    results[self.name] = list(nodes)
            return True
    return False

</t>
<t tx="ekr.20090302123851.453">def generate_matches(self, nodes):
    """Generator yielding matches for a sequence of nodes.

    Args:
        nodes: sequence of nodes

    Yields:
        (count, results) tuples where:
        count: the match comprises nodes[:count];
        results: dict containing named submatches.
    """
    if self.content is None:
        # Shortcut for special case (see __init__.__doc__)
        for count in xrange(self.min, 1 + min(len(nodes), self.max)):
            r = {}
            if self.name:
                r[self.name] = nodes[:count]
            yield count, r
    elif self.name == "bare_name":
        yield self._bare_name_matches(nodes)
    else:
        for count, r in self._recursive_matches(nodes, 0):
            if self.name:
                r[self.name] = nodes[:count]
            yield count, r

</t>
<t tx="ekr.20090302123851.454">def _bare_name_matches(self, nodes):
    """Special optimized matcher for bare_name."""
    count = 0
    r = {}
    done = False
    max = len(nodes)
    while not done and count &lt; max:
        done = True
        for leaf in self.content:
            if leaf[0].match(nodes[count], r):
                count += 1
                done = False
                break
    r[self.name] = nodes[:count]
    return count, r

</t>
<t tx="ekr.20090302123851.455">def _recursive_matches(self, nodes, count):
    """Helper to recursively yield the matches."""
    assert self.content is not None
    if count &gt;= self.min:
        yield 0, {}
    if count &lt; self.max:
        for alt in self.content:
            for c0, r0 in generate_matches(alt, nodes):
                for c1, r1 in self._recursive_matches(nodes[c0:], count+1):
                    r = {}
                    r.update(r0)
                    r.update(r1)
                    yield c0 + c1, r


</t>
<t tx="ekr.20090302123851.456">class NegatedPattern(BasePattern):
    @others
</t>
<t tx="ekr.20090302123851.457">
def __init__(self, content=None):
    """Initializer.

    The argument is either a pattern or None.  If it is None, this
    only matches an empty sequence (effectively '$' in regex
    lingo).  If it is not None, this matches whenever the argument
    pattern doesn't have any matches.
    """
    if content is not None:
        assert isinstance(content, BasePattern), repr(content)
    self.content = content

</t>
<t tx="ekr.20090302123851.458">def match(self, node):
    # We never match a node in its entirety
    return False

</t>
<t tx="ekr.20090302123851.459">def match_seq(self, nodes):
    # We only match an empty sequence of nodes in its entirety
    return len(nodes) == 0

</t>
<t tx="ekr.20090302123851.46">@language python
@tabwidth -4
@others
log = logging.getLogger('pythoscope')
setup_logger()
</t>
<t tx="ekr.20090302123851.460">def generate_matches(self, nodes):
    if self.content is None:
        # Return a match if there is an empty sequence
        if len(nodes) == 0:
            yield 0, {}
    else:
        # Return a match if the argument pattern has no matches
        for c, r in self.content.generate_matches(nodes):
            return
        yield 0, {}


</t>
<t tx="ekr.20090302123851.461">def generate_matches(patterns, nodes):
    """Generator yielding matches for a sequence of patterns and nodes.

    Args:
        patterns: a sequence of patterns
        nodes: a sequence of nodes

    Yields:
        (count, results) tuples where:
        count: the entire sequence of patterns matches nodes[:count];
        results: dict containing named submatches.
        """
    if not patterns:
        yield 0, {}
    else:
        p, rest = patterns[0], patterns[1:]
        for c0, r0 in p.generate_matches(nodes):
            if not rest:
                yield c0, r0
            else:
                for c1, r1 in generate_matches(rest, nodes[c0:]):
                    r = {}
                    r.update(r0)
                    r.update(r1)
                    yield c0 + c1, r
</t>
<t tx="ekr.20090302123851.462">@language python
@tabwidth -4
@others
#empty
</t>
<t tx="ekr.20090302123851.463"></t>
<t tx="ekr.20090302123851.464">@nocolor-node

/*

Description
-----------

The parser's interface is different than usual: the function addtoken()
must be called for each token in the input.  This makes it possible to
turn it into an incremental parsing system later.  The parsing system
constructs a parse tree as it goes.

A parsing rule is represented as a Deterministic Finite-state Automaton
(DFA).  A node in a DFA represents a state of the parser; an arc represents
a transition.  Transitions are either labeled with terminal symbols or
with non-terminals.  When the parser decides to follow an arc labeled
with a non-terminal, it is invoked recursively with the DFA representing
the parsing rule for that as its initial state; when that DFA accepts,
the parser that invoked it continues.  The parse tree constructed by the
recursively called parser is inserted as a child in the current parse tree.

The DFA's can be constructed automatically from a more conventional
language description.  An extended LL(1) grammar (ELL(1)) is suitable.
Certain restrictions make the parser's life easier: rules that can produce
the empty string should be outlawed (there are other ways to put loops
or optional parts in the language).  To avoid the need to construct
FIRST sets, we can require that all but the last alternative of a rule
(really: arc going out of a DFA's state) must begin with a terminal
symbol.

As an example, consider this grammar:

expr:	term (OP term)*
term:	CONSTANT | '(' expr ')'

The DFA corresponding to the rule for expr is:

-------&gt;.---term--&gt;.-------&gt;
	^          |
	|          |
	\----OP----/

The parse tree generated for the input a+b is:

(expr: (term: (NAME: a)), (OP: +), (term: (NAME: b)))

*/
</t>
<t tx="ekr.20090302123851.465">@language python
@tabwidth -4
@others
</t>
<t tx="ekr.20090302123851.466"># Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
# Licensed to PSF under a Contributor Agreement.

"""Convert graminit.[ch] spit out by pgen to Python code.

Pgen is the Python parser generator.  It is useful to quickly create a
parser from a grammar file in Python's grammar notation.  But I don't
want my parsers to be written in C (yet), so I'm translating the
parsing tables to Python data structures and writing a Python parse
engine.

Note that the token numbers are constants determined by the standard
Python tokenizer.  The standard token module defines these numbers and
their names (the names are not used much).  The token numbers are
hardcoded into the Python tokenizer and into pgen.  A Python
implementation of the Python tokenizer is also available, in the
standard tokenize module.

On the other hand, symbol numbers (representing the grammar's
non-terminals) are assigned by pgen based on the actual grammar
input.

Note: this module is pretty much obsolete; the pgen module generates
equivalent grammar tables directly from the Grammar.txt input file
without having to invoke the Python pgen C program.

"""

# Python imports
import re

# Local imports
import grammar, token


</t>
<t tx="ekr.20090302123851.467">class Converter(grammar.Grammar):
    """Grammar subclass that reads classic pgen output files.

    The run() method reads the tables as produced by the pgen parser
    generator, typically contained in two C files, graminit.h and
    graminit.c.  The other methods are for internal use only.

    See the base class for more documentation.

    """
    @others
</t>
<t tx="ekr.20090302123851.468">
def run(self, graminit_h, graminit_c):
    """Load the grammar tables from the text files written by pgen."""
    self.parse_graminit_h(graminit_h)
    self.parse_graminit_c(graminit_c)
    self.finish_off()

</t>
<t tx="ekr.20090302123851.469">def parse_graminit_h(self, filename):
    """Parse the .h file writen by pgen.  (Internal)

    This file is a sequence of #define statements defining the
    nonterminals of the grammar as numbers.  We build two tables
    mapping the numbers to names and back.

    """
    try:
        f = open(filename)
    except IOError, err:
        print "Can't open %s: %s" % (filename, err)
        return False
    self.symbol2number = {}
    self.number2symbol = {}
    lineno = 0
    for line in f:
        lineno += 1
        mo = re.match(r"^#define\s+(\w+)\s+(\d+)$", line)
        if not mo and line.strip():
            print "%s(%s): can't parse %s" % (filename, lineno,
                                              line.strip())
        else:
            symbol, number = mo.groups()
            number = int(number)
            assert symbol not in self.symbol2number
            assert number not in self.number2symbol
            self.symbol2number[symbol] = number
            self.number2symbol[number] = symbol
    return True

</t>
<t tx="ekr.20090302123851.47">"""This module defines the logging system.

To change the logging level, assign INFO, DEBUG or ERROR to log.level. Default
is INFO.

To change the output stream, call the set_output() function. Default is
sys.stderr.
"""

import logging
import re

from time import strftime, localtime

from pythoscope.util import module_path_to_name


INFO  = logging.INFO
DEBUG = logging.DEBUG
ERROR = logging.ERROR

</t>
<t tx="ekr.20090302123851.470">def parse_graminit_c(self, filename):
    """Parse the .c file writen by pgen.  (Internal)

    The file looks as follows.  The first two lines are always this:

    #include "pgenheaders.h"
    #include "grammar.h"

    After that come four blocks:

    1) one or more state definitions
    2) a table defining dfas
    3) a table defining labels
    4) a struct defining the grammar

    A state definition has the following form:
    - one or more arc arrays, each of the form:
      static arc arcs_&lt;n&gt;_&lt;m&gt;[&lt;k&gt;] = {
              {&lt;i&gt;, &lt;j&gt;},
              ...
      };
    - followed by a state array, of the form:
      static state states_&lt;s&gt;[&lt;t&gt;] = {
              {&lt;k&gt;, arcs_&lt;n&gt;_&lt;m&gt;},
              ...
      };

    """
    try:
        f = open(filename)
    except IOError, err:
        print "Can't open %s: %s" % (filename, err)
        return False
    # The code below essentially uses f's iterator-ness!
    lineno = 0

    # Expect the two #include lines
    lineno, line = lineno+1, f.next()
    assert line == '#include "pgenheaders.h"\n', (lineno, line)
    lineno, line = lineno+1, f.next()
    assert line == '#include "grammar.h"\n', (lineno, line)

    # Parse the state definitions
    lineno, line = lineno+1, f.next()
    allarcs = {}
    states = []
    while line.startswith("static arc "):
        while line.startswith("static arc "):
            mo = re.match(r"static arc arcs_(\d+)_(\d+)\[(\d+)\] = {$",
                          line)
            assert mo, (lineno, line)
            n, m, k = map(int, mo.groups())
            arcs = []
            for _ in range(k):
                lineno, line = lineno+1, f.next()
                mo = re.match(r"\s+{(\d+), (\d+)},$", line)
                assert mo, (lineno, line)
                i, j = map(int, mo.groups())
                arcs.append((i, j))
            lineno, line = lineno+1, f.next()
            assert line == "};\n", (lineno, line)
            allarcs[(n, m)] = arcs
            lineno, line = lineno+1, f.next()
        mo = re.match(r"static state states_(\d+)\[(\d+)\] = {$", line)
        assert mo, (lineno, line)
        s, t = map(int, mo.groups())
        assert s == len(states), (lineno, line)
        state = []
        for _ in range(t):
            lineno, line = lineno+1, f.next()
            mo = re.match(r"\s+{(\d+), arcs_(\d+)_(\d+)},$", line)
            assert mo, (lineno, line)
            k, n, m = map(int, mo.groups())
            arcs = allarcs[n, m]
            assert k == len(arcs), (lineno, line)
            state.append(arcs)
        states.append(state)
        lineno, line = lineno+1, f.next()
        assert line == "};\n", (lineno, line)
        lineno, line = lineno+1, f.next()
    self.states = states

    # Parse the dfas
    dfas = {}
    mo = re.match(r"static dfa dfas\[(\d+)\] = {$", line)
    assert mo, (lineno, line)
    ndfas = int(mo.group(1))
    for i in range(ndfas):
        lineno, line = lineno+1, f.next()
        mo = re.match(r'\s+{(\d+), "(\w+)", (\d+), (\d+), states_(\d+),$',
                      line)
        assert mo, (lineno, line)
        symbol = mo.group(2)
        number, x, y, z = map(int, mo.group(1, 3, 4, 5))
        assert self.symbol2number[symbol] == number, (lineno, line)
        assert self.number2symbol[number] == symbol, (lineno, line)
        assert x == 0, (lineno, line)
        state = states[z]
        assert y == len(state), (lineno, line)
        lineno, line = lineno+1, f.next()
        mo = re.match(r'\s+("(?:\\\d\d\d)*")},$', line)
        assert mo, (lineno, line)
        first = {}
        rawbitset = eval(mo.group(1))
        for i, c in enumerate(rawbitset):
            byte = ord(c)
            for j in range(8):
                if byte &amp; (1&lt;&lt;j):
                    first[i*8 + j] = 1
        dfas[number] = (state, first)
    lineno, line = lineno+1, f.next()
    assert line == "};\n", (lineno, line)
    self.dfas = dfas

    # Parse the labels
    labels = []
    lineno, line = lineno+1, f.next()
    mo = re.match(r"static label labels\[(\d+)\] = {$", line)
    assert mo, (lineno, line)
    nlabels = int(mo.group(1))
    for i in range(nlabels):
        lineno, line = lineno+1, f.next()
        mo = re.match(r'\s+{(\d+), (0|"\w+")},$', line)
        assert mo, (lineno, line)
        x, y = mo.groups()
        x = int(x)
        if y == "0":
            y = None
        else:
            y = eval(y)
        labels.append((x, y))
    lineno, line = lineno+1, f.next()
    assert line == "};\n", (lineno, line)
    self.labels = labels

    # Parse the grammar struct
    lineno, line = lineno+1, f.next()
    assert line == "grammar _PyParser_Grammar = {\n", (lineno, line)
    lineno, line = lineno+1, f.next()
    mo = re.match(r"\s+(\d+),$", line)
    assert mo, (lineno, line)
    ndfas = int(mo.group(1))
    assert ndfas == len(self.dfas)
    lineno, line = lineno+1, f.next()
    assert line == "\tdfas,\n", (lineno, line)
    lineno, line = lineno+1, f.next()
    mo = re.match(r"\s+{(\d+), labels},$", line)
    assert mo, (lineno, line)
    nlabels = int(mo.group(1))
    assert nlabels == len(self.labels), (lineno, line)
    lineno, line = lineno+1, f.next()
    mo = re.match(r"\s+(\d+)$", line)
    assert mo, (lineno, line)
    start = int(mo.group(1))
    assert start in self.number2symbol, (lineno, line)
    self.start = start
    lineno, line = lineno+1, f.next()
    assert line == "};\n", (lineno, line)
    try:
        lineno, line = lineno+1, f.next()
    except StopIteration:
        pass
    else:
        assert 0, (lineno, line)

</t>
<t tx="ekr.20090302123851.471">def finish_off(self):
    """Create additional useful structures.  (Internal)."""
    self.keywords = {} # map from keyword strings to arc labels
    self.tokens = {}   # map from numeric token values to arc labels
    for ilabel, (type, value) in enumerate(self.labels):
        if type == token.NAME and value is not None:
            self.keywords[value] = ilabel
        elif value is None:
            self.tokens[type] = ilabel
</t>
<t tx="ekr.20090302123851.472">@language python
@tabwidth -4
@others
</t>
<t tx="ekr.20090302123851.473"># Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
# Licensed to PSF under a Contributor Agreement.

# Modifications:
# Copyright 2006 Google, Inc. All Rights Reserved.
# Licensed to PSF under a Contributor Agreement.

"""Parser driver.

This provides a high-level interface to parse a file into a syntax tree.

"""

__author__ = "Guido van Rossum &lt;guido@python.org&gt;"

__all__ = ["Driver", "load_grammar"]

# Python imports
import os
import logging
import sys

# Pgen imports
import grammar, parse, token, tokenize, pgen


</t>
<t tx="ekr.20090302123851.474">class Driver(object):
    @others
</t>
<t tx="ekr.20090302123851.475">
def __init__(self, grammar, convert=None, logger=None):
    self.grammar = grammar
    if logger is None:
        logger = logging.getLogger()
    self.logger = logger
    self.convert = convert

</t>
<t tx="ekr.20090302123851.476">def parse_tokens(self, tokens, debug=False):
    """Parse a series of tokens and return the syntax tree."""
    # XXX Move the prefix computation into a wrapper around tokenize.
    p = parse.Parser(self.grammar, self.convert)
    p.setup()
    lineno = 1
    column = 0
    type = value = start = end = line_text = None
    prefix = ""
    for quintuple in tokens:
        type, value, start, end, line_text = quintuple
        if start != (lineno, column):
            assert (lineno, column) &lt;= start, ((lineno, column), start)
            s_lineno, s_column = start
            if lineno &lt; s_lineno:
                prefix += "\n" * (s_lineno - lineno)
                lineno = s_lineno
                column = 0
            if column &lt; s_column:
                prefix += line_text[column:s_column]
                column = s_column
        if type in (tokenize.COMMENT, tokenize.NL):
            prefix += value
            lineno, column = end
            if value.endswith("\n"):
                lineno += 1
                column = 0
            continue
        if type == token.OP:
            type = grammar.opmap[value]
        if debug:
            self.logger.debug("%s %r (prefix=%r)",
                              token.tok_name[type], value, prefix)
        if p.addtoken(type, value, (prefix, start)):
            if debug:
                self.logger.debug("Stop.")
            break
        prefix = ""
        lineno, column = end
        if value.endswith("\n"):
            lineno += 1
            column = 0
    else:
        # We never broke out -- EOF is too soon (how can this happen???)
        raise parse.ParseError("incomplete input", t, v, x)
    return p.rootnode

</t>
<t tx="ekr.20090302123851.477">def parse_stream_raw(self, stream, debug=False):
    """Parse a stream and return the syntax tree."""
    tokens = tokenize.generate_tokens(stream.readline)
    return self.parse_tokens(tokens, debug)

</t>
<t tx="ekr.20090302123851.478">def parse_stream(self, stream, debug=False):
    """Parse a stream and return the syntax tree."""
    return self.parse_stream_raw(stream, debug)

</t>
<t tx="ekr.20090302123851.479">def parse_file(self, filename, debug=False):
    """Parse a file and return the syntax tree."""
    stream = open(filename)
    try:
        return self.parse_stream(stream, debug)
    finally:
        stream.close()

</t>
<t tx="ekr.20090302123851.48">def path2modname(path, default=""):
    """Take a path to a pythoscope module and return a module name in dot-style
    notation. Return default if path doesn't point to a pythoscope module.

    &gt;&gt;&gt; path2modname("sth/pythoscope/astvisitor.py")
    'astvisitor'
    &gt;&gt;&gt; path2modname("sth/pythoscope/generator/__init__.py")
    'generator'
    &gt;&gt;&gt; path2modname("sth/pythoscope/generator/adder.py")
    'generator.adder'
    """
    match = re.search(r'.*pythoscope/(.*)$', path)
    if match:
        return module_path_to_name(match.group(1), newsep=".")
    else:
        return default

</t>
<t tx="ekr.20090302123851.480">def parse_string(self, text, debug=False):
    """Parse a string and return the syntax tree."""
    tokens = tokenize.generate_tokens(generate_lines(text).next)
    return self.parse_tokens(tokens, debug)


</t>
<t tx="ekr.20090302123851.481">def generate_lines(text):
    """Generator that behaves like readline without using StringIO."""
    for line in text.splitlines(True):
        yield line
    while True:
        yield ""


</t>
<t tx="ekr.20090302123851.482">def load_grammar(gt="Grammar.txt", gp=None,
                 save=True, force=False, logger=None):
    """Load the grammar (maybe from a pickle)."""
    if logger is None:
        logger = logging.getLogger()
    if gp is None:
        head, tail = os.path.splitext(gt)
        if tail == ".txt":
            tail = ""
        gp = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
    if force or not _newer(gp, gt):
        logger.info("Generating grammar tables from %s", gt)
        g = pgen.generate_grammar(gt)
        if save:
            logger.info("Writing grammar tables to %s", gp)
            try:
                g.dump(gp)
            except IOError, e:
                logger.info("Writing failed:"+str(e))
    else:
        g = grammar.Grammar()
        g.load(gp)
    return g


</t>
<t tx="ekr.20090302123851.483">def _newer(a, b):
    """Inquire whether file a was written since file b."""
    if not os.path.exists(a):
        return False
    if not os.path.exists(b):
        return True
    return os.path.getmtime(a) &gt;= os.path.getmtime(b)
</t>
<t tx="ekr.20090302123851.484">@language python
@tabwidth -4
@others
@ignore
    # First mismatch at line 140
    # The @ sign in column 1 confuses Leo's importer.
# Map from operator to number (since tokenize doesn't do this)

opmap_raw = """
( LPAR
) RPAR
[ LSQB
] RSQB
: COLON
, COMMA
; SEMI
+ PLUS
- MINUS
* STAR
/ SLASH
| VBAR
&amp; AMPER
&lt; LESS
&gt; GREATER
= EQUAL
. DOT
% PERCENT
` BACKQUOTE
{ LBRACE
} RBRACE
@ AT
== EQEQUAL
!= NOTEQUAL
&lt;&gt; NOTEQUAL
&lt;= LESSEQUAL
&gt;= GREATEREQUAL
~ TILDE
^ CIRCUMFLEX
&lt;&lt; LEFTSHIFT
&gt;&gt; RIGHTSHIFT
** DOUBLESTAR
+= PLUSEQUAL
-= MINEQUAL
*= STAREQUAL
/= SLASHEQUAL
%= PERCENTEQUAL
&amp;= AMPEREQUAL
|= VBAREQUAL
^= CIRCUMFLEXEQUAL
&lt;&lt;= LEFTSHIFTEQUAL
&gt;&gt;= RIGHTSHIFTEQUAL
**= DOUBLESTAREQUAL
// DOUBLESLASH
//= DOUBLESLASHEQUAL
-&gt; RARROW
"""

opmap = {}
for line in opmap_raw.splitlines():
    if line:
        op, name = line.split()
        opmap[op] = getattr(token, name)
</t>
<t tx="ekr.20090302123851.485"># Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
# Licensed to PSF under a Contributor Agreement.

"""This module defines the data structures used to represent a grammar.

These are a bit arcane because they are derived from the data
structures used by Python's 'pgen' parser generator.

There's also a table here mapping operators to their names in the
token module; the Python tokenize module reports all operators as the
fallback token code OP, but the parser needs the actual token code.

"""

# Python imports
import pickle

# Local imports
import token, tokenize


</t>
<t tx="ekr.20090302123851.486">class Grammar(object):
    """Pgen parsing tables tables conversion class.

    Once initialized, this class supplies the grammar tables for the
    parsing engine implemented by parse.py.  The parsing engine
    accesses the instance variables directly.  The class here does not
    provide initialization of the tables; several subclasses exist to
    do this (see the conv and pgen modules).

    The load() method reads the tables from a pickle file, which is
    much faster than the other ways offered by subclasses.  The pickle
    file is written by calling dump() (after loading the grammar
    tables using a subclass).  The report() method prints a readable
    representation of the tables to stdout, for debugging.

    The instance variables are as follows:

    symbol2number -- a dict mapping symbol names to numbers.  Symbol
                     numbers are always 256 or higher, to distinguish
                     them from token numbers, which are between 0 and
                     255 (inclusive).

    number2symbol -- a dict mapping numbers to symbol names;
                     these two are each other's inverse.

    states        -- a list of DFAs, where each DFA is a list of
                     states, each state is is a list of arcs, and each
                     arc is a (i, j) pair where i is a label and j is
                     a state number.  The DFA number is the index into
                     this list.  (This name is slightly confusing.)
                     Final states are represented by a special arc of
                     the form (0, j) where j is its own state number.

    dfas          -- a dict mapping symbol numbers to (DFA, first)
                     pairs, where DFA is an item from the states list
                     above, and first is a set of tokens that can
                     begin this grammar rule (represented by a dict
                     whose values are always 1).

    labels        -- a list of (x, y) pairs where x is either a token
                     number or a symbol number, and y is either None
                     or a string; the strings are keywords.  The label
                     number is the index in this list; label numbers
                     are used to mark state transitions (arcs) in the
                     DFAs.

    start         -- the number of the grammar's start symbol.

    keywords      -- a dict mapping keyword strings to arc labels.

    tokens        -- a dict mapping token numbers to arc labels.

    """
    @others
</t>
<t tx="ekr.20090302123851.487">
def __init__(self):
    self.symbol2number = {}
    self.number2symbol = {}
    self.states = []
    self.dfas = {}
    self.labels = [(0, "EMPTY")]
    self.keywords = {}
    self.tokens = {}
    self.symbol2label = {}
    self.start = 256

</t>
<t tx="ekr.20090302123851.488">def dump(self, filename):
    """Dump the grammar tables to a pickle file."""
    f = open(filename, "wb")
    pickle.dump(self.__dict__, f, 2)
    f.close()

</t>
<t tx="ekr.20090302123851.489">def load(self, filename):
    """Load the grammar tables from a pickle file."""
    f = open(filename, "rb")
    d = pickle.load(f)
    f.close()
    self.__dict__.update(d)

</t>
<t tx="ekr.20090302123851.49">class LogFormatter(logging.Formatter):
    @others
</t>
<t tx="ekr.20090302123851.490">def report(self):
    """Dump the grammar tables to standard output, for debugging."""
    from pprint import pprint
    print "s2n"
    pprint(self.symbol2number)
    print "n2s"
    pprint(self.number2symbol)
    print "states"
    pprint(self.states)
    print "dfas"
    pprint(self.dfas)
    print "labels"
    pprint(self.labels)
    print "start", self.start


</t>
<t tx="ekr.20090302123851.491">@language python
@tabwidth -4
@others
if __name__ == "__main__":
    test()
</t>
<t tx="ekr.20090302123851.492"># Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
# Licensed to PSF under a Contributor Agreement.

"""Safely evaluate Python string literals without using eval()."""

import re

simple_escapes = {"a": "\a",
                  "b": "\b",
                  "f": "\f",
                  "n": "\n",
                  "r": "\r",
                  "t": "\t",
                  "v": "\v",
                  "'": "'",
                  '"': '"',
                  "\\": "\\"}

</t>
<t tx="ekr.20090302123851.493">def escape(m):
    all, tail = m.group(0, 1)
    assert all.startswith("\\")
    esc = simple_escapes.get(tail)
    if esc is not None:
        return esc
    if tail.startswith("x"):
        hexes = tail[1:]
        if len(hexes) &lt; 2:
            raise ValueError("invalid hex string escape ('\\%s')" % tail)
        try:
            i = int(hexes, 16)
        except ValueError:
            raise ValueError("invalid hex string escape ('\\%s')" % tail)
    else:
        try:
            i = int(tail, 8)
        except ValueError:
            raise ValueError("invalid octal string escape ('\\%s')" % tail)
    return chr(i)

</t>
<t tx="ekr.20090302123851.494">def evalString(s):
    assert s.startswith("'") or s.startswith('"'), repr(s[:1])
    q = s[0]
    if s[:3] == q*3:
        q = q*3
    assert s.endswith(q), repr(s[-len(q):])
    assert len(s) &gt;= 2*len(q)
    s = s[len(q):-len(q)]
    return re.sub(r"\\(\'|\"|\\|[abfnrtv]|x.{0,2}|[0-7]{1,3})", escape, s)

</t>
<t tx="ekr.20090302123851.495">def test():
    for i in range(256):
        c = chr(i)
        s = repr(c)
        e = evalString(s)
        if e != c:
            print i, c, s, e


</t>
<t tx="ekr.20090302123851.496">@language python
@tabwidth -4
@others
</t>
<t tx="ekr.20090302123851.497"># Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
# Licensed to PSF under a Contributor Agreement.

"""Parser engine for the grammar tables generated by pgen.

The grammar table must be loaded first.

See Parser/parser.c in the Python distribution for additional info on
how this parsing engine works.

"""

# Get a usable 'set' constructor
try:
    set
except NameError:
    from sets import Set as set

# Local imports
import token

</t>
<t tx="ekr.20090302123851.498">class ParseError(Exception):
    """Exception to signal the parser is stuck."""
    @others
</t>
<t tx="ekr.20090302123851.499">
def __init__(self, msg, type, value, context):
    Exception.__init__(self, "%s: type=%r, value=%r, context=%r" %
                       (msg, type, value, context))
    self.msg = msg
    self.type = type
    self.value = value
    self.context = context

</t>
<t tx="ekr.20090302123851.5"></t>
<t tx="ekr.20090302123851.50">def format(self, record):
    """Show a message with a loglevel in normal verbosity mode and much more
    in debug mode.
    """
    message = "%s: %s" % (record.levelname, record.getMessage())
    if log.level == DEBUG:
        return "%s.%d %s:%d %s" % \
            (strftime("%H%M%S", localtime(record.created)),
             record.msecs,
             path2modname(record.pathname, default=record.module),
             record.lineno,
             message)
    return message

</t>
<t tx="ekr.20090302123851.500">def __reduce__(self):
    """Implemented so pickle can serialize this object.

    &gt;&gt;&gt; import pickle
    &gt;&gt;&gt; pickle.loads(pickle.dumps(ParseError(1, 2, 3, 4)))
    ParseError('1: type=2, value=3, context=4',)
    """
    return (ParseError, (self.msg, self.type, self.value, self.context))

</t>
<t tx="ekr.20090302123851.501">class Parser(object):
    """Parser engine.

    The proper usage sequence is:

    p = Parser(grammar, [converter])  # create instance
    p.setup([start])                  # prepare for parsing
    &lt;for each input token&gt;:
        if p.addtoken(...):           # parse a token; may raise ParseError
            break
    root = p.rootnode                 # root of abstract syntax tree

    A Parser instance may be reused by calling setup() repeatedly.

    A Parser instance contains state pertaining to the current token
    sequence, and should not be used concurrently by different threads
    to parse separate token sequences.

    See driver.py for how to get input tokens by tokenizing a file or
    string.

    Parsing is complete when addtoken() returns True; the root of the
    abstract syntax tree can then be retrieved from the rootnode
    instance variable.  When a syntax error occurs, addtoken() raises
    the ParseError exception.  There is no error recovery; the parser
    cannot be used after a syntax error was reported (but it can be
    reinitialized by calling setup()).

    """
    @others
</t>
<t tx="ekr.20090302123851.502">
def __init__(self, grammar, convert=None):
    """Constructor.

    The grammar argument is a grammar.Grammar instance; see the
    grammar module for more information.

    The parser is not ready yet for parsing; you must call the
    setup() method to get it started.

    The optional convert argument is a function mapping concrete
    syntax tree nodes to abstract syntax tree nodes.  If not
    given, no conversion is done and the syntax tree produced is
    the concrete syntax tree.  If given, it must be a function of
    two arguments, the first being the grammar (a grammar.Grammar
    instance), and the second being the concrete syntax tree node
    to be converted.  The syntax tree is converted from the bottom
    up.

    A concrete syntax tree node is a (type, value, context, nodes)
    tuple, where type is the node type (a token or symbol number),
    value is None for symbols and a string for tokens, context is
    None or an opaque value used for error reporting (typically a
    (lineno, offset) pair), and nodes is a list of children for
    symbols, and None for tokens.

    An abstract syntax tree node may be anything; this is entirely
    up to the converter function.

    """
    self.grammar = grammar
    self.convert = convert or (lambda grammar, node: node)

</t>
<t tx="ekr.20090302123851.503">def setup(self, start=None):
    """Prepare for parsing.

    This *must* be called before starting to parse.

    The optional argument is an alternative start symbol; it
    defaults to the grammar's start symbol.

    You can use a Parser instance to parse any number of programs;
    each time you call setup() the parser is reset to an initial
    state determined by the (implicit or explicit) start symbol.

    """
    if start is None:
        start = self.grammar.start
    # Each stack entry is a tuple: (dfa, state, node).
    # A node is a tuple: (type, value, context, children),
    # where children is a list of nodes or None, and context may be None.
    newnode = (start, None, None, [])
    stackentry = (self.grammar.dfas[start], 0, newnode)
    self.stack = [stackentry]
    self.rootnode = None
    self.used_names = set() # Aliased to self.rootnode.used_names in pop()

</t>
<t tx="ekr.20090302123851.504">def addtoken(self, type, value, context):
    """Add a token; return True iff this is the end of the program."""
    # Map from token to label
    ilabel = self.classify(type, value, context)
    # Loop until the token is shifted; may raise exceptions
    while True:
        dfa, state, node = self.stack[-1]
        states, first = dfa
        arcs = states[state]
        # Look for a state with this label
        for i, newstate in arcs:
            t, v = self.grammar.labels[i]
            if ilabel == i:
                # Look it up in the list of labels
                assert t &lt; 256
                # Shift a token; we're done with it
                self.shift(type, value, newstate, context)
                # Pop while we are in an accept-only state
                state = newstate
                while states[state] == [(0, state)]:
                    self.pop()
                    if not self.stack:
                        # Done parsing!
                        return True
                    dfa, state, node = self.stack[-1]
                    states, first = dfa
                # Done with this token
                return False
            elif t &gt;= 256:
                # See if it's a symbol and if we're in its first set
                itsdfa = self.grammar.dfas[t]
                itsstates, itsfirst = itsdfa
                if ilabel in itsfirst:
                    # Push a symbol
                    self.push(t, self.grammar.dfas[t], newstate, context)
                    break # To continue the outer while loop
        else:
            if (0, state) in arcs:
                # An accepting state, pop it and try something else
                self.pop()
                if not self.stack:
                    # Done parsing, but another token is input
                    raise ParseError("too much input",
                                     type, value, context)
            else:
                # No success finding a transition
                raise ParseError("bad input", type, value, context)

</t>
<t tx="ekr.20090302123851.505">def classify(self, type, value, context):
    """Turn a token into a label.  (Internal)"""
    if type == token.NAME:
        # Keep a listing of all used names
        self.used_names.add(value)
        # Check for reserved words
        ilabel = self.grammar.keywords.get(value)
        if ilabel is not None:
            return ilabel
    ilabel = self.grammar.tokens.get(type)
    if ilabel is None:
        raise ParseError("bad token", type, value, context)
    return ilabel

</t>
<t tx="ekr.20090302123851.506">def shift(self, type, value, newstate, context):
    """Shift a token.  (Internal)"""
    dfa, state, node = self.stack[-1]
    newnode = (type, value, context, None)
    newnode = self.convert(self.grammar, newnode)
    if newnode is not None:
        node[-1].append(newnode)
    self.stack[-1] = (dfa, newstate, node)

</t>
<t tx="ekr.20090302123851.507">def push(self, type, newdfa, newstate, context):
    """Push a nonterminal.  (Internal)"""
    dfa, state, node = self.stack[-1]
    newnode = (type, None, context, [])
    self.stack[-1] = (dfa, newstate, node)
    self.stack.append((newdfa, 0, newnode))

</t>
<t tx="ekr.20090302123851.508">def pop(self):
    """Pop a nonterminal.  (Internal)"""
    popdfa, popstate, popnode = self.stack.pop()
    newnode = self.convert(self.grammar, popnode)
    if newnode is not None:
        if self.stack:
            dfa, state, node = self.stack[-1]
            node[-1].append(newnode)
        else:
            self.rootnode = newnode
            self.rootnode.used_names = self.used_names
</t>
<t tx="ekr.20090302123851.509">@language python
@tabwidth -4
@others
</t>
<t tx="ekr.20090302123851.51"># Don't call this "setup" or nose will assume this is the fixture setup
# function for this module.
def setup_logger():
    handler = logging.StreamHandler()
    handler.setFormatter(LogFormatter())
    log.addHandler(handler)
    log.level = INFO

</t>
<t tx="ekr.20090302123851.510"># Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
# Licensed to PSF under a Contributor Agreement.

# Pgen imports
import grammar, token, tokenize

</t>
<t tx="ekr.20090302123851.511">class PgenGrammar(grammar.Grammar):
    pass

</t>
<t tx="ekr.20090302123851.512">class ParserGenerator(object):
    @others
</t>
<t tx="ekr.20090302123851.513">
def __init__(self, filename, stream=None):
    close_stream = None
    if stream is None:
        stream = open(filename)
        close_stream = stream.close
    self.filename = filename
    self.stream = stream
    self.generator = tokenize.generate_tokens(stream.readline)
    self.gettoken() # Initialize lookahead
    self.dfas, self.startsymbol = self.parse()
    if close_stream is not None:
        close_stream()
    self.first = {} # map from symbol name to set of tokens
    self.addfirstsets()

</t>
<t tx="ekr.20090302123851.514">def make_grammar(self):
    c = PgenGrammar()
    names = self.dfas.keys()
    names.sort()
    names.remove(self.startsymbol)
    names.insert(0, self.startsymbol)
    for name in names:
        i = 256 + len(c.symbol2number)
        c.symbol2number[name] = i
        c.number2symbol[i] = name
    for name in names:
        dfa = self.dfas[name]
        states = []
        for state in dfa:
            arcs = []
            for label, next in state.arcs.iteritems():
                arcs.append((self.make_label(c, label), dfa.index(next)))
            if state.isfinal:
                arcs.append((0, dfa.index(state)))
            states.append(arcs)
        c.states.append(states)
        c.dfas[c.symbol2number[name]] = (states, self.make_first(c, name))
    c.start = c.symbol2number[self.startsymbol]
    return c

</t>
<t tx="ekr.20090302123851.515">def make_first(self, c, name):
    rawfirst = self.first[name]
    first = {}
    for label in rawfirst:
        ilabel = self.make_label(c, label)
        ##assert ilabel not in first # XXX failed on &lt;&gt; ... !=
        first[ilabel] = 1
    return first

</t>
<t tx="ekr.20090302123851.516">def make_label(self, c, label):
    # XXX Maybe this should be a method on a subclass of converter?
    ilabel = len(c.labels)
    if label[0].isalpha():
        # Either a symbol name or a named token
        if label in c.symbol2number:
            # A symbol name (a non-terminal)
            if label in c.symbol2label:
                return c.symbol2label[label]
            else:
                c.labels.append((c.symbol2number[label], None))
                c.symbol2label[label] = ilabel
                return ilabel
        else:
            # A named token (NAME, NUMBER, STRING)
            itoken = getattr(token, label, None)
            assert isinstance(itoken, int), label
            assert itoken in token.tok_name, label
            if itoken in c.tokens:
                return c.tokens[itoken]
            else:
                c.labels.append((itoken, None))
                c.tokens[itoken] = ilabel
                return ilabel
    else:
        # Either a keyword or an operator
        assert label[0] in ('"', "'"), label
        value = eval(label)
        if value[0].isalpha():
            # A keyword
            if value in c.keywords:
                return c.keywords[value]
            else:
                c.labels.append((token.NAME, value))
                c.keywords[value] = ilabel
                return ilabel
        else:
            # An operator (any non-numeric token)
            itoken = grammar.opmap[value] # Fails if unknown token
            if itoken in c.tokens:
                return c.tokens[itoken]
            else:
                c.labels.append((itoken, None))
                c.tokens[itoken] = ilabel
                return ilabel

</t>
<t tx="ekr.20090302123851.517">def addfirstsets(self):
    names = self.dfas.keys()
    names.sort()
    for name in names:
        if name not in self.first:
            self.calcfirst(name)
        #print name, self.first[name].keys()

</t>
<t tx="ekr.20090302123851.518">def calcfirst(self, name):
    dfa = self.dfas[name]
    self.first[name] = None # dummy to detect left recursion
    state = dfa[0]
    totalset = {}
    overlapcheck = {}
    for label, next in state.arcs.iteritems():
        if label in self.dfas:
            if label in self.first:
                fset = self.first[label]
                if fset is None:
                    raise ValueError("recursion for rule %r" % name)
            else:
                self.calcfirst(label)
                fset = self.first[label]
            totalset.update(fset)
            overlapcheck[label] = fset
        else:
            totalset[label] = 1
            overlapcheck[label] = {label: 1}
    inverse = {}
    for label, itsfirst in overlapcheck.iteritems():
        for symbol in itsfirst:
            if symbol in inverse:
                raise ValueError("rule %s is ambiguous; %s is in the"
                                 " first sets of %s as well as %s" %
                                 (name, symbol, label, inverse[symbol]))
            inverse[symbol] = label
    self.first[name] = totalset

</t>
<t tx="ekr.20090302123851.519">def parse(self):
    dfas = {}
    startsymbol = None
    # MSTART: (NEWLINE | RULE)* ENDMARKER
    while self.type != token.ENDMARKER:
        while self.type == token.NEWLINE:
            self.gettoken()
        # RULE: NAME ':' RHS NEWLINE
        name = self.expect(token.NAME)
        self.expect(token.OP, ":")
        a, z = self.parse_rhs()
        self.expect(token.NEWLINE)
        #self.dump_nfa(name, a, z)
        dfa = self.make_dfa(a, z)
        #self.dump_dfa(name, dfa)
        oldlen = len(dfa)
        self.simplify_dfa(dfa)
        newlen = len(dfa)
        dfas[name] = dfa
        #print name, oldlen, newlen
        if startsymbol is None:
            startsymbol = name
    return dfas, startsymbol

</t>
<t tx="ekr.20090302123851.52">def get_output():
    return log.handlers[0].stream

</t>
<t tx="ekr.20090302123851.520">def make_dfa(self, start, finish):
    # To turn an NFA into a DFA, we define the states of the DFA
    # to correspond to *sets* of states of the NFA.  Then do some
    # state reduction.  Let's represent sets as dicts with 1 for
    # values.
    assert isinstance(start, NFAState)
    assert isinstance(finish, NFAState)
    def closure(state):
        base = {}
        addclosure(state, base)
        return base
    def addclosure(state, base):
        assert isinstance(state, NFAState)
        if state in base:
            return
        base[state] = 1
        for label, next in state.arcs:
            if label is None:
                addclosure(next, base)
    states = [DFAState(closure(start), finish)]
    for state in states: # NB states grows while we're iterating
        arcs = {}
        for nfastate in state.nfaset:
            for label, next in nfastate.arcs:
                if label is not None:
                    addclosure(next, arcs.setdefault(label, {}))
        for label, nfaset in arcs.iteritems():
            for st in states:
                if st.nfaset == nfaset:
                    break
            else:
                st = DFAState(nfaset, finish)
                states.append(st)
            state.addarc(st, label)
    return states # List of DFAState instances; first one is start

</t>
<t tx="ekr.20090302123851.521">def dump_nfa(self, name, start, finish):
    print "Dump of NFA for", name
    todo = [start]
    for i, state in enumerate(todo):
        print "  State", i, state is finish and "(final)" or ""
        for label, next in state.arcs:
            if next in todo:
                j = todo.index(next)
            else:
                j = len(todo)
                todo.append(next)
            if label is None:
                print "    -&gt; %d" % j
            else:
                print "    %s -&gt; %d" % (label, j)

</t>
<t tx="ekr.20090302123851.522">def dump_dfa(self, name, dfa):
    print "Dump of DFA for", name
    for i, state in enumerate(dfa):
        print "  State", i, state.isfinal and "(final)" or ""
        for label, next in state.arcs.iteritems():
            print "    %s -&gt; %d" % (label, dfa.index(next))

</t>
<t tx="ekr.20090302123851.523">def simplify_dfa(self, dfa):
    # This is not theoretically optimal, but works well enough.
    # Algorithm: repeatedly look for two states that have the same
    # set of arcs (same labels pointing to the same nodes) and
    # unify them, until things stop changing.

    # dfa is a list of DFAState instances
    changes = True
    while changes:
        changes = False
        for i, state_i in enumerate(dfa):
            for j in range(i+1, len(dfa)):
                state_j = dfa[j]
                if state_i == state_j:
                    #print "  unify", i, j
                    del dfa[j]
                    for state in dfa:
                        state.unifystate(state_j, state_i)
                    changes = True
                    break

</t>
<t tx="ekr.20090302123851.524">def parse_rhs(self):
    # RHS: ALT ('|' ALT)*
    a, z = self.parse_alt()
    if self.value != "|":
        return a, z
    else:
        aa = NFAState()
        zz = NFAState()
        aa.addarc(a)
        z.addarc(zz)
        while self.value == "|":
            self.gettoken()
            a, z = self.parse_alt()
            aa.addarc(a)
            z.addarc(zz)
        return aa, zz

</t>
<t tx="ekr.20090302123851.525">def parse_alt(self):
    # ALT: ITEM+
    a, b = self.parse_item()
    while (self.value in ("(", "[") or
           self.type in (token.NAME, token.STRING)):
        c, d = self.parse_item()
        b.addarc(c)
        b = d
    return a, b

</t>
<t tx="ekr.20090302123851.526">def parse_item(self):
    # ITEM: '[' RHS ']' | ATOM ['+' | '*']
    if self.value == "[":
        self.gettoken()
        a, z = self.parse_rhs()
        self.expect(token.OP, "]")
        a.addarc(z)
        return a, z
    else:
        a, z = self.parse_atom()
        value = self.value
        if value not in ("+", "*"):
            return a, z
        self.gettoken()
        z.addarc(a)
        if value == "+":
            return a, z
        else:
            return a, a

</t>
<t tx="ekr.20090302123851.527">def parse_atom(self):
    # ATOM: '(' RHS ')' | NAME | STRING
    if self.value == "(":
        self.gettoken()
        a, z = self.parse_rhs()
        self.expect(token.OP, ")")
        return a, z
    elif self.type in (token.NAME, token.STRING):
        a = NFAState()
        z = NFAState()
        a.addarc(z, self.value)
        self.gettoken()
        return a, z
    else:
        self.raise_error("expected (...) or NAME or STRING, got %s/%s",
                         self.type, self.value)

</t>
<t tx="ekr.20090302123851.528">def expect(self, type, value=None):
    if self.type != type or (value is not None and self.value != value):
        self.raise_error("expected %s/%s, got %s/%s",
                         type, value, self.type, self.value)
    value = self.value
    self.gettoken()
    return value

</t>
<t tx="ekr.20090302123851.529">def gettoken(self):
    tup = self.generator.next()
    while tup[0] in (tokenize.COMMENT, tokenize.NL):
        tup = self.generator.next()
    self.type, self.value, self.begin, self.end, self.line = tup
    #print token.tok_name[self.type], repr(self.value)

</t>
<t tx="ekr.20090302123851.53">def set_output(stream):
    "Change the output of all the logging calls to go to given stream."
    log.handlers[0].stream = stream

</t>
<t tx="ekr.20090302123851.530">def raise_error(self, msg, *args):
    if args:
        try:
            msg = msg % args
        except:
            msg = " ".join([msg] + map(str, args))
    raise SyntaxError(msg, (self.filename, self.end[0],
                            self.end[1], self.line))

</t>
<t tx="ekr.20090302123851.531">class NFAState(object):
    @others
</t>
<t tx="ekr.20090302123851.532">
def __init__(self):
    self.arcs = [] # list of (label, NFAState) pairs

</t>
<t tx="ekr.20090302123851.533">def addarc(self, next, label=None):
    assert label is None or isinstance(label, str)
    assert isinstance(next, NFAState)
    self.arcs.append((label, next))

</t>
<t tx="ekr.20090302123851.534">class DFAState(object):
    @others
</t>
<t tx="ekr.20090302123851.535">
def __init__(self, nfaset, final):
    assert isinstance(nfaset, dict)
    assert isinstance(iter(nfaset).next(), NFAState)
    assert isinstance(final, NFAState)
    self.nfaset = nfaset
    self.isfinal = final in nfaset
    self.arcs = {} # map from label to DFAState

</t>
<t tx="ekr.20090302123851.536">def addarc(self, next, label):
    assert isinstance(label, str)
    assert label not in self.arcs
    assert isinstance(next, DFAState)
    self.arcs[label] = next

</t>
<t tx="ekr.20090302123851.537">def unifystate(self, old, new):
    for label, next in self.arcs.iteritems():
        if next is old:
            self.arcs[label] = new

</t>
<t tx="ekr.20090302123851.538">def __eq__(self, other):
    # Equality test -- ignore the nfaset instance variable
    assert isinstance(other, DFAState)
    if self.isfinal != other.isfinal:
        return False
    # Can't just return self.arcs == other.arcs, because that
    # would invoke this method recursively, with cycles...
    if len(self.arcs) != len(other.arcs):
        return False
    for label, next in self.arcs.iteritems():
        if next is not other.arcs.get(label):
            return False
    return True

</t>
<t tx="ekr.20090302123851.539">def generate_grammar(filename="Grammar.txt"):
    p = ParserGenerator(filename)
    return p.make_grammar()
</t>
<t tx="ekr.20090302123851.54">@language python
@tabwidth -4
@others
</t>
<t tx="ekr.20090302123851.540">@language python
@tabwidth -4
@others
</t>
<t tx="ekr.20090302123851.541">#! /usr/bin/env python

"""Token constants (from "token.h")."""

#  Taken from Python (r53757) and modified to include some tokens
#   originally monkeypatched in by pgen2.tokenize

#--start constants--
ENDMARKER = 0
NAME = 1
NUMBER = 2
STRING = 3
NEWLINE = 4
INDENT = 5
DEDENT = 6
LPAR = 7
RPAR = 8
LSQB = 9
RSQB = 10
COLON = 11
COMMA = 12
SEMI = 13
PLUS = 14
MINUS = 15
STAR = 16
SLASH = 17
VBAR = 18
AMPER = 19
LESS = 20
GREATER = 21
EQUAL = 22
DOT = 23
PERCENT = 24
BACKQUOTE = 25
LBRACE = 26
RBRACE = 27
EQEQUAL = 28
NOTEQUAL = 29
LESSEQUAL = 30
GREATEREQUAL = 31
TILDE = 32
CIRCUMFLEX = 33
LEFTSHIFT = 34
RIGHTSHIFT = 35
DOUBLESTAR = 36
PLUSEQUAL = 37
MINEQUAL = 38
STAREQUAL = 39
SLASHEQUAL = 40
PERCENTEQUAL = 41
AMPEREQUAL = 42
VBAREQUAL = 43
CIRCUMFLEXEQUAL = 44
LEFTSHIFTEQUAL = 45
RIGHTSHIFTEQUAL = 46
DOUBLESTAREQUAL = 47
DOUBLESLASH = 48
DOUBLESLASHEQUAL = 49
AT = 50
OP = 51
COMMENT = 52
NL = 53
RARROW = 54
ERRORTOKEN = 55
N_TOKENS = 56
NT_OFFSET = 256
#--end constants--

tok_name = {}
for _name, _value in globals().items():
    if type(_value) is type(0):
        tok_name[_value] = _name


</t>
<t tx="ekr.20090302123851.542">def ISTERMINAL(x):
    return x &lt; NT_OFFSET

</t>
<t tx="ekr.20090302123851.543">def ISNONTERMINAL(x):
    return x &gt;= NT_OFFSET

</t>
<t tx="ekr.20090302123851.544">def ISEOF(x):
    return x == ENDMARKER
</t>
<t tx="ekr.20090302123851.545">@language python
@tabwidth -4
@others
if __name__ == '__main__':                     # testing
    import sys
    if len(sys.argv) &gt; 1: tokenize(open(sys.argv[1]).readline)
    else: tokenize(sys.stdin.readline)
</t>
<t tx="ekr.20090302123851.546"># Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006 Python Software Foundation.
# All rights reserved.

"""Tokenization help for Python programs.

generate_tokens(readline) is a generator that breaks a stream of
text into Python tokens.  It accepts a readline-like method which is called
repeatedly to get the next line of input (or "" for EOF).  It generates
5-tuples with these members:

    the token type (see token.py)
    the token (a string)
    the starting (row, column) indices of the token (a 2-tuple of ints)
    the ending (row, column) indices of the token (a 2-tuple of ints)
    the original line (string)

It is designed to match the working of the Python tokenizer exactly, except
that it produces COMMENT tokens for comments and gives type OP for all
operators

Older entry points
    tokenize_loop(readline, tokeneater)
    tokenize(readline, tokeneater=printtoken)
are the same, except instead of generating tokens, tokeneater is a callback
function to which the 5 fields described above are passed as 5 arguments,
each time a new token is found."""

__author__ = 'Ka-Ping Yee &lt;ping@lfw.org&gt;'
__credits__ = \
    'GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro'

import string, re
from token import *

import token
__all__ = [x for x in dir(token) if x[0] != '_'] + ["tokenize",
           "generate_tokens", "untokenize"]
del token

</t>
<t tx="ekr.20090302123851.547">def group(*choices): return '(' + '|'.join(choices) + ')'
</t>
<t tx="ekr.20090302123851.548">def any(*choices): return group(*choices) + '*'
</t>
<t tx="ekr.20090302123851.549">def maybe(*choices): return group(*choices) + '?'

</t>
<t tx="ekr.20090302123851.55">import array
import re
import sets
import types

from pythoscope.astvisitor import parse_fragment, ParseError
from pythoscope.util import RePatternType, all, frozenset, \
    regexp_flags_as_string, set, underscore


</t>
<t tx="ekr.20090302123851.550">Whitespace = r'[ \f\t]*'
Comment = r'#[^\r\n]*'
Ignore = Whitespace + any(r'\\\r?\n' + Whitespace) + maybe(Comment)
Name = r'[a-zA-Z_]\w*'

Binnumber = r'0[bB][01]*'
Hexnumber = r'0[xX][\da-fA-F]*[lL]?'
Octnumber = r'0[oO]?[0-7]*[lL]?'
Decnumber = r'[1-9]\d*[lL]?'
Intnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)
Exponent = r'[eE][-+]?\d+'
Pointfloat = group(r'\d+\.\d*', r'\.\d+') + maybe(Exponent)
Expfloat = r'\d+' + Exponent
Floatnumber = group(Pointfloat, Expfloat)
Imagnumber = group(r'\d+[jJ]', Floatnumber + r'[jJ]')
Number = group(Imagnumber, Floatnumber, Intnumber)

# Tail end of ' string.
Single = r"[^'\\]*(?:\\.[^'\\]*)*'"
# Tail end of " string.
Double = r'[^"\\]*(?:\\.[^"\\]*)*"'
# Tail end of ''' string.
Single3 = r"[^'\\]*(?:(?:\\.|'(?!''))[^'\\]*)*'''"
# Tail end of """ string.
Double3 = r'[^"\\]*(?:(?:\\.|"(?!""))[^"\\]*)*"""'
Triple = group("[ubUB]?[rR]?'''", '[ubUB]?[rR]?"""')
# Single-line ' or " string.
String = group(r"[uU]?[rR]?'[^\n'\\]*(?:\\.[^\n'\\]*)*'",
               r'[uU]?[rR]?"[^\n"\\]*(?:\\.[^\n"\\]*)*"')

# Because of leftmost-then-longest match semantics, be sure to put the
# longest operators first (e.g., if = came before ==, == would get
# recognized as two instances of =).
Operator = group(r"\*\*=?", r"&gt;&gt;=?", r"&lt;&lt;=?", r"&lt;&gt;", r"!=",
                 r"//=?", r"-&gt;",
                 r"[+\-*/%&amp;|^=&lt;&gt;]=?",
                 r"~")

Bracket = '[][(){}]'
Special = group(r'\r?\n', r'[:;.,`@]')
Funny = group(Operator, Bracket, Special)

PlainToken = group(Number, Funny, String, Name)
Token = Ignore + PlainToken

# First (or only) line of ' or " string.
ContStr = group(r"[uUbB]?[rR]?'[^\n'\\]*(?:\\.[^\n'\\]*)*" +
                group("'", r'\\\r?\n'),
                r'[uUbB]?[rR]?"[^\n"\\]*(?:\\.[^\n"\\]*)*' +
                group('"', r'\\\r?\n'))
PseudoExtras = group(r'\\\r?\n', Comment, Triple)
PseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)

tokenprog, pseudoprog, single3prog, double3prog = map(
    re.compile, (Token, PseudoToken, Single3, Double3))
endprogs = {"'": re.compile(Single), '"': re.compile(Double),
            "'''": single3prog, '"""': double3prog,
            "r'''": single3prog, 'r"""': double3prog,
            "u'''": single3prog, 'u"""': double3prog,
            "b'''": single3prog, 'b"""': double3prog,
            "ur'''": single3prog, 'ur"""': double3prog,
            "br'''": single3prog, 'br"""': double3prog,
            "R'''": single3prog, 'R"""': double3prog,
            "U'''": single3prog, 'U"""': double3prog,
            "B'''": single3prog, 'B"""': double3prog,
            "uR'''": single3prog, 'uR"""': double3prog,
            "Ur'''": single3prog, 'Ur"""': double3prog,
            "UR'''": single3prog, 'UR"""': double3prog,
            "bR'''": single3prog, 'bR"""': double3prog,
            "Br'''": single3prog, 'Br"""': double3prog,
            "BR'''": single3prog, 'BR"""': double3prog,
            'r': None, 'R': None,
            'u': None, 'U': None,
            'b': None, 'B': None}

triple_quoted = {}
for t in ("'''", '"""',
          "r'''", 'r"""', "R'''", 'R"""',
          "u'''", 'u"""', "U'''", 'U"""',
          "b'''", 'b"""', "B'''", 'B"""',
          "ur'''", 'ur"""', "Ur'''", 'Ur"""',
          "uR'''", 'uR"""', "UR'''", 'UR"""',
          "br'''", 'br"""', "Br'''", 'Br"""',
          "bR'''", 'bR"""', "BR'''", 'BR"""',):
    triple_quoted[t] = t
single_quoted = {}
for t in ("'", '"',
          "r'", 'r"', "R'", 'R"',
          "u'", 'u"', "U'", 'U"',
          "b'", 'b"', "B'", 'B"',
          "ur'", 'ur"', "Ur'", 'Ur"',
          "uR'", 'uR"', "UR'", 'UR"',
          "br'", 'br"', "Br'", 'Br"',
          "bR'", 'bR"', "BR'", 'BR"', ):
    single_quoted[t] = t

tabsize = 8

class TokenError(Exception): pass

</t>
<t tx="ekr.20090302123851.551">class StopTokenizing(Exception): pass

</t>
<t tx="ekr.20090302123851.552">def printtoken(type, token, (srow, scol), (erow, ecol), line): # for testing
    print "%d,%d-%d,%d:\t%s\t%s" % \
        (srow, scol, erow, ecol, tok_name[type], repr(token))

</t>
<t tx="ekr.20090302123851.553">def tokenize(readline, tokeneater=printtoken):
    """
    The tokenize() function accepts two parameters: one representing the
    input stream, and one providing an output mechanism for tokenize().

    The first parameter, readline, must be a callable object which provides
    the same interface as the readline() method of built-in file objects.
    Each call to the function should return one line of input as a string.

    The second parameter, tokeneater, must also be a callable object. It is
    called once for each token, with five arguments, corresponding to the
    tuples generated by generate_tokens().
    """
    try:
        tokenize_loop(readline, tokeneater)
    except StopTokenizing:
        pass

</t>
<t tx="ekr.20090302123851.554"># backwards compatible interface
def tokenize_loop(readline, tokeneater):
    for token_info in generate_tokens(readline):
        tokeneater(*token_info)

</t>
<t tx="ekr.20090302123851.555">class Untokenizer:
    @others
</t>
<t tx="ekr.20090302123851.556">
def __init__(self):
    self.tokens = []
    self.prev_row = 1
    self.prev_col = 0

</t>
<t tx="ekr.20090302123851.557">def add_whitespace(self, start):
    row, col = start
    assert row &lt;= self.prev_row
    col_offset = col - self.prev_col
    if col_offset:
        self.tokens.append(" " * col_offset)

</t>
<t tx="ekr.20090302123851.558">def untokenize(self, iterable):
    for t in iterable:
        if len(t) == 2:
            self.compat(t, iterable)
            break
        tok_type, token, start, end, line = t
        self.add_whitespace(start)
        self.tokens.append(token)
        self.prev_row, self.prev_col = end
        if tok_type in (NEWLINE, NL):
            self.prev_row += 1
            self.prev_col = 0
    return "".join(self.tokens)

</t>
<t tx="ekr.20090302123851.559">def compat(self, token, iterable):
    startline = False
    indents = []
    toks_append = self.tokens.append
    toknum, tokval = token
    if toknum in (NAME, NUMBER):
        tokval += ' '
    if toknum in (NEWLINE, NL):
        startline = True
    for tok in iterable:
        toknum, tokval = tok[:2]

        if toknum in (NAME, NUMBER):
            tokval += ' '

        if toknum == INDENT:
            indents.append(tokval)
            continue
        elif toknum == DEDENT:
            indents.pop()
            continue
        elif toknum in (NEWLINE, NL):
            startline = True
        elif startline and indents:
            toks_append(indents[-1])
            startline = False
        toks_append(tokval)

</t>
<t tx="ekr.20090302123851.56"># :: SerializedObject | [SerializedObject] -&gt; bool
def can_be_constructed(obj):
    if isinstance(obj, list):
        return all(map(can_be_constructed, obj))
    return obj.reconstructor_with_imports is not None

</t>
<t tx="ekr.20090302123851.560">def untokenize(iterable):
    """Transform tokens back into Python source code.

    Each element returned by the iterable must be a token sequence
    with at least two elements, a token number and token value.  If
    only two tokens are passed, the resulting output is poor.

    Round-trip invariant for full input:
        Untokenized source will match input source exactly

    Round-trip invariant for limited intput:
        # Output text will tokenize the back to the input
        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
        newcode = untokenize(t1)
        readline = iter(newcode.splitlines(1)).next
        t2 = [tok[:2] for tokin generate_tokens(readline)]
        assert t1 == t2
    """
    ut = Untokenizer()
    return ut.untokenize(iterable)

</t>
<t tx="ekr.20090302123851.561">def generate_tokens(readline):
    """
    The generate_tokens() generator requires one argment, readline, which
    must be a callable object which provides the same interface as the
    readline() method of built-in file objects. Each call to the function
    should return one line of input as a string.  Alternately, readline
    can be a callable function terminating with StopIteration:
        readline = open(myfile).next    # Example of alternate readline

    The generator produces 5-tuples with these members: the token type; the
    token string; a 2-tuple (srow, scol) of ints specifying the row and
    column where the token begins in the source; a 2-tuple (erow, ecol) of
    ints specifying the row and column where the token ends in the source;
    and the line on which the token was found. The line passed is the
    logical line; continuation lines are included.
    """
    lnum = parenlev = continued = 0
    namechars, numchars = string.ascii_letters + '_', '0123456789'
    contstr, needcont = '', 0
    contline = None
    indents = [0]

    while 1:                                   # loop over lines in stream
        try:
            line = readline()
        except StopIteration:
            line = ''
        lnum = lnum + 1
        pos, max = 0, len(line)

        if contstr:                            # continued string
            if not line:
                raise TokenError, ("EOF in multi-line string", strstart)
            endmatch = endprog.match(line)
            if endmatch:
                pos = end = endmatch.end(0)
                yield (STRING, contstr + line[:end],
                       strstart, (lnum, end), contline + line)
                contstr, needcont = '', 0
                contline = None
            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
                yield (ERRORTOKEN, contstr + line,
                           strstart, (lnum, len(line)), contline)
                contstr = ''
                contline = None
                continue
            else:
                contstr = contstr + line
                contline = contline + line
                continue

        elif parenlev == 0 and not continued:  # new statement
            if not line: break
            column = 0
            while pos &lt; max:                   # measure leading whitespace
                if line[pos] == ' ': column = column + 1
                elif line[pos] == '\t': column = (column/tabsize + 1)*tabsize
                elif line[pos] == '\f': column = 0
                else: break
                pos = pos + 1
            if pos == max: break

            if line[pos] in '#\r\n':           # skip comments or blank lines
                if line[pos] == '#':
                    comment_token = line[pos:].rstrip('\r\n')
                    nl_pos = pos + len(comment_token)
                    yield (COMMENT, comment_token,
                           (lnum, pos), (lnum, pos + len(comment_token)), line)
                    yield (NL, line[nl_pos:],
                           (lnum, nl_pos), (lnum, len(line)), line)
                else:
                    yield ((NL, COMMENT)[line[pos] == '#'], line[pos:],
                           (lnum, pos), (lnum, len(line)), line)
                continue

            if column &gt; indents[-1]:           # count indents or dedents
                indents.append(column)
                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
            while column &lt; indents[-1]:
                if column not in indents:
                    raise IndentationError(
                        "unindent does not match any outer indentation level",
                        ("&lt;tokenize&gt;", lnum, pos, line))
                indents = indents[:-1]
                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)

        else:                                  # continued statement
            if not line:
                raise TokenError, ("EOF in multi-line statement", (lnum, 0))
            continued = 0

        while pos &lt; max:
            pseudomatch = pseudoprog.match(line, pos)
            if pseudomatch:                                # scan for tokens
                start, end = pseudomatch.span(1)
                spos, epos, pos = (lnum, start), (lnum, end), end
                token, initial = line[start:end], line[start]

                if initial in numchars or \
                   (initial == '.' and token != '.'):      # ordinary number
                    yield (NUMBER, token, spos, epos, line)
                elif initial in '\r\n':
                    newline = NEWLINE
                    if parenlev &gt; 0:
                        newline = NL
                    yield (newline, token, spos, epos, line)
                elif initial == '#':
                    assert not token.endswith("\n")
                    yield (COMMENT, token, spos, epos, line)
                elif token in triple_quoted:
                    endprog = endprogs[token]
                    endmatch = endprog.match(line, pos)
                    if endmatch:                           # all on one line
                        pos = endmatch.end(0)
                        token = line[start:pos]
                        yield (STRING, token, spos, (lnum, pos), line)
                    else:
                        strstart = (lnum, start)           # multiple lines
                        contstr = line[start:]
                        contline = line
                        break
                elif initial in single_quoted or \
                    token[:2] in single_quoted or \
                    token[:3] in single_quoted:
                    if token[-1] == '\n':                  # continued string
                        strstart = (lnum, start)
                        endprog = (endprogs[initial] or endprogs[token[1]] or
                                   endprogs[token[2]])
                        contstr, needcont = line[start:], 1
                        contline = line
                        break
                    else:                                  # ordinary string
                        yield (STRING, token, spos, epos, line)
                elif initial in namechars:                 # ordinary name
                    yield (NAME, token, spos, epos, line)
                elif initial == '\\':                      # continued stmt
                    # This yield is new; needed for better idempotency:
                    yield (NL, token, spos, (lnum, pos), line)
                    continued = 1
                else:
                    if initial in '([{': parenlev = parenlev + 1
                    elif initial in ')]}': parenlev = parenlev - 1
                    yield (OP, token, spos, epos, line)
            else:
                yield (ERRORTOKEN, line[pos],
                           (lnum, pos), (lnum, pos+1), line)
                pos = pos + 1

    for indent in indents[1:]:                 # pop remaining indent levels
        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')

</t>
<t tx="ekr.20090302123851.562">@language python
@tabwidth -4
@others
</t>
<t tx="ekr.20090302123851.563"># Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
# Licensed to PSF under a Contributor Agreement.

"""The pgen2 package."""
</t>
<t tx="ekr.20090302123851.57"># :: string -&gt; string
def string2id(string):
    """Remove from string all characters that cannot be used in an identifier.
    """
    return re.sub(r'[^a-zA-Z0-9_]', '', re.sub(r'\s+', '_', string.strip()))

</t>
<t tx="ekr.20090302123851.58"># :: object -&gt; string
def get_type_name(obj):
    """A canonical representation of the type.

    &gt;&gt;&gt; get_type_name([])
    'list'
    &gt;&gt;&gt; get_type_name({})
    'dict'

    May contain dots, if type is not builtin.
        &gt;&gt;&gt; get_type_name(lambda: None)
        'types.FunctionType'
    """
    mapping = {types.FunctionType: 'types.FunctionType',
               types.GeneratorType: 'types.GeneratorType'}
    objtype = type(obj)
    return mapping.get(objtype, objtype.__name__)

</t>
<t tx="ekr.20090302123851.59"># :: object -&gt; string
def get_module_name(obj):
    return type(obj).__module__

</t>
<t tx="ekr.20090302123851.6">@language python
@tabwidth -4
@others
</t>
<t tx="ekr.20090302123851.60"># :: object -&gt; string
def get_partial_reconstructor(obj):
    """A string representation of a partial object reconstructor.

    It doesn't have to be parsable, as it will be part of a comment. Partial
    reconstructor should give all possible hints about an object to help
    the user correct the code.
    """
    mapping = {types.FunctionType: 'function',
               types.GeneratorType: 'generator'}
    objtype = type(obj)
    default = "%s.%s" % (objtype.__module__, objtype.__name__)
    return mapping.get(objtype, default)

</t>
<t tx="ekr.20090302123851.61"># :: object -&gt; string
def get_human_readable_id(obj):
    """A human-readable description of an object, suitable to be used as
    an identifier.
    """
    # Get human readable id based on object's value,
    if obj is True:
        return 'true'
    elif obj is False:
        return 'false'

    # ... based on object's type,
    objtype = type(obj)
    mapping = {list: 'list',
               dict: 'dict',
               tuple: 'tuple',
               unicode: 'unicode_string',
               types.GeneratorType: 'generator'}
    objid = mapping.get(objtype)
    if objid:
        return objid

    # ... or based on its supertype.
    if isinstance(obj, Exception):
        return underscore(objtype.__name__)
    elif isinstance(obj, RePatternType):
        return "%s_pattern" % string2id(obj.pattern)
    elif isinstance(obj, types.FunctionType):
        if obj.func_name == '&lt;lambda&gt;':
            return "function"
        return "%s_function" % obj.func_name
    else:
        string = str(obj)
        # Looks like an instance without a custom __str__ defined.
        if string.startswith("&lt;"):
            return "%s_instance" % underscore(objtype.__name__)
        else:
            return string2id(string)

</t>
<t tx="ekr.20090302123851.62"># :: string -&gt; bool
def is_parsable(string):
    try:
        parse_fragment(string)
        return True
    except ParseError:
        return False

</t>
<t tx="ekr.20090302123851.63"># :: object -&gt; (string, set) | None
def get_reconstructor_with_imports(obj):
    """A string representing code that will construct the object plus
    a set of import descriptions needed for that code to work.

    Returns None when given object cannot be reconstructed.

    &gt;&gt;&gt; get_reconstructor_with_imports(array.array('I', [1, 2, 3, 4]))
    ("array.array('I', [1L, 2L, 3L, 4L])", ['array'])
    &gt;&gt;&gt; get_reconstructor_with_imports(array.array('d', [1, 2, 3, 4]))
    ("array.array('d', [1.0, 2.0, 3.0, 4.0])", ['array'])

    &gt;&gt;&gt; get_reconstructor_with_imports(re.compile('abcd'))
    ("re.compile('abcd')", ['re'])
    &gt;&gt;&gt; get_reconstructor_with_imports(re.compile('abcd', re.I | re.M))
    ("re.compile('abcd', re.IGNORECASE | re.MULTILINE)", ['re'])
    """
    if isinstance(obj, RePatternType):
        flags = regexp_flags_as_string(obj.flags)
        if flags:
            return ('re.compile(%r, %s)' % (obj.pattern, flags), ['re'])
        else:
            return ('re.compile(%r)' % obj.pattern, ['re'])
    elif isinstance(obj, types.FunctionType):
        function = obj.func_name
        if function != '&lt;lambda&gt;':
            module = obj.__module__
            return (function, [(module, function)])
    elif isinstance(obj, (int, long, float, str, unicode, types.NoneType)):
        # Bultin types has very convienient representation.
        return repr(obj), []
    elif isinstance(obj, array.array):
        return "array." + repr(obj), ["array"]
    elif isinstance(obj, (dict, frozenset, list, set, sets.ImmutableSet, sets.Set, tuple)):
        imports = set()
        if isinstance(obj, sets.ImmutableSet):
            imports.add(("sets", "ImmutableSet"))
        elif isinstance(obj, sets.Set):
            imports.add(("sets", "Set"))
        # Be careful not to generate wrong code.
        # TODO: Current solution is a hack. Right way to do this is to make
        # composite types call get_reconstructor_with_imports on all of their
        # elements recursively.
        if is_parsable(repr(obj)):
            return repr(obj), imports

</t>
<t tx="ekr.20090302123851.64">class SerializedObject(object):
    __slots__ = ("human_readable_id", "module_name", "partial_reconstructor",
                 "reconstructor_with_imports", "type_import", "type_name")

    @others
</t>
<t tx="ekr.20090302123851.65">def __init__(self, obj):
    self.human_readable_id = get_human_readable_id(obj)
    self.module_name = get_module_name(obj)
    self.partial_reconstructor = get_partial_reconstructor(obj)
    self.reconstructor_with_imports = get_reconstructor_with_imports(obj)
    self.type_name = get_type_name(obj)

    # An import needed for the type to be available in the testing
    # environment.
    self.type_import = (self.module_name, self.type_name)

</t>
<t tx="ekr.20090302123851.66">def __eq__(self, other):
    if not isinstance(other, SerializedObject):
        return False
    for attr in SerializedObject.__slots__:
        if getattr(self, attr) != getattr(other, attr):
            return False
    return True

</t>
<t tx="ekr.20090302123851.67">def __hash__(self):
    return hash(self.partial_reconstructor)

</t>
<t tx="ekr.20090302123851.68">def __repr__(self):
    if self.reconstructor_with_imports is not None:
        return "SerializedObject(%r)" % self.reconstructor_with_imports[0]
    else:
        return "SerializedObject(%r)" % self.partial_reconstructor

</t>
<t tx="ekr.20090302123851.69">def serialize(obj):
    return SerializedObject(obj)

</t>
<t tx="ekr.20090302123851.7">from pythoscope.logger import log
from pythoscope.util import quoted_block

from lib2to3 import pygram
from lib2to3 import pytree
from lib2to3.patcomp import compile_pattern
from lib2to3.pgen2 import driver
from lib2to3.pgen2 import token
from lib2to3.pgen2.parse import ParseError
from lib2to3.pygram import python_symbols as syms
from lib2to3.pytree import Node, Leaf


__all__ = ["EmptyCode", "Newline", "clone", "create_import", "parse",
           "regenerate", "ASTError", "ASTVisitor"]

EmptyCode = lambda: Node(syms.file_input, [])
Newline = lambda: Leaf(token.NEWLINE, "\n")

</t>
<t tx="ekr.20090302123851.70">def serialize_call_arguments(input):
    new_input = {}
    for key, value in input.iteritems():
        new_input[key] = serialize(value)
    return new_input
</t>
<t tx="ekr.20090302123851.71">@language python
@tabwidth -4
@others
</t>
<t tx="ekr.20090302123851.72">import os
import cPickle
import re
import time

from pythoscope.astvisitor import EmptyCode, Newline, create_import, find_last_leaf, \
     get_starting_whitespace, is_node_of_type, regenerate, \
     remove_trailing_whitespace
from pythoscope.serializer import serialize, serialize_call_arguments, SerializedObject
from pythoscope.util import all_of_type, cname, set, module_path_to_name, \
     write_string_to_file, ensure_directory, DirectoryException, \
     get_last_modification_time, read_file_contents, is_generator_code, \
     extract_subpath, directories_under, findfirst, contains_active_generator


</t>
<t tx="ekr.20090302123851.73">class ModuleNeedsAnalysis(Exception):
    @others
</t>
<t tx="ekr.20090302123851.74">def __init__(self, path, out_of_sync=False):
    Exception.__init__(self, "Destination test module %r needs analysis." % path)
    self.path = path
    self.out_of_sync = out_of_sync

</t>
<t tx="ekr.20090302123851.75">class ModuleNotFound(Exception):
    @others
</t>
<t tx="ekr.20090302123851.76">def __init__(self, module):
    Exception.__init__(self, "Couldn't find module %r." % module)
    self.module = module

</t>
<t tx="ekr.20090302123851.77">class ModuleSaveError(Exception):
    @others
</t>
<t tx="ekr.20090302123851.78">def __init__(self, module, reason):
    Exception.__init__(self, "Couldn't save module %r: %s." % (module, reason))
    self.module = module
    self.reason = reason

</t>
<t tx="ekr.20090302123851.79">def get_pythoscope_path(project_path):
    return os.path.join(project_path, ".pythoscope")

</t>
<t tx="ekr.20090302123851.8">def clone(tree):
    """Clone the tree, preserving its add_newline attribute.
    """
    if tree is None:
        return None

    new_tree = tree.clone()
    if hasattr(tree, 'added_newline') and tree.added_newline:
        new_tree.added_newline = True
    return new_tree

</t>
<t tx="ekr.20090302123851.80">def get_pickle_path(project_path):
    return os.path.join(get_pythoscope_path(project_path), "project.pickle")

</t>
<t tx="ekr.20090302123851.81">def get_points_of_entry_path(project_path):
    return os.path.join(get_pythoscope_path(project_path), "points-of-entry")

</t>
<t tx="ekr.20090302123851.82">def get_test_objects(objects):
    def is_test_object(object):
        return isinstance(object, TestCase)
    return filter(is_test_object, objects)

</t>
<t tx="ekr.20090302123851.83">class Project(object):
    """Object representing the whole project under Pythoscope wings.

    No modifications are final until you call save().
    """
    @others
</t>
<t tx="ekr.20090302123851.84">def from_directory(cls, project_path):
    """Read the project information from the .pythoscope/ directory of
    the given project.

    The pickle file may not exist for project that is analyzed the
    first time and that's OK.
    """
    project_path = os.path.realpath(project_path)
    try:
        fd = open(get_pickle_path(project_path))
        project = cPickle.load(fd)
        fd.close()
        # Update project's path, as the directory could've been moved.
        project.path = project_path
    except IOError:
        project = Project(project_path)
    return project
</t>
<t tx="ekr.20090302123851.85">from_directory = classmethod(from_directory)

def __init__(self, path):
    self.path = path
    self.new_tests_directory = "tests"
    self.points_of_entry = {}
    self._modules = {}

    self._find_new_tests_directory()

</t>
<t tx="ekr.20090302123851.86">def _get_pickle_path(self):
    return get_pickle_path(self.path)

</t>
<t tx="ekr.20090302123851.87">def _get_points_of_entry_path(self):
    return get_points_of_entry_path(self.path)

</t>
<t tx="ekr.20090302123851.88">def _find_new_tests_directory(self):
    for path in directories_under(self.path):
        if re.search(r'[_-]?tests?([_-]|$)', path):
            self.new_tests_directory = path

</t>
<t tx="ekr.20090302123851.89">def save(self):
    # Try pickling the project first, because if this fails, we shouldn't
    # save any changes at all.
    pickled_project = cPickle.dumps(self, cPickle.HIGHEST_PROTOCOL)

    # To avoid inconsistencies try to save all project's modules first. If
    # any of those saves fail, the pickle file won't get updated.
    for module in self.get_modules():
        module.save()

    write_string_to_file(pickled_project, self._get_pickle_path())

</t>
<t tx="ekr.20090302123851.9">def create_import(import_desc):
    """Create an AST representing import statement from given description.

    &gt;&gt;&gt; regenerate(create_import("unittest"))
    'import unittest\\n'
    &gt;&gt;&gt; regenerate(create_import(("nose", "SkipTest")))
    'from nose import SkipTest\\n'
    """
    if isinstance(import_desc, tuple):
        package, name = import_desc
        return Node(syms.import_from,
                    [Leaf(token.NAME, 'from'),
                     Leaf(token.NAME, package, prefix=" "),
                     Leaf(token.NAME, 'import', prefix=" "),
                     Leaf(token.NAME, name, prefix=" "),
                     Newline()])
    else:
        return Node(syms.import_name,
                    [Leaf(token.NAME, 'import'),
                     Leaf(token.NAME, import_desc, prefix=" "),
                     Newline()])

</t>
<t tx="ekr.20090302123851.90">def find_module_by_full_path(self, path):
    subpath = self._extract_subpath(path)
    return self[subpath]

</t>
<t tx="ekr.20090302123851.91">def ensure_point_of_entry(self, path):
    name = self._extract_point_of_entry_subpath(path)
    if name not in self.points_of_entry:
        poe = PointOfEntry(project=self, name=name)
        self.points_of_entry[name] = poe
    return self.points_of_entry[name]

</t>
<t tx="ekr.20090302123851.92">def remove_point_of_entry(self, name):
    poe = self.points_of_entry.pop(name)
    poe.clear_previous_run()

</t>
<t tx="ekr.20090302123851.93">def create_module(self, path, **kwds):
    """Create a module for this project located under given path.

    If there already was a module with given subpath, it will get replaced
    with a new instance using the _replace_references_to_module method.

    Returns the new Module object.
    """
    module = Module(subpath=self._extract_subpath(path), project=self, **kwds)

    if module.subpath in self._modules.keys():
        self._replace_references_to_module(module)

    self._modules[module.subpath] = module

    return module

</t>
<t tx="ekr.20090302123851.94">def create_test_module_from_name(self, test_name):
    """Create a module with given name in project tests directory.

    If the test module already exists, ModuleNeedsAnalysis exception will
    be raised.
    """
    test_path = self._path_for_test(test_name)
    if os.path.exists(test_path):
        raise ModuleNeedsAnalysis(test_path)
    return self.create_module(test_path)

</t>
<t tx="ekr.20090302123851.95">def remove_module(self, subpath):
    """Remove a module from this Project along with all references to it
    from other modules.
    """
    module = self[subpath]
    for test_case in self.iter_test_cases():
        try:
            test_case.associated_modules.remove(module)
        except ValueError:
            pass
    del self._modules[subpath]

</t>
<t tx="ekr.20090302123851.96">def _replace_references_to_module(self, module):
    """Remove a module with the same subpath as given module from this
    Project and replace all references to it with the new instance.
    """
    old_module = self[module.subpath]
    for test_case in self.iter_test_cases():
        try:
            test_case.associated_modules.remove(old_module)
            test_case.associated_modules.append(module)
        except ValueError:
            pass

</t>
<t tx="ekr.20090302123851.97">def _extract_point_of_entry_subpath(self, path):
    """Takes the file path and returns subpath relative to the
    points of entry path.

    Assumes the given path is under points of entry path.
    """
    return extract_subpath(path, self._get_points_of_entry_path())

</t>
<t tx="ekr.20090302123851.98">def _extract_subpath(self, path):
    """Takes the file path and returns subpath relative to the
    project.

    Assumes the given path is under Project.path.
    """
    return extract_subpath(path, self.path)

</t>
<t tx="ekr.20090302123851.99">def iter_test_cases(self):
    for module in self.iter_modules():
        for test_case in module.test_cases:
            yield test_case

</t>
<t tx="ekr.20100315100121.2449"></t>
</tnodes>
</leo_file>
