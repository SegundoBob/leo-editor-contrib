<?xml version="1.0" encoding="utf-8"?>
<!-- Created by Leo: http://leoeditor.com/leo_toc.html -->
<leo_file xmlns:leo="http://leoeditor.com/namespaces/leo-python-editor/1.1" >
<leo_header file_format="2" tnodes="0" max_tnode_index="0" clone_windows="0"/>
<globals body_outline_ratio="0.5" body_secondary_ratio="0.5">
	<global_window_position top="50" left="50" height="500" width="700"/>
	<global_log_window_position top="0" left="0" height="0" width="0"/>
</globals>
<preferences/>
<find_panel_settings/>
<vnodes>
<v t="ekr.20180516073049.1"><vh>@settings</vh>
<v t="ekr.20180516073122.1"><vh>@bool run-pyflakes-on-write = False</vh></v>
</v>
<v t="ekr.20180516071358.1"><vh>Recursive import script</vh></v>
<v t="ekr.20180516073042.1"><vh>Logs</vh>
<v t="ekr.20180516071825.1"><vh>log</vh></v>
<v t="ekr.20180516072403.1"><vh>pyflakes errors</vh></v>
<v t="ekr.20180516072650.1"><vh>log: writing @ignore nodes</vh></v>
</v>
<v t="ekr.20180516071749.2"><vh>@path C:/Anaconda3/Lib/site-packages/jedi</vh>
<v t="ekr.20180516071749.3"><vh>@clean cache.py</vh>
<v t="ekr.20180516071749.4"><vh>Declarations</vh></v>
<v t="ekr.20180516071749.5"><vh>underscore_memoization</vh></v>
<v t="ekr.20180516071749.6"><vh>clear_time_caches</vh></v>
<v t="ekr.20180516071749.7"><vh>time_cache</vh></v>
<v t="ekr.20180516071749.8"><vh>memoize_method</vh></v>
</v>
<v t="ekr.20180516071749.9"><vh>@clean common.py</vh>
<v t="ekr.20180516071749.10"><vh>Declarations</vh></v>
<v t="ekr.20180516071749.11"><vh>class UncaughtAttributeError</vh></v>
<v t="ekr.20180516071749.12"><vh>safe_property</vh></v>
<v t="ekr.20180516071749.13"><vh>reraise_uncaught</vh></v>
<v t="ekr.20180516071749.14"><vh>class PushBackIterator</vh>
<v t="ekr.20180516071749.15"><vh>__init__</vh></v>
<v t="ekr.20180516071749.16"><vh>push_back</vh></v>
<v t="ekr.20180516071749.17"><vh>__iter__</vh></v>
<v t="ekr.20180516071749.18"><vh>next</vh></v>
<v t="ekr.20180516071749.19"><vh>__next__</vh></v>
</v>
<v t="ekr.20180516071749.20"><vh>scale_speed_settings</vh></v>
<v t="ekr.20180516071749.21"><vh>indent_block</vh></v>
<v t="ekr.20180516071749.22"><vh>ignored</vh></v>
<v t="ekr.20180516071749.23"><vh>source_to_unicode</vh></v>
<v t="ekr.20180516071749.24"><vh>splitlines</vh></v>
<v t="ekr.20180516071749.25"><vh>unite</vh></v>
<v t="ekr.20180516071749.26"><vh>to_list</vh></v>
</v>
<v t="ekr.20180516071749.27"><vh>@clean debug.py</vh>
<v t="ekr.20180516071749.28"><vh>Declarations</vh></v>
<v t="ekr.20180516071749.29"><vh>_lazy_colorama_init</vh></v>
<v t="ekr.20180516071749.30"><vh>reset_time</vh></v>
<v t="ekr.20180516071749.31"><vh>increase_indent</vh></v>
<v t="ekr.20180516071749.32"><vh>dbg</vh></v>
<v t="ekr.20180516071749.33"><vh>warning</vh></v>
<v t="ekr.20180516071749.34"><vh>speed</vh></v>
<v t="ekr.20180516071749.35"><vh>print_to_stdout</vh></v>
</v>
<v t="ekr.20180516071749.36"><vh>@clean refactoring.py</vh>
<v t="ekr.20180516071749.37"><vh>Declarations</vh></v>
<v t="ekr.20180516071749.38"><vh>class Refactoring</vh>
<v t="ekr.20180516071749.39"><vh>__init__</vh></v>
<v t="ekr.20180516071749.40"><vh>old_files</vh></v>
<v t="ekr.20180516071749.41"><vh>new_files</vh></v>
<v t="ekr.20180516071749.42"><vh>diff</vh></v>
</v>
<v t="ekr.20180516071749.43"><vh>rename</vh></v>
<v t="ekr.20180516071749.44"><vh>_rename</vh></v>
<v t="ekr.20180516071749.45"><vh>extract</vh></v>
<v t="ekr.20180516071749.46"><vh>inline</vh></v>
</v>
<v t="ekr.20180516071750.1"><vh>@clean settings.py</vh>
<v t="ekr.20180516071750.2"><vh>Declarations</vh></v>
</v>
<v t="ekr.20180516071750.3"><vh>@clean utils.py</vh>
<v t="ekr.20180516071750.4"><vh>Declarations</vh></v>
<v t="ekr.20180516071750.5"><vh>setup_readline</vh></v>
<v t="ekr.20180516071750.6"><vh>version_info</vh></v>
</v>
<v t="ekr.20180516071750.7"><vh>@clean _compatibility.py</vh>
<v t="ekr.20180516071750.8"><vh>Declarations</vh></v>
<v t="ekr.20180516071750.9"><vh>class DummyFile</vh>
<v t="ekr.20180516071750.10"><vh>__init__</vh></v>
<v t="ekr.20180516071750.11"><vh>read</vh></v>
<v t="ekr.20180516071750.12"><vh>close</vh></v>
</v>
<v t="ekr.20180516071750.13"><vh>find_module_py34</vh></v>
<v t="ekr.20180516071750.14"><vh>find_module_py33</vh></v>
<v t="ekr.20180516071750.15"><vh>find_module_pre_py33</vh></v>
<v t="ekr.20180516071750.16"><vh>class ImplicitNSInfo</vh>
<v t="ekr.20180516071750.17"><vh>__init__</vh></v>
</v>
<v t="ekr.20180516071750.18"><vh>class Python3Method</vh>
<v t="ekr.20180516071750.19"><vh>__init__</vh></v>
<v t="ekr.20180516071750.20"><vh>__get__</vh></v>
</v>
<v t="ekr.20180516071750.21"><vh>use_metaclass</vh></v>
<v t="ekr.20180516071750.22"><vh>u</vh></v>
<v t="ekr.20180516071750.23"><vh>literal_eval</vh></v>
<v t="ekr.20180516071750.24"><vh>no_unicode_pprint</vh></v>
<v t="ekr.20180516071750.25"><vh>utf8_repr</vh></v>
</v>
<v t="ekr.20180516071750.26"><vh>@clean __init__.py</vh>
<v t="ekr.20180516071750.27"><vh>Declarations</vh></v>
</v>
<v t="ekr.20180516071750.28"><vh>@clean __main__.py</vh>
<v t="ekr.20180516071750.29"><vh>Declarations</vh></v>
<v t="ekr.20180516071750.30"><vh>_start_linter</vh></v>
</v>
<v t="ekr.20180516071750.98"><vh>@path api</vh>
<v t="ekr.20180516071750.99"><vh>@clean classes.py</vh>
<v t="ekr.20180516071750.100"><vh>Declarations</vh></v>
<v t="ekr.20180516071750.101"><vh>_sort_names_by_start_pos</vh></v>
<v t="ekr.20180516071750.102"><vh>defined_names</vh></v>
<v t="ekr.20180516071750.103"><vh>class BaseDefinition</vh>
<v t="ekr.20180516071750.104"><vh>__init__</vh></v>
<v t="ekr.20180516071750.105"><vh>name</vh></v>
<v t="ekr.20180516071750.106"><vh>type</vh></v>
<v t="ekr.20180516071750.107"><vh>_path</vh></v>
<v t="ekr.20180516071750.108"><vh>module_name</vh></v>
<v t="ekr.20180516071750.109"><vh>in_builtin_module</vh></v>
<v t="ekr.20180516071750.110"><vh>line</vh></v>
<v t="ekr.20180516071750.111"><vh>column</vh></v>
<v t="ekr.20180516071750.112"><vh>docstring</vh></v>
<v t="ekr.20180516071750.113"><vh>doc</vh></v>
<v t="ekr.20180516071750.114"><vh>raw_doc</vh></v>
<v t="ekr.20180516071750.115"><vh>description</vh></v>
<v t="ekr.20180516071750.116"><vh>full_name</vh></v>
<v t="ekr.20180516071750.117"><vh>goto_assignments</vh></v>
<v t="ekr.20180516071750.118"><vh>_goto_definitions</vh></v>
<v t="ekr.20180516071750.119"><vh>params</vh></v>
<v t="ekr.20180516071750.120"><vh>parent</vh></v>
<v t="ekr.20180516071750.121"><vh>__repr__</vh></v>
<v t="ekr.20180516071750.122"><vh>get_line_code</vh></v>
</v>
<v t="ekr.20180516071750.123"><vh>class Completion</vh>
<v t="ekr.20180516071750.124"><vh>__init__</vh></v>
<v t="ekr.20180516071750.125"><vh>_complete</vh></v>
<v t="ekr.20180516071750.126"><vh>complete</vh></v>
<v t="ekr.20180516071750.127"><vh>name_with_symbols</vh></v>
<v t="ekr.20180516071750.128"><vh>docstring</vh></v>
<v t="ekr.20180516071750.129"><vh>description</vh></v>
<v t="ekr.20180516071750.130"><vh>__repr__</vh></v>
<v t="ekr.20180516071750.131"><vh>follow_definition</vh></v>
</v>
<v t="ekr.20180516071750.132"><vh>class Definition</vh>
<v t="ekr.20180516071750.133"><vh>__init__</vh></v>
<v t="ekr.20180516071750.134"><vh>description</vh></v>
<v t="ekr.20180516071750.135"><vh>desc_with_module</vh></v>
<v t="ekr.20180516071750.136"><vh>defined_names</vh></v>
<v t="ekr.20180516071750.137"><vh>is_definition</vh></v>
<v t="ekr.20180516071750.138"><vh>__eq__</vh></v>
<v t="ekr.20180516071750.139"><vh>__ne__</vh></v>
<v t="ekr.20180516071750.140"><vh>__hash__</vh></v>
</v>
<v t="ekr.20180516071750.141"><vh>class CallSignature</vh>
<v t="ekr.20180516071750.142"><vh>__init__</vh></v>
<v t="ekr.20180516071750.143"><vh>index</vh></v>
<v t="ekr.20180516071750.144"><vh>bracket_start</vh></v>
<v t="ekr.20180516071750.145"><vh>call_name</vh></v>
<v t="ekr.20180516071750.146"><vh>module</vh></v>
<v t="ekr.20180516071750.147"><vh>__repr__</vh></v>
</v>
<v t="ekr.20180516071750.148"><vh>class _Param</vh>
<v t="ekr.20180516071750.149"><vh>get_code</vh></v>
</v>
<v t="ekr.20180516071750.150"><vh>class _Help</vh>
<v t="ekr.20180516071750.151"><vh>__init__</vh></v>
<v t="ekr.20180516071750.152"><vh>_get_node</vh></v>
<v t="ekr.20180516071750.153"><vh>full</vh></v>
<v t="ekr.20180516071750.154"><vh>raw</vh></v>
</v>
</v>
<v t="ekr.20180516071750.155"><vh>@clean completion.py</vh>
<v t="ekr.20180516071750.156"><vh>Declarations</vh></v>
<v t="ekr.20180516071750.157"><vh>get_call_signature_param_names</vh></v>
<v t="ekr.20180516071750.158"><vh>filter_names</vh></v>
<v t="ekr.20180516071750.159"><vh>get_user_scope</vh></v>
<v t="ekr.20180516071750.160"><vh>get_flow_scope_node</vh></v>
<v t="ekr.20180516071750.161"><vh>class Completion</vh>
<v t="ekr.20180516071750.162"><vh>__init__</vh></v>
<v t="ekr.20180516071750.163"><vh>completions</vh></v>
<v t="ekr.20180516071750.164"><vh>_get_context_completions</vh></v>
<v t="ekr.20180516071750.165"><vh>_get_keyword_completion_names</vh></v>
<v t="ekr.20180516071750.166"><vh>_global_completions</vh></v>
<v t="ekr.20180516071750.167"><vh>_trailer_completions</vh></v>
<v t="ekr.20180516071750.168"><vh>_parse_dotted_names</vh></v>
<v t="ekr.20180516071750.169"><vh>_get_importer_names</vh></v>
<v t="ekr.20180516071750.170"><vh>_get_class_context_completions</vh></v>
</v>
</v>
<v t="ekr.20180516071750.171"><vh>@clean helpers.py</vh>
<v t="ekr.20180516071750.172"><vh>Declarations</vh></v>
<v t="ekr.20180516071750.173"><vh>sorted_definitions</vh></v>
<v t="ekr.20180516071750.174"><vh>get_on_completion_name</vh></v>
<v t="ekr.20180516071750.175"><vh>_get_code</vh></v>
<v t="ekr.20180516071750.176"><vh>class OnErrorLeaf</vh>
<v t="ekr.20180516071750.177"><vh>error_leaf</vh></v>
</v>
<v t="ekr.20180516071750.178"><vh>_is_on_comment</vh></v>
<v t="ekr.20180516071750.179"><vh>_get_code_for_stack</vh></v>
<v t="ekr.20180516071750.180"><vh>get_stack_at_position</vh></v>
<v t="ekr.20180516071750.181"><vh>class Stack</vh>
<v t="ekr.20180516071750.182"><vh>get_node_names</vh></v>
<v t="ekr.20180516071750.183"><vh>get_nodes</vh></v>
</v>
<v t="ekr.20180516071750.184"><vh>get_possible_completion_types</vh></v>
<v t="ekr.20180516071750.185"><vh>evaluate_goto_definition</vh></v>
<v t="ekr.20180516071750.186"><vh>_get_index_and_key</vh></v>
<v t="ekr.20180516071750.187"><vh>_get_call_signature_details_from_error_node</vh></v>
<v t="ekr.20180516071750.188"><vh>get_call_signature_details</vh></v>
<v t="ekr.20180516071750.189"><vh>cache_call_signatures</vh></v>
</v>
<v t="ekr.20180516071750.190"><vh>@clean interpreter.py</vh>
<v t="ekr.20180516071750.191"><vh>Declarations</vh></v>
<v t="ekr.20180516071750.192"><vh>class MixedModuleContext</vh>
<v t="ekr.20180516071750.193"><vh>__init__</vh></v>
<v t="ekr.20180516071750.194"><vh>get_node</vh></v>
<v t="ekr.20180516071750.195"><vh>get_filters</vh></v>
<v t="ekr.20180516071750.196"><vh>__getattr__</vh></v>
</v>
</v>
<v t="ekr.20180516071750.197"><vh>@clean keywords.py</vh>
<v t="ekr.20180516071750.198"><vh>Declarations</vh></v>
<v t="ekr.20180516071750.199"><vh>has_inappropriate_leaf_keyword</vh></v>
<v t="ekr.20180516071750.200"><vh>completion_names</vh></v>
<v t="ekr.20180516071750.201"><vh>all_keywords</vh></v>
<v t="ekr.20180516071750.202"><vh>keyword</vh></v>
<v t="ekr.20180516071750.203"><vh>get_operator</vh></v>
<v t="ekr.20180516071750.204"><vh>class KeywordName</vh>
<v t="ekr.20180516071750.205"><vh>__init__</vh></v>
<v t="ekr.20180516071750.206"><vh>eval</vh></v>
</v>
<v t="ekr.20180516071750.207"><vh>class Keyword</vh>
<v t="ekr.20180516071750.208"><vh>__init__</vh></v>
<v t="ekr.20180516071750.209"><vh>only_valid_as_leaf</vh></v>
<v t="ekr.20180516071750.210"><vh>names</vh></v>
<v t="ekr.20180516071750.211"><vh>docstr</vh></v>
<v t="ekr.20180516071750.212"><vh>__repr__</vh></v>
</v>
<v t="ekr.20180516071750.213"><vh>imitate_pydoc</vh></v>
</v>
<v t="ekr.20180516071750.214"><vh>@clean replstartup.py</vh>
<v t="ekr.20180516071750.215"><vh>Declarations</vh></v>
</v>
<v t="ekr.20180516071750.216"><vh>@clean usages.py</vh>
<v t="ekr.20180516071750.217"><vh>Declarations</vh></v>
<v t="ekr.20180516071750.218"><vh>compare_contexts</vh></v>
<v t="ekr.20180516071750.219"><vh>usages</vh></v>
<v t="ekr.20180516071750.220"><vh>resolve_potential_imports</vh></v>
</v>
<v t="ekr.20180516071750.221"><vh>@clean __init__.py</vh>
<v t="ekr.20180516071750.222"><vh>Declarations</vh></v>
<v t="ekr.20180516071750.223"><vh>class NotFoundError</vh></v>
<v t="ekr.20180516071750.224"><vh>class Script</vh>
<v t="ekr.20180516071750.225"><vh>__init__</vh></v>
<v t="ekr.20180516071750.226"><vh>_get_module_node</vh></v>
<v t="ekr.20180516071750.227"><vh>_get_module</vh></v>
<v t="ekr.20180516071750.228"><vh>source_path</vh></v>
<v t="ekr.20180516071750.229"><vh>__repr__</vh></v>
<v t="ekr.20180516071750.230"><vh>completions</vh></v>
<v t="ekr.20180516071750.231"><vh>goto_definitions</vh></v>
<v t="ekr.20180516071750.232"><vh>goto_assignments</vh></v>
<v t="ekr.20180516071750.233"><vh>_goto</vh></v>
<v t="ekr.20180516071750.234"><vh>usages</vh></v>
<v t="ekr.20180516071750.235"><vh>call_signatures</vh></v>
<v t="ekr.20180516071750.236"><vh>_analysis</vh></v>
</v>
<v t="ekr.20180516071750.237"><vh>class Interpreter</vh>
<v t="ekr.20180516071750.238"><vh>__init__</vh></v>
<v t="ekr.20180516071750.239"><vh>_get_module</vh></v>
</v>
<v t="ekr.20180516071750.240"><vh>defined_names</vh></v>
<v t="ekr.20180516071750.241"><vh>names</vh></v>
<v t="ekr.20180516071750.242"><vh>preload_module</vh></v>
<v t="ekr.20180516071750.243"><vh>set_debug_function</vh></v>
</v>
</v>
<v t="ekr.20180516071750.245"><vh>@path evaluate</vh>
<v t="ekr.20180516071750.246"><vh>@clean analysis.py</vh>
<v t="ekr.20180516071750.247"><vh>Declarations</vh></v>
<v t="ekr.20180516071750.248"><vh>class Error</vh>
<v t="ekr.20180516071750.249"><vh>__init__</vh></v>
<v t="ekr.20180516071750.250"><vh>line</vh></v>
<v t="ekr.20180516071750.251"><vh>column</vh></v>
<v t="ekr.20180516071750.252"><vh>code</vh></v>
<v t="ekr.20180516071750.253"><vh>__unicode__</vh></v>
<v t="ekr.20180516071750.254"><vh>__str__</vh></v>
<v t="ekr.20180516071750.255"><vh>__eq__</vh></v>
<v t="ekr.20180516071750.256"><vh>__ne__</vh></v>
<v t="ekr.20180516071750.257"><vh>__hash__</vh></v>
<v t="ekr.20180516071750.258"><vh>__repr__</vh></v>
</v>
<v t="ekr.20180516071750.259"><vh>class Warning</vh></v>
<v t="ekr.20180516071750.260"><vh>add</vh></v>
<v t="ekr.20180516071750.261"><vh>_check_for_setattr</vh></v>
<v t="ekr.20180516071750.262"><vh>add_attribute_error</vh></v>
<v t="ekr.20180516071750.263"><vh>_check_for_exception_catch</vh></v>
</v>
<v t="ekr.20180516071750.264"><vh>@clean cache.py</vh>
<v t="ekr.20180516071750.265"><vh>Declarations</vh></v>
<v t="ekr.20180516071750.266"><vh>memoize_default</vh></v>
<v t="ekr.20180516071750.267"><vh>class CachedMetaClass</vh>
<v t="ekr.20180516071750.268"><vh>__call__</vh></v>
</v>
</v>
<v t="ekr.20180516071750.269"><vh>@clean context.py</vh>
<v t="ekr.20180516071750.270"><vh>Declarations</vh></v>
<v t="ekr.20180516071750.271"><vh>class Context</vh>
<v t="ekr.20180516071750.272"><vh>__init__</vh></v>
<v t="ekr.20180516071750.273"><vh>get_root_context</vh></v>
<v t="ekr.20180516071750.274"><vh>execute</vh></v>
<v t="ekr.20180516071750.275"><vh>execute_evaluated</vh></v>
<v t="ekr.20180516071750.276"><vh>eval_node</vh></v>
<v t="ekr.20180516071750.277"><vh>eval_stmt</vh></v>
<v t="ekr.20180516071750.278"><vh>eval_trailer</vh></v>
<v t="ekr.20180516071750.279"><vh>create_context</vh></v>
<v t="ekr.20180516071750.280"><vh>is_class</vh></v>
<v t="ekr.20180516071750.281"><vh>py__bool__</vh></v>
</v>
<v t="ekr.20180516071750.282"><vh>class TreeContext</vh>
<v t="ekr.20180516071750.283"><vh>__init__</vh></v>
<v t="ekr.20180516071750.284"><vh>__repr__</vh></v>
</v>
<v t="ekr.20180516071750.285"><vh>class AbstractLazyContext</vh>
<v t="ekr.20180516071750.286"><vh>__init__</vh></v>
<v t="ekr.20180516071750.287"><vh>__repr__</vh></v>
<v t="ekr.20180516071750.288"><vh>infer</vh></v>
</v>
<v t="ekr.20180516071750.289"><vh>class LazyKnownContext</vh>
<v t="ekr.20180516071750.290"><vh>infer</vh></v>
</v>
<v t="ekr.20180516071750.291"><vh>class LazyKnownContexts</vh>
<v t="ekr.20180516071750.292"><vh>infer</vh></v>
</v>
<v t="ekr.20180516071750.293"><vh>class LazyUnknownContext</vh>
<v t="ekr.20180516071750.294"><vh>__init__</vh></v>
<v t="ekr.20180516071750.295"><vh>infer</vh></v>
</v>
<v t="ekr.20180516071750.296"><vh>class LazyTreeContext</vh>
<v t="ekr.20180516071750.297"><vh>__init__</vh></v>
<v t="ekr.20180516071750.298"><vh>infer</vh></v>
</v>
<v t="ekr.20180516071750.299"><vh>get_merged_lazy_context</vh></v>
<v t="ekr.20180516071750.300"><vh>class MergedLazyContexts</vh>
<v t="ekr.20180516071750.301"><vh>infer</vh></v>
</v>
<v t="ekr.20180516071750.302"><vh>class ContextualizedNode</vh>
<v t="ekr.20180516071750.303"><vh>__init__</vh></v>
<v t="ekr.20180516071750.304"><vh>get_root_context</vh></v>
<v t="ekr.20180516071750.305"><vh>infer</vh></v>
</v>
<v t="ekr.20180516071750.306"><vh>class ContextualizedName</vh>
<v t="ekr.20180516071750.307"><vh>name</vh></v>
<v t="ekr.20180516071750.308"><vh>assignment_indexes</vh></v>
</v>
</v>
<v t="ekr.20180516071750.309"><vh>@clean docstrings.py</vh>
<v t="ekr.20180516071750.310"><vh>Declarations</vh></v>
<v t="ekr.20180516071750.311"><vh>_search_param_in_docstr</vh></v>
<v t="ekr.20180516071750.312"><vh>_strip_rst_role</vh></v>
<v t="ekr.20180516071750.313"><vh>_evaluate_for_statement_string</vh></v>
<v t="ekr.20180516071750.314"><vh>_execute_types_in_stmt</vh></v>
<v t="ekr.20180516071750.315"><vh>_execute_array_values</vh></v>
<v t="ekr.20180516071750.316"><vh>follow_param</vh></v>
<v t="ekr.20180516071750.317"><vh>find_return_types</vh></v>
</v>
<v t="ekr.20180516071750.318"><vh>@clean dynamic.py</vh>
<v t="ekr.20180516071750.319"><vh>Declarations</vh></v>
<v t="ekr.20180516071750.320"><vh>class ParamListener</vh>
<v t="ekr.20180516071750.321"><vh>__init__</vh></v>
<v t="ekr.20180516071750.322"><vh>execute</vh></v>
</v>
<v t="ekr.20180516071750.323"><vh>class MergedExecutedParams</vh>
<v t="ekr.20180516071750.324"><vh>__init__</vh></v>
<v t="ekr.20180516071750.325"><vh>infer</vh></v>
</v>
<v t="ekr.20180516071750.326"><vh>search_params</vh></v>
<v t="ekr.20180516071750.327"><vh>_search_function_executions</vh></v>
<v t="ekr.20180516071750.328"><vh>_get_possible_nodes</vh></v>
<v t="ekr.20180516071750.329"><vh>_check_name_for_execution</vh></v>
</v>
<v t="ekr.20180516071750.330"><vh>@clean filters.py</vh>
<v t="ekr.20180516071750.331"><vh>Declarations</vh></v>
<v t="ekr.20180516071750.332"><vh>class AbstractNameDefinition</vh>
<v t="ekr.20180516071750.333"><vh>infer</vh></v>
<v t="ekr.20180516071750.334"><vh>get_root_context</vh></v>
<v t="ekr.20180516071750.335"><vh>__repr__</vh></v>
<v t="ekr.20180516071750.336"><vh>execute</vh></v>
<v t="ekr.20180516071750.337"><vh>execute_evaluated</vh></v>
<v t="ekr.20180516071750.338"><vh>api_type</vh></v>
</v>
<v t="ekr.20180516071750.339"><vh>class AbstractTreeName</vh>
<v t="ekr.20180516071750.340"><vh>__init__</vh></v>
<v t="ekr.20180516071750.341"><vh>string_name</vh></v>
<v t="ekr.20180516071750.342"><vh>start_pos</vh></v>
</v>
<v t="ekr.20180516071750.343"><vh>class ContextNameMixin</vh>
<v t="ekr.20180516071750.344"><vh>infer</vh></v>
<v t="ekr.20180516071750.345"><vh>get_root_context</vh></v>
<v t="ekr.20180516071750.346"><vh>api_type</vh></v>
</v>
<v t="ekr.20180516071750.347"><vh>class ContextName</vh>
<v t="ekr.20180516071750.348"><vh>__init__</vh></v>
</v>
<v t="ekr.20180516071750.349"><vh>class TreeNameDefinition</vh>
<v t="ekr.20180516071750.350"><vh>infer</vh></v>
<v t="ekr.20180516071750.351"><vh>api_type</vh></v>
</v>
<v t="ekr.20180516071750.352"><vh>class ParamName</vh>
<v t="ekr.20180516071750.353"><vh>__init__</vh></v>
<v t="ekr.20180516071750.354"><vh>infer</vh></v>
<v t="ekr.20180516071750.355"><vh>get_param</vh></v>
</v>
<v t="ekr.20180516071750.356"><vh>class AnonymousInstanceParamName</vh>
<v t="ekr.20180516071750.357"><vh>infer</vh></v>
</v>
<v t="ekr.20180516071750.358"><vh>class AbstractFilter</vh>
<v t="ekr.20180516071750.359"><vh>_filter</vh></v>
<v t="ekr.20180516071750.360"><vh>get</vh></v>
<v t="ekr.20180516071750.361"><vh>values</vh></v>
</v>
<v t="ekr.20180516071750.362"><vh>class AbstractUsedNamesFilter</vh>
<v t="ekr.20180516071750.363"><vh>__init__</vh></v>
<v t="ekr.20180516071750.364"><vh>get</vh></v>
<v t="ekr.20180516071750.365"><vh>_convert_names</vh></v>
<v t="ekr.20180516071750.366"><vh>values</vh></v>
<v t="ekr.20180516071750.367"><vh>__repr__</vh></v>
</v>
<v t="ekr.20180516071750.368"><vh>class ParserTreeFilter</vh>
<v t="ekr.20180516071750.369"><vh>__init__</vh></v>
<v t="ekr.20180516071750.370"><vh>_filter</vh></v>
<v t="ekr.20180516071750.371"><vh>_is_name_reachable</vh></v>
<v t="ekr.20180516071750.372"><vh>_check_flows</vh></v>
</v>
<v t="ekr.20180516071750.373"><vh>class FunctionExecutionFilter</vh>
<v t="ekr.20180516071750.374"><vh>__init__</vh></v>
<v t="ekr.20180516071750.375"><vh>_convert_names</vh></v>
</v>
<v t="ekr.20180516071750.376"><vh>class AnonymousInstanceFunctionExecutionFilter</vh></v>
<v t="ekr.20180516071750.377"><vh>class GlobalNameFilter</vh>
<v t="ekr.20180516071750.378"><vh>__init__</vh></v>
<v t="ekr.20180516071750.379"><vh>_filter</vh></v>
</v>
<v t="ekr.20180516071750.380"><vh>class DictFilter</vh>
<v t="ekr.20180516071750.381"><vh>__init__</vh></v>
<v t="ekr.20180516071750.382"><vh>get</vh></v>
<v t="ekr.20180516071750.383"><vh>values</vh></v>
<v t="ekr.20180516071750.384"><vh>_convert</vh></v>
</v>
<v t="ekr.20180516071750.385"><vh>get_global_filters</vh></v>
</v>
<v t="ekr.20180516071751.1"><vh>@clean finder.py</vh>
<v t="ekr.20180516071751.2"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.3"><vh>class NameFinder</vh>
<v t="ekr.20180516071751.4"><vh>__init__</vh></v>
<v t="ekr.20180516071751.5"><vh>find</vh></v>
<v t="ekr.20180516071751.6"><vh>_get_origin_scope</vh></v>
<v t="ekr.20180516071751.7"><vh>get_filters</vh></v>
<v t="ekr.20180516071751.8"><vh>filter_name</vh></v>
<v t="ekr.20180516071751.9"><vh>_check_getattr</vh></v>
<v t="ekr.20180516071751.10"><vh>_names_to_types</vh></v>
</v>
<v t="ekr.20180516071751.11"><vh>_name_to_types</vh></v>
<v t="ekr.20180516071751.12"><vh>_apply_decorators</vh></v>
<v t="ekr.20180516071751.13"><vh>_remove_statements</vh></v>
<v t="ekr.20180516071751.14"><vh>_check_flow_information</vh></v>
<v t="ekr.20180516071751.15"><vh>_check_isinstance_type</vh></v>
<v t="ekr.20180516071751.16"><vh>check_tuple_assignments</vh></v>
</v>
<v t="ekr.20180516071751.17"><vh>@clean flow_analysis.py</vh>
<v t="ekr.20180516071751.18"><vh>class Status</vh>
<v t="ekr.20180516071751.19"><vh>__init__</vh></v>
<v t="ekr.20180516071751.20"><vh>invert</vh></v>
<v t="ekr.20180516071751.21"><vh>__and__</vh></v>
<v t="ekr.20180516071751.22"><vh>__repr__</vh></v>
</v>
<v t="ekr.20180516071751.23"><vh>_get_flow_scopes</vh></v>
<v t="ekr.20180516071751.24"><vh>reachability_check</vh></v>
<v t="ekr.20180516071751.25"><vh>_break_check</vh></v>
<v t="ekr.20180516071751.26"><vh>_check_if</vh></v>
</v>
<v t="ekr.20180516071751.27"><vh>@clean helpers.py</vh>
<v t="ekr.20180516071751.28"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.29"><vh>deep_ast_copy</vh></v>
<v t="ekr.20180516071751.30"><vh>evaluate_call_of_leaf</vh></v>
<v t="ekr.20180516071751.31"><vh>call_of_leaf</vh></v>
<v t="ekr.20180516071751.32"><vh>get_names_of_node</vh></v>
<v t="ekr.20180516071751.33"><vh>get_module_names</vh></v>
<v t="ekr.20180516071751.34"><vh>class FakeName</vh>
<v t="ekr.20180516071751.35"><vh>__init__</vh></v>
<v t="ekr.20180516071751.36"><vh>get_definition</vh></v>
<v t="ekr.20180516071751.37"><vh>is_definition</vh></v>
</v>
<v t="ekr.20180516071751.38"><vh>predefine_names</vh></v>
</v>
<v t="ekr.20180516071751.39"><vh>@clean imports.py</vh>
<v t="ekr.20180516071751.40"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.41"><vh>infer_import</vh></v>
<v t="ekr.20180516071751.42"><vh>class NestedImportModule</vh>
<v t="ekr.20180516071751.43"><vh>__init__</vh></v>
<v t="ekr.20180516071751.44"><vh>_get_nested_import_name</vh></v>
<v t="ekr.20180516071751.45"><vh>__getattr__</vh></v>
<v t="ekr.20180516071751.46"><vh>__repr__</vh></v>
</v>
<v t="ekr.20180516071751.47"><vh>_add_error</vh></v>
<v t="ekr.20180516071751.48"><vh>get_init_path</vh></v>
<v t="ekr.20180516071751.49"><vh>class ImportName</vh>
<v t="ekr.20180516071751.50"><vh>__init__</vh></v>
<v t="ekr.20180516071751.51"><vh>infer</vh></v>
<v t="ekr.20180516071751.52"><vh>get_root_context</vh></v>
<v t="ekr.20180516071751.53"><vh>api_type</vh></v>
</v>
<v t="ekr.20180516071751.54"><vh>class SubModuleName</vh>
<v t="ekr.20180516071751.55"><vh>infer</vh></v>
</v>
<v t="ekr.20180516071751.56"><vh>class Importer</vh>
<v t="ekr.20180516071751.57"><vh>__init__</vh></v>
<v t="ekr.20180516071751.58"><vh>str_import_path</vh></v>
<v t="ekr.20180516071751.59"><vh>sys_path_with_modifications</vh></v>
<v t="ekr.20180516071751.60"><vh>follow</vh></v>
<v t="ekr.20180516071751.61"><vh>_do_import</vh></v>
<v t="ekr.20180516071751.62"><vh>_generate_name</vh></v>
<v t="ekr.20180516071751.63"><vh>_get_module_names</vh></v>
<v t="ekr.20180516071751.64"><vh>completion_names</vh></v>
</v>
<v t="ekr.20180516071751.65"><vh>_load_module</vh></v>
<v t="ekr.20180516071751.66"><vh>add_module</vh></v>
<v t="ekr.20180516071751.67"><vh>get_modules_containing_name</vh></v>
</v>
<v t="ekr.20180516071751.68"><vh>@clean instance.py</vh>
<v t="ekr.20180516071751.69"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.70"><vh>class AbstractInstanceContext</vh>
<v t="ekr.20180516071751.71"><vh>__init__</vh></v>
<v t="ekr.20180516071751.72"><vh>is_class</vh></v>
<v t="ekr.20180516071751.73"><vh>py__call__</vh></v>
<v t="ekr.20180516071751.74"><vh>py__class__</vh></v>
<v t="ekr.20180516071751.75"><vh>py__bool__</vh></v>
<v t="ekr.20180516071751.76"><vh>get_function_slot_names</vh></v>
<v t="ekr.20180516071751.77"><vh>execute_function_slots</vh></v>
<v t="ekr.20180516071751.78"><vh>py__get__</vh></v>
<v t="ekr.20180516071751.79"><vh>get_filters</vh></v>
<v t="ekr.20180516071751.80"><vh>py__getitem__</vh></v>
<v t="ekr.20180516071751.81"><vh>py__iter__</vh></v>
<v t="ekr.20180516071751.82"><vh>name</vh></v>
<v t="ekr.20180516071751.83"><vh>_create_init_execution</vh></v>
<v t="ekr.20180516071751.84"><vh>create_init_executions</vh></v>
<v t="ekr.20180516071751.85"><vh>create_instance_context</vh></v>
<v t="ekr.20180516071751.86"><vh>__repr__</vh></v>
</v>
<v t="ekr.20180516071751.87"><vh>class CompiledInstance</vh>
<v t="ekr.20180516071751.88"><vh>__init__</vh></v>
<v t="ekr.20180516071751.89"><vh>name</vh></v>
<v t="ekr.20180516071751.90"><vh>create_instance_context</vh></v>
</v>
<v t="ekr.20180516071751.91"><vh>class TreeInstance</vh>
<v t="ekr.20180516071751.92"><vh>name</vh></v>
</v>
<v t="ekr.20180516071751.93"><vh>class AnonymousInstance</vh>
<v t="ekr.20180516071751.94"><vh>__init__</vh></v>
</v>
<v t="ekr.20180516071751.95"><vh>class CompiledInstanceName</vh>
<v t="ekr.20180516071751.96"><vh>__init__</vh></v>
<v t="ekr.20180516071751.97"><vh>infer</vh></v>
</v>
<v t="ekr.20180516071751.98"><vh>class CompiledInstanceClassFilter</vh>
<v t="ekr.20180516071751.99"><vh>__init__</vh></v>
<v t="ekr.20180516071751.100"><vh>_create_name</vh></v>
</v>
<v t="ekr.20180516071751.101"><vh>class BoundMethod</vh>
<v t="ekr.20180516071751.102"><vh>__init__</vh></v>
<v t="ekr.20180516071751.103"><vh>get_function_execution</vh></v>
</v>
<v t="ekr.20180516071751.104"><vh>class CompiledBoundMethod</vh>
<v t="ekr.20180516071751.105"><vh>__init__</vh></v>
<v t="ekr.20180516071751.106"><vh>get_param_names</vh></v>
</v>
<v t="ekr.20180516071751.107"><vh>class InstanceNameDefinition</vh>
<v t="ekr.20180516071751.108"><vh>infer</vh></v>
</v>
<v t="ekr.20180516071751.109"><vh>class LazyInstanceName</vh>
<v t="ekr.20180516071751.110"><vh>__init__</vh></v>
<v t="ekr.20180516071751.111"><vh>parent_context</vh></v>
</v>
<v t="ekr.20180516071751.112"><vh>class LazyInstanceClassName</vh>
<v t="ekr.20180516071751.113"><vh>infer</vh></v>
</v>
<v t="ekr.20180516071751.114"><vh>class InstanceClassFilter</vh>
<v t="ekr.20180516071751.115"><vh>__init__</vh></v>
<v t="ekr.20180516071751.116"><vh>_equals_origin_scope</vh></v>
<v t="ekr.20180516071751.117"><vh>_access_possible</vh></v>
<v t="ekr.20180516071751.118"><vh>_filter</vh></v>
<v t="ekr.20180516071751.119"><vh>_convert_names</vh></v>
</v>
<v t="ekr.20180516071751.120"><vh>class SelfNameFilter</vh>
<v t="ekr.20180516071751.121"><vh>_filter</vh></v>
<v t="ekr.20180516071751.122"><vh>_filter_self_names</vh></v>
<v t="ekr.20180516071751.123"><vh>_check_flows</vh></v>
</v>
<v t="ekr.20180516071751.124"><vh>class ParamArguments</vh>
<v t="ekr.20180516071751.125"><vh>class LazyParamContext</vh>
<v t="ekr.20180516071751.126"><vh>__init__</vh></v>
<v t="ekr.20180516071751.127"><vh>infer</vh></v>
</v>
<v t="ekr.20180516071751.128"><vh>__init__</vh></v>
<v t="ekr.20180516071751.129"><vh>unpack</vh></v>
</v>
<v t="ekr.20180516071751.130"><vh>class InstanceVarArgs</vh>
<v t="ekr.20180516071751.131"><vh>__init__</vh></v>
<v t="ekr.20180516071751.132"><vh>_get_var_args</vh></v>
<v t="ekr.20180516071751.133"><vh>unpack</vh></v>
<v t="ekr.20180516071751.134"><vh>get_calling_nodes</vh></v>
<v t="ekr.20180516071751.135"><vh>__getattr__</vh></v>
</v>
<v t="ekr.20180516071751.136"><vh>class InstanceFunctionExecution</vh>
<v t="ekr.20180516071751.137"><vh>__init__</vh></v>
</v>
<v t="ekr.20180516071751.138"><vh>class AnonymousInstanceFunctionExecution</vh>
<v t="ekr.20180516071751.139"><vh>__init__</vh></v>
</v>
</v>
<v t="ekr.20180516071751.140"><vh>@clean iterable.py</vh>
<v t="ekr.20180516071751.141"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.142"><vh>class AbstractSequence</vh>
<v t="ekr.20180516071751.143"><vh>__init__</vh></v>
<v t="ekr.20180516071751.144"><vh>get_filters</vh></v>
<v t="ekr.20180516071751.145"><vh>name</vh></v>
</v>
<v t="ekr.20180516071751.146"><vh>class BuiltinMethod</vh>
<v t="ekr.20180516071751.147"><vh>__init__</vh></v>
<v t="ekr.20180516071751.148"><vh>py__call__</vh></v>
<v t="ekr.20180516071751.149"><vh>__getattr__</vh></v>
</v>
<v t="ekr.20180516071751.150"><vh>class SpecialMethodFilter</vh>
<v t="ekr.20180516071751.151"><vh>class SpecialMethodName</vh>
<v t="ekr.20180516071751.152"><vh>__init__</vh></v>
<v t="ekr.20180516071751.153"><vh>infer</vh></v>
</v>
<v t="ekr.20180516071751.154"><vh>__init__</vh></v>
<v t="ekr.20180516071751.155"><vh>_convert</vh></v>
</v>
<v t="ekr.20180516071751.156"><vh>has_builtin_methods</vh></v>
<v t="ekr.20180516071751.157"><vh>register_builtin_method</vh></v>
<v t="ekr.20180516071751.158"><vh>class GeneratorMixin</vh>
<v t="ekr.20180516071751.159"><vh>py__next__</vh></v>
<v t="ekr.20180516071751.160"><vh>get_filters</vh></v>
<v t="ekr.20180516071751.161"><vh>py__bool__</vh></v>
<v t="ekr.20180516071751.162"><vh>py__class__</vh></v>
<v t="ekr.20180516071751.163"><vh>name</vh></v>
</v>
<v t="ekr.20180516071751.164"><vh>class Generator</vh>
<v t="ekr.20180516071751.165"><vh>__init__</vh></v>
<v t="ekr.20180516071751.166"><vh>py__iter__</vh></v>
<v t="ekr.20180516071751.167"><vh>__repr__</vh></v>
</v>
<v t="ekr.20180516071751.168"><vh>class CompForContext</vh>
<v t="ekr.20180516071751.169"><vh>from_comp_for</vh></v>
<v t="ekr.20180516071751.170"><vh>__init__</vh></v>
<v t="ekr.20180516071751.171"><vh>get_node</vh></v>
<v t="ekr.20180516071751.172"><vh>get_filters</vh></v>
</v>
<v t="ekr.20180516071751.173"><vh>class Comprehension</vh>
<v t="ekr.20180516071751.174"><vh>from_atom</vh></v>
<v t="ekr.20180516071751.175"><vh>__init__</vh></v>
<v t="ekr.20180516071751.176"><vh>_get_comprehension</vh></v>
<v t="ekr.20180516071751.177"><vh>_get_comp_for</vh></v>
<v t="ekr.20180516071751.178"><vh>_eval_node</vh></v>
<v t="ekr.20180516071751.179"><vh>_get_comp_for_context</vh></v>
<v t="ekr.20180516071751.180"><vh>_nested</vh></v>
<v t="ekr.20180516071751.181"><vh>_iterate</vh></v>
<v t="ekr.20180516071751.182"><vh>py__iter__</vh></v>
<v t="ekr.20180516071751.183"><vh>__repr__</vh></v>
</v>
<v t="ekr.20180516071751.184"><vh>class ArrayMixin</vh>
<v t="ekr.20180516071751.185"><vh>get_filters</vh></v>
<v t="ekr.20180516071751.186"><vh>py__bool__</vh></v>
<v t="ekr.20180516071751.187"><vh>py__class__</vh></v>
<v t="ekr.20180516071751.188"><vh>parent</vh></v>
<v t="ekr.20180516071751.189"><vh>dict_values</vh></v>
</v>
<v t="ekr.20180516071751.190"><vh>class ListComprehension</vh>
<v t="ekr.20180516071751.191"><vh>py__getitem__</vh></v>
</v>
<v t="ekr.20180516071751.192"><vh>class SetComprehension</vh></v>
<v t="ekr.20180516071751.193"><vh>class DictComprehension</vh>
<v t="ekr.20180516071751.194"><vh>_get_comp_for</vh></v>
<v t="ekr.20180516071751.195"><vh>py__iter__</vh></v>
<v t="ekr.20180516071751.196"><vh>py__getitem__</vh></v>
<v t="ekr.20180516071751.197"><vh>dict_values</vh></v>
<v t="ekr.20180516071751.198"><vh>_imitate_values</vh></v>
<v t="ekr.20180516071751.199"><vh>_imitate_items</vh></v>
</v>
<v t="ekr.20180516071751.200"><vh>class GeneratorComprehension</vh></v>
<v t="ekr.20180516071751.201"><vh>class SequenceLiteralContext</vh>
<v t="ekr.20180516071751.202"><vh>__init__</vh></v>
<v t="ekr.20180516071751.203"><vh>py__getitem__</vh></v>
<v t="ekr.20180516071751.204"><vh>py__iter__</vh></v>
<v t="ekr.20180516071751.205"><vh>_values</vh></v>
<v t="ekr.20180516071751.206"><vh>_items</vh></v>
<v t="ekr.20180516071751.207"><vh>exact_key_items</vh></v>
<v t="ekr.20180516071751.208"><vh>__repr__</vh></v>
</v>
<v t="ekr.20180516071751.209"><vh>class DictLiteralContext</vh>
<v t="ekr.20180516071751.210"><vh>__init__</vh></v>
<v t="ekr.20180516071751.211"><vh>_imitate_values</vh></v>
<v t="ekr.20180516071751.212"><vh>_imitate_items</vh></v>
</v>
<v t="ekr.20180516071751.213"><vh>class _FakeArray</vh>
<v t="ekr.20180516071751.214"><vh>__init__</vh></v>
</v>
<v t="ekr.20180516071751.215"><vh>class FakeSequence</vh>
<v t="ekr.20180516071751.216"><vh>__init__</vh></v>
<v t="ekr.20180516071751.217"><vh>_items</vh></v>
<v t="ekr.20180516071751.218"><vh>py__getitem__</vh></v>
<v t="ekr.20180516071751.219"><vh>py__iter__</vh></v>
<v t="ekr.20180516071751.220"><vh>__repr__</vh></v>
</v>
<v t="ekr.20180516071751.221"><vh>class FakeDict</vh>
<v t="ekr.20180516071751.222"><vh>__init__</vh></v>
<v t="ekr.20180516071751.223"><vh>py__iter__</vh></v>
<v t="ekr.20180516071751.224"><vh>py__getitem__</vh></v>
<v t="ekr.20180516071751.225"><vh>dict_values</vh></v>
<v t="ekr.20180516071751.226"><vh>_items</vh></v>
<v t="ekr.20180516071751.227"><vh>exact_key_items</vh></v>
</v>
<v t="ekr.20180516071751.228"><vh>class MergedArray</vh>
<v t="ekr.20180516071751.229"><vh>__init__</vh></v>
<v t="ekr.20180516071751.230"><vh>py__iter__</vh></v>
<v t="ekr.20180516071751.231"><vh>py__getitem__</vh></v>
<v t="ekr.20180516071751.232"><vh>_items</vh></v>
<v t="ekr.20180516071751.233"><vh>__len__</vh></v>
</v>
<v t="ekr.20180516071751.234"><vh>unpack_tuple_to_dict</vh></v>
<v t="ekr.20180516071751.235"><vh>py__iter__</vh></v>
<v t="ekr.20180516071751.236"><vh>py__iter__types</vh></v>
<v t="ekr.20180516071751.237"><vh>py__getitem__</vh></v>
<v t="ekr.20180516071751.238"><vh>check_array_additions</vh></v>
<v t="ekr.20180516071751.239"><vh>_check_array_additions</vh></v>
<v t="ekr.20180516071751.240"><vh>get_dynamic_array_instance</vh></v>
<v t="ekr.20180516071751.241"><vh>class _ArrayInstance</vh>
<v t="ekr.20180516071751.242"><vh>__init__</vh></v>
<v t="ekr.20180516071751.243"><vh>py__iter__</vh></v>
</v>
<v t="ekr.20180516071751.244"><vh>class Slice</vh>
<v t="ekr.20180516071751.245"><vh>__init__</vh></v>
<v t="ekr.20180516071751.246"><vh>obj</vh></v>
</v>
<v t="ekr.20180516071751.247"><vh>create_index_types</vh></v>
</v>
<v t="ekr.20180516071751.248"><vh>@clean jedi_typing.py</vh>
<v t="ekr.20180516071751.249"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.250"><vh>factory</vh></v>
</v>
<v t="ekr.20180516071751.251"><vh>@clean param.py</vh>
<v t="ekr.20180516071751.252"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.253"><vh>add_argument_issue</vh></v>
<v t="ekr.20180516071751.254"><vh>try_iter_content</vh></v>
<v t="ekr.20180516071751.255"><vh>class AbstractArguments</vh>
<v t="ekr.20180516071751.256"><vh>eval_argument_clinic</vh></v>
<v t="ekr.20180516071751.257"><vh>eval_all</vh></v>
</v>
<v t="ekr.20180516071751.258"><vh>class TreeArguments</vh>
<v t="ekr.20180516071751.259"><vh>__init__</vh></v>
<v t="ekr.20180516071751.260"><vh>_split</vh></v>
<v t="ekr.20180516071751.261"><vh>unpack</vh></v>
<v t="ekr.20180516071751.262"><vh>as_tree_tuple_objects</vh></v>
<v t="ekr.20180516071751.263"><vh>__repr__</vh></v>
<v t="ekr.20180516071751.264"><vh>get_calling_nodes</vh></v>
</v>
<v t="ekr.20180516071751.265"><vh>class ValuesArguments</vh>
<v t="ekr.20180516071751.266"><vh>__init__</vh></v>
<v t="ekr.20180516071751.267"><vh>unpack</vh></v>
<v t="ekr.20180516071751.268"><vh>get_calling_nodes</vh></v>
<v t="ekr.20180516071751.269"><vh>__repr__</vh></v>
</v>
<v t="ekr.20180516071751.270"><vh>class ExecutedParam</vh>
<v t="ekr.20180516071751.271"><vh>__init__</vh></v>
<v t="ekr.20180516071751.272"><vh>infer</vh></v>
<v t="ekr.20180516071751.273"><vh>position_nr</vh></v>
<v t="ekr.20180516071751.274"><vh>__repr__</vh></v>
</v>
<v t="ekr.20180516071751.275"><vh>get_params</vh></v>
<v t="ekr.20180516071751.276"><vh>_iterate_star_args</vh></v>
<v t="ekr.20180516071751.277"><vh>_star_star_dict</vh></v>
<v t="ekr.20180516071751.278"><vh>_error_argument_count</vh></v>
<v t="ekr.20180516071751.279"><vh>create_default_param</vh></v>
</v>
<v t="ekr.20180516071751.280"><vh>@clean pep0484.py</vh>
<v t="ekr.20180516071751.281"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.282"><vh>_evaluate_for_annotation</vh></v>
<v t="ekr.20180516071751.283"><vh>_fix_forward_reference</vh></v>
<v t="ekr.20180516071751.284"><vh>follow_param</vh></v>
<v t="ekr.20180516071751.285"><vh>py__annotations__</vh></v>
<v t="ekr.20180516071751.286"><vh>find_return_types</vh></v>
<v t="ekr.20180516071751.287"><vh>_get_typing_replacement_module</vh></v>
<v t="ekr.20180516071751.288"><vh>py__getitem__</vh></v>
<v t="ekr.20180516071751.289"><vh>find_type_from_comment_hint_for</vh></v>
<v t="ekr.20180516071751.290"><vh>find_type_from_comment_hint_with</vh></v>
<v t="ekr.20180516071751.291"><vh>find_type_from_comment_hint_assign</vh></v>
<v t="ekr.20180516071751.292"><vh>_find_type_from_comment_hint</vh></v>
</v>
<v t="ekr.20180516071751.293"><vh>@clean precedence.py</vh>
<v t="ekr.20180516071751.294"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.295"><vh>literals_to_types</vh></v>
<v t="ekr.20180516071751.296"><vh>calculate_children</vh></v>
<v t="ekr.20180516071751.297"><vh>calculate</vh></v>
<v t="ekr.20180516071751.298"><vh>factor_calculate</vh></v>
<v t="ekr.20180516071751.299"><vh>_is_number</vh></v>
<v t="ekr.20180516071751.300"><vh>is_string</vh></v>
<v t="ekr.20180516071751.301"><vh>is_literal</vh></v>
<v t="ekr.20180516071751.302"><vh>_is_tuple</vh></v>
<v t="ekr.20180516071751.303"><vh>_is_list</vh></v>
<v t="ekr.20180516071751.304"><vh>_element_calculate</vh></v>
</v>
<v t="ekr.20180516071751.305"><vh>@clean recursion.py</vh>
<v t="ekr.20180516071751.306"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.307"><vh>class RecursionDetector</vh>
<v t="ekr.20180516071751.308"><vh>__init__</vh></v>
</v>
<v t="ekr.20180516071751.309"><vh>execution_allowed</vh></v>
<v t="ekr.20180516071751.310"><vh>execution_recursion_decorator</vh></v>
<v t="ekr.20180516071751.311"><vh>class ExecutionRecursionDetector</vh>
<v t="ekr.20180516071751.312"><vh>__init__</vh></v>
<v t="ekr.20180516071751.313"><vh>__call__</vh></v>
<v t="ekr.20180516071751.314"><vh>pop_execution</vh></v>
<v t="ekr.20180516071751.315"><vh>push_execution</vh></v>
</v>
</v>
<v t="ekr.20180516071751.316"><vh>@clean representation.py</vh>
<v t="ekr.20180516071751.317"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.318"><vh>apply_py__get__</vh></v>
<v t="ekr.20180516071751.319"><vh>class ClassName</vh>
<v t="ekr.20180516071751.320"><vh>__init__</vh></v>
<v t="ekr.20180516071751.321"><vh>infer</vh></v>
</v>
<v t="ekr.20180516071751.322"><vh>class ClassFilter</vh>
<v t="ekr.20180516071751.323"><vh>_convert_names</vh></v>
</v>
<v t="ekr.20180516071751.324"><vh>class ClassContext</vh>
<v t="ekr.20180516071751.325"><vh>__init__</vh></v>
<v t="ekr.20180516071751.326"><vh>py__mro__</vh></v>
<v t="ekr.20180516071751.327"><vh>py__bases__</vh></v>
<v t="ekr.20180516071751.328"><vh>py__call__</vh></v>
<v t="ekr.20180516071751.329"><vh>py__class__</vh></v>
<v t="ekr.20180516071751.330"><vh>get_params</vh></v>
<v t="ekr.20180516071751.331"><vh>get_filters</vh></v>
<v t="ekr.20180516071751.332"><vh>is_class</vh></v>
<v t="ekr.20180516071751.333"><vh>get_subscope_by_name</vh></v>
<v t="ekr.20180516071751.334"><vh>get_function_slot_names</vh></v>
<v t="ekr.20180516071751.335"><vh>get_param_names</vh></v>
<v t="ekr.20180516071751.336"><vh>name</vh></v>
</v>
<v t="ekr.20180516071751.337"><vh>class FunctionContext</vh>
<v t="ekr.20180516071751.338"><vh>__init__</vh></v>
<v t="ekr.20180516071751.339"><vh>get_filters</vh></v>
<v t="ekr.20180516071751.340"><vh>infer_function_execution</vh></v>
<v t="ekr.20180516071751.341"><vh>get_function_execution</vh></v>
<v t="ekr.20180516071751.342"><vh>py__call__</vh></v>
<v t="ekr.20180516071751.343"><vh>py__class__</vh></v>
<v t="ekr.20180516071751.344"><vh>name</vh></v>
<v t="ekr.20180516071751.345"><vh>get_param_names</vh></v>
</v>
<v t="ekr.20180516071751.346"><vh>class FunctionExecutionContext</vh>
<v t="ekr.20180516071751.347"><vh>__init__</vh></v>
<v t="ekr.20180516071751.348"><vh>get_return_values</vh></v>
<v t="ekr.20180516071751.349"><vh>_eval_yield</vh></v>
<v t="ekr.20180516071751.350"><vh>get_yield_values</vh></v>
<v t="ekr.20180516071751.351"><vh>get_filters</vh></v>
<v t="ekr.20180516071751.352"><vh>get_params</vh></v>
</v>
<v t="ekr.20180516071751.353"><vh>class AnonymousFunctionExecution</vh>
<v t="ekr.20180516071751.354"><vh>__init__</vh></v>
<v t="ekr.20180516071751.355"><vh>get_params</vh></v>
</v>
<v t="ekr.20180516071751.356"><vh>class ModuleAttributeName</vh>
<v t="ekr.20180516071751.357"><vh>__init__</vh></v>
<v t="ekr.20180516071751.358"><vh>infer</vh></v>
</v>
<v t="ekr.20180516071751.359"><vh>class ModuleName</vh>
<v t="ekr.20180516071751.360"><vh>__init__</vh></v>
<v t="ekr.20180516071751.361"><vh>string_name</vh></v>
</v>
<v t="ekr.20180516071751.362"><vh>class ModuleContext</vh>
<v t="ekr.20180516071751.363"><vh>__init__</vh></v>
<v t="ekr.20180516071751.364"><vh>get_filters</vh></v>
<v t="ekr.20180516071751.365"><vh>star_imports</vh></v>
<v t="ekr.20180516071751.366"><vh>_module_attributes_dict</vh></v>
<v t="ekr.20180516071751.367"><vh>_string_name</vh></v>
<v t="ekr.20180516071751.368"><vh>name</vh></v>
<v t="ekr.20180516071751.369"><vh>_get_init_directory</vh></v>
<v t="ekr.20180516071751.370"><vh>py__name__</vh></v>
<v t="ekr.20180516071751.371"><vh>py__file__</vh></v>
<v t="ekr.20180516071751.372"><vh>py__package__</vh></v>
<v t="ekr.20180516071751.373"><vh>_py__path__</vh></v>
<v t="ekr.20180516071751.374"><vh>py__path__</vh></v>
<v t="ekr.20180516071751.375"><vh>_sub_modules_dict</vh></v>
<v t="ekr.20180516071751.376"><vh>py__class__</vh></v>
<v t="ekr.20180516071751.377"><vh>__repr__</vh></v>
</v>
<v t="ekr.20180516071751.378"><vh>class ImplicitNSName</vh>
<v t="ekr.20180516071751.379"><vh>__init__</vh></v>
<v t="ekr.20180516071751.380"><vh>infer</vh></v>
<v t="ekr.20180516071751.381"><vh>get_root_context</vh></v>
</v>
<v t="ekr.20180516071751.382"><vh>class ImplicitNamespaceContext</vh>
<v t="ekr.20180516071751.383"><vh>__init__</vh></v>
<v t="ekr.20180516071751.384"><vh>get_filters</vh></v>
<v t="ekr.20180516071751.385"><vh>name</vh></v>
<v t="ekr.20180516071751.386"><vh>py__file__</vh></v>
<v t="ekr.20180516071751.387"><vh>py__package__</vh></v>
<v t="ekr.20180516071751.388"><vh>py__path__</vh></v>
<v t="ekr.20180516071751.389"><vh>_sub_modules_dict</vh></v>
</v>
</v>
<v t="ekr.20180516071751.390"><vh>@clean site.py</vh>
<v t="ekr.20180516071751.391"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.392"><vh>makepath</vh></v>
<v t="ekr.20180516071751.393"><vh>_init_pathinfo</vh></v>
<v t="ekr.20180516071751.394"><vh>addpackage</vh></v>
<v t="ekr.20180516071751.395"><vh>addsitedir</vh></v>
</v>
<v t="ekr.20180516071751.396"><vh>@clean stdlib.py</vh>
<v t="ekr.20180516071751.397"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.398"><vh>class NotInStdLib</vh></v>
<v t="ekr.20180516071751.399"><vh>execute</vh></v>
<v t="ekr.20180516071751.400"><vh>_follow_param</vh></v>
<v t="ekr.20180516071751.401"><vh>argument_clinic</vh></v>
<v t="ekr.20180516071751.402"><vh>builtins_next</vh></v>
<v t="ekr.20180516071751.403"><vh>builtins_getattr</vh></v>
<v t="ekr.20180516071751.404"><vh>builtins_type</vh></v>
<v t="ekr.20180516071751.405"><vh>class SuperInstance</vh>
<v t="ekr.20180516071751.406"><vh>__init__</vh></v>
</v>
<v t="ekr.20180516071751.407"><vh>builtins_super</vh></v>
<v t="ekr.20180516071751.408"><vh>builtins_reversed</vh></v>
<v t="ekr.20180516071751.409"><vh>builtins_isinstance</vh></v>
<v t="ekr.20180516071751.410"><vh>collections_namedtuple</vh></v>
<v t="ekr.20180516071751.411"><vh>_return_first_param</vh></v>
</v>
<v t="ekr.20180516071751.412"><vh>@clean sys_path.py</vh>
<v t="ekr.20180516071751.413"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.414"><vh>get_venv_path</vh></v>
<v t="ekr.20180516071751.415"><vh>_get_sys_path_with_egglinks</vh></v>
<v t="ekr.20180516071751.416"><vh>_get_venv_path_dirs</vh></v>
<v t="ekr.20180516071751.417"><vh>_get_venv_sitepackages</vh></v>
<v t="ekr.20180516071751.418"><vh>_execute_code</vh></v>
<v t="ekr.20180516071751.419"><vh>_paths_from_assignment</vh></v>
<v t="ekr.20180516071751.420"><vh>_paths_from_list_modifications</vh></v>
<v t="ekr.20180516071751.421"><vh>_check_module</vh></v>
<v t="ekr.20180516071751.422"><vh>sys_path_with_modifications</vh></v>
<v t="ekr.20180516071751.423"><vh>_get_paths_from_buildout_script</vh></v>
<v t="ekr.20180516071751.424"><vh>traverse_parents</vh></v>
<v t="ekr.20180516071751.425"><vh>_get_parent_dir_with_file</vh></v>
<v t="ekr.20180516071751.426"><vh>_detect_django_path</vh></v>
<v t="ekr.20180516071751.427"><vh>_get_buildout_script_paths</vh></v>
</v>
<v t="ekr.20180516071751.428"><vh>@clean __init__.py</vh>
<v t="ekr.20180516071751.429"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.430"><vh>class Evaluator</vh>
<v t="ekr.20180516071751.431"><vh>__init__</vh></v>
<v t="ekr.20180516071751.432"><vh>reset_recursion_limitations</vh></v>
<v t="ekr.20180516071751.433"><vh>find_types</vh></v>
<v t="ekr.20180516071751.434"><vh>eval_statement</vh></v>
<v t="ekr.20180516071751.435"><vh>_eval_stmt</vh></v>
<v t="ekr.20180516071751.436"><vh>eval_element</vh></v>
<v t="ekr.20180516071751.437"><vh>_eval_element_if_evaluated</vh></v>
<v t="ekr.20180516071751.438"><vh>_eval_element_cached</vh></v>
<v t="ekr.20180516071751.439"><vh>_eval_element_not_cached</vh></v>
<v t="ekr.20180516071751.440"><vh>eval_atom</vh></v>
<v t="ekr.20180516071751.441"><vh>eval_trailer</vh></v>
<v t="ekr.20180516071751.442"><vh>execute</vh></v>
<v t="ekr.20180516071751.443"><vh>goto_definitions</vh></v>
<v t="ekr.20180516071751.444"><vh>goto</vh></v>
<v t="ekr.20180516071751.445"><vh>create_context</vh></v>
</v>
</v>
<v t="ekr.20180516071751.447"><vh>@path compiled</vh>
<v t="ekr.20180516071751.448"><vh>@clean fake.py</vh>
<v t="ekr.20180516071751.449"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.450"><vh>class FakeDoesNotExist</vh></v>
<v t="ekr.20180516071751.451"><vh>_load_faked_module</vh></v>
<v t="ekr.20180516071751.452"><vh>_search_scope</vh></v>
<v t="ekr.20180516071751.453"><vh>get_module</vh></v>
<v t="ekr.20180516071751.454"><vh>_faked</vh></v>
<v t="ekr.20180516071751.455"><vh>memoize_faked</vh></v>
<v t="ekr.20180516071751.456"><vh>_get_faked</vh></v>
<v t="ekr.20180516071751.457"><vh>get_faked</vh></v>
<v t="ekr.20180516071751.458"><vh>is_class_instance</vh></v>
</v>
<v t="ekr.20180516071751.459"><vh>@clean mixed.py</vh>
<v t="ekr.20180516071751.460"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.461"><vh>class MixedObject</vh>
<v t="ekr.20180516071751.462"><vh>__init__</vh></v>
<v t="ekr.20180516071751.463"><vh>eval_trailer</vh></v>
<v t="ekr.20180516071751.464"><vh>py__getattribute__</vh></v>
<v t="ekr.20180516071751.465"><vh>get_filters</vh></v>
<v t="ekr.20180516071751.466"><vh>__repr__</vh></v>
<v t="ekr.20180516071751.467"><vh>__getattr__</vh></v>
</v>
<v t="ekr.20180516071751.468"><vh>class MixedName</vh>
<v t="ekr.20180516071751.469"><vh>start_pos</vh></v>
<v t="ekr.20180516071751.470"><vh>start_pos</vh></v>
<v t="ekr.20180516071751.471"><vh>infer</vh></v>
<v t="ekr.20180516071751.472"><vh>api_type</vh></v>
</v>
<v t="ekr.20180516071751.473"><vh>class MixedObjectFilter</vh>
<v t="ekr.20180516071751.474"><vh>__init__</vh></v>
</v>
<v t="ekr.20180516071751.475"><vh>_load_module</vh></v>
<v t="ekr.20180516071751.476"><vh>find_syntax_node_name</vh></v>
<v t="ekr.20180516071751.477"><vh>create</vh></v>
</v>
<v t="ekr.20180516071751.478"><vh>@clean __init__.py</vh>
<v t="ekr.20180516071751.479"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.480"><vh>class CheckAttribute</vh>
<v t="ekr.20180516071751.481"><vh>__init__</vh></v>
<v t="ekr.20180516071751.482"><vh>__get__</vh></v>
</v>
<v t="ekr.20180516071751.483"><vh>class CompiledObject</vh>
<v t="ekr.20180516071751.484"><vh>__init__</vh></v>
<v t="ekr.20180516071751.485"><vh>get_root_node</vh></v>
<v t="ekr.20180516071751.486"><vh>py__call__</vh></v>
<v t="ekr.20180516071751.487"><vh>py__class__</vh></v>
<v t="ekr.20180516071751.488"><vh>py__mro__</vh></v>
<v t="ekr.20180516071751.489"><vh>py__bases__</vh></v>
<v t="ekr.20180516071751.490"><vh>py__bool__</vh></v>
<v t="ekr.20180516071751.491"><vh>py__file__</vh></v>
<v t="ekr.20180516071751.492"><vh>is_class</vh></v>
<v t="ekr.20180516071751.493"><vh>doc</vh></v>
<v t="ekr.20180516071751.494"><vh>get_params</vh></v>
<v t="ekr.20180516071751.495"><vh>get_param_names</vh></v>
<v t="ekr.20180516071751.496"><vh>__repr__</vh></v>
<v t="ekr.20180516071751.497"><vh>_parse_function_doc</vh></v>
<v t="ekr.20180516071751.498"><vh>api_type</vh></v>
<v t="ekr.20180516071751.499"><vh>type</vh></v>
<v t="ekr.20180516071751.500"><vh>_cls</vh></v>
<v t="ekr.20180516071751.501"><vh>_get_class</vh></v>
<v t="ekr.20180516071751.502"><vh>get_filters</vh></v>
<v t="ekr.20180516071751.503"><vh>_ensure_one_filter</vh></v>
<v t="ekr.20180516071751.504"><vh>get_subscope_by_name</vh></v>
<v t="ekr.20180516071751.505"><vh>py__getitem__</vh></v>
<v t="ekr.20180516071751.506"><vh>py__iter__</vh></v>
<v t="ekr.20180516071751.507"><vh>py__name__</vh></v>
<v t="ekr.20180516071751.508"><vh>name</vh></v>
<v t="ekr.20180516071751.509"><vh>_execute_function</vh></v>
<v t="ekr.20180516071751.510"><vh>is_scope</vh></v>
<v t="ekr.20180516071751.511"><vh>get_self_attributes</vh></v>
<v t="ekr.20180516071751.512"><vh>get_imports</vh></v>
</v>
<v t="ekr.20180516071751.513"><vh>class CompiledName</vh>
<v t="ekr.20180516071751.514"><vh>__init__</vh></v>
<v t="ekr.20180516071751.515"><vh>__repr__</vh></v>
<v t="ekr.20180516071751.516"><vh>api_type</vh></v>
<v t="ekr.20180516071751.517"><vh>infer</vh></v>
</v>
<v t="ekr.20180516071751.518"><vh>class UnresolvableParamName</vh>
<v t="ekr.20180516071751.519"><vh>__init__</vh></v>
<v t="ekr.20180516071751.520"><vh>infer</vh></v>
</v>
<v t="ekr.20180516071751.521"><vh>class CompiledContextName</vh>
<v t="ekr.20180516071751.522"><vh>__init__</vh></v>
</v>
<v t="ekr.20180516071751.523"><vh>class EmptyCompiledName</vh>
<v t="ekr.20180516071751.524"><vh>__init__</vh></v>
<v t="ekr.20180516071751.525"><vh>infer</vh></v>
</v>
<v t="ekr.20180516071751.526"><vh>class CompiledObjectFilter</vh>
<v t="ekr.20180516071751.527"><vh>__init__</vh></v>
<v t="ekr.20180516071751.528"><vh>get</vh></v>
<v t="ekr.20180516071751.529"><vh>values</vh></v>
<v t="ekr.20180516071751.530"><vh>_create_name</vh></v>
</v>
<v t="ekr.20180516071751.531"><vh>dotted_from_fs_path</vh></v>
<v t="ekr.20180516071751.532"><vh>load_module</vh></v>
<v t="ekr.20180516071751.533"><vh>_parse_function_doc</vh></v>
<v t="ekr.20180516071751.534"><vh>_create_from_name</vh></v>
<v t="ekr.20180516071751.535"><vh>builtin_from_name</vh></v>
<v t="ekr.20180516071751.536"><vh>_a_generator</vh></v>
<v t="ekr.20180516071751.537"><vh>get_special_object</vh></v>
<v t="ekr.20180516071751.538"><vh>compiled_objects_cache</vh></v>
<v t="ekr.20180516071751.539"><vh>create</vh></v>
</v>
</v>
</v>
<v t="ekr.20180516071751.542"><vh>@path parser</vh>
<v t="ekr.20180516071751.543"><vh>@clean cache.py</vh>
<v t="ekr.20180516071751.544"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.545"><vh>class _NodeCacheItem</vh>
<v t="ekr.20180516071751.546"><vh>__init__</vh></v>
</v>
<v t="ekr.20180516071751.547"><vh>load_module</vh></v>
<v t="ekr.20180516071751.548"><vh>_load_from_file_system</vh></v>
<v t="ekr.20180516071751.549"><vh>save_module</vh></v>
<v t="ekr.20180516071751.550"><vh>_save_to_file_system</vh></v>
<v t="ekr.20180516071751.551"><vh>remove_old_modules</vh></v>
<v t="ekr.20180516071751.552"><vh>clear_cache</vh></v>
<v t="ekr.20180516071751.553"><vh>_get_hashed_path</vh></v>
<v t="ekr.20180516071751.554"><vh>_get_cache_directory_path</vh></v>
</v>
<v t="ekr.20180516071751.555"><vh>@clean parser.py</vh>
<v t="ekr.20180516071751.556"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.557"><vh>class ParserSyntaxError</vh>
<v t="ekr.20180516071751.558"><vh>__init__</vh></v>
</v>
<v t="ekr.20180516071751.559"><vh>class BaseParser</vh>
<v t="ekr.20180516071751.560"><vh>__init__</vh></v>
<v t="ekr.20180516071751.561"><vh>parse</vh></v>
<v t="ekr.20180516071751.562"><vh>error_recovery</vh></v>
<v t="ekr.20180516071751.563"><vh>convert_node</vh></v>
<v t="ekr.20180516071751.564"><vh>convert_leaf</vh></v>
</v>
</v>
<v t="ekr.20180516071751.565"><vh>@@clean token.py</vh>
<v t="ekr.20180516071751.566"><vh>Declarations</vh></v>
</v>
<v t="ekr.20180516071751.567"><vh>@clean tokenize.py</vh>
<v t="ekr.20180516071751.568"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.569"><vh>group</vh></v>
<v t="ekr.20180516071751.570"><vh>any</vh></v>
<v t="ekr.20180516071751.571"><vh>maybe</vh></v>
<v t="ekr.20180516071751.572"><vh>_all_string_prefixes</vh></v>
<v t="ekr.20180516071751.573"><vh>_compile</vh></v>
<v t="ekr.20180516071751.574"><vh>class TokenInfo</vh>
<v t="ekr.20180516071751.575"><vh>__repr__</vh></v>
<v t="ekr.20180516071751.576"><vh>get_type_name</vh></v>
<v t="ekr.20180516071751.577"><vh>exact_type</vh></v>
<v t="ekr.20180516071751.578"><vh>end_pos</vh></v>
</v>
<v t="ekr.20180516071751.579"><vh>source_tokens</vh></v>
<v t="ekr.20180516071751.580"><vh>generate_tokens</vh></v>
</v>
<v t="ekr.20180516071751.581"><vh>@clean tree.py</vh>
<v t="ekr.20180516071751.582"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.583"><vh>class _NodeOrLeaf</vh>
<v t="ekr.20180516071751.584"><vh>get_root_node</vh></v>
<v t="ekr.20180516071751.585"><vh>get_next_sibling</vh></v>
<v t="ekr.20180516071751.586"><vh>get_previous_sibling</vh></v>
<v t="ekr.20180516071751.587"><vh>get_previous_leaf</vh></v>
<v t="ekr.20180516071751.588"><vh>get_next_leaf</vh></v>
</v>
<v t="ekr.20180516071751.589"><vh>class Leaf</vh>
<v t="ekr.20180516071751.590"><vh>__init__</vh></v>
<v t="ekr.20180516071751.591"><vh>start_pos</vh></v>
<v t="ekr.20180516071751.592"><vh>start_pos</vh></v>
<v t="ekr.20180516071751.593"><vh>get_start_pos_of_prefix</vh></v>
<v t="ekr.20180516071751.594"><vh>move</vh></v>
<v t="ekr.20180516071751.595"><vh>get_first_leaf</vh></v>
<v t="ekr.20180516071751.596"><vh>get_last_leaf</vh></v>
<v t="ekr.20180516071751.597"><vh>get_code</vh></v>
<v t="ekr.20180516071751.598"><vh>nodes_to_execute</vh></v>
<v t="ekr.20180516071751.599"><vh>end_pos</vh></v>
<v t="ekr.20180516071751.600"><vh>__repr__</vh></v>
</v>
<v t="ekr.20180516071751.601"><vh>class BaseNode</vh>
<v t="ekr.20180516071751.602"><vh>__init__</vh></v>
<v t="ekr.20180516071751.603"><vh>move</vh></v>
<v t="ekr.20180516071751.604"><vh>start_pos</vh></v>
<v t="ekr.20180516071751.605"><vh>get_start_pos_of_prefix</vh></v>
<v t="ekr.20180516071751.606"><vh>end_pos</vh></v>
<v t="ekr.20180516071751.607"><vh>_get_code_for_children</vh></v>
<v t="ekr.20180516071751.608"><vh>get_code</vh></v>
<v t="ekr.20180516071751.609"><vh>get_leaf_for_position</vh></v>
<v t="ekr.20180516071751.610"><vh>get_first_leaf</vh></v>
<v t="ekr.20180516071751.611"><vh>get_last_leaf</vh></v>
<v t="ekr.20180516071751.612"><vh>get_following_comment_same_line</vh></v>
<v t="ekr.20180516071751.613"><vh>__repr__</vh></v>
</v>
<v t="ekr.20180516071751.614"><vh>class Node</vh>
<v t="ekr.20180516071751.615"><vh>__init__</vh></v>
<v t="ekr.20180516071751.616"><vh>nodes_to_execute</vh></v>
<v t="ekr.20180516071751.617"><vh>__repr__</vh></v>
</v>
<v t="ekr.20180516071751.618"><vh>class ErrorNode</vh>
<v t="ekr.20180516071751.619"><vh>nodes_to_execute</vh></v>
</v>
<v t="ekr.20180516071751.620"><vh>class ErrorLeaf</vh>
<v t="ekr.20180516071751.621"><vh>__init__</vh></v>
<v t="ekr.20180516071751.622"><vh>__repr__</vh></v>
</v>
</v>
<v t="ekr.20180516071751.623"><vh>@clean __init__.py</vh>
<v t="ekr.20180516071751.624"><vh>Declarations</vh></v>
<v t="ekr.20180516071751.625"><vh>parse</vh></v>
</v>
<v t="ekr.20180516071752.2"><vh>@path pgen2</vh>
<v t="ekr.20180516071752.3"><vh>@clean grammar.py</vh>
<v t="ekr.20180516071752.4"><vh>Declarations</vh></v>
<v t="ekr.20180516071752.5"><vh>class Grammar</vh>
<v t="ekr.20180516071752.6"><vh>__init__</vh></v>
<v t="ekr.20180516071752.7"><vh>dump</vh></v>
<v t="ekr.20180516071752.8"><vh>load</vh></v>
<v t="ekr.20180516071752.9"><vh>copy</vh></v>
<v t="ekr.20180516071752.10"><vh>report</vh></v>
</v>
</v>
<v t="ekr.20180516071752.11"><vh>@clean parse.py</vh>
<v t="ekr.20180516071752.12"><vh>Declarations</vh></v>
<v t="ekr.20180516071752.13"><vh>class InternalParseError</vh>
<v t="ekr.20180516071752.14"><vh>__init__</vh></v>
</v>
<v t="ekr.20180516071752.15"><vh>token_to_ilabel</vh></v>
<v t="ekr.20180516071752.16"><vh>class PgenParser</vh>
<v t="ekr.20180516071752.17"><vh>__init__</vh></v>
<v t="ekr.20180516071752.18"><vh>parse</vh></v>
<v t="ekr.20180516071752.19"><vh>addtoken</vh></v>
<v t="ekr.20180516071752.20"><vh>_shift</vh></v>
<v t="ekr.20180516071752.21"><vh>_push</vh></v>
<v t="ekr.20180516071752.22"><vh>_pop</vh></v>
</v>
</v>
<v t="ekr.20180516071752.23"><vh>@clean pgen.py</vh>
<v t="ekr.20180516071752.24"><vh>Declarations</vh></v>
<v t="ekr.20180516071752.25"><vh>class ParserGenerator</vh>
<v t="ekr.20180516071752.26"><vh>__init__</vh></v>
<v t="ekr.20180516071752.27"><vh>make_grammar</vh></v>
<v t="ekr.20180516071752.28"><vh>make_first</vh></v>
<v t="ekr.20180516071752.29"><vh>make_label</vh></v>
<v t="ekr.20180516071752.30"><vh>addfirstsets</vh></v>
<v t="ekr.20180516071752.31"><vh>calcfirst</vh></v>
<v t="ekr.20180516071752.32"><vh>parse</vh></v>
<v t="ekr.20180516071752.33"><vh>make_dfa</vh></v>
<v t="ekr.20180516071752.34"><vh>dump_nfa</vh></v>
<v t="ekr.20180516071752.35"><vh>dump_dfa</vh></v>
<v t="ekr.20180516071752.36"><vh>simplify_dfa</vh></v>
<v t="ekr.20180516071752.37"><vh>parse_rhs</vh></v>
<v t="ekr.20180516071752.38"><vh>parse_alt</vh></v>
<v t="ekr.20180516071752.39"><vh>parse_item</vh></v>
<v t="ekr.20180516071752.40"><vh>parse_atom</vh></v>
<v t="ekr.20180516071752.41"><vh>expect</vh></v>
<v t="ekr.20180516071752.42"><vh>gettoken</vh></v>
<v t="ekr.20180516071752.43"><vh>raise_error</vh></v>
</v>
<v t="ekr.20180516071752.44"><vh>class NFAState</vh>
<v t="ekr.20180516071752.45"><vh>__init__</vh></v>
<v t="ekr.20180516071752.46"><vh>addarc</vh></v>
</v>
<v t="ekr.20180516071752.47"><vh>class DFAState</vh>
<v t="ekr.20180516071752.48"><vh>__init__</vh></v>
<v t="ekr.20180516071752.49"><vh>addarc</vh></v>
<v t="ekr.20180516071752.50"><vh>unifystate</vh></v>
<v t="ekr.20180516071752.51"><vh>__eq__</vh></v>
</v>
<v t="ekr.20180516071752.52"><vh>generate_grammar</vh></v>
</v>
<v t="ekr.20180516071752.53"><vh>@clean __init__.py</vh></v>
</v>
<v t="ekr.20180516071752.55"><vh>@path python</vh>
<v t="ekr.20180516071752.56"><vh>@clean diff.py</vh>
<v t="ekr.20180516071752.57"><vh>Declarations</vh></v>
<v t="ekr.20180516071752.58"><vh>_get_last_line</vh></v>
<v t="ekr.20180516071752.59"><vh>_ends_with_newline</vh></v>
<v t="ekr.20180516071752.60"><vh>_flows_finished</vh></v>
<v t="ekr.20180516071752.61"><vh>suite_or_file_input_is_valid</vh></v>
<v t="ekr.20180516071752.62"><vh>_is_flow_node</vh></v>
<v t="ekr.20180516071752.63"><vh>class _PositionUpdatingFinished</vh></v>
<v t="ekr.20180516071752.64"><vh>_update_positions</vh></v>
<v t="ekr.20180516071752.65"><vh>class DiffParser</vh>
<v t="ekr.20180516071752.66"><vh>__init__</vh></v>
<v t="ekr.20180516071752.67"><vh>_reset</vh></v>
<v t="ekr.20180516071752.68"><vh>update</vh></v>
<v t="ekr.20180516071752.69"><vh>_enabled_debugging</vh></v>
<v t="ekr.20180516071752.70"><vh>_copy_from_old_parser</vh></v>
<v t="ekr.20180516071752.71"><vh>_get_old_line_stmt</vh></v>
<v t="ekr.20180516071752.72"><vh>_get_before_insertion_node</vh></v>
<v t="ekr.20180516071752.73"><vh>_parse</vh></v>
<v t="ekr.20180516071752.74"><vh>_get_children_nodes</vh></v>
<v t="ekr.20180516071752.75"><vh>_try_parse_part</vh></v>
<v t="ekr.20180516071752.76"><vh>_diff_tokenize</vh></v>
</v>
<v t="ekr.20180516071752.77"><vh>class _NodesStackNode</vh>
<v t="ekr.20180516071752.78"><vh>__init__</vh></v>
<v t="ekr.20180516071752.79"><vh>close</vh></v>
<v t="ekr.20180516071752.80"><vh>add</vh></v>
<v t="ekr.20180516071752.81"><vh>get_last_line</vh></v>
</v>
<v t="ekr.20180516071752.82"><vh>class _NodesStack</vh>
<v t="ekr.20180516071752.83"><vh>__init__</vh></v>
<v t="ekr.20180516071752.84"><vh>is_empty</vh></v>
<v t="ekr.20180516071752.85"><vh>parsed_until_line</vh></v>
<v t="ekr.20180516071752.86"><vh>_get_insertion_node</vh></v>
<v t="ekr.20180516071752.87"><vh>_close_tos</vh></v>
<v t="ekr.20180516071752.88"><vh>add_parsed_nodes</vh></v>
<v t="ekr.20180516071752.89"><vh>_remove_endmarker</vh></v>
<v t="ekr.20180516071752.90"><vh>copy_nodes</vh></v>
<v t="ekr.20180516071752.91"><vh>_copy_nodes</vh></v>
<v t="ekr.20180516071752.92"><vh>_update_tos</vh></v>
<v t="ekr.20180516071752.93"><vh>close</vh></v>
</v>
</v>
<v t="ekr.20180516071752.94"><vh>@clean parser.py</vh>
<v t="ekr.20180516071752.95"><vh>Declarations</vh></v>
<v t="ekr.20180516071752.96"><vh>class Parser</vh>
<v t="ekr.20180516071752.97"><vh>__init__</vh></v>
<v t="ekr.20180516071752.98"><vh>parse</vh></v>
<v t="ekr.20180516071752.99"><vh>convert_node</vh></v>
<v t="ekr.20180516071752.100"><vh>convert_leaf</vh></v>
<v t="ekr.20180516071752.101"><vh>error_recovery</vh></v>
<v t="ekr.20180516071752.102"><vh>_stack_removal</vh></v>
<v t="ekr.20180516071752.103"><vh>_recovery_tokenize</vh></v>
</v>
<v t="ekr.20180516071752.104"><vh>_remove_last_newline</vh></v>
</v>
<v t="ekr.20180516071752.105"><vh>@clean tree.py</vh>
<v t="ekr.20180516071752.106"><vh>Declarations</vh></v>
<v t="ekr.20180516071752.107"><vh>_safe_literal_eval</vh></v>
<v t="ekr.20180516071752.108"><vh>search_ancestor</vh></v>
<v t="ekr.20180516071752.109"><vh>class DocstringMixin</vh>
<v t="ekr.20180516071752.110"><vh>raw_doc</vh></v>
</v>
<v t="ekr.20180516071752.111"><vh>class PythonMixin</vh>
<v t="ekr.20180516071752.112"><vh>get_parent_scope</vh></v>
<v t="ekr.20180516071752.113"><vh>get_definition</vh></v>
<v t="ekr.20180516071752.114"><vh>is_scope</vh></v>
<v t="ekr.20180516071752.115"><vh>nodes_to_execute</vh></v>
<v t="ekr.20180516071752.116"><vh>name_for_position</vh></v>
<v t="ekr.20180516071752.117"><vh>get_statement_for_position</vh></v>
</v>
<v t="ekr.20180516071752.118"><vh>class PythonLeaf</vh></v>
<v t="ekr.20180516071752.119"><vh>class _LeafWithoutNewlines</vh>
<v t="ekr.20180516071752.120"><vh>end_pos</vh></v>
</v>
<v t="ekr.20180516071752.121"><vh>class PythonBaseNode</vh></v>
<v t="ekr.20180516071752.122"><vh>class PythonNode</vh></v>
<v t="ekr.20180516071752.123"><vh>class PythonErrorNode</vh></v>
<v t="ekr.20180516071752.124"><vh>class PythonErrorLeaf</vh></v>
<v t="ekr.20180516071752.125"><vh>class EndMarker</vh></v>
<v t="ekr.20180516071752.126"><vh>class Newline</vh>
<v t="ekr.20180516071752.127"><vh>__repr__</vh></v>
</v>
<v t="ekr.20180516071752.128"><vh>class Name</vh>
<v t="ekr.20180516071752.129"><vh>__str__</vh></v>
<v t="ekr.20180516071752.130"><vh>__unicode__</vh></v>
<v t="ekr.20180516071752.131"><vh>__repr__</vh></v>
<v t="ekr.20180516071752.132"><vh>is_definition</vh></v>
<v t="ekr.20180516071752.133"><vh>nodes_to_execute</vh></v>
</v>
<v t="ekr.20180516071752.134"><vh>class Literal</vh>
<v t="ekr.20180516071752.135"><vh>eval</vh></v>
</v>
<v t="ekr.20180516071752.136"><vh>class Number</vh></v>
<v t="ekr.20180516071752.137"><vh>class String</vh></v>
<v t="ekr.20180516071752.138"><vh>class Operator</vh>
<v t="ekr.20180516071752.139"><vh>__str__</vh></v>
<v t="ekr.20180516071752.140"><vh>__eq__</vh></v>
<v t="ekr.20180516071752.141"><vh>__ne__</vh></v>
<v t="ekr.20180516071752.142"><vh>__hash__</vh></v>
</v>
<v t="ekr.20180516071752.143"><vh>class Keyword</vh>
<v t="ekr.20180516071752.144"><vh>__eq__</vh></v>
<v t="ekr.20180516071752.145"><vh>__ne__</vh></v>
<v t="ekr.20180516071752.146"><vh>__hash__</vh></v>
</v>
<v t="ekr.20180516071752.147"><vh>class Scope</vh>
<v t="ekr.20180516071752.148"><vh>__init__</vh></v>
<v t="ekr.20180516071752.149"><vh>returns</vh></v>
<v t="ekr.20180516071752.150"><vh>subscopes</vh></v>
<v t="ekr.20180516071752.151"><vh>flows</vh></v>
<v t="ekr.20180516071752.152"><vh>imports</vh></v>
<v t="ekr.20180516071752.153"><vh>_search_in_scope</vh></v>
<v t="ekr.20180516071752.154"><vh>statements</vh></v>
<v t="ekr.20180516071752.155"><vh>is_scope</vh></v>
<v t="ekr.20180516071752.156"><vh>__repr__</vh></v>
<v t="ekr.20180516071752.157"><vh>walk</vh></v>
</v>
<v t="ekr.20180516071752.158"><vh>class Module</vh>
<v t="ekr.20180516071752.159"><vh>__init__</vh></v>
<v t="ekr.20180516071752.160"><vh>has_explicit_absolute_import</vh></v>
<v t="ekr.20180516071752.161"><vh>nodes_to_execute</vh></v>
<v t="ekr.20180516071752.162"><vh>used_names</vh></v>
</v>
<v t="ekr.20180516071752.163"><vh>class Decorator</vh>
<v t="ekr.20180516071752.164"><vh>nodes_to_execute</vh></v>
</v>
<v t="ekr.20180516071752.165"><vh>class ClassOrFunc</vh>
<v t="ekr.20180516071752.166"><vh>name</vh></v>
<v t="ekr.20180516071752.167"><vh>get_decorators</vh></v>
</v>
<v t="ekr.20180516071752.168"><vh>class Class</vh>
<v t="ekr.20180516071752.169"><vh>__init__</vh></v>
<v t="ekr.20180516071752.170"><vh>get_super_arglist</vh></v>
<v t="ekr.20180516071752.171"><vh>doc</vh></v>
<v t="ekr.20180516071752.172"><vh>nodes_to_execute</vh></v>
</v>
<v t="ekr.20180516071752.173"><vh>_create_params</vh></v>
<v t="ekr.20180516071752.174"><vh>class Function</vh>
<v t="ekr.20180516071752.175"><vh>__init__</vh></v>
<v t="ekr.20180516071752.176"><vh>params</vh></v>
<v t="ekr.20180516071752.177"><vh>name</vh></v>
<v t="ekr.20180516071752.178"><vh>yields</vh></v>
<v t="ekr.20180516071752.179"><vh>is_generator</vh></v>
<v t="ekr.20180516071752.180"><vh>annotation</vh></v>
<v t="ekr.20180516071752.181"><vh>get_call_signature</vh></v>
<v t="ekr.20180516071752.182"><vh>_get_paramlist_code</vh></v>
<v t="ekr.20180516071752.183"><vh>doc</vh></v>
<v t="ekr.20180516071752.184"><vh>nodes_to_execute</vh></v>
</v>
<v t="ekr.20180516071752.185"><vh>class Lambda</vh>
<v t="ekr.20180516071752.186"><vh>__init__</vh></v>
<v t="ekr.20180516071752.187"><vh>name</vh></v>
<v t="ekr.20180516071752.188"><vh>_get_paramlist_code</vh></v>
<v t="ekr.20180516071752.189"><vh>params</vh></v>
<v t="ekr.20180516071752.190"><vh>is_generator</vh></v>
<v t="ekr.20180516071752.191"><vh>annotation</vh></v>
<v t="ekr.20180516071752.192"><vh>yields</vh></v>
<v t="ekr.20180516071752.193"><vh>nodes_to_execute</vh></v>
<v t="ekr.20180516071752.194"><vh>__repr__</vh></v>
</v>
<v t="ekr.20180516071752.195"><vh>class Flow</vh>
<v t="ekr.20180516071752.196"><vh>nodes_to_execute</vh></v>
<v t="ekr.20180516071752.197"><vh>get_branch_keyword</vh></v>
</v>
<v t="ekr.20180516071752.198"><vh>class IfStmt</vh>
<v t="ekr.20180516071752.199"><vh>check_nodes</vh></v>
<v t="ekr.20180516071752.200"><vh>node_in_which_check_node</vh></v>
<v t="ekr.20180516071752.201"><vh>node_after_else</vh></v>
</v>
<v t="ekr.20180516071752.202"><vh>class WhileStmt</vh></v>
<v t="ekr.20180516071752.203"><vh>class ForStmt</vh>
<v t="ekr.20180516071752.204"><vh>get_input_node</vh></v>
<v t="ekr.20180516071752.205"><vh>defines_one_name</vh></v>
</v>
<v t="ekr.20180516071752.206"><vh>class TryStmt</vh>
<v t="ekr.20180516071752.207"><vh>except_clauses</vh></v>
<v t="ekr.20180516071752.208"><vh>nodes_to_execute</vh></v>
</v>
<v t="ekr.20180516071752.209"><vh>class WithStmt</vh>
<v t="ekr.20180516071752.210"><vh>get_defined_names</vh></v>
<v t="ekr.20180516071752.211"><vh>node_from_name</vh></v>
<v t="ekr.20180516071752.212"><vh>nodes_to_execute</vh></v>
</v>
<v t="ekr.20180516071752.213"><vh>class Import</vh>
<v t="ekr.20180516071752.214"><vh>path_for_name</vh></v>
<v t="ekr.20180516071752.215"><vh>is_nested</vh></v>
<v t="ekr.20180516071752.216"><vh>is_star_import</vh></v>
<v t="ekr.20180516071752.217"><vh>nodes_to_execute</vh></v>
</v>
<v t="ekr.20180516071752.218"><vh>class ImportFrom</vh>
<v t="ekr.20180516071752.219"><vh>get_defined_names</vh></v>
<v t="ekr.20180516071752.220"><vh>aliases</vh></v>
<v t="ekr.20180516071752.221"><vh>get_from_names</vh></v>
<v t="ekr.20180516071752.222"><vh>level</vh></v>
<v t="ekr.20180516071752.223"><vh>_as_name_tuples</vh></v>
<v t="ekr.20180516071752.224"><vh>star_import_name</vh></v>
<v t="ekr.20180516071752.225"><vh>paths</vh></v>
</v>
<v t="ekr.20180516071752.226"><vh>class ImportName</vh>
<v t="ekr.20180516071752.227"><vh>get_defined_names</vh></v>
<v t="ekr.20180516071752.228"><vh>level</vh></v>
<v t="ekr.20180516071752.229"><vh>paths</vh></v>
<v t="ekr.20180516071752.230"><vh>_dotted_as_names</vh></v>
<v t="ekr.20180516071752.231"><vh>is_nested</vh></v>
<v t="ekr.20180516071752.232"><vh>aliases</vh></v>
</v>
<v t="ekr.20180516071752.233"><vh>class KeywordStatement</vh>
<v t="ekr.20180516071752.234"><vh>type</vh></v>
<v t="ekr.20180516071752.235"><vh>keyword</vh></v>
<v t="ekr.20180516071752.236"><vh>nodes_to_execute</vh></v>
</v>
<v t="ekr.20180516071752.237"><vh>class AssertStmt</vh>
<v t="ekr.20180516071752.238"><vh>assertion</vh></v>
</v>
<v t="ekr.20180516071752.239"><vh>class GlobalStmt</vh>
<v t="ekr.20180516071752.240"><vh>get_defined_names</vh></v>
<v t="ekr.20180516071752.241"><vh>get_global_names</vh></v>
<v t="ekr.20180516071752.242"><vh>nodes_to_execute</vh></v>
</v>
<v t="ekr.20180516071752.243"><vh>class ReturnStmt</vh></v>
<v t="ekr.20180516071752.244"><vh>class YieldExpr</vh>
<v t="ekr.20180516071752.245"><vh>type</vh></v>
<v t="ekr.20180516071752.246"><vh>nodes_to_execute</vh></v>
</v>
<v t="ekr.20180516071752.247"><vh>_defined_names</vh></v>
<v t="ekr.20180516071752.248"><vh>class ExprStmt</vh>
<v t="ekr.20180516071752.249"><vh>get_defined_names</vh></v>
<v t="ekr.20180516071752.250"><vh>get_rhs</vh></v>
<v t="ekr.20180516071752.251"><vh>first_operation</vh></v>
<v t="ekr.20180516071752.252"><vh>nodes_to_execute</vh></v>
</v>
<v t="ekr.20180516071752.253"><vh>class Param</vh>
<v t="ekr.20180516071752.254"><vh>__init__</vh></v>
<v t="ekr.20180516071752.255"><vh>stars</vh></v>
<v t="ekr.20180516071752.256"><vh>default</vh></v>
<v t="ekr.20180516071752.257"><vh>annotation</vh></v>
<v t="ekr.20180516071752.258"><vh>_tfpdef</vh></v>
<v t="ekr.20180516071752.259"><vh>name</vh></v>
<v t="ekr.20180516071752.260"><vh>position_nr</vh></v>
<v t="ekr.20180516071752.261"><vh>get_parent_function</vh></v>
<v t="ekr.20180516071752.262"><vh>__repr__</vh></v>
<v t="ekr.20180516071752.263"><vh>get_description</vh></v>
</v>
<v t="ekr.20180516071752.264"><vh>class CompFor</vh>
<v t="ekr.20180516071752.265"><vh>get_comp_fors</vh></v>
<v t="ekr.20180516071752.266"><vh>is_scope</vh></v>
<v t="ekr.20180516071752.267"><vh>get_defined_names</vh></v>
<v t="ekr.20180516071752.268"><vh>nodes_to_execute</vh></v>
</v>
</v>
<v t="ekr.20180516071752.269"><vh>@clean __init__.py</vh>
<v t="ekr.20180516071752.270"><vh>Declarations</vh></v>
<v t="ekr.20180516071752.271"><vh>load_grammar</vh></v>
<v t="ekr.20180516071752.272"><vh>parse</vh></v>
</v>
</v>
</v>
</v>
</vnodes>
<tnodes>
<t tx="ekr.20180516071358.1">'''Recursively import all python files in a directory and clean the result.'''
@tabwidth -4 # For a better match.
g.cls()
c.recursiveImport(
    
    dir_ = r'C:\Anaconda3\Lib\site-packages\jedi',
    kind = '@clean', # '@auto', '@clean', '@nosent','@file',
    add_path=True,
    recursive = True,
    safe_at_file = False,
    theTypes = ['.py',] # ['.py', '.js','.vue',],
)
# c.expandAllSubheads()
</t>
<t tx="ekr.20180516071749.10">""" A universal module with functions / classes without dependencies. """
import sys
import contextlib
import functools
import re
from ast import literal_eval

from jedi._compatibility import unicode, reraise
from jedi import settings


</t>
<t tx="ekr.20180516071749.11">class UncaughtAttributeError(Exception):
    """
    Important, because `__getattr__` and `hasattr` catch AttributeErrors
    implicitly. This is really evil (mainly because of `__getattr__`).
    `hasattr` in Python 2 is even more evil, because it catches ALL exceptions.
    Therefore this class originally had to be derived from `BaseException`
    instead of `Exception`.  But because I removed relevant `hasattr` from
    the code base, we can now switch back to `Exception`.

    :param base: return values of sys.exc_info().
    """


</t>
<t tx="ekr.20180516071749.12">def safe_property(func):
    return property(reraise_uncaught(func))


</t>
<t tx="ekr.20180516071749.13">def reraise_uncaught(func):
    """
    Re-throw uncaught `AttributeError`.

    Usage:  Put ``@rethrow_uncaught`` in front of the function
    which does **not** suppose to raise `AttributeError`.

    AttributeError is easily get caught by `hasattr` and another
    ``except AttributeError`` clause.  This becomes problem when you use
    a lot of "dynamic" attributes (e.g., using ``@property``) because you
    can't distinguish if the property does not exist for real or some code
    inside of the "dynamic" attribute through that error.  In a well
    written code, such error should not exist but getting there is very
    difficult.  This decorator is to help us getting there by changing
    `AttributeError` to `UncaughtAttributeError` to avoid unexpected catch.
    This helps us noticing bugs earlier and facilitates debugging.

    .. note:: Treating StopIteration here is easy.
              Add that feature when needed.
    """
    @functools.wraps(func)
    def wrapper(*args, **kwds):
        try:
            return func(*args, **kwds)
        except AttributeError:
            exc_info = sys.exc_info()
            reraise(UncaughtAttributeError(exc_info[1]), exc_info[2])
    return wrapper


</t>
<t tx="ekr.20180516071749.14">class PushBackIterator(object):
@others
</t>
<t tx="ekr.20180516071749.15">    def __init__(self, iterator):
        self.pushes = []
        self.iterator = iterator
        self.current = None

</t>
<t tx="ekr.20180516071749.16">    def push_back(self, value):
        self.pushes.append(value)

</t>
<t tx="ekr.20180516071749.17">    def __iter__(self):
        return self

</t>
<t tx="ekr.20180516071749.18">    def next(self):
        """ Python 2 Compatibility """
        return self.__next__()

</t>
<t tx="ekr.20180516071749.19">    def __next__(self):
        if self.pushes:
            self.current = self.pushes.pop()
        else:
            self.current = next(self.iterator)
        return self.current


</t>
<t tx="ekr.20180516071749.2"></t>
<t tx="ekr.20180516071749.20">@contextlib.contextmanager
def scale_speed_settings(factor):
    a = settings.max_executions
    b = settings.max_until_execution_unique
    settings.max_executions *= factor
    settings.max_until_execution_unique *= factor
    try:
        yield
    finally:
        settings.max_executions = a
        settings.max_until_execution_unique = b


</t>
<t tx="ekr.20180516071749.21">def indent_block(text, indention='    '):
    """This function indents a text block with a default of four spaces."""
    temp = ''
    while text and text[-1] == '\n':
        temp += text[-1]
        text = text[:-1]
    lines = text.split('\n')
    return '\n'.join(map(lambda s: indention + s, lines)) + temp


</t>
<t tx="ekr.20180516071749.22">@contextlib.contextmanager
def ignored(*exceptions):
    """
    Context manager that ignores all of the specified exceptions. This will
    be in the standard library starting with Python 3.4.
    """
    try:
        yield
    except exceptions:
        pass


</t>
<t tx="ekr.20180516071749.23">def source_to_unicode(source, encoding=None):
    def detect_encoding():
        """
        For the implementation of encoding definitions in Python, look at:
        - http://www.python.org/dev/peps/pep-0263/
        - http://docs.python.org/2/reference/lexical_analysis.html#encoding-declarations
        """
        byte_mark = literal_eval(r"b'\xef\xbb\xbf'")
        if source.startswith(byte_mark):
            # UTF-8 byte-order mark
            return 'utf-8'

        first_two_lines = re.match(br'(?:[^\n]*\n){0,2}', source).group(0)
        possible_encoding = re.search(br"coding[=:]\s*([-\w.]+)",
                                      first_two_lines)
        if possible_encoding:
            return possible_encoding.group(1)
        else:
            # the default if nothing else has been set -&gt; PEP 263
            return encoding if encoding is not None else 'utf-8'

    if isinstance(source, unicode):
        # only cast str/bytes
        return source

    encoding = detect_encoding()
    if not isinstance(encoding, unicode):
        encoding = unicode(encoding, 'utf-8', 'replace')
    # cast to unicode by default
    return unicode(source, encoding, 'replace')


</t>
<t tx="ekr.20180516071749.24">def splitlines(string, keepends=False):
    """
    A splitlines for Python code. In contrast to Python's ``str.splitlines``,
    looks at form feeds and other special characters as normal text. Just
    splits ``\n`` and ``\r\n``.
    Also different: Returns ``['']`` for an empty string input.

    In Python 2.7 form feeds are used as normal characters when using
    str.splitlines. However in Python 3 somewhere there was a decision to split
    also on form feeds.
    """
    if keepends:
        lst = string.splitlines(True)

        # We have to merge lines that were broken by form feed characters.
        merge = []
        for i, line in enumerate(lst):
            if line.endswith('\f'):
                merge.append(i)

        for index in reversed(merge):
            try:
                lst[index] = lst[index] + lst[index + 1]
                del lst[index + 1]
            except IndexError:
                # index + 1 can be empty and therefore there's no need to
                # merge.
                pass

        # The stdlib's implementation of the end is inconsistent when calling
        # it with/without keepends. One time there's an empty string in the
        # end, one time there's none.
        if string.endswith('\n') or string == '':
            lst.append('')
        return lst
    else:
        return re.split('\n|\r\n', string)


</t>
<t tx="ekr.20180516071749.25">def unite(iterable):
    """Turns a two dimensional array into a one dimensional."""
    return set(typ for types in iterable for typ in types)


</t>
<t tx="ekr.20180516071749.26">def to_list(func):
    def wrapper(*args, **kwargs):
        return list(func(*args, **kwargs))
    return wrapper
</t>
<t tx="ekr.20180516071749.27">@path C:/Anaconda3/Lib/site-packages/jedi/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071749.28">from jedi._compatibility import encoding, is_py3, u
import inspect
import os
import time

</t>
<t tx="ekr.20180516071749.29">def _lazy_colorama_init():
    """
    Lazily init colorama if necessary, not to screw up stdout is debug not
    enabled.

    This version of the function does nothing.
    """
    pass

_inited=False

try:
    if os.name == 'nt':
        # Does not work on Windows, as pyreadline and colorama interfere
        raise ImportError
    else:
        # Use colorama for nicer console output.
        from colorama import Fore, init
        from colorama import initialise
        def _lazy_colorama_init():
            """
            Lazily init colorama if necessary, not to screw up stdout is
            debug not enabled.

            This version of the function does init colorama.
            """
            global _inited
            if not _inited:
                # pytest resets the stream at the end - causes troubles. Since
                # after every output the stream is reset automatically we don't
                # need this.
                initialise.atexit_done = True
                try:
                    init()
                except Exception:
                    # Colorama fails with initializing under vim and is buggy in
                    # version 0.3.6.
                    pass
            _inited = True

except ImportError:
    class Fore(object):
        RED = ''
        GREEN = ''
        YELLOW = ''
        MAGENTA = ''
        RESET = ''

NOTICE = object()
WARNING = object()
SPEED = object()

enable_speed = False
enable_warning = False
enable_notice = False

# callback, interface: level, str
debug_function = None
ignored_modules = ['jedi.parser']
_debug_indent = 0
_start_time = time.time()


</t>
<t tx="ekr.20180516071749.3">@path C:/Anaconda3/Lib/site-packages/jedi/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071749.30">def reset_time():
    global _start_time, _debug_indent
    _start_time = time.time()
    _debug_indent = 0


</t>
<t tx="ekr.20180516071749.31">def increase_indent(func):
    """Decorator for makin """
    def wrapper(*args, **kwargs):
        global _debug_indent
        _debug_indent += 1
        try:
            return func(*args, **kwargs)
        finally:
            _debug_indent -= 1
    return wrapper


</t>
<t tx="ekr.20180516071749.32">def dbg(message, *args, **kwargs):
    """ Looks at the stack, to see if a debug message should be printed. """
    # Python 2 compatibility, because it doesn't understand default args
    color = kwargs.pop('color', 'GREEN')
    assert color

    if debug_function and enable_notice:
        frm = inspect.stack()[1]
        mod = inspect.getmodule(frm[0])
        if not (mod.__name__ in ignored_modules):
            i = ' ' * _debug_indent
            _lazy_colorama_init()
            debug_function(color, i + 'dbg: ' + message % tuple(u(repr(a)) for a in args))


</t>
<t tx="ekr.20180516071749.33">def warning(message, *args, **kwargs):
    format = kwargs.pop('format', True)
    assert not kwargs

    if debug_function and enable_warning:
        i = ' ' * _debug_indent
        if format:
            message = message % tuple(u(repr(a)) for a in args)
        debug_function('RED', i + 'warning: ' + message)


</t>
<t tx="ekr.20180516071749.34">def speed(name):
    if debug_function and enable_speed:
        now = time.time()
        i = ' ' * _debug_indent
        debug_function('YELLOW', i + 'speed: ' + '%s %s' % (name, now - _start_time))


</t>
<t tx="ekr.20180516071749.35">def print_to_stdout(color, str_out):
    """
    The default debug function that prints to standard out.

    :param str color: A string that is an attribute of ``colorama.Fore``.
    """
    col = getattr(Fore, color)
    _lazy_colorama_init()
    if not is_py3:
        str_out = str_out.encode(encoding, 'replace')
    print(col + str_out + Fore.RESET)


# debug_function = print_to_stdout
</t>
<t tx="ekr.20180516071749.36">@path C:/Anaconda3/Lib/site-packages/jedi/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071749.37">"""
Introduce some basic refactoring functions to |jedi|. This module is still in a
very early development stage and needs much testing and improvement.

.. warning:: I won't do too much here, but if anyone wants to step in, please
             do. Refactoring is none of my priorities

It uses the |jedi| `API &lt;plugin-api.html&gt;`_ and supports currently the
following functions (sometimes bug-prone):

- rename
- extract variable
- inline variable
"""
import difflib

from jedi import common
from jedi.evaluate import helpers
from jedi.parser.python import tree as pt


</t>
<t tx="ekr.20180516071749.38">class Refactoring(object):
    @others
</t>
<t tx="ekr.20180516071749.39">def __init__(self, change_dct):
    """
    :param change_dct: dict(old_path=(new_path, old_lines, new_lines))
    """
    self.change_dct = change_dct

</t>
<t tx="ekr.20180516071749.4">"""
This caching is very important for speed and memory optimizations. There's
nothing really spectacular, just some decorators. The following cache types are
available:

- ``time_cache`` can be used to cache something for just a limited time span,
  which can be useful if there's user interaction and the user cannot react
  faster than a certain time.

This module is one of the reasons why |jedi| is not thread-safe. As you can see
there are global variables, which are holding the cache information. Some of
these variables are being cleaned after every API usage.
"""
import time
import inspect

from jedi import settings
from jedi.parser.cache import parser_cache

_time_caches = {}


</t>
<t tx="ekr.20180516071749.40">def old_files(self):
    dct = {}
    for old_path, (new_path, old_l, new_l) in self.change_dct.items():
        dct[new_path] = '\n'.join(new_l)
    return dct

</t>
<t tx="ekr.20180516071749.41">def new_files(self):
    dct = {}
    for old_path, (new_path, old_l, new_l) in self.change_dct.items():
        dct[new_path] = '\n'.join(new_l)
    return dct

</t>
<t tx="ekr.20180516071749.42">def diff(self):
    texts = []
    for old_path, (new_path, old_l, new_l) in self.change_dct.items():
        if old_path:
            udiff = difflib.unified_diff(old_l, new_l)
        else:
            udiff = difflib.unified_diff(old_l, new_l, old_path, new_path)
        texts.append('\n'.join(udiff))
    return '\n'.join(texts)


</t>
<t tx="ekr.20180516071749.43">def rename(script, new_name):
    """ The `args` / `kwargs` params are the same as in `api.Script`.
    :param operation: The refactoring operation to execute.
    :type operation: str
    :type source: str
    :return: list of changed lines/changed files
    """
    return Refactoring(_rename(script.usages(), new_name))


</t>
<t tx="ekr.20180516071749.44">def _rename(names, replace_str):
    """ For both rename and inline. """
    order = sorted(names, key=lambda x: (x.module_path, x.line, x.column),
                   reverse=True)

    def process(path, old_lines, new_lines):
        if new_lines is not None:  # goto next file, save last
            dct[path] = path, old_lines, new_lines

    dct = {}
    current_path = object()
    new_lines = old_lines = None
    for name in order:
        if name.in_builtin_module():
            continue
        if current_path != name.module_path:
            current_path = name.module_path

            process(current_path, old_lines, new_lines)
            if current_path is not None:
                # None means take the source that is a normal param.
                with open(current_path) as f:
                    source = f.read()

            new_lines = common.splitlines(common.source_to_unicode(source))
            old_lines = new_lines[:]

        nr, indent = name.line, name.column
        line = new_lines[nr - 1]
        new_lines[nr - 1] = line[:indent] + replace_str + \
            line[indent + len(name.name):]
    process(current_path, old_lines, new_lines)
    return dct


</t>
<t tx="ekr.20180516071749.45">def extract(script, new_name):
    """ The `args` / `kwargs` params are the same as in `api.Script`.
    :param operation: The refactoring operation to execute.
    :type operation: str
    :type source: str
    :return: list of changed lines/changed files
    """
    new_lines = common.splitlines(common.source_to_unicode(script.source))
    old_lines = new_lines[:]

    user_stmt = script._parser.user_stmt()

    # TODO care for multiline extracts
    dct = {}
    if user_stmt:
        pos = script._pos
        line_index = pos[0] - 1
        arr, index = helpers.array_for_pos(user_stmt, pos)
        if arr is not None:
            start_pos = arr[index].start_pos
            end_pos = arr[index].end_pos

            # take full line if the start line is different from end line
            e = end_pos[1] if end_pos[0] == start_pos[0] else None
            start_line = new_lines[start_pos[0] - 1]
            text = start_line[start_pos[1]:e]
            for l in range(start_pos[0], end_pos[0] - 1):
                text += '\n' + l
            if e is None:
                end_line = new_lines[end_pos[0] - 1]
                text += '\n' + end_line[:end_pos[1]]

            # remove code from new lines
            t = text.lstrip()
            del_start = start_pos[1] + len(text) - len(t)

            text = t.rstrip()
            del_end = len(t) - len(text)
            if e is None:
                new_lines[end_pos[0] - 1] = end_line[end_pos[1] - del_end:]
                e = len(start_line)
            else:
                e = e - del_end
            start_line = start_line[:del_start] + new_name + start_line[e:]
            new_lines[start_pos[0] - 1] = start_line
            new_lines[start_pos[0]:end_pos[0] - 1] = []

            # add parentheses in multiline case
            open_brackets = ['(', '[', '{']
            close_brackets = [')', ']', '}']
            if '\n' in text and not (text[0] in open_brackets and text[-1] ==
                                     close_brackets[open_brackets.index(text[0])]):
                text = '(%s)' % text

            # add new line before statement
            indent = user_stmt.start_pos[1]
            new = "%s%s = %s" % (' ' * indent, new_name, text)
            new_lines.insert(line_index, new)
    dct[script.path] = script.path, old_lines, new_lines
    return Refactoring(dct)


</t>
<t tx="ekr.20180516071749.46">def inline(script):
    """
    :type script: api.Script
    """
    new_lines = common.splitlines(common.source_to_unicode(script.source))

    dct = {}

    definitions = script.goto_assignments()
    with common.ignored(AssertionError):
        assert len(definitions) == 1
        stmt = definitions[0]._definition
        usages = script.usages()
        inlines = [r for r in usages
                   if not stmt.start_pos &lt;= (r.line, r.column) &lt;= stmt.end_pos]
        inlines = sorted(inlines, key=lambda x: (x.module_path, x.line, x.column),
                         reverse=True)
        expression_list = stmt.expression_list()
        # don't allow multiline refactorings for now.
        assert stmt.start_pos[0] == stmt.end_pos[0]
        index = stmt.start_pos[0] - 1

        line = new_lines[index]
        replace_str = line[expression_list[0].start_pos[1]:stmt.end_pos[1] + 1]
        replace_str = replace_str.strip()
        # tuples need parentheses
        if expression_list and isinstance(expression_list[0], pr.Array):
            arr = expression_list[0]
            if replace_str[0] not in ['(', '[', '{'] and len(arr) &gt; 1:
                replace_str = '(%s)' % replace_str

        # if it's the only assignment, remove the statement
        if len(stmt.get_defined_names()) == 1:
            line = line[:stmt.start_pos[1]] + line[stmt.end_pos[1]:]

        dct = _rename(inlines, replace_str)
        # remove the empty line
        new_lines = dct[script.path][2]
        if line.strip():
            new_lines[index] = line
        else:
            new_lines.pop(index)

    return Refactoring(dct)
</t>
<t tx="ekr.20180516071749.5">def underscore_memoization(func):
    """
    Decorator for methods::

        class A(object):
            def x(self):
                if self._x:
                    self._x = 10
                return self._x

    Becomes::

        class A(object):
            @underscore_memoization
            def x(self):
                return 10

    A now has an attribute ``_x`` written by this decorator.
    """
    name = '_' + func.__name__

    def wrapper(self):
        try:
            return getattr(self, name)
        except AttributeError:
            result = func(self)
            if inspect.isgenerator(result):
                result = list(result)
            setattr(self, name, result)
            return result

    return wrapper


</t>
<t tx="ekr.20180516071749.6">def clear_time_caches(delete_all=False):
    """ Jedi caches many things, that should be completed after each completion
    finishes.

    :param delete_all: Deletes also the cache that is normally not deleted,
        like parser cache, which is important for faster parsing.
    """
    global _time_caches

    if delete_all:
        for cache in _time_caches.values():
            cache.clear()
        parser_cache.clear()
    else:
        # normally just kill the expired entries, not all
        for tc in _time_caches.values():
            # check time_cache for expired entries
            for key, (t, value) in list(tc.items()):
                if t &lt; time.time():
                    # delete expired entries
                    del tc[key]


</t>
<t tx="ekr.20180516071749.7">def time_cache(time_add_setting):
    """
    This decorator works as follows: Call it with a setting and after that
    use the function with a callable that returns the key.
    But: This function is only called if the key is not available. After a
    certain amount of time (`time_add_setting`) the cache is invalid.

    If the given key is None, the function will not be cached.
    """
    def _temp(key_func):
        dct = {}
        _time_caches[time_add_setting] = dct

        def wrapper(*args, **kwargs):
            generator = key_func(*args, **kwargs)
            key = next(generator)
            try:
                expiry, value = dct[key]
                if expiry &gt; time.time():
                    return value
            except KeyError:
                pass

            value = next(generator)
            time_add = getattr(settings, time_add_setting)
            if key is not None:
                dct[key] = time.time() + time_add, value
            return value
        return wrapper
    return _temp


</t>
<t tx="ekr.20180516071749.8">def memoize_method(method):
    """A normal memoize function."""
    def wrapper(self, *args, **kwargs):
        cache_dict = self.__dict__.setdefault('_memoize_method_dct', {})
        dct = cache_dict.setdefault(method, {})
        key = (args, frozenset(kwargs.items()))
        try:
            return dct[key]
        except KeyError:
            result = method(self, *args, **kwargs)
            dct[key] = result
            return result
    return wrapper
</t>
<t tx="ekr.20180516071749.9">@path C:/Anaconda3/Lib/site-packages/jedi/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071750.1">@path C:/Anaconda3/Lib/site-packages/jedi/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071750.10">def __init__(self, loader, string):
    self.loader = loader
    self.string = string

</t>
<t tx="ekr.20180516071750.100">"""
The :mod:`jedi.api.classes` module contains the return classes of the API.
These classes are the much bigger part of the whole API, because they contain
the interesting information about completion and goto operations.
"""
import warnings
import re

from jedi._compatibility import u
from jedi import settings
from jedi import common
from jedi.parser.cache import parser_cache
from jedi.cache import memoize_method
from jedi.evaluate import representation as er
from jedi.evaluate import instance
from jedi.evaluate import imports
from jedi.evaluate import compiled
from jedi.evaluate.filters import ParamName
from jedi.api.keywords import KeywordName


</t>
<t tx="ekr.20180516071750.101">def _sort_names_by_start_pos(names):
    return sorted(names, key=lambda s: s.start_pos or (0, 0))


</t>
<t tx="ekr.20180516071750.102">def defined_names(evaluator, context):
    """
    List sub-definitions (e.g., methods in class).

    :type scope: Scope
    :rtype: list of Definition
    """
    filter = next(context.get_filters(search_global=True))
    names = [name for name in filter.values()]
    return [Definition(evaluator, n) for n in _sort_names_by_start_pos(names)]


</t>
<t tx="ekr.20180516071750.103">class BaseDefinition(object):
    _mapping = {
        'posixpath': 'os.path',
        'riscospath': 'os.path',
        'ntpath': 'os.path',
        'os2emxpath': 'os.path',
        'macpath': 'os.path',
        'genericpath': 'os.path',
        'posix': 'os',
        '_io': 'io',
        '_functools': 'functools',
        '_sqlite3': 'sqlite3',
        '__builtin__': '',
        'builtins': '',
    }

    _tuple_mapping = dict((tuple(k.split('.')), v) for (k, v) in {
        'argparse._ActionsContainer': 'argparse.ArgumentParser',
    }.items())

    @others
</t>
<t tx="ekr.20180516071750.104">def __init__(self, evaluator, name):
    self._evaluator = evaluator
    self._name = name
    """
    An instance of :class:`jedi.parser.reprsentation.Name` subclass.
    """
    self.is_keyword = isinstance(self._name, KeywordName)

    # generate a path to the definition
    self._module = name.get_root_context()
    if self.in_builtin_module():
        self.module_path = None
    else:
        self.module_path = self._module.py__file__()
        """Shows the file path of a module. e.g. ``/usr/lib/python2.7/os.py``"""

</t>
<t tx="ekr.20180516071750.105">@property
def name(self):
    """
    Name of variable/function/class/module.

    For example, for ``x = None`` it returns ``'x'``.

    :rtype: str or None
    """
    return self._name.string_name

</t>
<t tx="ekr.20180516071750.106">@property
def type(self):
    """
    The type of the definition.

    Here is an example of the value of this attribute.  Let's consider
    the following source.  As what is in ``variable`` is unambiguous
    to Jedi, :meth:`jedi.Script.goto_definitions` should return a list of
    definition for ``sys``, ``f``, ``C`` and ``x``.

    &gt;&gt;&gt; from jedi import Script
    &gt;&gt;&gt; source = '''
    ... import keyword
    ...
    ... class C:
    ...     pass
    ...
    ... class D:
    ...     pass
    ...
    ... x = D()
    ...
    ... def f():
    ...     pass
    ...
    ... for variable in [keyword, f, C, x]:
    ...     variable'''

    &gt;&gt;&gt; script = Script(source)
    &gt;&gt;&gt; defs = script.goto_definitions()

    Before showing what is in ``defs``, let's sort it by :attr:`line`
    so that it is easy to relate the result to the source code.

    &gt;&gt;&gt; defs = sorted(defs, key=lambda d: d.line)
    &gt;&gt;&gt; defs                           # doctest: +NORMALIZE_WHITESPACE
    [&lt;Definition module keyword&gt;, &lt;Definition class C&gt;,
     &lt;Definition instance D&gt;, &lt;Definition def f&gt;]

    Finally, here is what you can get from :attr:`type`:

    &gt;&gt;&gt; defs[0].type
    'module'
    &gt;&gt;&gt; defs[1].type
    'class'
    &gt;&gt;&gt; defs[2].type
    'instance'
    &gt;&gt;&gt; defs[3].type
    'function'

    """
    tree_name = self._name.tree_name
    resolve = False
    if tree_name is not None:
        # TODO move this to their respective names.
        definition = tree_name.get_definition()
        if definition.type == 'import_from' and \
                tree_name in definition.get_defined_names():
            resolve = True

    if isinstance(self._name, imports.SubModuleName) or resolve:
        for context in self._name.infer():
            return context.api_type
    return self._name.api_type

</t>
<t tx="ekr.20180516071750.107">def _path(self):
    """The path to a module/class/function definition."""
    def to_reverse():
        name = self._name
        if name.api_type == 'module':
            try:
                name = list(name.infer())[0].name
            except IndexError:
                pass

        if name.api_type == 'module':
            module_context, = name.infer()
            for n in reversed(module_context.py__name__().split('.')):
                yield n
        else:
            yield name.string_name

        parent_context = name.parent_context
        while parent_context is not None:
            try:
                method = parent_context.py__name__
            except AttributeError:
                try:
                    yield parent_context.name.string_name
                except AttributeError:
                    pass
            else:
                for name in reversed(method().split('.')):
                    yield name
            parent_context = parent_context.parent_context
    return reversed(list(to_reverse()))

</t>
<t tx="ekr.20180516071750.108">@property
def module_name(self):
    """
    The module name.

    &gt;&gt;&gt; from jedi import Script
    &gt;&gt;&gt; source = 'import json'
    &gt;&gt;&gt; script = Script(source, path='example.py')
    &gt;&gt;&gt; d = script.goto_definitions()[0]
    &gt;&gt;&gt; print(d.module_name)                       # doctest: +ELLIPSIS
    json
    """
    return self._module.name.string_name

</t>
<t tx="ekr.20180516071750.109">def in_builtin_module(self):
    """Whether this is a builtin module."""
    return isinstance(self._module, compiled.CompiledObject)

</t>
<t tx="ekr.20180516071750.11">def read(self):
    return self.loader.get_source(self.string)

</t>
<t tx="ekr.20180516071750.110">@property
def line(self):
    """The line where the definition occurs (starting with 1)."""
    start_pos = self._name.start_pos
    if start_pos is None:
        return None
    return start_pos[0]

</t>
<t tx="ekr.20180516071750.111">@property
def column(self):
    """The column where the definition occurs (starting with 0)."""
    start_pos = self._name.start_pos
    if start_pos is None:
        return None
    return start_pos[1]

</t>
<t tx="ekr.20180516071750.112">def docstring(self, raw=False, fast=True):
    r"""
    Return a document string for this completion object.

    Example:

    &gt;&gt;&gt; from jedi import Script
    &gt;&gt;&gt; source = '''\
    ... def f(a, b=1):
    ...     "Document for function f."
    ... '''
    &gt;&gt;&gt; script = Script(source, 1, len('def f'), 'example.py')
    &gt;&gt;&gt; doc = script.goto_definitions()[0].docstring()
    &gt;&gt;&gt; print(doc)
    f(a, b=1)
    &lt;BLANKLINE&gt;
    Document for function f.

    Notice that useful extra information is added to the actual
    docstring.  For function, it is call signature.  If you need
    actual docstring, use ``raw=True`` instead.

    &gt;&gt;&gt; print(script.goto_definitions()[0].docstring(raw=True))
    Document for function f.

    :param fast: Don't follow imports that are only one level deep like
        ``import foo``, but follow ``from foo import bar``. This makes
        sense for speed reasons. Completing `import a` is slow if you use
        the ``foo.docstring(fast=False)`` on every object, because it
        parses all libraries starting with ``a``.
    """
    if raw:
        return _Help(self._name).raw(fast=fast)
    else:
        return _Help(self._name).full(fast=fast)

</t>
<t tx="ekr.20180516071750.113">@property
def doc(self):
    """
    .. deprecated:: 0.8.0
       Use :meth:`.docstring` instead.
    .. todo:: Remove!
    """
    warnings.warn("Use docstring() instead.", DeprecationWarning)
    return self.docstring()

</t>
<t tx="ekr.20180516071750.114">@property
def raw_doc(self):
    """
    .. deprecated:: 0.8.0
       Use :meth:`.docstring` instead.
    .. todo:: Remove!
    """
    warnings.warn("Use docstring() instead.", DeprecationWarning)
    return self.docstring(raw=True)

</t>
<t tx="ekr.20180516071750.115">@property
def description(self):
    """A textual description of the object."""
    return u(self._name.string_name)

</t>
<t tx="ekr.20180516071750.116">@property
def full_name(self):
    """
    Dot-separated path of this object.

    It is in the form of ``&lt;module&gt;[.&lt;submodule&gt;[...]][.&lt;object&gt;]``.
    It is useful when you want to look up Python manual of the
    object at hand.

    Example:

    &gt;&gt;&gt; from jedi import Script
    &gt;&gt;&gt; source = '''
    ... import os
    ... os.path.join'''
    &gt;&gt;&gt; script = Script(source, 3, len('os.path.join'), 'example.py')
    &gt;&gt;&gt; print(script.goto_definitions()[0].full_name)
    os.path.join

    Notice that it returns ``'os.path.join'`` instead of (for example)
    ``'posixpath.join'``. This is not correct, since the modules name would
    be ``&lt;module 'posixpath' ...&gt;```. However most users find the latter
    more practical.
    """
    path = list(self._path())
    # TODO add further checks, the mapping should only occur on stdlib.
    if not path:
        return None  # for keywords the path is empty

    with common.ignored(KeyError):
        path[0] = self._mapping[path[0]]
    for key, repl in self._tuple_mapping.items():
        if tuple(path[:len(key)]) == key:
            path = [repl] + path[len(key):]

    return '.'.join(path if path[0] else path[1:])

</t>
<t tx="ekr.20180516071750.117">def goto_assignments(self):
    if self._name.tree_name is None:
        return self

    names = self._evaluator.goto(self._name.parent_context, self._name.tree_name)
    return [Definition(self._evaluator, n) for n in names]

</t>
<t tx="ekr.20180516071750.118">def _goto_definitions(self):
    # TODO make this function public.
    return [Definition(self._evaluator, d.name) for d in self._name.infer()]

</t>
<t tx="ekr.20180516071750.119">@property
@memoize_method
def params(self):
    """
    Raises an ``AttributeError``if the definition is not callable.
    Otherwise returns a list of `Definition` that represents the params.
    """
    def get_param_names(context):
        param_names = []
        if context.api_type == 'function':
            param_names = list(context.get_param_names())
            if isinstance(context, instance.BoundMethod):
                param_names = param_names[1:]
        elif isinstance(context, (instance.AbstractInstanceContext, er.ClassContext)):
            if isinstance(context, er.ClassContext):
                search = '__init__'
            else:
                search = '__call__'
            names = context.get_function_slot_names(search)
            if not names:
                return []

            # Just take the first one here, not optimal, but currently
            # there's no better solution.
            inferred = names[0].infer()
            param_names = get_param_names(next(iter(inferred)))
            if isinstance(context, er.ClassContext):
                param_names = param_names[1:]
            return param_names
        elif isinstance(context, compiled.CompiledObject):
            return list(context.get_param_names())
        return param_names

    followed = list(self._name.infer())
    if not followed or not hasattr(followed[0], 'py__call__'):
        raise AttributeError()
    context = followed[0]  # only check the first one.

    return [_Param(self._evaluator, n) for n in get_param_names(context)]

</t>
<t tx="ekr.20180516071750.12">def close(self):
    del self.loader


</t>
<t tx="ekr.20180516071750.120">def parent(self):
    context = self._name.parent_context
    if context is None:
        return None

    if isinstance(context, er.FunctionExecutionContext):
        # TODO the function context should be a part of the function
        # execution context.
        context = er.FunctionContext(
            self._evaluator, context.parent_context, context.tree_node)
    return Definition(self._evaluator, context.name)

</t>
<t tx="ekr.20180516071750.121">def __repr__(self):
    return "&lt;%s %s&gt;" % (type(self).__name__, self.description)

</t>
<t tx="ekr.20180516071750.122">def get_line_code(self, before=0, after=0):
    """
    Returns the line of code where this object was defined.

    :param before: Add n lines before the current line to the output.
    :param after: Add n lines after the current line to the output.

    :return str: Returns the line(s) of code or an empty string if it's a
                 builtin.
    """
    if self.in_builtin_module():
        return ''

    path = self._name.get_root_context().py__file__()
    lines = parser_cache[path].lines

    line_nr = self._name.start_pos[0]
    start_line_nr = line_nr - before
    return ''.join(lines[start_line_nr:line_nr + after + 1])


</t>
<t tx="ekr.20180516071750.123">class Completion(BaseDefinition):
    """
    `Completion` objects are returned from :meth:`api.Script.completions`. They
    provide additional information about a completion.
    """
    @others
</t>
<t tx="ekr.20180516071750.124">def __init__(self, evaluator, name, stack, like_name_length):
    super(Completion, self).__init__(evaluator, name)

    self._like_name_length = like_name_length
    self._stack = stack

    # Completion objects with the same Completion name (which means
    # duplicate items in the completion)
    self._same_name_completions = []

</t>
<t tx="ekr.20180516071750.125">def _complete(self, like_name):
    append = ''
    if settings.add_bracket_after_function \
            and self.type == 'Function':
        append = '('

    if isinstance(self._name, ParamName) and self._stack is not None:
        node_names = list(self._stack.get_node_names(self._evaluator.grammar))
        if 'trailer' in node_names and 'argument' not in node_names:
            append += '='

    name = self._name.string_name
    if like_name:
        name = name[self._like_name_length:]
    return name + append

</t>
<t tx="ekr.20180516071750.126">@property
def complete(self):
    """
    Return the rest of the word, e.g. completing ``isinstance``::

        isinstan# &lt;-- Cursor is here

    would return the string 'ce'. It also adds additional stuff, depending
    on your `settings.py`.

    Assuming the following function definition::

        def foo(param=0):
            pass

    completing ``foo(par`` would give a ``Completion`` which `complete`
    would be `am=`


    """
    return self._complete(True)

</t>
<t tx="ekr.20180516071750.127">@property
def name_with_symbols(self):
    """
    Similar to :attr:`name`, but like :attr:`name` returns also the
    symbols, for example assuming the following function definition::

        def foo(param=0):
            pass

    completing ``foo(`` would give a ``Completion`` which
    ``name_with_symbols`` would be "param=".

    """
    return self._complete(False)

</t>
<t tx="ekr.20180516071750.128">def docstring(self, raw=False, fast=True):
    if self._like_name_length &gt;= 3:
        # In this case we can just resolve the like name, because we
        # wouldn't load like &gt; 100 Python modules anymore.
        fast = False
    return super(Completion, self,).docstring(raw, fast)

</t>
<t tx="ekr.20180516071750.129">@property
def description(self):
    """Provide a description of the completion object."""
    # TODO improve the class structure.
    return Definition.description.__get__(self)

</t>
<t tx="ekr.20180516071750.13">def find_module_py34(string, path=None, fullname=None):
    implicit_namespace_pkg = False
    spec = None
    loader = None

    spec = importlib.machinery.PathFinder.find_spec(string, path)
    if hasattr(spec, 'origin'):
        origin = spec.origin
        implicit_namespace_pkg = origin == 'namespace'

    # We try to disambiguate implicit namespace pkgs with non implicit namespace pkgs
    if implicit_namespace_pkg:
        fullname = string if not path else fullname
        implicit_ns_info = ImplicitNSInfo(fullname, spec.submodule_search_locations._path)
        return None, implicit_ns_info, False

    # we have found the tail end of the dotted path
    if hasattr(spec, 'loader'):
        loader = spec.loader
    return find_module_py33(string, path, loader)

</t>
<t tx="ekr.20180516071750.130">def __repr__(self):
    return '&lt;%s: %s&gt;' % (type(self).__name__, self._name.string_name)

</t>
<t tx="ekr.20180516071750.131">@memoize_method
def follow_definition(self):
    """
    Return the original definitions. I strongly recommend not using it for
    your completions, because it might slow down |jedi|. If you want to
    read only a few objects (&lt;=20), it might be useful, especially to get
    the original docstrings. The basic problem of this function is that it
    follows all results. This means with 1000 completions (e.g.  numpy),
    it's just PITA-slow.
    """
    defs = self._name.infer()
    return [Definition(self._evaluator, d.name) for d in defs]


</t>
<t tx="ekr.20180516071750.132">class Definition(BaseDefinition):
    """
    *Definition* objects are returned from :meth:`api.Script.goto_assignments`
    or :meth:`api.Script.goto_definitions`.
    """
    @others
</t>
<t tx="ekr.20180516071750.133">def __init__(self, evaluator, definition):
    super(Definition, self).__init__(evaluator, definition)

</t>
<t tx="ekr.20180516071750.134">@property
def description(self):
    """
    A description of the :class:`.Definition` object, which is heavily used
    in testing. e.g. for ``isinstance`` it returns ``def isinstance``.

    Example:

    &gt;&gt;&gt; from jedi import Script
    &gt;&gt;&gt; source = '''
    ... def f():
    ...     pass
    ...
    ... class C:
    ...     pass
    ...
    ... variable = f if random.choice([0,1]) else C'''
    &gt;&gt;&gt; script = Script(source, column=3)  # line is maximum by default
    &gt;&gt;&gt; defs = script.goto_definitions()
    &gt;&gt;&gt; defs = sorted(defs, key=lambda d: d.line)
    &gt;&gt;&gt; defs
    [&lt;Definition def f&gt;, &lt;Definition class C&gt;]
    &gt;&gt;&gt; str(defs[0].description)  # strip literals in python2
    'def f'
    &gt;&gt;&gt; str(defs[1].description)
    'class C'

    """
    typ = self.type
    tree_name = self._name.tree_name
    if typ in ('function', 'class', 'module', 'instance') or tree_name is None:
        if typ == 'function':
            # For the description we want a short and a pythonic way.
            typ = 'def'
        return typ + ' ' + u(self._name.string_name)
    elif typ == 'param':
        return typ + ' ' + tree_name.get_definition().get_description()

    definition = tree_name.get_definition()
    # Remove the prefix, because that's not what we want for get_code
    # here.
    txt = definition.get_code(include_prefix=False)
    # Delete comments:
    txt = re.sub('#[^\n]+\n', ' ', txt)
    # Delete multi spaces/newlines
    txt = re.sub('\s+', ' ', txt).strip()
    return txt

</t>
<t tx="ekr.20180516071750.135">@property
def desc_with_module(self):
    """
    In addition to the definition, also return the module.

    .. warning:: Don't use this function yet, its behaviour may change. If
        you really need it, talk to me.

    .. todo:: Add full path. This function is should return a
        `module.class.function` path.
    """
    position = '' if self.in_builtin_module else '@%s' % (self.line)
    return "%s:%s%s" % (self.module_name, self.description, position)

</t>
<t tx="ekr.20180516071750.136">@memoize_method
def defined_names(self):
    """
    List sub-definitions (e.g., methods in class).

    :rtype: list of Definition
    """
    defs = self._name.infer()
    return sorted(
        common.unite(defined_names(self._evaluator, d) for d in defs),
        key=lambda s: s._name.start_pos or (0, 0)
    )

</t>
<t tx="ekr.20180516071750.137">def is_definition(self):
    """
    Returns True, if defined as a name in a statement, function or class.
    Returns False, if it's a reference to such a definition.
    """
    if self._name.tree_name is None:
        return True
    else:
        return self._name.tree_name.is_definition()

</t>
<t tx="ekr.20180516071750.138">def __eq__(self, other):
    return self._name.start_pos == other._name.start_pos \
        and self.module_path == other.module_path \
        and self.name == other.name \
        and self._evaluator == other._evaluator

</t>
<t tx="ekr.20180516071750.139">def __ne__(self, other):
    return not self.__eq__(other)

</t>
<t tx="ekr.20180516071750.14">def find_module_py33(string, path=None, loader=None, fullname=None):
    loader = loader or importlib.machinery.PathFinder.find_module(string, path)

    if loader is None and path is None:  # Fallback to find builtins
        try:
            loader = importlib.find_loader(string)
        except ValueError as e:
            # See #491. Importlib might raise a ValueError, to avoid this, we
            # just raise an ImportError to fix the issue.
            raise ImportError("Originally  " + repr(e))

    if loader is None:
        raise ImportError("Couldn't find a loader for {0}".format(string))

    try:
        is_package = loader.is_package(string)
        if is_package:
            if hasattr(loader, 'path'):
                module_path = os.path.dirname(loader.path)
            else:
                # At least zipimporter does not have path attribute
                module_path = os.path.dirname(loader.get_filename(string))
            if hasattr(loader, 'archive'):
                module_file = DummyFile(loader, string)
            else:
                module_file = None
        else:
            module_path = loader.get_filename(string)
            module_file = DummyFile(loader, string)
    except AttributeError:
        # ExtensionLoader has not attribute get_filename, instead it has a
        # path attribute that we can use to retrieve the module path
        try:
            module_path = loader.path
            module_file = DummyFile(loader, string)
        except AttributeError:
            module_path = string
            module_file = None
        finally:
            is_package = False

    if hasattr(loader, 'archive'):
        module_path = loader.archive

    return module_file, module_path, is_package


</t>
<t tx="ekr.20180516071750.140">def __hash__(self):
    return hash((self._name.start_pos, self.module_path, self.name, self._evaluator))


</t>
<t tx="ekr.20180516071750.141">class CallSignature(Definition):
    """
    `CallSignature` objects is the return value of `Script.function_definition`.
    It knows what functions you are currently in. e.g. `isinstance(` would
    return the `isinstance` function. without `(` it would return nothing.
    """
    @others
</t>
<t tx="ekr.20180516071750.142">def __init__(self, evaluator, executable_name, bracket_start_pos, index, key_name_str):
    super(CallSignature, self).__init__(evaluator, executable_name)
    self._index = index
    self._key_name_str = key_name_str
    self._bracket_start_pos = bracket_start_pos

</t>
<t tx="ekr.20180516071750.143">@property
def index(self):
    """
    The Param index of the current call.
    Returns None if the index cannot be found in the curent call.
    """
    if self._key_name_str is not None:
        for i, param in enumerate(self.params):
            if self._key_name_str == param.name:
                return i
        if self.params:
            param_name = self.params[-1]._name
            if param_name.tree_name is not None:
                if param_name.tree_name.get_definition().stars == 2:
                    return i
        return None

    if self._index &gt;= len(self.params):
        for i, param in enumerate(self.params):
            tree_name = param._name.tree_name
            if tree_name is not None:
                # *args case
                if tree_name.get_definition().stars == 1:
                    return i
        return None
    return self._index

</t>
<t tx="ekr.20180516071750.144">@property
def bracket_start(self):
    """
    The indent of the bracket that is responsible for the last function
    call.
    """
    return self._bracket_start_pos

</t>
<t tx="ekr.20180516071750.145">@property
def call_name(self):
    """
    .. deprecated:: 0.8.0
       Use :attr:`.name` instead.
    .. todo:: Remove!

    The name (e.g. 'isinstance') as a string.
    """
    warnings.warn("Use name instead.", DeprecationWarning)
    return self.name

</t>
<t tx="ekr.20180516071750.146">@property
def module(self):
    """
    .. deprecated:: 0.8.0
       Use :attr:`.module_name` for the module name.
    .. todo:: Remove!
    """
    return self._executable.get_root_node()

</t>
<t tx="ekr.20180516071750.147">def __repr__(self):
    return '&lt;%s: %s index %s&gt;' % \
        (type(self).__name__, self._name.string_name, self.index)


</t>
<t tx="ekr.20180516071750.148">class _Param(Definition):
    """
    Just here for backwards compatibility.
    """
    @others
</t>
<t tx="ekr.20180516071750.149">def get_code(self):
    """
    .. deprecated:: 0.8.0
       Use :attr:`.description` and :attr:`.name` instead.
    .. todo:: Remove!

    A function to get the whole code of the param.
    """
    warnings.warn("Use description instead.", DeprecationWarning)
    return self.description


</t>
<t tx="ekr.20180516071750.15">def find_module_pre_py33(string, path=None, fullname=None):
    try:
        module_file, module_path, description = imp.find_module(string, path)
        module_type = description[2]
        return module_file, module_path, module_type is imp.PKG_DIRECTORY
    except ImportError:
        pass

    if path is None:
        path = sys.path
    for item in path:
        loader = pkgutil.get_importer(item)
        if loader:
            try:
                loader = loader.find_module(string)
                if loader:
                    is_package = loader.is_package(string)
                    is_archive = hasattr(loader, 'archive')
                    try:
                        module_path = loader.get_filename(string)
                    except AttributeError:
                        # fallback for py26
                        try:
                            module_path = loader._get_filename(string)
                        except AttributeError:
                            continue
                    if is_package:
                        module_path = os.path.dirname(module_path)
                    if is_archive:
                        module_path = loader.archive
                    file = None
                    if not is_package or is_archive:
                        file = DummyFile(loader, string)
                    return (file, module_path, is_package)
            except ImportError:
                pass
    raise ImportError("No module named {0}".format(string))


find_module = find_module_py33 if is_py33 else find_module_pre_py33
find_module = find_module_py34 if is_py34  else find_module
find_module.__doc__ = """
Provides information about a module.

This function isolates the differences in importing libraries introduced with
python 3.3 on; it gets a module name and optionally a path. It will return a
tuple containin an open file for the module (if not builtin), the filename
or the name of the module if it is a builtin one and a boolean indicating
if the module is contained in a package.
"""


</t>
<t tx="ekr.20180516071750.150">class _Help(object):
    """
    Temporary implementation, will be used as `Script.help() or something in
    the future.
    """
    @others
</t>
<t tx="ekr.20180516071750.151">def __init__(self, definition):
    self._name = definition

</t>
<t tx="ekr.20180516071750.152">@memoize_method
def _get_node(self, fast):
    if isinstance(self._name, (compiled.CompiledContextName, compiled.CompiledName)):
        followed = self._name.infer()
        if followed:
            return next(iter(followed))
        return None

    if self._name.api_type == 'module' and not fast:
        followed = self._name.infer()
        if followed:
            # TODO: Use all of the followed objects as input to Documentation.
            context = next(iter(followed))
            return context.tree_node
    if self._name.tree_name is None:
        return None
    return self._name.tree_name.get_definition()

</t>
<t tx="ekr.20180516071750.153">def full(self, fast=True):
    node = self._get_node(fast)
    try:
        return node.doc
    except AttributeError:
        return self.raw(fast)

</t>
<t tx="ekr.20180516071750.154">def raw(self, fast=True):
    """
    The raw docstring ``__doc__`` for any object.

    See :attr:`doc` for example.
    """
    node = self._get_node(fast)
    if node is None:
        return ''

    try:
        return node.raw_doc
    except AttributeError:
        return ''
</t>
<t tx="ekr.20180516071750.155">@path C:/Anaconda3/Lib/site-packages/jedi/api/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071750.156">from jedi.parser import token
from jedi.parser.python import tree
from jedi import debug
from jedi import settings
from jedi.api import classes
from jedi.api import helpers
from jedi.evaluate import imports
from jedi.api import keywords
from jedi.evaluate.helpers import evaluate_call_of_leaf
from jedi.evaluate.filters import get_global_filters


</t>
<t tx="ekr.20180516071750.157">def get_call_signature_param_names(call_signatures):
    # add named params
    for call_sig in call_signatures:
        for p in call_sig.params:
            # Allow protected access, because it's a public API.
            tree_name = p._name.tree_name
            # Compiled modules typically don't allow keyword arguments.
            if tree_name is not None:
                # Allow access on _definition here, because it's a
                # public API and we don't want to make the internal
                # Name object public.
                tree_param = tree.search_ancestor(tree_name, 'param')
                if tree_param.stars == 0:  # no *args/**kwargs
                    yield p._name


</t>
<t tx="ekr.20180516071750.158">def filter_names(evaluator, completion_names, stack, like_name):
    comp_dct = {}
    for name in completion_names:
        if settings.case_insensitive_completion \
                and name.string_name.lower().startswith(like_name.lower()) \
                or name.string_name.startswith(like_name):

            new = classes.Completion(
                evaluator,
                name,
                stack,
                len(like_name)
            )
            k = (new.name, new.complete)  # key
            if k in comp_dct and settings.no_completion_duplicates:
                comp_dct[k]._same_name_completions.append(new)
            else:
                comp_dct[k] = new
                yield new


</t>
<t tx="ekr.20180516071750.159">def get_user_scope(module_context, position):
    """
    Returns the scope in which the user resides. This includes flows.
    """
    user_stmt = module_context.tree_node.get_statement_for_position(position)
    if user_stmt is None:
        def scan(scope):
            for s in scope.children:
                if s.start_pos &lt;= position &lt;= s.end_pos:
                    if isinstance(s, (tree.Scope, tree.Flow)):
                        return scan(s) or s
                    elif s.type in ('suite', 'decorated'):
                        return scan(s)
            return None

        scanned_node = scan(module_context.tree_node)
        if scanned_node:
            return module_context.create_context(scanned_node, node_is_context=True)
        return module_context
    else:
        return module_context.create_context(user_stmt)


</t>
<t tx="ekr.20180516071750.16">class ImplicitNSInfo(object):
    """Stores information returned from an implicit namespace spec"""
    @others
# unicode function
try:
    unicode = unicode
except NameError:
    unicode = str

if is_py3:
    u = lambda s: s
else:
    u = lambda s: s.decode('utf-8')

u.__doc__ = """
Decode a raw string into unicode object.  Do nothing in Python 3.
"""

# exec function
if is_py3:
    def exec_function(source, global_map):
        exec(source, global_map)
else:
    eval(compile("""def exec_function(source, global_map):
                        exec source in global_map """, 'blub', 'exec'))

# re-raise function
if is_py3:
    def reraise(exception, traceback):
        raise exception.with_traceback(traceback)
else:
    eval(compile("""
def reraise(exception, traceback):
    raise exception, None, traceback
""", 'blub', 'exec'))

reraise.__doc__ = """
Re-raise `exception` with a `traceback` object.

Usage::

    reraise(Exception, sys.exc_info()[2])

"""

</t>
<t tx="ekr.20180516071750.160">def get_flow_scope_node(module_node, position):
    node = module_node.get_leaf_for_position(position, include_prefixes=True)
    while not isinstance(node, (tree.Scope, tree.Flow)):
        node = node.parent

    return node


</t>
<t tx="ekr.20180516071750.161">class Completion:
    @others
</t>
<t tx="ekr.20180516071750.162">def __init__(self, evaluator, module, code_lines, position, call_signatures_method):
    self._evaluator = evaluator
    self._module_context = module
    self._module_node = module.tree_node
    self._code_lines = code_lines

    # The first step of completions is to get the name
    self._like_name = helpers.get_on_completion_name(self._module_node, code_lines, position)
    # The actual cursor position is not what we need to calculate
    # everything. We want the start of the name we're on.
    self._position = position[0], position[1] - len(self._like_name)
    self._call_signatures_method = call_signatures_method

</t>
<t tx="ekr.20180516071750.163">def completions(self):
    completion_names = self._get_context_completions()

    completions = filter_names(self._evaluator, completion_names,
                               self.stack, self._like_name)

    return sorted(completions, key=lambda x: (x.name.startswith('__'),
                                              x.name.startswith('_'),
                                              x.name.lower()))

</t>
<t tx="ekr.20180516071750.164">def _get_context_completions(self):
    """
    Analyzes the context that a completion is made in and decides what to
    return.

    Technically this works by generating a parser stack and analysing the
    current stack for possible grammar nodes.

    Possible enhancements:
    - global/nonlocal search global
    - yield from / raise from &lt;- could be only exceptions/generators
    - In args: */**: no completion
    - In params (also lambda): no completion before =
    """

    grammar = self._evaluator.grammar

    try:
        self.stack = helpers.get_stack_at_position(
            grammar, self._code_lines, self._module_node, self._position
        )
    except helpers.OnErrorLeaf as e:
        self.stack = None
        if e.error_leaf.value == '.':
            # After ErrorLeaf's that are dots, we will not do any
            # completions since this probably just confuses the user.
            return []
        # If we don't have a context, just use global completion.

        return self._global_completions()

    allowed_keywords, allowed_tokens = \
        helpers.get_possible_completion_types(grammar, self.stack)

    completion_names = list(self._get_keyword_completion_names(allowed_keywords))

    if token.NAME in allowed_tokens or token.INDENT in allowed_tokens:
        # This means that we actually have to do type inference.

        symbol_names = list(self.stack.get_node_names(grammar))

        nodes = list(self.stack.get_nodes())

        if "import_stmt" in symbol_names:
            level = 0
            only_modules = True
            level, names = self._parse_dotted_names(nodes)
            if "import_from" in symbol_names:
                if 'import' in nodes:
                    only_modules = False
            else:
                assert "import_name" in symbol_names

            completion_names += self._get_importer_names(
                names,
                level,
                only_modules
            )
        elif nodes and nodes[-1] in ('as', 'def', 'class'):
            # No completions for ``with x as foo`` and ``import x as foo``.
            # Also true for defining names as a class or function.
            return list(self._get_class_context_completions(is_function=True))
        elif symbol_names[-1] in ('trailer', 'dotted_name') and nodes[-1] == '.':
            dot = self._module_node.get_leaf_for_position(self._position)
            completion_names += self._trailer_completions(dot.get_previous_leaf())
        else:
            completion_names += self._global_completions()
            completion_names += self._get_class_context_completions(is_function=False)

        if 'trailer' in symbol_names:
            call_signatures = self._call_signatures_method()
            completion_names += get_call_signature_param_names(call_signatures)

    return completion_names

</t>
<t tx="ekr.20180516071750.165">def _get_keyword_completion_names(self, keywords_):
    for k in keywords_:
        yield keywords.keyword(self._evaluator, k).name

</t>
<t tx="ekr.20180516071750.166">def _global_completions(self):
    context = get_user_scope(self._module_context, self._position)
    debug.dbg('global completion scope: %s', context)
    flow_scope_node = get_flow_scope_node(self._module_node, self._position)
    filters = get_global_filters(
        self._evaluator,
        context,
        self._position,
        origin_scope=flow_scope_node
    )
    completion_names = []
    for filter in filters:
        completion_names += filter.values()
    return completion_names

</t>
<t tx="ekr.20180516071750.167">def _trailer_completions(self, previous_leaf):
    user_context = get_user_scope(self._module_context, self._position)
    evaluation_context = self._evaluator.create_context(
        self._module_context, previous_leaf
    )
    contexts = evaluate_call_of_leaf(evaluation_context, previous_leaf)
    completion_names = []
    debug.dbg('trailer completion contexts: %s', contexts)
    for context in contexts:
        for filter in context.get_filters(
                search_global=False, origin_scope=user_context.tree_node):
            completion_names += filter.values()
    return completion_names

</t>
<t tx="ekr.20180516071750.168">def _parse_dotted_names(self, nodes):
    level = 0
    names = []
    for node in nodes[1:]:
        if node in ('.', '...'):
            if not names:
                level += len(node.value)
        elif node.type == 'dotted_name':
            names += node.children[::2]
        elif node.type == 'name':
            names.append(node)
        else:
            break
    return level, names

</t>
<t tx="ekr.20180516071750.169">def _get_importer_names(self, names, level=0, only_modules=True):
    names = [n.value for n in names]
    i = imports.Importer(self._evaluator, names, self._module_context, level)
    return i.completion_names(self._evaluator, only_modules=only_modules)

</t>
<t tx="ekr.20180516071750.17">def __init__(self, name, paths):
    self.name = name
    self.paths = paths

</t>
<t tx="ekr.20180516071750.170">def _get_class_context_completions(self, is_function=True):
    """
    Autocomplete inherited methods when overriding in child class.
    """
    leaf = self._module_node.get_leaf_for_position(self._position, include_prefixes=True)
    cls = tree.search_ancestor(leaf, 'classdef')
    if isinstance(cls, (tree.Class, tree.Function)):
        # Complete the methods that are defined in the super classes.
        random_context = self._module_context.create_context(
            cls,
            node_is_context=True
        )
    else:
        return

    if cls.start_pos[1] &gt;= leaf.start_pos[1]:
        return

    filters = random_context.get_filters(search_global=False, is_instance=True)
    # The first dict is the dictionary of class itself.
    next(filters)
    for filter in filters:
        for name in filter.values():
            if (name.api_type == 'function') == is_function:
                yield name
</t>
<t tx="ekr.20180516071750.171">@path C:/Anaconda3/Lib/site-packages/jedi/api/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071750.172">"""
Helpers for the API
"""
import re
from collections import namedtuple

from jedi._compatibility import u
from jedi.evaluate.helpers import evaluate_call_of_leaf
from jedi.parser.python.parser import Parser
from jedi.parser.python import tree
from jedi.parser import tokenize
from jedi.cache import time_cache
from jedi import common


CompletionParts = namedtuple('CompletionParts', ['path', 'has_dot', 'name'])


</t>
<t tx="ekr.20180516071750.173">def sorted_definitions(defs):
    # Note: `or ''` below is required because `module_path` could be
    return sorted(defs, key=lambda x: (x.module_path or '', x.line or 0, x.column or 0))


</t>
<t tx="ekr.20180516071750.174">def get_on_completion_name(module_node, lines, position):
    leaf = module_node.get_leaf_for_position(position)
    if leaf is None or leaf.type in ('string', 'error_leaf'):
        # Completions inside strings are a bit special, we need to parse the
        # string. The same is true for comments and error_leafs.
        line = lines[position[0] - 1]
        # The first step of completions is to get the name
        return re.search(r'(?!\d)\w+$|$', line[:position[1]]).group(0)
    elif leaf.type not in ('name', 'keyword'):
        return ''

    return leaf.value[:position[1] - leaf.start_pos[1]]


</t>
<t tx="ekr.20180516071750.175">def _get_code(code_lines, start_pos, end_pos):
    # Get relevant lines.
    lines = code_lines[start_pos[0] - 1:end_pos[0]]
    # Remove the parts at the end of the line.
    lines[-1] = lines[-1][:end_pos[1]]
    # Remove first line indentation.
    lines[0] = lines[0][start_pos[1]:]
    return '\n'.join(lines)


</t>
<t tx="ekr.20180516071750.176">class OnErrorLeaf(Exception):
    @others
</t>
<t tx="ekr.20180516071750.177">@property
def error_leaf(self):
    return self.args[0]


</t>
<t tx="ekr.20180516071750.178">def _is_on_comment(leaf, position):
    comment_lines = common.splitlines(leaf.prefix)
    difference = leaf.start_pos[0] - position[0]
    prefix_start_pos = leaf.get_start_pos_of_prefix()
    if difference == 0:
        indent = leaf.start_pos[1]
    elif position[0] == prefix_start_pos[0]:
        indent = prefix_start_pos[1]
    else:
        indent = 0
    line = comment_lines[-difference - 1][:position[1] - indent]
    return '#' in line


</t>
<t tx="ekr.20180516071750.179">def _get_code_for_stack(code_lines, module_node, position):
    leaf = module_node.get_leaf_for_position(position, include_prefixes=True)
    # It might happen that we're on whitespace or on a comment. This means
    # that we would not get the right leaf.
    if leaf.start_pos &gt;= position:
        if _is_on_comment(leaf, position):
            return u('')

        # If we're not on a comment simply get the previous leaf and proceed.
        leaf = leaf.get_previous_leaf()
        if leaf is None:
            return u('')  # At the beginning of the file.

    is_after_newline = leaf.type == 'newline'
    while leaf.type == 'newline':
        leaf = leaf.get_previous_leaf()
        if leaf is None:
            return u('')

    if leaf.type == 'error_leaf' or leaf.type == 'string':
        if leaf.start_pos[0] &lt; position[0]:
            # On a different line, we just begin anew.
            return u('')

        # Error leafs cannot be parsed, completion in strings is also
        # impossible.
        raise OnErrorLeaf(leaf)
    else:
        if leaf == ';':
            user_stmt = leaf.parent
        else:
            user_stmt = leaf.get_definition()
        if user_stmt.parent.type == 'simple_stmt':
            user_stmt = user_stmt.parent

        if is_after_newline:
            if user_stmt.start_pos[1] &gt; position[1]:
                # This means that it's actually a dedent and that means that we
                # start without context (part of a suite).
                return u('')

        # This is basically getting the relevant lines.
        return _get_code(code_lines, user_stmt.get_start_pos_of_prefix(), position)


</t>
<t tx="ekr.20180516071750.18">class Python3Method(object):
    @others
</t>
<t tx="ekr.20180516071750.180">def get_stack_at_position(grammar, code_lines, module_node, pos):
    """
    Returns the possible node names (e.g. import_from, xor_test or yield_stmt).
    """
    class EndMarkerReached(Exception):
        pass

    def tokenize_without_endmarker(code):
        tokens = tokenize.source_tokens(code, use_exact_op_types=True)
        for token_ in tokens:
            if token_.string == safeword:
                raise EndMarkerReached()
            else:
                yield token_

    code = _get_code_for_stack(code_lines, module_node, pos)
    # We use a word to tell Jedi when we have reached the start of the
    # completion.
    # Use Z as a prefix because it's not part of a number suffix.
    safeword = 'ZZZ_USER_WANTS_TO_COMPLETE_HERE_WITH_JEDI'
    code = code + safeword

    p = Parser(grammar, error_recovery=True)
    try:
        p.parse(tokens=tokenize_without_endmarker(code))
    except EndMarkerReached:
        return Stack(p.pgen_parser.stack)
    raise SystemError("This really shouldn't happen. There's a bug in Jedi.")


</t>
<t tx="ekr.20180516071750.181">class Stack(list):
    @others
</t>
<t tx="ekr.20180516071750.182">def get_node_names(self, grammar):
    for dfa, state, (node_number, nodes) in self:
        yield grammar.number2symbol[node_number]

</t>
<t tx="ekr.20180516071750.183">def get_nodes(self):
    for dfa, state, (node_number, nodes) in self:
        for node in nodes:
            yield node


</t>
<t tx="ekr.20180516071750.184">def get_possible_completion_types(grammar, stack):
    def add_results(label_index):
        try:
            grammar_labels.append(inversed_tokens[label_index])
        except KeyError:
            try:
                keywords.append(inversed_keywords[label_index])
            except KeyError:
                t, v = grammar.labels[label_index]
                assert t &gt;= 256
                # See if it's a symbol and if we're in its first set
                inversed_keywords
                itsdfa = grammar.dfas[t]
                itsstates, itsfirst = itsdfa
                for first_label_index in itsfirst.keys():
                    add_results(first_label_index)

    inversed_keywords = dict((v, k) for k, v in grammar.keywords.items())
    inversed_tokens = dict((v, k) for k, v in grammar.tokens.items())

    keywords = []
    grammar_labels = []

    def scan_stack(index):
        dfa, state, node = stack[index]
        states, first = dfa
        arcs = states[state]

        for label_index, new_state in arcs:
            if label_index == 0:
                # An accepting state, check the stack below.
                scan_stack(index - 1)
            else:
                add_results(label_index)

    scan_stack(-1)

    return keywords, grammar_labels


</t>
<t tx="ekr.20180516071750.185">def evaluate_goto_definition(evaluator, context, leaf):
    if leaf.type == 'name':
        # In case of a name we can just use goto_definition which does all the
        # magic itself.
        return evaluator.goto_definitions(context, leaf)

    parent = leaf.parent
    if parent.type == 'atom':
        return context.eval_node(leaf.parent)
    elif parent.type == 'trailer':
        return evaluate_call_of_leaf(context, leaf)
    elif isinstance(leaf, tree.Literal):
        return context.evaluator.eval_atom(context, leaf)
    return []


CallSignatureDetails = namedtuple(
    'CallSignatureDetails',
    ['bracket_leaf', 'call_index', 'keyword_name_str']
)


</t>
<t tx="ekr.20180516071750.186">def _get_index_and_key(nodes, position):
    """
    Returns the amount of commas and the keyword argument string.
    """
    nodes_before = [c for c in nodes if c.start_pos &lt; position]
    if nodes_before[-1].type == 'arglist':
        nodes_before = [c for c in nodes_before[-1].children if c.start_pos &lt; position]

    key_str = None

    if nodes_before:
        last = nodes_before[-1]
        if last.type == 'argument' and last.children[1].end_pos &lt;= position:
            # Checked if the argument
            key_str = last.children[0].value
        elif last == '=':
            key_str = nodes_before[-2].value

    return nodes_before.count(','), key_str


</t>
<t tx="ekr.20180516071750.187">def _get_call_signature_details_from_error_node(node, position):
    for index, element in reversed(list(enumerate(node.children))):
        # `index &gt; 0` means that it's a trailer and not an atom.
        if element == '(' and element.end_pos &lt;= position and index &gt; 0:
            # It's an error node, we don't want to match too much, just
            # until the parentheses is enough.
            children = node.children[index:]
            name = element.get_previous_leaf()
            if name is None:
                continue
            if name.type == 'name' or name.parent.type in ('trailer', 'atom'):
                return CallSignatureDetails(
                    element,
                    *_get_index_and_key(children, position)
                )


</t>
<t tx="ekr.20180516071750.188">def get_call_signature_details(module, position):
    leaf = module.get_leaf_for_position(position, include_prefixes=True)
    if leaf.start_pos &gt;= position:
        # Whitespace / comments after the leaf count towards the previous leaf.
        leaf = leaf.get_previous_leaf()
        if leaf is None:
            return None

    if leaf == ')':
        if leaf.end_pos == position:
            leaf = leaf.get_next_leaf()

    # Now that we know where we are in the syntax tree, we start to look at
    # parents for possible function definitions.
    node = leaf.parent
    while node is not None:
        if node.type in ('funcdef', 'classdef'):
            # Don't show call signatures if there's stuff before it that just
            # makes it feel strange to have a call signature.
            return None

        for n in node.children[::-1]:
            if n.start_pos &lt; position and n.type == 'error_node':
                result = _get_call_signature_details_from_error_node(n, position)
                if result is not None:
                    return result

        if node.type == 'trailer' and node.children[0] == '(':
            leaf = node.get_previous_leaf()
            if leaf is None:
                return None
            return CallSignatureDetails(
                node.children[0], *_get_index_and_key(node.children, position))

        node = node.parent

    return None


</t>
<t tx="ekr.20180516071750.189">@time_cache("call_signatures_validity")
def cache_call_signatures(evaluator, context, bracket_leaf, code_lines, user_pos):
    """This function calculates the cache key."""
    index = user_pos[0] - 1

    before_cursor = code_lines[index][:user_pos[1]]
    other_lines = code_lines[bracket_leaf.start_pos[0]:index]
    whole = '\n'.join(other_lines + [before_cursor])
    before_bracket = re.match(r'.*\(', whole, re.DOTALL)

    module_path = context.get_root_context().py__file__()
    if module_path is None:
        yield None  # Don't cache!
    else:
        yield (module_path, before_bracket, bracket_leaf.start_pos)
    yield evaluate_goto_definition(
        evaluator,
        context,
        bracket_leaf.get_previous_leaf()
    )
</t>
<t tx="ekr.20180516071750.19">def __init__(self, func):
    self.func = func

</t>
<t tx="ekr.20180516071750.190">@path C:/Anaconda3/Lib/site-packages/jedi/api/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071750.191">"""
TODO Some parts of this module are still not well documented.
"""

from jedi.evaluate.representation import ModuleContext
from jedi.evaluate import compiled
from jedi.evaluate.compiled import mixed
from jedi.evaluate.context import Context


</t>
<t tx="ekr.20180516071750.192">class MixedModuleContext(Context):
    resets_positions = True
    type = 'mixed_module'

    @others
</t>
<t tx="ekr.20180516071750.193">def __init__(self, evaluator, tree_module, namespaces, path):
    self.evaluator = evaluator
    self._namespaces = namespaces

    self._namespace_objects = [type('jedi_namespace', (), n) for n in namespaces]
    self._module_context = ModuleContext(evaluator, tree_module, path=path)
    self.tree_node = tree_module

</t>
<t tx="ekr.20180516071750.194">def get_node(self):
    return self.tree_node

</t>
<t tx="ekr.20180516071750.195">def get_filters(self, *args, **kwargs):
    for filter in self._module_context.get_filters(*args, **kwargs):
        yield filter

    for namespace_obj in self._namespace_objects:
        compiled_object = compiled.create(self.evaluator, namespace_obj)
        mixed_object = mixed.MixedObject(
            self.evaluator,
            parent_context=self,
            compiled_object=compiled_object,
            tree_context=self._module_context
        )
        for filter in mixed_object.get_filters(*args, **kwargs):
            yield filter

</t>
<t tx="ekr.20180516071750.196">def __getattr__(self, name):
    return getattr(self._module_context, name)
</t>
<t tx="ekr.20180516071750.197">@path C:/Anaconda3/Lib/site-packages/jedi/api/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071750.198">import pydoc
import keyword

from jedi._compatibility import is_py3, is_py35
from jedi import common
from jedi.evaluate.filters import AbstractNameDefinition
from jedi.parser.python.tree import Leaf

try:
    from pydoc_data import topics as pydoc_topics
except ImportError:
    # Python 2
    try:
        import pydoc_topics
    except ImportError:
        # This is for Python 3 embeddable version, which dont have
        # pydoc_data module in its file python3x.zip.
        pydoc_topics = None

if is_py3:
    if is_py35:
        # in python 3.5 async and await are not proper keywords, but for
        # completion pursposes should as as though they are
        keys = keyword.kwlist + ["async", "await"]
    else:
        keys = keyword.kwlist
else:
    keys = keyword.kwlist + ['None', 'False', 'True']


</t>
<t tx="ekr.20180516071750.199">def has_inappropriate_leaf_keyword(pos, module):
    relevant_errors = filter(
        lambda error: error.first_pos[0] == pos[0],
        module.error_statement_stacks)

    for error in relevant_errors:
        if error.next_token in keys:
            return True

    return False


</t>
<t tx="ekr.20180516071750.2">"""
This module contains variables with global |jedi| settings. To change the
behavior of |jedi|, change the variables defined in :mod:`jedi.settings`.

Plugins should expose an interface so that the user can adjust the
configuration.


Example usage::

    from jedi import settings
    settings.case_insensitive_completion = True


Completion output
~~~~~~~~~~~~~~~~~

.. autodata:: case_insensitive_completion
.. autodata:: add_bracket_after_function
.. autodata:: no_completion_duplicates


Filesystem cache
~~~~~~~~~~~~~~~~

.. autodata:: cache_directory
.. autodata:: use_filesystem_cache


Parser
~~~~~~

.. autodata:: fast_parser


Dynamic stuff
~~~~~~~~~~~~~

.. autodata:: dynamic_array_additions
.. autodata:: dynamic_params
.. autodata:: dynamic_params_for_other_modules
.. autodata:: additional_dynamic_modules
.. autodata:: auto_import_modules


.. _settings-recursion:

Recursions
~~~~~~~~~~

Recursion settings are important if you don't want extremly
recursive python code to go absolutely crazy. First of there is a
global limit :data:`max_executions`. This limit is important, to set
a maximum amount of time, the completion may use.

The default values are based on experiments while completing the |jedi| library
itself (inception!). But I don't think there's any other Python library that
uses recursion in a similarly extreme way. These settings make the completion
definitely worse in some cases. But a completion should also be fast.

.. autodata:: max_until_execution_unique
.. autodata:: max_function_recursion_level
.. autodata:: max_executions_without_builtins
.. autodata:: max_executions
.. autodata:: scale_call_signatures


Caching
~~~~~~~

.. autodata:: call_signatures_validity


"""
import os
import platform

# ----------------
# completion output settings
# ----------------

case_insensitive_completion = True
"""
The completion is by default case insensitive.
"""

add_bracket_after_function = False
"""
Adds an opening bracket after a function, because that's normal behaviour.
Removed it again, because in VIM that is not very practical.
"""

no_completion_duplicates = True
"""
If set, completions with the same name don't appear in the output anymore,
but are in the `same_name_completions` attribute.
"""

# ----------------
# Filesystem cache
# ----------------

use_filesystem_cache = True
"""
Use filesystem cache to save once parsed files with pickle.
"""

if platform.system().lower() == 'windows':
    _cache_directory = os.path.join(os.getenv('APPDATA') or '~', 'Jedi',
                                    'Jedi')
elif platform.system().lower() == 'darwin':
    _cache_directory = os.path.join('~', 'Library', 'Caches', 'Jedi')
else:
    _cache_directory = os.path.join(os.getenv('XDG_CACHE_HOME') or '~/.cache',
                                    'jedi')
cache_directory = os.path.expanduser(_cache_directory)
"""
The path where all the caches can be found.

On Linux, this defaults to ``~/.cache/jedi/``, on OS X to
``~/Library/Caches/Jedi/`` and on Windows to ``%APPDATA%\\Jedi\\Jedi\\``.
On Linux, if environment variable ``$XDG_CACHE_HOME`` is set,
``$XDG_CACHE_HOME/jedi`` is used instead of the default one.
"""

# ----------------
# parser
# ----------------

fast_parser = True
"""
Use the fast parser. This means that reparsing is only being done if
something has been changed e.g. to a function. If this happens, only the
function is being reparsed.
"""

# ----------------
# dynamic stuff
# ----------------

dynamic_array_additions = True
"""
check for `append`, etc. on arrays: [], {}, () as well as list/set calls.
"""

dynamic_params = True
"""
A dynamic param completion, finds the callees of the function, which define
the params of a function.
"""

dynamic_params_for_other_modules = True
"""
Do the same for other modules.
"""

additional_dynamic_modules = []
"""
Additional modules in which |jedi| checks if statements are to be found. This
is practical for IDEs, that want to administrate their modules themselves.
"""

dynamic_flow_information = True
"""
Check for `isinstance` and other information to infer a type.
"""

auto_import_modules = [
    'hashlib',  # setattr
]
"""
Modules that are not analyzed but imported, although they contain Python code.
This improves autocompletion for libraries that use ``setattr`` or
``globals()`` modifications a lot.
"""

# ----------------
# recursions
# ----------------

max_until_execution_unique = 50
"""
This limit is probably the most important one, because if this limit is
exceeded, functions can only be one time executed. So new functions will be
executed, complex recursions with the same functions again and again, are
ignored.
"""

max_function_recursion_level = 5
"""
`max_function_recursion_level` is more about whether the recursions are
stopped in deepth or in width. The ratio beetween this and
`max_until_execution_unique` is important here. It stops a recursion (after
the number of function calls in the recursion), if it was already used
earlier.
"""

max_executions_without_builtins = 200
"""
.. todo:: Document this.
"""

max_executions = 250
"""
A maximum amount of time, the completion may use.
"""

scale_call_signatures = 0.1
"""
Because call_signatures is normally used on every single key hit, it has
to be faster than a normal completion. This is the factor that is used to
scale `max_executions` and `max_until_execution_unique`:
"""

# ----------------
# caching validity (time)
# ----------------

call_signatures_validity = 3.0
"""
Finding function calls might be slow (0.1-0.5s). This is not acceptible for
normal writing. Therefore cache it for a short time.
"""
</t>
<t tx="ekr.20180516071750.20">def __get__(self, obj, objtype):
    if obj is None:
        return lambda *args, **kwargs: self.func(*args, **kwargs)
    else:
        return lambda *args, **kwargs: self.func(obj, *args, **kwargs)


</t>
<t tx="ekr.20180516071750.200">def completion_names(evaluator, stmt, pos, module):
    keyword_list = all_keywords(evaluator)

    if not isinstance(stmt, Leaf) or has_inappropriate_leaf_keyword(pos, module):
        keyword_list = filter(
            lambda keyword: not keyword.only_valid_as_leaf,
            keyword_list
        )
    return [keyword.name for keyword in keyword_list]


</t>
<t tx="ekr.20180516071750.201">def all_keywords(evaluator, pos=(0, 0)):
    return set([Keyword(evaluator, k, pos) for k in keys])


</t>
<t tx="ekr.20180516071750.202">def keyword(evaluator, string, pos=(0, 0)):
    if string in keys:
        return Keyword(evaluator, string, pos)
    else:
        return None


</t>
<t tx="ekr.20180516071750.203">def get_operator(evaluator, string, pos):
    return Keyword(evaluator, string, pos)


keywords_only_valid_as_leaf = (
    'continue',
    'break',
)


</t>
<t tx="ekr.20180516071750.204">class KeywordName(AbstractNameDefinition):
    api_type = 'keyword'

    @others
</t>
<t tx="ekr.20180516071750.205">def __init__(self, evaluator, name):
    self.string_name = name
    self.parent_context = evaluator.BUILTINS

</t>
<t tx="ekr.20180516071750.206">def eval(self):
    return set()


</t>
<t tx="ekr.20180516071750.207">class Keyword(object):
    api_type = 'keyword'

    @others
</t>
<t tx="ekr.20180516071750.208">def __init__(self, evaluator, name, pos):
    self.name = KeywordName(evaluator, name)
    self.start_pos = pos
    self.parent = evaluator.BUILTINS

</t>
<t tx="ekr.20180516071750.209">@property
def only_valid_as_leaf(self):
    return self.name.value in keywords_only_valid_as_leaf

</t>
<t tx="ekr.20180516071750.21">def use_metaclass(meta, *bases):
    """ Create a class with a metaclass. """
    if not bases:
        bases = (object,)
    return meta("HackClass", bases, {})


try:
    encoding = sys.stdout.encoding
    if encoding is None:
        encoding = 'utf-8'
except AttributeError:
    encoding = 'ascii'


</t>
<t tx="ekr.20180516071750.210">@property
def names(self):
    """ For a `parsing.Name` like comparision """
    return [self.name]

</t>
<t tx="ekr.20180516071750.211">@property
def docstr(self):
    return imitate_pydoc(self.name)

</t>
<t tx="ekr.20180516071750.212">def __repr__(self):
    return '&lt;%s: %s&gt;' % (type(self).__name__, self.name)


</t>
<t tx="ekr.20180516071750.213">def imitate_pydoc(string):
    """
    It's not possible to get the pydoc's without starting the annoying pager
    stuff.
    """
    if pydoc_topics is None:
        return ''

    # str needed because of possible unicode stuff in py2k (pydoc doesn't work
    # with unicode strings)
    string = str(string)
    h = pydoc.help
    with common.ignored(KeyError):
        # try to access symbols
        string = h.symbols[string]
        string, _, related = string.partition(' ')

    get_target = lambda s: h.topics.get(s, h.keywords.get(s))
    while isinstance(string, str):
        string = get_target(string)

    try:
        # is a tuple now
        label, related = string
    except TypeError:
        return ''

    try:
        return pydoc_topics.topics[label] if pydoc_topics else ''
    except KeyError:
        return ''
</t>
<t tx="ekr.20180516071750.214">@path C:/Anaconda3/Lib/site-packages/jedi/api/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071750.215">"""
To use Jedi completion in Python interpreter, add the following in your shell
setup (e.g., ``.bashrc``)::

    export PYTHONSTARTUP="$(python -m jedi repl)"

Then you will be able to use Jedi completer in your Python interpreter::

    $ python
    Python 2.7.2+ (default, Jul 20 2012, 22:15:08)
    [GCC 4.6.1] on linux2
    Type "help", "copyright", "credits" or "license" for more information.
    &gt;&gt;&gt; import os
    &gt;&gt;&gt; os.path.join().split().in&lt;TAB&gt;                     # doctest: +SKIP
    os.path.join().split().index   os.path.join().split().insert

"""
import jedi.utils
from jedi import __version__ as __jedi_version__

print('REPL completion using Jedi %s' % __jedi_version__)
jedi.utils.setup_readline()

del jedi

# Note: try not to do many things here, as it will contaminate global
# namespace of the interpreter.
</t>
<t tx="ekr.20180516071750.216">@path C:/Anaconda3/Lib/site-packages/jedi/api/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071750.217">from jedi.api import classes
from jedi.parser.python import tree
from jedi.evaluate import imports
from jedi.evaluate.filters import TreeNameDefinition
from jedi.evaluate.representation import ModuleContext


</t>
<t tx="ekr.20180516071750.218">def compare_contexts(c1, c2):
    return c1 == c2 or (c1[1] == c2[1] and c1[0].tree_node == c2[0].tree_node)


</t>
<t tx="ekr.20180516071750.219">def usages(evaluator, definition_names, mods):
    """
    :param definitions: list of Name
    """
    def resolve_names(definition_names):
        for name in definition_names:
            if name.api_type == 'module':
                found = False
                for context in name.infer():
                    if isinstance(context, ModuleContext):
                        found = True
                        yield context.name
                if not found:
                    yield name
            else:
                yield name

    def compare_array(definition_names):
        """ `definitions` are being compared by module/start_pos, because
        sometimes the id's of the objects change (e.g. executions).
        """
        return [
            (name.get_root_context(), name.start_pos)
            for name in resolve_names(definition_names)
        ]

    search_name = list(definition_names)[0].string_name
    compare_definitions = compare_array(definition_names)
    mods = mods | set([d.get_root_context() for d in definition_names])
    definition_names = set(resolve_names(definition_names))
    for m in imports.get_modules_containing_name(evaluator, mods, search_name):
        if isinstance(m, ModuleContext):
            for name_node in m.tree_node.used_names.get(search_name, []):
                context = evaluator.create_context(m, name_node)
                result = evaluator.goto(context, name_node)
                if any(compare_contexts(c1, c2)
                       for c1 in compare_array(result)
                       for c2 in compare_definitions):
                    name = TreeNameDefinition(context, name_node)
                    definition_names.add(name)
                    # Previous definitions might be imports, so include them
                    # (because goto might return that import name).
                    compare_definitions += compare_array([name])
        else:
            # compiled objects
            definition_names.add(m.name)

    return [classes.Definition(evaluator, n) for n in definition_names]


</t>
<t tx="ekr.20180516071750.22">def u(string):
    """Cast to unicode DAMMIT!
    Written because Python2 repr always implicitly casts to a string, so we
    have to cast back to a unicode (and we now that we always deal with valid
    unicode, because we check that in the beginning).
    """
    if is_py3:
        return str(string)

    if not isinstance(string, unicode):
        return unicode(str(string), 'UTF-8')
    return string

try:
    import builtins  # module name in python 3
except ImportError:
    import __builtin__ as builtins


import ast


</t>
<t tx="ekr.20180516071750.220">def resolve_potential_imports(evaluator, definitions):
    """ Adds the modules of the imports """
    new = set()
    for d in definitions:
        if isinstance(d, TreeNameDefinition):
            imp_or_stmt = d.tree_name.get_definition()
            if isinstance(imp_or_stmt, tree.Import):
                new |= resolve_potential_imports(
                    evaluator,
                    set(imports.infer_import(
                        d.parent_context, d.tree_name, is_goto=True
                    ))
                )
    return set(definitions) | new
</t>
<t tx="ekr.20180516071750.221">@path C:/Anaconda3/Lib/site-packages/jedi/api/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071750.222">"""
The API basically only provides one class. You can create a :class:`Script` and
use its methods.

Additionally you can add a debug function with :func:`set_debug_function`.
Alternatively, if you don't need a custom function and are happy with printing
debug messages to stdout, simply call :func:`set_debug_function` without
arguments.

.. warning:: Please, note that Jedi is **not thread safe**.
"""
import os
import warnings
import sys

from jedi.parser.python import load_grammar
from jedi.parser.python import tree
from jedi.parser.python import parse
from jedi import debug
from jedi import settings
from jedi import common
from jedi import cache
from jedi.api import classes
from jedi.api import interpreter
from jedi.api import usages
from jedi.api import helpers
from jedi.api.completion import Completion
from jedi.evaluate import Evaluator
from jedi.evaluate import representation as er
from jedi.evaluate import imports
from jedi.evaluate.param import try_iter_content
from jedi.evaluate.helpers import get_module_names
from jedi.evaluate.sys_path import get_venv_path
from jedi.evaluate.iterable import unpack_tuple_to_dict
from jedi.evaluate.filters import TreeNameDefinition

# Jedi uses lots and lots of recursion. By setting this a little bit higher, we
# can remove some "maximum recursion depth" errors.
sys.setrecursionlimit(2000)


</t>
<t tx="ekr.20180516071750.223">class NotFoundError(Exception):
    """A custom error to avoid catching the wrong exceptions.

    .. deprecated:: 0.9.0
       Not in use anymore, Jedi just returns no goto result if you're not on a
       valid name.
    .. todo:: Remove!
    """


</t>
<t tx="ekr.20180516071750.224">class Script(object):
    """
    A Script is the base for completions, goto or whatever you want to do with
    |jedi|.

    You can either use the ``source`` parameter or ``path`` to read a file.
    Usually you're going to want to use both of them (in an editor).

    The script might be analyzed in a different ``sys.path`` than |jedi|:

    - if `sys_path` parameter is not ``None``, it will be used as ``sys.path``
      for the script;

    - if `sys_path` parameter is ``None`` and ``VIRTUAL_ENV`` environment
      variable is defined, ``sys.path`` for the specified environment will be
      guessed (see :func:`jedi.evaluate.sys_path.get_venv_path`) and used for
      the script;

    - otherwise ``sys.path`` will match that of |jedi|.

    :param source: The source code of the current file, separated by newlines.
    :type source: str
    :param line: The line to perform actions on (starting with 1).
    :type line: int
    :param column: The column of the cursor (starting with 0).
    :type column: int
    :param path: The path of the file in the file system, or ``''`` if
        it hasn't been saved yet.
    :type path: str or None
    :param encoding: The encoding of ``source``, if it is not a
        ``unicode`` object (default ``'utf-8'``).
    :type encoding: str
    :param source_encoding: The encoding of ``source``, if it is not a
        ``unicode`` object (default ``'utf-8'``).
    :type encoding: str
    :param sys_path: ``sys.path`` to use during analysis of the script
    :type sys_path: list

    """
    @others
</t>
<t tx="ekr.20180516071750.225">def __init__(self, source=None, line=None, column=None, path=None,
             encoding='utf-8', source_path=None, source_encoding=None,
             sys_path=None):
    if source_path is not None:
        warnings.warn("Use path instead of source_path.", DeprecationWarning)
        path = source_path
    if source_encoding is not None:
        warnings.warn("Use encoding instead of source_encoding.", DeprecationWarning)
        encoding = source_encoding

    self._orig_path = path
    # An empty path (also empty string) should always result in no path.
    self.path = os.path.abspath(path) if path else None

    if source is None:
        # TODO add a better warning than the traceback!
        with open(path, 'rb') as f:
            source = f.read()

    self._source = common.source_to_unicode(source, encoding)
    self._code_lines = common.splitlines(self._source)
    line = max(len(self._code_lines), 1) if line is None else line
    if not (0 &lt; line &lt;= len(self._code_lines)):
        raise ValueError('`line` parameter is not in a valid range.')

    line_len = len(self._code_lines[line - 1])
    column = line_len if column is None else column
    if not (0 &lt;= column &lt;= line_len):
        raise ValueError('`column` parameter is not in a valid range.')
    self._pos = line, column
    self._path = path

    cache.clear_time_caches()
    debug.reset_time()
    self._grammar = load_grammar(version='%s.%s' % sys.version_info[:2])
    if sys_path is None:
        venv = os.getenv('VIRTUAL_ENV')
        if venv:
            sys_path = list(get_venv_path(venv))
    self._evaluator = Evaluator(self._grammar, sys_path=sys_path)
    debug.speed('init')

</t>
<t tx="ekr.20180516071750.226">@cache.memoize_method
def _get_module_node(self):
    return parse(
        code=self._source,
        path=self.path,
        grammar=self._grammar,
        cache=False,  # No disk cache, because the current script often changes.
        diff_cache=True,
    )

</t>
<t tx="ekr.20180516071750.227">@cache.memoize_method
def _get_module(self):
    module = er.ModuleContext(
        self._evaluator,
        self._get_module_node(),
        self.path
    )
    imports.add_module(self._evaluator, module.name.string_name, module)
    return module

</t>
<t tx="ekr.20180516071750.228">@property
def source_path(self):
    """
    .. deprecated:: 0.7.0
       Use :attr:`.path` instead.
    .. todo:: Remove!
    """
    warnings.warn("Use path instead of source_path.", DeprecationWarning)
    return self.path

</t>
<t tx="ekr.20180516071750.229">def __repr__(self):
    return '&lt;%s: %s&gt;' % (self.__class__.__name__, repr(self._orig_path))

</t>
<t tx="ekr.20180516071750.23">def literal_eval(string):
    # py3.0, py3.1 and py32 don't support unicode literals. Support those, I
    # don't want to write two versions of the tokenizer.
    if is_py3 and sys.version_info.minor &lt; 3:
        if re.match('[uU][\'"]', string):
            string = string[1:]
    return ast.literal_eval(string)


try:
    from itertools import zip_longest
except ImportError:
    from itertools import izip_longest as zip_longest  # Python 2

try:
    FileNotFoundError = FileNotFoundError
except NameError:
    FileNotFoundError = IOError


</t>
<t tx="ekr.20180516071750.230">def completions(self):
    """
    Return :class:`classes.Completion` objects. Those objects contain
    information about the completions, more than just names.

    :return: Completion objects, sorted by name and __ comes last.
    :rtype: list of :class:`classes.Completion`
    """
    debug.speed('completions start')
    completion = Completion(
        self._evaluator, self._get_module(), self._code_lines,
        self._pos, self.call_signatures
    )
    completions = completion.completions()
    debug.speed('completions end')
    return completions

</t>
<t tx="ekr.20180516071750.231">def goto_definitions(self):
    """
    Return the definitions of a the path under the cursor.  goto function!
    This follows complicated paths and returns the end, not the first
    definition. The big difference between :meth:`goto_assignments` and
    :meth:`goto_definitions` is that :meth:`goto_assignments` doesn't
    follow imports and statements. Multiple objects may be returned,
    because Python itself is a dynamic language, which means depending on
    an option you can have two different versions of a function.

    :rtype: list of :class:`classes.Definition`
    """
    module_node = self._get_module_node()
    leaf = module_node.name_for_position(self._pos)
    if leaf is None:
        leaf = module_node.get_leaf_for_position(self._pos)
        if leaf is None:
            return []

    context = self._evaluator.create_context(self._get_module(), leaf)
    definitions = helpers.evaluate_goto_definition(self._evaluator, context, leaf)

    names = [s.name for s in definitions]
    defs = [classes.Definition(self._evaluator, name) for name in names]
    # The additional set here allows the definitions to become unique in an
    # API sense. In the internals we want to separate more things than in
    # the API.
    return helpers.sorted_definitions(set(defs))

</t>
<t tx="ekr.20180516071750.232">def goto_assignments(self, follow_imports=False):
    """
    Return the first definition found, while optionally following imports.
    Multiple objects may be returned, because Python itself is a
    dynamic language, which means depending on an option you can have two
    different versions of a function.

    :rtype: list of :class:`classes.Definition`
    """
    def filter_follow_imports(names):
        for name in names:
            if isinstance(name, (imports.ImportName, TreeNameDefinition)):
                for context in name.infer():
                    yield context.name
            else:
                yield name

    names = self._goto()
    if follow_imports:
        names = filter_follow_imports(names)

    defs = [classes.Definition(self._evaluator, d) for d in set(names)]
    return helpers.sorted_definitions(defs)

</t>
<t tx="ekr.20180516071750.233">def _goto(self):
    """
    Used for goto_assignments and usages.
    """
    name = self._get_module_node().name_for_position(self._pos)
    if name is None:
        return []
    context = self._evaluator.create_context(self._get_module(), name)
    return list(self._evaluator.goto(context, name))

</t>
<t tx="ekr.20180516071750.234">def usages(self, additional_module_paths=()):
    """
    Return :class:`classes.Definition` objects, which contain all
    names that point to the definition of the name under the cursor. This
    is very useful for refactoring (renaming), or to show all usages of a
    variable.

    .. todo:: Implement additional_module_paths

    :rtype: list of :class:`classes.Definition`
    """
    temp, settings.dynamic_flow_information = \
        settings.dynamic_flow_information, False
    try:
        module_node = self._get_module_node()
        user_stmt = module_node.get_statement_for_position(self._pos)
        definition_names = self._goto()
        if not definition_names and isinstance(user_stmt, tree.Import):
            # For not defined imports (goto doesn't find something, we take
            # the name as a definition. This is enough, because every name
            # points to it.
            name = user_stmt.name_for_position(self._pos)
            if name is None:
                # Must be syntax
                return []
            definition_names = [TreeNameDefinition(self._get_module(), name)]

        if not definition_names:
            # Without a definition for a name we cannot find references.
            return []

        definition_names = usages.resolve_potential_imports(self._evaluator,
                                                            definition_names)

        modules = set([d.get_root_context() for d in definition_names])
        modules.add(self._get_module())
        definitions = usages.usages(self._evaluator, definition_names, modules)
    finally:
        settings.dynamic_flow_information = temp

    return helpers.sorted_definitions(set(definitions))

</t>
<t tx="ekr.20180516071750.235">def call_signatures(self):
    """
    Return the function object of the call you're currently in.

    E.g. if the cursor is here::

        abs(# &lt;-- cursor is here

    This would return the ``abs`` function. On the other hand::

        abs()# &lt;-- cursor is here

    This would return an empty list..

    :rtype: list of :class:`classes.CallSignature`
    """
    call_signature_details = \
        helpers.get_call_signature_details(self._get_module_node(), self._pos)
    if call_signature_details is None:
        return []

    context = self._evaluator.create_context(
        self._get_module(),
        call_signature_details.bracket_leaf
    )
    with common.scale_speed_settings(settings.scale_call_signatures):
        definitions = helpers.cache_call_signatures(
            self._evaluator,
            context,
            call_signature_details.bracket_leaf,
            self._code_lines,
            self._pos
        )
    debug.speed('func_call followed')

    return [classes.CallSignature(self._evaluator, d.name,
                                  call_signature_details.bracket_leaf.start_pos,
                                  call_signature_details.call_index,
                                  call_signature_details.keyword_name_str)
            for d in definitions if hasattr(d, 'py__call__')]

</t>
<t tx="ekr.20180516071750.236">def _analysis(self):
    self._evaluator.is_analysis = True
    module_node = self._get_module_node()
    self._evaluator.analysis_modules = [module_node]
    try:
        for node in module_node.nodes_to_execute():
            context = self._get_module().create_context(node)
            if node.type in ('funcdef', 'classdef'):
                # TODO This is stupid, should be private
                from jedi.evaluate.finder import _name_to_types
                # Resolve the decorators.
                _name_to_types(self._evaluator, context, node.children[1])
            elif isinstance(node, tree.Import):
                import_names = set(node.get_defined_names())
                if node.is_nested():
                    import_names |= set(path[-1] for path in node.paths())
                for n in import_names:
                    imports.infer_import(context, n)
            elif node.type == 'expr_stmt':
                types = context.eval_node(node)
                for testlist in node.children[:-1:2]:
                    # Iterate tuples.
                    unpack_tuple_to_dict(context, types, testlist)
            else:
                try_iter_content(self._evaluator.goto_definitions(context, node))
            self._evaluator.reset_recursion_limitations()

        ana = [a for a in self._evaluator.analysis if self.path == a.path]
        return sorted(set(ana), key=lambda x: x.line)
    finally:
        self._evaluator.is_analysis = False


</t>
<t tx="ekr.20180516071750.237">class Interpreter(Script):
    """
    Jedi API for Python REPLs.

    In addition to completion of simple attribute access, Jedi
    supports code completion based on static code analysis.
    Jedi can complete attributes of object which is not initialized
    yet.

    &gt;&gt;&gt; from os.path import join
    &gt;&gt;&gt; namespace = locals()
    &gt;&gt;&gt; script = Interpreter('join("").up', [namespace])
    &gt;&gt;&gt; print(script.completions()[0].name)
    upper
    """

    @others
</t>
<t tx="ekr.20180516071750.238">def __init__(self, source, namespaces, **kwds):
    """
    Parse `source` and mixin interpreted Python objects from `namespaces`.

    :type source: str
    :arg  source: Code to parse.
    :type namespaces: list of dict
    :arg  namespaces: a list of namespace dictionaries such as the one
                      returned by :func:`locals`.

    Other optional arguments are same as the ones for :class:`Script`.
    If `line` and `column` are None, they are assumed be at the end of
    `source`.
    """
    try:
        namespaces = [dict(n) for n in namespaces]
    except Exception:
        raise TypeError("namespaces must be a non-empty list of dicts.")

    super(Interpreter, self).__init__(source, **kwds)
    self.namespaces = namespaces

</t>
<t tx="ekr.20180516071750.239">def _get_module(self):
    parser_module = super(Interpreter, self)._get_module_node()
    return interpreter.MixedModuleContext(
        self._evaluator,
        parser_module,
        self.namespaces,
        path=self.path
    )


</t>
<t tx="ekr.20180516071750.24">def no_unicode_pprint(dct):
    """
    Python 2/3 dict __repr__ may be different, because of unicode differens
    (with or without a `u` prefix). Normally in doctests we could use `pprint`
    to sort dicts and check for equality, but here we have to write a separate
    function to do that.
    """
    import pprint
    s = pprint.pformat(dct)
    print(re.sub("u'", "'", s))


</t>
<t tx="ekr.20180516071750.240">def defined_names(source, path=None, encoding='utf-8'):
    """
    Get all definitions in `source` sorted by its position.

    This functions can be used for listing functions, classes and
    data defined in a file.  This can be useful if you want to list
    them in "sidebar".  Each element in the returned list also has
    `defined_names` method which can be used to get sub-definitions
    (e.g., methods in class).

    :rtype: list of classes.Definition

    .. deprecated:: 0.9.0
       Use :func:`names` instead.
    .. todo:: Remove!
    """
    warnings.warn("Use call_signatures instead.", DeprecationWarning)
    return names(source, path, encoding)


</t>
<t tx="ekr.20180516071750.241">def names(source=None, path=None, encoding='utf-8', all_scopes=False,
          definitions=True, references=False):
    """
    Returns a list of `Definition` objects, containing name parts.
    This means you can call ``Definition.goto_assignments()`` and get the
    reference of a name.
    The parameters are the same as in :py:class:`Script`, except or the
    following ones:

    :param all_scopes: If True lists the names of all scopes instead of only
        the module namespace.
    :param definitions: If True lists the names that have been defined by a
        class, function or a statement (``a = b`` returns ``a``).
    :param references: If True lists all the names that are not listed by
        ``definitions=True``. E.g. ``a = b`` returns ``b``.
    """
    def def_ref_filter(_def):
        is_def = _def._name.tree_name.is_definition()
        return definitions and is_def or references and not is_def

    # Set line/column to a random position, because they don't matter.
    script = Script(source, line=1, column=0, path=path, encoding=encoding)
    module_context = script._get_module()
    defs = [
        classes.Definition(
            script._evaluator,
            TreeNameDefinition(
                module_context.create_context(name.parent),
                name
            )
        ) for name in get_module_names(script._get_module_node(), all_scopes)
    ]
    return sorted(filter(def_ref_filter, defs), key=lambda x: (x.line, x.column))


</t>
<t tx="ekr.20180516071750.242">def preload_module(*modules):
    """
    Preloading modules tells Jedi to load a module now, instead of lazy parsing
    of modules. Usful for IDEs, to control which modules to load on startup.

    :param modules: different module names, list of string.
    """
    for m in modules:
        s = "import %s as x; x." % m
        Script(s, 1, len(s), None).completions()


</t>
<t tx="ekr.20180516071750.243">def set_debug_function(func_cb=debug.print_to_stdout, warnings=True,
                       notices=True, speed=True):
    """
    Define a callback debug function to get all the debug messages.

    If you don't specify any arguments, debug messages will be printed to stdout.

    :param func_cb: The callback function for debug messages, with n params.
    """
    debug.debug_function = func_cb
    debug.enable_warning = warnings
    debug.enable_notice = notices
    debug.enable_speed = speed
</t>
<t tx="ekr.20180516071750.245"></t>
<t tx="ekr.20180516071750.246">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071750.247">"""
Module for statical analysis.
"""
from jedi import debug
from jedi.parser.python import tree
from jedi.evaluate.compiled import CompiledObject


CODES = {
    'attribute-error': (1, AttributeError, 'Potential AttributeError.'),
    'name-error': (2, NameError, 'Potential NameError.'),
    'import-error': (3, ImportError, 'Potential ImportError.'),
    'type-error-too-many-arguments': (4, TypeError, None),
    'type-error-too-few-arguments': (5, TypeError, None),
    'type-error-keyword-argument': (6, TypeError, None),
    'type-error-multiple-values': (7, TypeError, None),
    'type-error-star-star': (8, TypeError, None),
    'type-error-star': (9, TypeError, None),
    'type-error-operation': (10, TypeError, None),
    'type-error-not-iterable': (11, TypeError, None),
    'type-error-isinstance': (12, TypeError, None),
    'type-error-not-subscriptable': (13, TypeError, None),
    'value-error-too-many-values': (14, ValueError, None),
    'value-error-too-few-values': (15, ValueError, None),
}


</t>
<t tx="ekr.20180516071750.248">class Error(object):
    @others
</t>
<t tx="ekr.20180516071750.249">def __init__(self, name, module_path, start_pos, message=None):
    self.path = module_path
    self._start_pos = start_pos
    self.name = name
    if message is None:
        message = CODES[self.name][2]
    self.message = message

</t>
<t tx="ekr.20180516071750.25">def utf8_repr(func):
    """
    ``__repr__`` methods in Python 2 don't allow unicode objects to be
    returned. Therefore cast them to utf-8 bytes in this decorator.
    """
    def wrapper(self):
        result = func(self)
        if isinstance(result, unicode):
            return result.encode('utf-8')
        else:
            return result

    if is_py3:
        return func
    else:
        return wrapper
</t>
<t tx="ekr.20180516071750.250">@property
def line(self):
    return self._start_pos[0]

</t>
<t tx="ekr.20180516071750.251">@property
def column(self):
    return self._start_pos[1]

</t>
<t tx="ekr.20180516071750.252">@property
def code(self):
    # The class name start
    first = self.__class__.__name__[0]
    return first + str(CODES[self.name][0])

</t>
<t tx="ekr.20180516071750.253">def __unicode__(self):
    return '%s:%s:%s: %s %s' % (self.path, self.line, self.column,
                                self.code, self.message)

</t>
<t tx="ekr.20180516071750.254">def __str__(self):
    return self.__unicode__()

</t>
<t tx="ekr.20180516071750.255">def __eq__(self, other):
    return (self.path == other.path and self.name == other.name and
            self._start_pos == other._start_pos)

</t>
<t tx="ekr.20180516071750.256">def __ne__(self, other):
    return not self.__eq__(other)

</t>
<t tx="ekr.20180516071750.257">def __hash__(self):
    return hash((self.path, self._start_pos, self.name))

</t>
<t tx="ekr.20180516071750.258">def __repr__(self):
    return '&lt;%s %s: %s@%s,%s&gt;' % (self.__class__.__name__,
                                  self.name, self.path,
                                  self._start_pos[0], self._start_pos[1])


</t>
<t tx="ekr.20180516071750.259">class Warning(Error):
    pass


</t>
<t tx="ekr.20180516071750.26">@path C:/Anaconda3/Lib/site-packages/jedi/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071750.260">def add(node_context, error_name, node, message=None, typ=Error, payload=None):
    exception = CODES[error_name][1]
    if _check_for_exception_catch(node_context, node, exception, payload):
        return

    # TODO this path is probably not right
    module_context = node_context.get_root_context()
    module_path = module_context.py__file__()
    instance = typ(error_name, module_path, node.start_pos, message)
    debug.warning(str(instance), format=False)
    node_context.evaluator.analysis.append(instance)


</t>
<t tx="ekr.20180516071750.261">def _check_for_setattr(instance):
    """
    Check if there's any setattr method inside an instance. If so, return True.
    """
    from jedi.evaluate.representation import ModuleContext
    module = instance.get_root_context()
    if not isinstance(module, ModuleContext):
        return False

    node = module.tree_node
    try:
        stmts = node.used_names['setattr']
    except KeyError:
        return False

    return any(node.start_pos &lt; stmt.start_pos &lt; node.end_pos
               for stmt in stmts)


</t>
<t tx="ekr.20180516071750.262">def add_attribute_error(name_context, lookup_context, name):
    message = ('AttributeError: %s has no attribute %s.' % (lookup_context, name))
    from jedi.evaluate.instance import AbstractInstanceContext, CompiledInstanceName
    # Check for __getattr__/__getattribute__ existance and issue a warning
    # instead of an error, if that happens.
    typ = Error
    if isinstance(lookup_context, AbstractInstanceContext):
        slot_names = lookup_context.get_function_slot_names('__getattr__') + \
            lookup_context.get_function_slot_names('__getattribute__')
        for n in slot_names:
            if isinstance(name, CompiledInstanceName) and \
                    n.parent_context.obj == object:
                typ = Warning
                break

        if _check_for_setattr(lookup_context):
            typ = Warning

    payload = lookup_context, name
    add(name_context, 'attribute-error', name, message, typ, payload)


</t>
<t tx="ekr.20180516071750.263">def _check_for_exception_catch(node_context, jedi_name, exception, payload=None):
    """
    Checks if a jedi object (e.g. `Statement`) sits inside a try/catch and
    doesn't count as an error (if equal to `exception`).
    Also checks `hasattr` for AttributeErrors and uses the `payload` to compare
    it.
    Returns True if the exception was catched.
    """
    def check_match(cls, exception):
        try:
            return isinstance(cls, CompiledObject) and issubclass(exception, cls.obj)
        except TypeError:
            return False

    def check_try_for_except(obj, exception):
        # Only nodes in try
        iterator = iter(obj.children)
        for branch_type in iterator:
            colon = next(iterator)
            suite = next(iterator)
            if branch_type == 'try' \
                    and not (branch_type.start_pos &lt; jedi_name.start_pos &lt;= suite.end_pos):
                return False

        for node in obj.except_clauses():
            if node is None:
                return True  # An exception block that catches everything.
            else:
                except_classes = node_context.eval_node(node)
                for cls in except_classes:
                    from jedi.evaluate import iterable
                    if isinstance(cls, iterable.AbstractSequence) and \
                            cls.array_type == 'tuple':
                        # multiple exceptions
                        for lazy_context in cls.py__iter__():
                            for typ in lazy_context.infer():
                                if check_match(typ, exception):
                                    return True
                    else:
                        if check_match(cls, exception):
                            return True

    def check_hasattr(node, suite):
        try:
            assert suite.start_pos &lt;= jedi_name.start_pos &lt; suite.end_pos
            assert node.type in ('power', 'atom_expr')
            base = node.children[0]
            assert base.type == 'name' and base.value == 'hasattr'
            trailer = node.children[1]
            assert trailer.type == 'trailer'
            arglist = trailer.children[1]
            assert arglist.type == 'arglist'
            from jedi.evaluate.param import TreeArguments
            args = list(TreeArguments(node_context.evaluator, node_context, arglist).unpack())
            # Arguments should be very simple
            assert len(args) == 2

            # Check name
            key, lazy_context = args[1]
            names = list(lazy_context.infer())
            assert len(names) == 1 and isinstance(names[0], CompiledObject)
            assert names[0].obj == str(payload[1])

            # Check objects
            key, lazy_context = args[0]
            objects = lazy_context.infer()
            return payload[0] in objects
        except AssertionError:
            return False

    obj = jedi_name
    while obj is not None and not isinstance(obj, (tree.Function, tree.Class)):
        if isinstance(obj, tree.Flow):
            # try/except catch check
            if obj.type == 'try_stmt' and check_try_for_except(obj, exception):
                return True
            # hasattr check
            if exception == AttributeError and obj.type in ('if_stmt', 'while_stmt'):
                if check_hasattr(obj.children[1], obj.children[3]):
                    return True
        obj = obj.parent

    return False
</t>
<t tx="ekr.20180516071750.264">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071750.265">"""
- the popular ``memoize_default`` works like a typical memoize and returns the
  default otherwise.
- ``CachedMetaClass`` uses ``memoize_default`` to do the same with classes.
"""

import inspect

NO_DEFAULT = object()


</t>
<t tx="ekr.20180516071750.266">def memoize_default(default=NO_DEFAULT, evaluator_is_first_arg=False, second_arg_is_evaluator=False):
    """ This is a typical memoization decorator, BUT there is one difference:
    To prevent recursion it sets defaults.

    Preventing recursion is in this case the much bigger use than speed. I
    don't think, that there is a big speed difference, but there are many cases
    where recursion could happen (think about a = b; b = a).
    """
    def func(function):
        def wrapper(obj, *args, **kwargs):
            if evaluator_is_first_arg:
                cache = obj.memoize_cache
            elif second_arg_is_evaluator:  # needed for meta classes
                cache = args[0].memoize_cache
            else:
                cache = obj.evaluator.memoize_cache

            try:
                memo = cache[function]
            except KeyError:
                memo = {}
                cache[function] = memo

            key = (obj, args, frozenset(kwargs.items()))
            if key in memo:
                return memo[key]
            else:
                if default is not NO_DEFAULT:
                    memo[key] = default
                rv = function(obj, *args, **kwargs)
                if inspect.isgenerator(rv):
                    rv = list(rv)
                memo[key] = rv
                return rv
        return wrapper
    return func


</t>
<t tx="ekr.20180516071750.267">class CachedMetaClass(type):
    """
    This is basically almost the same than the decorator above, it just caches
    class initializations. Either you do it this way or with decorators, but
    with decorators you lose class access (isinstance, etc).
    """
    @others
</t>
<t tx="ekr.20180516071750.268">@memoize_default(None, second_arg_is_evaluator=True)
def __call__(self, *args, **kwargs):
    return super(CachedMetaClass, self).__call__(*args, **kwargs)
</t>
<t tx="ekr.20180516071750.269">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071750.27">"""
Jedi is a static analysis tool for Python that can be used in IDEs/editors. Its
historic focus is autocompletion, but does static analysis for now as well.
Jedi is fast and is very well tested. It understands Python on a deeper level
than all other static analysis frameworks for Python.

Jedi has support for two different goto functions. It's possible to search for
related names and to list all names in a Python file and infer them. Jedi
understands docstrings and you can use Jedi autocompletion in your REPL as
well.

Jedi uses a very simple API to connect with IDE's. There's a reference
implementation as a `VIM-Plugin &lt;https://github.com/davidhalter/jedi-vim&gt;`_,
which uses Jedi's autocompletion.  We encourage you to use Jedi in your IDEs.
It's really easy.

To give you a simple example how you can use the Jedi library, here is an
example for the autocompletion feature:

&gt;&gt;&gt; import jedi
&gt;&gt;&gt; source = '''
... import datetime
... datetime.da'''
&gt;&gt;&gt; script = jedi.Script(source, 3, len('datetime.da'), 'example.py')
&gt;&gt;&gt; script
&lt;Script: 'example.py'&gt;
&gt;&gt;&gt; completions = script.completions()
&gt;&gt;&gt; completions                                         #doctest: +ELLIPSIS
[&lt;Completion: date&gt;, &lt;Completion: datetime&gt;, ...]
&gt;&gt;&gt; print(completions[0].complete)
te
&gt;&gt;&gt; print(completions[0].name)
date

As you see Jedi is pretty simple and allows you to concentrate on writing a
good text editor, while still having very good IDE features for Python.
"""

__version__ = '0.10.2'

from jedi.api import Script, Interpreter, NotFoundError, set_debug_function
from jedi.api import preload_module, defined_names, names
from jedi import settings
</t>
<t tx="ekr.20180516071750.270">from jedi._compatibility import Python3Method
from jedi.common import unite
from jedi.parser.python.tree import ExprStmt, CompFor


</t>
<t tx="ekr.20180516071750.271">class Context(object):
    api_type = None
    """
    To be defined by subclasses.
    """
    predefined_names = {}
    tree_node = None

    @others
</t>
<t tx="ekr.20180516071750.272">def __init__(self, evaluator, parent_context=None):
    self.evaluator = evaluator
    self.parent_context = parent_context

</t>
<t tx="ekr.20180516071750.273">def get_root_context(self):
    context = self
    while True:
        if context.parent_context is None:
            return context
        context = context.parent_context

</t>
<t tx="ekr.20180516071750.274">def execute(self, arguments):
    return self.evaluator.execute(self, arguments)

</t>
<t tx="ekr.20180516071750.275">def execute_evaluated(self, *value_list):
    """
    Execute a function with already executed arguments.
    """
    from jedi.evaluate.param import ValuesArguments
    arguments = ValuesArguments([[value] for value in value_list])
    return self.execute(arguments)

</t>
<t tx="ekr.20180516071750.276">def eval_node(self, node):
    return self.evaluator.eval_element(self, node)

</t>
<t tx="ekr.20180516071750.277">def eval_stmt(self, stmt, seek_name=None):
    return self.evaluator.eval_statement(self, stmt, seek_name)

</t>
<t tx="ekr.20180516071750.278">@Python3Method
def eval_trailer(self, types, trailer):
    return self.evaluator.eval_trailer(self, types, trailer)

</t>
<t tx="ekr.20180516071750.279">@Python3Method
def py__getattribute__(self, name_or_str, name_context=None, position=None,
                       search_global=False, is_goto=False):
    if name_context is None:
        name_context = self
    return self.evaluator.find_types(
        self, name_or_str, name_context, position, search_global, is_goto)

def create_context(self, node, node_is_context=False, node_is_object=False):
    return self.evaluator.create_context(self, node, node_is_context, node_is_object)

</t>
<t tx="ekr.20180516071750.28">@path C:/Anaconda3/Lib/site-packages/jedi/
@others
if len(sys.argv) == 2 and sys.argv[1] == 'repl':
    # don't want to use __main__ only for repl yet, maybe we want to use it for
    # something else. So just use the keyword ``repl`` for now.
    print(join(dirname(abspath(__file__)), 'api', 'replstartup.py'))
elif len(sys.argv) &gt; 1 and sys.argv[1] == 'linter':
    _start_linter()
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071750.280">def is_class(self):
    return False

</t>
<t tx="ekr.20180516071750.281">def py__bool__(self):
    """
    Since Wrapper is a super class for classes, functions and modules,
    the return value will always be true.
    """
    return True


</t>
<t tx="ekr.20180516071750.282">class TreeContext(Context):
    @others
</t>
<t tx="ekr.20180516071750.283">def __init__(self, evaluator, parent_context=None):
    super(TreeContext, self).__init__(evaluator, parent_context)
    self.predefined_names = {}

</t>
<t tx="ekr.20180516071750.284">def __repr__(self):
    return '&lt;%s: %s&gt;' % (self.__class__.__name__, self.tree_node)


</t>
<t tx="ekr.20180516071750.285">class AbstractLazyContext(object):
    @others
</t>
<t tx="ekr.20180516071750.286">def __init__(self, data):
    self.data = data

</t>
<t tx="ekr.20180516071750.287">def __repr__(self):
    return '&lt;%s: %s&gt;' % (self.__class__.__name__, self.data)

</t>
<t tx="ekr.20180516071750.288">def infer(self):
    raise NotImplementedError


</t>
<t tx="ekr.20180516071750.289">class LazyKnownContext(AbstractLazyContext):
    """data is a context."""
    @others
</t>
<t tx="ekr.20180516071750.29">import sys
from os.path import join, dirname, abspath, isdir


</t>
<t tx="ekr.20180516071750.290">def infer(self):
    return set([self.data])


</t>
<t tx="ekr.20180516071750.291">class LazyKnownContexts(AbstractLazyContext):
    """data is a set of contexts."""
    @others
</t>
<t tx="ekr.20180516071750.292">def infer(self):
    return self.data


</t>
<t tx="ekr.20180516071750.293">class LazyUnknownContext(AbstractLazyContext):
    @others
</t>
<t tx="ekr.20180516071750.294">def __init__(self):
    super(LazyUnknownContext, self).__init__(None)

</t>
<t tx="ekr.20180516071750.295">def infer(self):
    return set()


</t>
<t tx="ekr.20180516071750.296">class LazyTreeContext(AbstractLazyContext):
    @others
</t>
<t tx="ekr.20180516071750.297">def __init__(self, context, node):
    super(LazyTreeContext, self).__init__(node)
    self._context = context
    # We need to save the predefined names. It's an unfortunate side effect
    # that needs to be tracked otherwise results will be wrong.
    self._predefined_names = dict(context.predefined_names)

</t>
<t tx="ekr.20180516071750.298">def infer(self):
    old, self._context.predefined_names = \
        self._context.predefined_names, self._predefined_names
    try:
        return self._context.eval_node(self.data)
    finally:
        self._context.predefined_names = old


</t>
<t tx="ekr.20180516071750.299">def get_merged_lazy_context(lazy_contexts):
    if len(lazy_contexts) &gt; 1:
        return MergedLazyContexts(lazy_contexts)
    else:
        return lazy_contexts[0]


</t>
<t tx="ekr.20180516071750.3">@path C:/Anaconda3/Lib/site-packages/jedi/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071750.30">def _start_linter():
    """
    This is a pre-alpha API. You're not supposed to use it at all, except for
    testing. It will very likely change.
    """
    import jedi

    if '--debug' in sys.argv:
        jedi.set_debug_function()

    for path in sys.argv[2:]:
        if path.startswith('--'):
            continue
        if isdir(path):
            import fnmatch
            import os

            paths = []
            for root, dirnames, filenames in os.walk(path):
                for filename in fnmatch.filter(filenames, '*.py'):
                    paths.append(os.path.join(root, filename))
        else:
            paths = [path]

        try:
            for path in paths:
                for error in jedi.Script(path=path)._analysis():
                    print(error)
        except Exception:
            if '--pdb' in sys.argv:
                import traceback
                traceback.print_exc()
                import pdb
                pdb.post_mortem()
            else:
                raise


</t>
<t tx="ekr.20180516071750.300">class MergedLazyContexts(AbstractLazyContext):
    """data is a list of lazy contexts."""
    @others
</t>
<t tx="ekr.20180516071750.301">def infer(self):
    return unite(l.infer() for l in self.data)


</t>
<t tx="ekr.20180516071750.302">class ContextualizedNode(object):
    @others
</t>
<t tx="ekr.20180516071750.303">def __init__(self, context, node):
    self.context = context
    self._node = node

</t>
<t tx="ekr.20180516071750.304">def get_root_context(self):
    return self.context.get_root_context()

</t>
<t tx="ekr.20180516071750.305">def infer(self):
    return self.context.eval_node(self._node)


</t>
<t tx="ekr.20180516071750.306">class ContextualizedName(ContextualizedNode):
    # TODO merge with TreeNameDefinition?!
    @others
</t>
<t tx="ekr.20180516071750.307">@property
def name(self):
    return self._node

</t>
<t tx="ekr.20180516071750.308">def assignment_indexes(self):
    """
    Returns an array of tuple(int, node) of the indexes that are used in
    tuple assignments.

    For example if the name is ``y`` in the following code::

        x, (y, z) = 2, ''

    would result in ``[(1, xyz_node), (0, yz_node)]``.
    """
    indexes = []
    node = self._node.parent
    compare = self._node
    while node is not None:
        if node.type in ('testlist_comp', 'testlist_star_expr', 'exprlist'):
            for i, child in enumerate(node.children):
                if child == compare:
                    indexes.insert(0, (int(i / 2), node))
                    break
            else:
                raise LookupError("Couldn't find the assignment.")
        elif isinstance(node, (ExprStmt, CompFor)):
            break

        compare = node
        node = node.parent
    return indexes
</t>
<t tx="ekr.20180516071750.309">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071750.310">"""
Docstrings are another source of information for functions and classes.
:mod:`jedi.evaluate.dynamic` tries to find all executions of functions, while
the docstring parsing is much easier. There are two different types of
docstrings that |jedi| understands:

- `Sphinx &lt;http://sphinx-doc.org/markup/desc.html#info-field-lists&gt;`_
- `Epydoc &lt;http://epydoc.sourceforge.net/manual-fields.html&gt;`_

For example, the sphinx annotation ``:type foo: str`` clearly states that the
type of ``foo`` is ``str``.

As an addition to parameter searching, this module also provides return
annotations.
"""

from ast import literal_eval
import re
from textwrap import dedent

from jedi._compatibility import u
from jedi.common import unite
from jedi.evaluate import context
from jedi.evaluate.cache import memoize_default
from jedi.parser.python import parse
from jedi.parser.python.tree import search_ancestor
from jedi.common import indent_block
from jedi.evaluate.iterable import SequenceLiteralContext, FakeSequence


DOCSTRING_PARAM_PATTERNS = [
    r'\s*:type\s+%s:\s*([^\n]+)',  # Sphinx
    r'\s*:param\s+(\w+)\s+%s:[^\n]+',  # Sphinx param with type
    r'\s*@type\s+%s:\s*([^\n]+)',  # Epydoc
]

DOCSTRING_RETURN_PATTERNS = [
    re.compile(r'\s*:rtype:\s*([^\n]+)', re.M),  # Sphinx
    re.compile(r'\s*@rtype:\s*([^\n]+)', re.M),  # Epydoc
]

REST_ROLE_PATTERN = re.compile(r':[^`]+:`([^`]+)`')


try:
    from numpydoc.docscrape import NumpyDocString
except ImportError:
    def _search_param_in_numpydocstr(docstr, param_str):
        return []
else:
    def _search_param_in_numpydocstr(docstr, param_str):
        """Search `docstr` (in numpydoc format) for type(-s) of `param_str`."""
        params = NumpyDocString(docstr)._parsed_data['Parameters']
        for p_name, p_type, p_descr in params:
            if p_name == param_str:
                m = re.match('([^,]+(,[^,]+)*?)(,[ ]*optional)?$', p_type)
                if m:
                    p_type = m.group(1)

                if p_type.startswith('{'):
                    types = set(type(x).__name__ for x in literal_eval(p_type))
                    return list(types)
                else:
                    return [p_type]
        return []


</t>
<t tx="ekr.20180516071750.311">def _search_param_in_docstr(docstr, param_str):
    """
    Search `docstr` for type(-s) of `param_str`.

    &gt;&gt;&gt; _search_param_in_docstr(':type param: int', 'param')
    ['int']
    &gt;&gt;&gt; _search_param_in_docstr('@type param: int', 'param')
    ['int']
    &gt;&gt;&gt; _search_param_in_docstr(
    ...   ':type param: :class:`threading.Thread`', 'param')
    ['threading.Thread']
    &gt;&gt;&gt; bool(_search_param_in_docstr('no document', 'param'))
    False
    &gt;&gt;&gt; _search_param_in_docstr(':param int param: some description', 'param')
    ['int']

    """
    # look at #40 to see definitions of those params
    patterns = [re.compile(p % re.escape(param_str))
                for p in DOCSTRING_PARAM_PATTERNS]
    for pattern in patterns:
        match = pattern.search(docstr)
        if match:
            return [_strip_rst_role(match.group(1))]

    return (_search_param_in_numpydocstr(docstr, param_str) or
            [])


</t>
<t tx="ekr.20180516071750.312">def _strip_rst_role(type_str):
    """
    Strip off the part looks like a ReST role in `type_str`.

    &gt;&gt;&gt; _strip_rst_role(':class:`ClassName`')  # strip off :class:
    'ClassName'
    &gt;&gt;&gt; _strip_rst_role(':py:obj:`module.Object`')  # works with domain
    'module.Object'
    &gt;&gt;&gt; _strip_rst_role('ClassName')  # do nothing when not ReST role
    'ClassName'

    See also:
    http://sphinx-doc.org/domains.html#cross-referencing-python-objects

    """
    match = REST_ROLE_PATTERN.match(type_str)
    if match:
        return match.group(1)
    else:
        return type_str


</t>
<t tx="ekr.20180516071750.313">def _evaluate_for_statement_string(module_context, string):
    code = dedent(u("""
    def pseudo_docstring_stuff():
        # Create a pseudo function for docstring statements.
    {0}
    """))
    if string is None:
        return []

    for element in re.findall('((?:\w+\.)*\w+)\.', string):
        # Try to import module part in dotted name.
        # (e.g., 'threading' in 'threading.Thread').
        string = 'import %s\n' % element + string

    # Take the default grammar here, if we load the Python 2.7 grammar here, it
    # will be impossible to use `...` (Ellipsis) as a token. Docstring types
    # don't need to conform with the current grammar.
    module = parse(code.format(indent_block(string)))
    try:
        funcdef = module.subscopes[0]
        # First pick suite, then simple_stmt and then the node,
        # which is also not the last item, because there's a newline.
        stmt = funcdef.children[-1].children[-1].children[-2]
    except (AttributeError, IndexError):
        return []

    from jedi.evaluate.param import ValuesArguments
    from jedi.evaluate.representation import FunctionContext
    function_context = FunctionContext(
        module_context.evaluator,
        module_context,
        funcdef
    )
    func_execution_context = function_context.get_function_execution(
        ValuesArguments([])
    )
    # Use the module of the param.
    # TODO this module is not the module of the param in case of a function
    # call. In that case it's the module of the function call.
    # stuffed with content from a function call.
    return list(_execute_types_in_stmt(func_execution_context, stmt))


</t>
<t tx="ekr.20180516071750.314">def _execute_types_in_stmt(module_context, stmt):
    """
    Executing all types or general elements that we find in a statement. This
    doesn't include tuple, list and dict literals, because the stuff they
    contain is executed. (Used as type information).
    """
    definitions = module_context.eval_node(stmt)
    return unite(_execute_array_values(module_context.evaluator, d) for d in definitions)


</t>
<t tx="ekr.20180516071750.315">def _execute_array_values(evaluator, array):
    """
    Tuples indicate that there's not just one return value, but the listed
    ones.  `(str, int)` means that it returns a tuple with both types.
    """
    if isinstance(array, SequenceLiteralContext):
        values = []
        for lazy_context in array.py__iter__():
            objects = unite(_execute_array_values(evaluator, typ) for typ in lazy_context.infer())
            values.append(context.LazyKnownContexts(objects))
        return set([FakeSequence(evaluator, array.array_type, values)])
    else:
        return array.execute_evaluated()


</t>
<t tx="ekr.20180516071750.316">@memoize_default()
def follow_param(module_context, param):
    def eval_docstring(docstring):
        return set(
            [p for param_str in _search_param_in_docstr(docstring, str(param.name))
                for p in _evaluate_for_statement_string(module_context, param_str)]
        )
    func = param.get_parent_function()
    types = eval_docstring(func.raw_doc)
    if func.name.value == '__init__':
        cls = search_ancestor(func, 'classdef')
        if cls is not None:
            types |= eval_docstring(cls.raw_doc)

    return types


</t>
<t tx="ekr.20180516071750.317">@memoize_default()
def find_return_types(module_context, func):
    def search_return_in_docstr(code):
        for p in DOCSTRING_RETURN_PATTERNS:
            match = p.search(code)
            if match:
                return _strip_rst_role(match.group(1))

    type_str = search_return_in_docstr(func.raw_doc)
    return _evaluate_for_statement_string(module_context, type_str)
</t>
<t tx="ekr.20180516071750.318">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071750.319">"""
One of the really important features of |jedi| is to have an option to
understand code like this::

    def foo(bar):
        bar. # completion here
    foo(1)

There's no doubt wheter bar is an ``int`` or not, but if there's also a call
like ``foo('str')``, what would happen? Well, we'll just show both. Because
that's what a human would expect.

It works as follows:

- |Jedi| sees a param
- search for function calls named ``foo``
- execute these calls and check the input. This work with a ``ParamListener``.
"""

from jedi.parser.python import tree
from jedi import settings
from jedi import debug
from jedi.evaluate.cache import memoize_default
from jedi.evaluate import imports
from jedi.evaluate.param import TreeArguments, create_default_param
from jedi.common import to_list, unite


MAX_PARAM_SEARCHES = 20


</t>
<t tx="ekr.20180516071750.320">class ParamListener(object):
    """
    This listener is used to get the params for a function.
    """
    @others
</t>
<t tx="ekr.20180516071750.321">def __init__(self):
    self.param_possibilities = []

</t>
<t tx="ekr.20180516071750.322">def execute(self, params):
    self.param_possibilities += params


</t>
<t tx="ekr.20180516071750.323">class MergedExecutedParams(object):
    """
    Simulates being a parameter while actually just being multiple params.
    """
    @others
</t>
<t tx="ekr.20180516071750.324">def __init__(self, executed_params):
    self._executed_params = executed_params

</t>
<t tx="ekr.20180516071750.325">def infer(self):
    return unite(p.infer() for p in self._executed_params)


</t>
<t tx="ekr.20180516071750.326">@debug.increase_indent
def search_params(evaluator, parent_context, funcdef):
    """
    A dynamic search for param values. If you try to complete a type:

    &gt;&gt;&gt; def func(foo):
    ...     foo
    &gt;&gt;&gt; func(1)
    &gt;&gt;&gt; func("")

    It is not known what the type ``foo`` without analysing the whole code. You
    have to look for all calls to ``func`` to find out what ``foo`` possibly
    is.
    """
    if not settings.dynamic_params:
        return set()

    evaluator.dynamic_params_depth += 1
    try:
        debug.dbg('Dynamic param search in %s.', funcdef.name.value, color='MAGENTA')
        module_context = parent_context.get_root_context()
        function_executions = _search_function_executions(
            evaluator,
            module_context,
            funcdef
        )
        if function_executions:
            zipped_params = zip(*list(
                function_execution.get_params()
                for function_execution in function_executions
            ))
            params = [MergedExecutedParams(executed_params) for executed_params in zipped_params]
            # Evaluate the ExecutedParams to types.
        else:
            params = [create_default_param(parent_context, p) for p in funcdef.params]
        debug.dbg('Dynamic param result finished', color='MAGENTA')
        return params
    finally:
        evaluator.dynamic_params_depth -= 1


</t>
<t tx="ekr.20180516071750.327">@memoize_default([], evaluator_is_first_arg=True)
@to_list
def _search_function_executions(evaluator, module_context, funcdef):
    """
    Returns a list of param names.
    """
    from jedi.evaluate import representation as er

    func_string_name = funcdef.name.value
    compare_node = funcdef
    if func_string_name == '__init__':
        cls = funcdef.get_parent_scope()
        if isinstance(cls, tree.Class):
            func_string_name = cls.name.value
            compare_node = cls

    found_executions = False
    i = 0
    for for_mod_context in imports.get_modules_containing_name(
            evaluator, [module_context], func_string_name):
        if not isinstance(module_context, er.ModuleContext):
            return
        for name, trailer in _get_possible_nodes(for_mod_context, func_string_name):
            i += 1

            # This is a simple way to stop Jedi's dynamic param recursion
            # from going wild: The deeper Jedi's in the recursion, the less
            # code should be evaluated.
            if i * evaluator.dynamic_params_depth &gt; MAX_PARAM_SEARCHES:
                return

            random_context = evaluator.create_context(for_mod_context, name)
            for function_execution in _check_name_for_execution(
                    evaluator, random_context, compare_node, name, trailer):
                found_executions = True
                yield function_execution

        # If there are results after processing a module, we're probably
        # good to process. This is a speed optimization.
        if found_executions:
            return


</t>
<t tx="ekr.20180516071750.328">def _get_possible_nodes(module_context, func_string_name):
    try:
        names = module_context.tree_node.used_names[func_string_name]
    except KeyError:
        return

    for name in names:
        bracket = name.get_next_leaf()
        trailer = bracket.parent
        if trailer.type == 'trailer' and bracket == '(':
            yield name, trailer


</t>
<t tx="ekr.20180516071750.329">def _check_name_for_execution(evaluator, context, compare_node, name, trailer):
    from jedi.evaluate import representation as er, instance

    def create_func_excs():
        arglist = trailer.children[1]
        if arglist == ')':
            arglist = ()
        args = TreeArguments(evaluator, context, arglist, trailer)
        if value_node.type == 'funcdef':
            yield value.get_function_execution(args)
        else:
            created_instance = instance.TreeInstance(
                evaluator,
                value.parent_context,
                value,
                args
            )
            for execution in created_instance.create_init_executions():
                yield execution

    for value in evaluator.goto_definitions(context, name):
        value_node = value.tree_node
        if compare_node == value_node:
            for func_execution in create_func_excs():
                yield func_execution
        elif isinstance(value.parent_context, er.FunctionExecutionContext) and \
                compare_node.type == 'funcdef':
            # Here we're trying to find decorators by checking the first
            # parameter. It's not very generic though. Should find a better
            # solution that also applies to nested decorators.
            params = value.parent_context.get_params()
            if len(params) != 1:
                continue
            values = params[0].infer()
            nodes = [v.tree_node for v in values]
            if nodes == [compare_node]:
                # Found a decorator.
                module_context = context.get_root_context()
                execution_context = next(create_func_excs())
                for name, trailer in _get_possible_nodes(module_context, params[0].string_name):
                    if value_node.start_pos &lt; name.start_pos &lt; value_node.end_pos:
                        random_context = evaluator.create_context(execution_context, name)
                        iterator = _check_name_for_execution(
                            evaluator,
                            random_context,
                            compare_node,
                            name,
                            trailer
                        )
                        for function_execution in iterator:
                            yield function_execution
</t>
<t tx="ekr.20180516071750.330">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071750.331">"""
Filters are objects that you can use to filter names in different scopes. They
are needed for name resolution.
"""
from abc import abstractmethod

from jedi.parser.python.tree import search_ancestor
from jedi.evaluate import flow_analysis
from jedi.common import to_list, unite


</t>
<t tx="ekr.20180516071750.332">class AbstractNameDefinition(object):
    start_pos = None
    string_name = None
    parent_context = None
    tree_name = None

    @others
</t>
<t tx="ekr.20180516071750.333">@abstractmethod
def infer(self):
    raise NotImplementedError

</t>
<t tx="ekr.20180516071750.334">def get_root_context(self):
    return self.parent_context.get_root_context()

</t>
<t tx="ekr.20180516071750.335">def __repr__(self):
    if self.start_pos is None:
        return '&lt;%s: %s&gt;' % (self.__class__.__name__, self.string_name)
    return '&lt;%s: %s@%s&gt;' % (self.__class__.__name__, self.string_name, self.start_pos)

</t>
<t tx="ekr.20180516071750.336">def execute(self, arguments):
    return unite(context.execute(arguments) for context in self.infer())

</t>
<t tx="ekr.20180516071750.337">def execute_evaluated(self, *args, **kwargs):
    return unite(context.execute_evaluated(*args, **kwargs) for context in self.infer())

</t>
<t tx="ekr.20180516071750.338">@property
def api_type(self):
    return self.parent_context.api_type


</t>
<t tx="ekr.20180516071750.339">class AbstractTreeName(AbstractNameDefinition):
    @others
</t>
<t tx="ekr.20180516071750.340">def __init__(self, parent_context, tree_name):
    self.parent_context = parent_context
    self.tree_name = tree_name

</t>
<t tx="ekr.20180516071750.341">@property
def string_name(self):
    return self.tree_name.value

</t>
<t tx="ekr.20180516071750.342">@property
def start_pos(self):
    return self.tree_name.start_pos


</t>
<t tx="ekr.20180516071750.343">class ContextNameMixin(object):
    @others
</t>
<t tx="ekr.20180516071750.344">def infer(self):
    return set([self._context])

</t>
<t tx="ekr.20180516071750.345">def get_root_context(self):
    if self.parent_context is None:
        return self._context
    return super(ContextNameMixin, self).get_root_context()

</t>
<t tx="ekr.20180516071750.346">@property
def api_type(self):
    return self._context.api_type


</t>
<t tx="ekr.20180516071750.347">class ContextName(ContextNameMixin, AbstractTreeName):
    @others
</t>
<t tx="ekr.20180516071750.348">def __init__(self, context, tree_name):
    super(ContextName, self).__init__(context.parent_context, tree_name)
    self._context = context


</t>
<t tx="ekr.20180516071750.349">class TreeNameDefinition(AbstractTreeName):
    @others
</t>
<t tx="ekr.20180516071750.350">def infer(self):
    # Refactor this, should probably be here.
    from jedi.evaluate.finder import _name_to_types
    return _name_to_types(self.parent_context.evaluator, self.parent_context, self.tree_name)

</t>
<t tx="ekr.20180516071750.351">@property
def api_type(self):
    definition = self.tree_name.get_definition()
    return dict(
        import_name='module',
        import_from='module',
        funcdef='function',
        param='param',
        classdef='class',
    ).get(definition.type, 'statement')


</t>
<t tx="ekr.20180516071750.352">class ParamName(AbstractTreeName):
    api_type = 'param'

    @others
</t>
<t tx="ekr.20180516071750.353">def __init__(self, parent_context, tree_name):
    self.parent_context = parent_context
    self.tree_name = tree_name

</t>
<t tx="ekr.20180516071750.354">def infer(self):
    return self.get_param().infer()

</t>
<t tx="ekr.20180516071750.355">def get_param(self):
    params = self.parent_context.get_params()
    param_node = search_ancestor(self.tree_name, 'param')
    return params[param_node.position_nr]


</t>
<t tx="ekr.20180516071750.356">class AnonymousInstanceParamName(ParamName):
    @others
</t>
<t tx="ekr.20180516071750.357">def infer(self):
    param_node = search_ancestor(self.tree_name, 'param')
    if param_node.position_nr == 0:
        # This is a speed optimization, to return the self param (because
        # it's known). This only affects anonymous instances.
        return set([self.parent_context.instance])
    else:
        return self.get_param().infer()


</t>
<t tx="ekr.20180516071750.358">class AbstractFilter(object):
    _until_position = None

    @others
</t>
<t tx="ekr.20180516071750.359">def _filter(self, names):
    if self._until_position is not None:
        return [n for n in names if n.start_pos &lt; self._until_position]
    return names

</t>
<t tx="ekr.20180516071750.360">@abstractmethod
def get(self, name):
    raise NotImplementedError

</t>
<t tx="ekr.20180516071750.361">@abstractmethod
def values(self):
    raise NotImplementedError


</t>
<t tx="ekr.20180516071750.362">class AbstractUsedNamesFilter(AbstractFilter):
    name_class = TreeNameDefinition

    @others
</t>
<t tx="ekr.20180516071750.363">def __init__(self, context, parser_scope):
    self._parser_scope = parser_scope
    self._used_names = self._parser_scope.get_root_node().used_names
    self.context = context

</t>
<t tx="ekr.20180516071750.364">def get(self, name):
    try:
        names = self._used_names[str(name)]
    except KeyError:
        return []

    return self._convert_names(self._filter(names))

</t>
<t tx="ekr.20180516071750.365">def _convert_names(self, names):
    return [self.name_class(self.context, name) for name in names]

</t>
<t tx="ekr.20180516071750.366">def values(self):
    return self._convert_names(name for name_list in self._used_names.values()
                               for name in self._filter(name_list))

</t>
<t tx="ekr.20180516071750.367">def __repr__(self):
    return '&lt;%s: %s&gt;' % (self.__class__.__name__, self.context)


</t>
<t tx="ekr.20180516071750.368">class ParserTreeFilter(AbstractUsedNamesFilter):
    @others
</t>
<t tx="ekr.20180516071750.369">def __init__(self, evaluator, context, node_context=None, until_position=None,
             origin_scope=None):
    """
    node_context is an option to specify a second context for use cases
    like the class mro where the parent class of a new name would be the
    context, but for some type inference it's important to have a local
    context of the other classes.
    """
    if node_context is None:
        node_context = context
    super(ParserTreeFilter, self).__init__(context, node_context.tree_node)
    self._node_context = node_context
    self._origin_scope = origin_scope
    self._until_position = until_position

</t>
<t tx="ekr.20180516071750.370">def _filter(self, names):
    names = super(ParserTreeFilter, self)._filter(names)
    names = [n for n in names if self._is_name_reachable(n)]
    return list(self._check_flows(names))

</t>
<t tx="ekr.20180516071750.371">def _is_name_reachable(self, name):
    if not name.is_definition():
        return False
    parent = name.parent
    if parent.type == 'trailer':
        return False
    base_node = parent if parent.type in ('classdef', 'funcdef') else name
    return base_node.get_parent_scope() == self._parser_scope

</t>
<t tx="ekr.20180516071750.372">def _check_flows(self, names):
    for name in sorted(names, key=lambda name: name.start_pos, reverse=True):
        check = flow_analysis.reachability_check(
            self._node_context, self._parser_scope, name, self._origin_scope
        )
        if check is not flow_analysis.UNREACHABLE:
            yield name

        if check is flow_analysis.REACHABLE:
            break


</t>
<t tx="ekr.20180516071750.373">class FunctionExecutionFilter(ParserTreeFilter):
    param_name = ParamName

    @others
</t>
<t tx="ekr.20180516071750.374">def __init__(self, evaluator, context, node_context=None,
             until_position=None, origin_scope=None):
    super(FunctionExecutionFilter, self).__init__(
        evaluator,
        context,
        node_context,
        until_position,
        origin_scope
    )

</t>
<t tx="ekr.20180516071750.375">@to_list
def _convert_names(self, names):
    for name in names:
        param = search_ancestor(name, 'param')
        if param:
            yield self.param_name(self.context, name)
        else:
            yield TreeNameDefinition(self.context, name)


</t>
<t tx="ekr.20180516071750.376">class AnonymousInstanceFunctionExecutionFilter(FunctionExecutionFilter):
    param_name = AnonymousInstanceParamName


</t>
<t tx="ekr.20180516071750.377">class GlobalNameFilter(AbstractUsedNamesFilter):
    @others
</t>
<t tx="ekr.20180516071750.378">def __init__(self, context, parser_scope):
    super(GlobalNameFilter, self).__init__(context, parser_scope)

</t>
<t tx="ekr.20180516071750.379">@to_list
def _filter(self, names):
    for name in names:
        if name.parent.type == 'global_stmt':
            yield name


</t>
<t tx="ekr.20180516071750.380">class DictFilter(AbstractFilter):
    @others
</t>
<t tx="ekr.20180516071750.381">def __init__(self, dct):
    self._dct = dct

</t>
<t tx="ekr.20180516071750.382">def get(self, name):
    try:
        value = self._convert(name, self._dct[str(name)])
    except KeyError:
        return []

    return list(self._filter([value]))

</t>
<t tx="ekr.20180516071750.383">def values(self):
    return self._filter(self._convert(*item) for item in self._dct.items())

</t>
<t tx="ekr.20180516071750.384">def _convert(self, name, value):
    return value


</t>
<t tx="ekr.20180516071750.385">def get_global_filters(evaluator, context, until_position, origin_scope):
    """
    Returns all filters in order of priority for name resolution.

    For global name lookups. The filters will handle name resolution
    themselves, but here we gather possible filters downwards.

    &gt;&gt;&gt; from jedi._compatibility import u, no_unicode_pprint
    &gt;&gt;&gt; from jedi import Script
    &gt;&gt;&gt; script = Script(u('''
    ... x = ['a', 'b', 'c']
    ... def func():
    ...     y = None
    ... '''))
    &gt;&gt;&gt; module_node = script._get_module_node()
    &gt;&gt;&gt; scope = module_node.subscopes[0]
    &gt;&gt;&gt; scope
    &lt;Function: func@3-5&gt;
    &gt;&gt;&gt; context = script._get_module().create_context(scope)
    &gt;&gt;&gt; filters = list(get_global_filters(context.evaluator, context, (4, 0), None))

    First we get the names names from the function scope.

    &gt;&gt;&gt; no_unicode_pprint(filters[0])
    &lt;ParserTreeFilter: &lt;ModuleContext: @2-5&gt;&gt;
    &gt;&gt;&gt; sorted(str(n) for n in filters[0].values())
    ['&lt;TreeNameDefinition: func@(3, 4)&gt;', '&lt;TreeNameDefinition: x@(2, 0)&gt;']
    &gt;&gt;&gt; filters[0]._until_position
    (4, 0)

    Then it yields the names from one level "lower". In this example, this is
    the module scope. As a side note, you can see, that the position in the
    filter is now None, because typically the whole module is loaded before the
    function is called.

    &gt;&gt;&gt; filters[1].values()  # global names -&gt; there are none in our example.
    []
    &gt;&gt;&gt; list(filters[2].values())  # package modules -&gt; Also empty.
    []
    &gt;&gt;&gt; sorted(name.string_name for name in filters[3].values())  # Module attributes
    ['__doc__', '__file__', '__name__', '__package__']
    &gt;&gt;&gt; print(filters[1]._until_position)
    None

    Finally, it yields the builtin filter, if `include_builtin` is
    true (default).

    &gt;&gt;&gt; filters[4].values()                              #doctest: +ELLIPSIS
    [&lt;CompiledName: ...&gt;, ...]
    """
    from jedi.evaluate.representation import FunctionExecutionContext
    while context is not None:
        # Names in methods cannot be resolved within the class.
        for filter in context.get_filters(
                search_global=True,
                until_position=until_position,
                origin_scope=origin_scope):
            yield filter
        if isinstance(context, FunctionExecutionContext):
            # The position should be reset if the current scope is a function.
            until_position = None

        context = context.parent_context

    # Add builtins to the global scope.
    for filter in evaluator.BUILTINS.get_filters(search_global=True):
        yield filter
</t>
<t tx="ekr.20180516071750.4">"""
Utilities for end-users.
"""

from __future__ import absolute_import
import __main__
from collections import namedtuple
import logging
import traceback
import re
import os
import sys

from jedi import Interpreter
from jedi.api.helpers import get_on_completion_name
from jedi import common


READLINE_DEBUG = False


</t>
<t tx="ekr.20180516071750.5">def setup_readline(namespace_module=__main__):
    """
    Install Jedi completer to :mod:`readline`.

    This function setups :mod:`readline` to use Jedi in Python interactive
    shell.  If you want to use a custom ``PYTHONSTARTUP`` file (typically
    ``$HOME/.pythonrc.py``), you can add this piece of code::

        try:
            from jedi.utils import setup_readline
            setup_readline()
        except ImportError:
            # Fallback to the stdlib readline completer if it is installed.
            # Taken from http://docs.python.org/2/library/rlcompleter.html
            print("Jedi is not installed, falling back to readline")
            try:
                import readline
                import rlcompleter
                readline.parse_and_bind("tab: complete")
            except ImportError:
                print("Readline is not installed either. No tab completion is enabled.")

    This will fallback to the readline completer if Jedi is not installed.
    The readline completer will only complete names in the global namespace,
    so for example::

        ran&lt;TAB&gt;

    will complete to ``range``

    with both Jedi and readline, but::

        range(10).cou&lt;TAB&gt;

    will show complete to ``range(10).count`` only with Jedi.

    You'll also need to add ``export PYTHONSTARTUP=$HOME/.pythonrc.py`` to
    your shell profile (usually ``.bash_profile`` or ``.profile`` if you use
    bash).

    """
    if READLINE_DEBUG:
        logging.basicConfig(
            filename='/tmp/jedi.log',
            filemode='a',
            level=logging.DEBUG
        )

    class JediRL(object):
        def complete(self, text, state):
            """
            This complete stuff is pretty weird, a generator would make
            a lot more sense, but probably due to backwards compatibility
            this is still the way how it works.

            The only important part is stuff in the ``state == 0`` flow,
            everything else has been copied from the ``rlcompleter`` std.
            library module.
            """
            if state == 0:
                sys.path.insert(0, os.getcwd())
                # Calling python doesn't have a path, so add to sys.path.
                try:
                    logging.debug("Start REPL completion: " + repr(text))
                    interpreter = Interpreter(text, [namespace_module.__dict__])

                    lines = common.splitlines(text)
                    position = (len(lines), len(lines[-1]))
                    name = get_on_completion_name(
                        interpreter._get_module_node(),
                        lines,
                        position
                    )
                    before = text[:len(text) - len(name)]
                    completions = interpreter.completions()
                except:
                    logging.error("REPL Completion error:\n" + traceback.format_exc())
                    raise
                finally:
                    sys.path.pop(0)

                self.matches = [before + c.name_with_symbols for c in completions]
            try:
                return self.matches[state]
            except IndexError:
                return None

    try:
        import readline
    except ImportError:
        print("Jedi: Module readline not available.")
    else:
        readline.set_completer(JediRL().complete)
        readline.parse_and_bind("tab: complete")
        # jedi itself does the case matching
        readline.parse_and_bind("set completion-ignore-case on")
        # because it's easier to hit the tab just once
        readline.parse_and_bind("set show-all-if-unmodified")
        readline.parse_and_bind("set show-all-if-ambiguous on")
        # don't repeat all the things written in the readline all the time
        readline.parse_and_bind("set completion-prefix-display-length 2")
        # No delimiters, Jedi handles that.
        readline.set_completer_delims('')


</t>
<t tx="ekr.20180516071750.6">def version_info():
    """
    Returns a namedtuple of Jedi's version, similar to Python's
    ``sys.version_info``.
    """
    Version = namedtuple('Version', 'major, minor, micro')
    from jedi import __version__
    tupl = re.findall('[a-z]+|\d+', __version__)
    return Version(*[x if i == 3 else int(x) for i, x in enumerate(tupl)])
</t>
<t tx="ekr.20180516071750.7">@path C:/Anaconda3/Lib/site-packages/jedi/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071750.8">"""
To ensure compatibility from Python ``2.6`` - ``3.3``, a module has been
created. Clearly there is huge need to use conforming syntax.
"""
import sys
import imp
import os
import re
import pkgutil
try:
    import importlib
except ImportError:
    pass

# Cannot use sys.version.major and minor names, because in Python 2.6 it's not
# a namedtuple.
is_py3 = sys.version_info[0] &gt;= 3
is_py33 = is_py3 and sys.version_info[1] &gt;= 3
is_py34 = is_py3 and sys.version_info[1] &gt;= 4
is_py35 = is_py3 and sys.version_info[1] &gt;= 5
is_py26 = not is_py3 and sys.version_info[1] &lt; 7
py_version = int(str(sys.version_info[0]) + str(sys.version_info[1]))


</t>
<t tx="ekr.20180516071750.9">class DummyFile(object):
    @others
</t>
<t tx="ekr.20180516071750.98"></t>
<t tx="ekr.20180516071750.99">@path C:/Anaconda3/Lib/site-packages/jedi/api/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.1">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.10">def _names_to_types(self, names, attribute_lookup):
    types = set()

    types = unite(name.infer() for name in names)

    debug.dbg('finder._names_to_types: %s -&gt; %s', names, types)
    if not names and isinstance(self._context, AbstractInstanceContext):
        # handling __getattr__ / __getattribute__
        return self._check_getattr(self._context)

    # Add isinstance and other if/assert knowledge.
    if not types and isinstance(self._name, tree.Name) and \
            not isinstance(self._name_context, AbstractInstanceContext):
        flow_scope = self._name
        base_node = self._name_context.tree_node
        if base_node.type == 'comp_for':
            return types
        while True:
            flow_scope = flow_scope.get_parent_scope(include_flows=True)
            n = _check_flow_information(self._name_context, flow_scope,
                                        self._name, self._position)
            if n is not None:
                return n
            if flow_scope == base_node:
                break
    return types


</t>
<t tx="ekr.20180516071751.100">def _create_name(self, name):
    return self.name_class(
        self._evaluator, self._instance, self._compiled_object, name)


</t>
<t tx="ekr.20180516071751.101">class BoundMethod(er.FunctionContext):
    @others
</t>
<t tx="ekr.20180516071751.102">def __init__(self, evaluator, instance, class_context, *args, **kwargs):
    super(BoundMethod, self).__init__(evaluator, *args, **kwargs)
    self._instance = instance
    self._class_context = class_context

</t>
<t tx="ekr.20180516071751.103">def get_function_execution(self, arguments=None):
    if arguments is None:
        return AnonymousInstanceFunctionExecution(
            self._instance, self.parent_context, self)
    else:
        return InstanceFunctionExecution(
            self._instance, self.parent_context, self, arguments)


</t>
<t tx="ekr.20180516071751.104">class CompiledBoundMethod(compiled.CompiledObject):
    @others
</t>
<t tx="ekr.20180516071751.105">def __init__(self, func):
    super(CompiledBoundMethod, self).__init__(
        func.evaluator, func.obj, func.parent_context, func.tree_node)

</t>
<t tx="ekr.20180516071751.106">def get_param_names(self):
    return list(super(CompiledBoundMethod, self).get_param_names())[1:]


</t>
<t tx="ekr.20180516071751.107">class InstanceNameDefinition(filters.TreeNameDefinition):
    @others
</t>
<t tx="ekr.20180516071751.108">def infer(self):
    contexts = super(InstanceNameDefinition, self).infer()
    for context in contexts:
        yield context


</t>
<t tx="ekr.20180516071751.109">class LazyInstanceName(filters.TreeNameDefinition):
    """
    This name calculates the parent_context lazily.
    """
    @others
</t>
<t tx="ekr.20180516071751.11">def _name_to_types(evaluator, context, tree_name):
    types = []
    node = tree_name.get_definition()
    typ = node.type
    if typ == 'for_stmt':
        types = pep0484.find_type_from_comment_hint_for(context, node, tree_name)
        if types:
            return types
    if typ == 'with_stmt':
        types = pep0484.find_type_from_comment_hint_with(context, node, tree_name)
        if types:
            return types
    if typ in ('for_stmt', 'comp_for'):
        try:
            types = context.predefined_names[node][tree_name.value]
        except KeyError:
            cn = ContextualizedNode(context, node.children[3])
            for_types = iterable.py__iter__types(evaluator, cn.infer(), cn)
            c_node = ContextualizedName(context, tree_name)
            types = check_tuple_assignments(evaluator, c_node, for_types)
    elif typ == 'expr_stmt':
        types = _remove_statements(evaluator, context, node, tree_name)
    elif typ == 'with_stmt':
        types = context.eval_node(node.node_from_name(tree_name))
    elif typ in ('import_from', 'import_name'):
        types = imports.infer_import(context, tree_name)
    elif typ in ('funcdef', 'classdef'):
        types = _apply_decorators(evaluator, context, node)
    elif typ == 'global_stmt':
        context = evaluator.create_context(context, tree_name)
        finder = NameFinder(evaluator, context, context, str(tree_name))
        filters = finder.get_filters(search_global=True)
        # For global_stmt lookups, we only need the first possible scope,
        # which means the function itself.
        filters = [next(filters)]
        types += finder.find(filters, attribute_lookup=False)
    elif typ == 'try_stmt':
        # TODO an exception can also be a tuple. Check for those.
        # TODO check for types that are not classes and add it to
        # the static analysis report.
        exceptions = context.eval_node(tree_name.get_previous_sibling().get_previous_sibling())
        types = unite(
            evaluator.execute(t, param.ValuesArguments([]))
            for t in exceptions
        )
    else:
        raise ValueError("Should not happen.")
    return types


</t>
<t tx="ekr.20180516071751.110">def __init__(self, instance, class_context, tree_name):
    self._instance = instance
    self.class_context = class_context
    self.tree_name = tree_name

</t>
<t tx="ekr.20180516071751.111">@property
def parent_context(self):
    return self._instance.create_instance_context(self.class_context, self.tree_name)


</t>
<t tx="ekr.20180516071751.112">class LazyInstanceClassName(LazyInstanceName):
    @others
</t>
<t tx="ekr.20180516071751.113">def infer(self):
    for result_context in super(LazyInstanceClassName, self).infer():
        if isinstance(result_context, er.FunctionContext):
            # Classes are never used to resolve anything within the
            # functions. Only other functions and modules will resolve
            # those things.
            parent_context = result_context.parent_context
            while parent_context.is_class():
                parent_context = parent_context.parent_context

            yield BoundMethod(
                result_context.evaluator, self._instance, self.class_context,
                parent_context, result_context.tree_node
            )
        else:
            for c in er.apply_py__get__(result_context, self._instance):
                yield c


</t>
<t tx="ekr.20180516071751.114">class InstanceClassFilter(filters.ParserTreeFilter):
    name_class = LazyInstanceClassName

    @others
</t>
<t tx="ekr.20180516071751.115">def __init__(self, evaluator, context, class_context, origin_scope):
    super(InstanceClassFilter, self).__init__(
        evaluator=evaluator,
        context=context,
        node_context=class_context,
        origin_scope=origin_scope
    )
    self._class_context = class_context

</t>
<t tx="ekr.20180516071751.116">def _equals_origin_scope(self):
    node = self._origin_scope
    while node is not None:
        if node == self._parser_scope or node == self.context:
            return True
        node = node.get_parent_scope()
    return False

</t>
<t tx="ekr.20180516071751.117">def _access_possible(self, name):
    return not name.value.startswith('__') or name.value.endswith('__') \
        or self._equals_origin_scope()

</t>
<t tx="ekr.20180516071751.118">def _filter(self, names):
    names = super(InstanceClassFilter, self)._filter(names)
    return [name for name in names if self._access_possible(name)]

</t>
<t tx="ekr.20180516071751.119">def _convert_names(self, names):
    return [self.name_class(self.context, self._class_context, name) for name in names]


</t>
<t tx="ekr.20180516071751.12">def _apply_decorators(evaluator, context, node):
    """
    Returns the function, that should to be executed in the end.
    This is also the places where the decorators are processed.
    """
    if node.type == 'classdef':
        decoratee_context = er.ClassContext(
            evaluator,
            parent_context=context,
            classdef=node
        )
    else:
        decoratee_context = er.FunctionContext(
            evaluator,
            parent_context=context,
            funcdef=node
        )
    initial = values = set([decoratee_context])
    for dec in reversed(node.get_decorators()):
        debug.dbg('decorator: %s %s', dec, values)
        dec_values = context.eval_node(dec.children[1])
        trailer_nodes = dec.children[2:-1]
        if trailer_nodes:
            # Create a trailer and evaluate it.
            trailer = tree.PythonNode('trailer', trailer_nodes)
            trailer.parent = dec
            dec_values = evaluator.eval_trailer(context, dec_values, trailer)

        if not len(dec_values):
            debug.warning('decorator not found: %s on %s', dec, node)
            return initial

        values = unite(dec_value.execute(param.ValuesArguments([values]))
                       for dec_value in dec_values)
        if not len(values):
            debug.warning('not possible to resolve wrappers found %s', node)
            return initial

        debug.dbg('decorator end %s', values)
    return values


</t>
<t tx="ekr.20180516071751.120">class SelfNameFilter(InstanceClassFilter):
    name_class = LazyInstanceName

    @others
</t>
<t tx="ekr.20180516071751.121">def _filter(self, names):
    names = self._filter_self_names(names)
    if isinstance(self._parser_scope, compiled.CompiledObject) and False:
        # This would be for builtin skeletons, which are not yet supported.
        return list(names)
    else:
        start, end = self._parser_scope.start_pos, self._parser_scope.end_pos
        return [n for n in names if start &lt; n.start_pos &lt; end]

</t>
<t tx="ekr.20180516071751.122">def _filter_self_names(self, names):
    for name in names:
        trailer = name.parent
        if trailer.type == 'trailer' \
                and len(trailer.children) == 2 \
                and trailer.children[0] == '.':
            if name.is_definition() and self._access_possible(name):
                yield name

</t>
<t tx="ekr.20180516071751.123">def _check_flows(self, names):
    return names


</t>
<t tx="ekr.20180516071751.124">class ParamArguments(object):
    """
    TODO This seems like a strange class, clean up?
    """
    @others
</t>
<t tx="ekr.20180516071751.125">class LazyParamContext(object):
    @others
</t>
<t tx="ekr.20180516071751.126">def __init__(self, fucking_param):
    self._param = fucking_param

</t>
<t tx="ekr.20180516071751.127">def infer(self):
    return self._param.infer()

</t>
<t tx="ekr.20180516071751.128">def __init__(self, class_context, funcdef):
    self._class_context = class_context
    self._funcdef = funcdef

</t>
<t tx="ekr.20180516071751.129">def unpack(self, func=None):
    params = search_params(
        self._class_context.evaluator,
        self._class_context,
        self._funcdef
    )
    is_first = True
    for p in params:
        # TODO Yeah, here at last, the class seems to be really wrong.
        if is_first:
            is_first = False
            continue
        yield None, self.LazyParamContext(p)


</t>
<t tx="ekr.20180516071751.13">def _remove_statements(evaluator, context, stmt, name):
    """
    This is the part where statements are being stripped.

    Due to lazy evaluation, statements like a = func; b = a; b() have to be
    evaluated.
    """
    types = set()
    check_instance = None

    pep0484types = \
        pep0484.find_type_from_comment_hint_assign(context, stmt, name)
    if pep0484types:
        return pep0484types
    types |= context.eval_stmt(stmt, seek_name=name)

    if check_instance is not None:
        # class renames
        types = set([er.get_instance_el(evaluator, check_instance, a, True)
                     if isinstance(a, er.Function) else a for a in types])
    return types


</t>
<t tx="ekr.20180516071751.130">class InstanceVarArgs(object):
    @others
</t>
<t tx="ekr.20180516071751.131">def __init__(self, instance, funcdef, var_args):
    self._instance = instance
    self._funcdef = funcdef
    self._var_args = var_args

</t>
<t tx="ekr.20180516071751.132">@memoize_method
def _get_var_args(self):
    if self._var_args is None:
        # TODO this parent_context might be wrong. test?!
        return ParamArguments(self._instance.class_context, self._funcdef)

    return self._var_args

</t>
<t tx="ekr.20180516071751.133">def unpack(self, func=None):
    yield None, LazyKnownContext(self._instance)
    for values in self._get_var_args().unpack(func):
        yield values

</t>
<t tx="ekr.20180516071751.134">def get_calling_nodes(self):
    return self._get_var_args().get_calling_nodes()

</t>
<t tx="ekr.20180516071751.135">def __getattr__(self, name):
    return getattr(self._var_args, name)


</t>
<t tx="ekr.20180516071751.136">class InstanceFunctionExecution(er.FunctionExecutionContext):
    @others
</t>
<t tx="ekr.20180516071751.137">def __init__(self, instance, parent_context, function_context, var_args):
    self.instance = instance
    var_args = InstanceVarArgs(instance, function_context.tree_node, var_args)

    super(InstanceFunctionExecution, self).__init__(
        instance.evaluator, parent_context, function_context, var_args)


</t>
<t tx="ekr.20180516071751.138">class AnonymousInstanceFunctionExecution(InstanceFunctionExecution):
    function_execution_filter = filters.AnonymousInstanceFunctionExecutionFilter

    @others
</t>
<t tx="ekr.20180516071751.139">def __init__(self, instance, parent_context, function_context):
    super(AnonymousInstanceFunctionExecution, self).__init__(
        instance, parent_context, function_context, None)
</t>
<t tx="ekr.20180516071751.14">def _check_flow_information(context, flow, search_name, pos):
    """ Try to find out the type of a variable just with the information that
    is given by the flows: e.g. It is also responsible for assert checks.::

        if isinstance(k, str):
            k.  # &lt;- completion here

    ensures that `k` is a string.
    """
    if not settings.dynamic_flow_information:
        return None

    result = None
    if flow.is_scope():
        # Check for asserts.
        module_node = flow.get_root_node()
        try:
            names = module_node.used_names[search_name.value]
        except KeyError:
            return None
        names = reversed([
            n for n in names
            if flow.start_pos &lt;= n.start_pos &lt; (pos or flow.end_pos)
        ])

        for name in names:
            ass = tree.search_ancestor(name, 'assert_stmt')
            if ass is not None:
                result = _check_isinstance_type(context, ass.assertion(), search_name)
                if result is not None:
                    return result

    if flow.type in ('if_stmt', 'while_stmt'):
        potential_ifs = [c for c in flow.children[1::4] if c != ':']
        for if_test in reversed(potential_ifs):
            if search_name.start_pos &gt; if_test.end_pos:
                return _check_isinstance_type(context, if_test, search_name)
    return result


</t>
<t tx="ekr.20180516071751.140">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.141">"""
Contains all classes and functions to deal with lists, dicts, generators and
iterators in general.

Array modifications
*******************

If the content of an array (``set``/``list``) is requested somewhere, the
current module will be checked for appearances of ``arr.append``,
``arr.insert``, etc.  If the ``arr`` name points to an actual array, the
content will be added

This can be really cpu intensive, as you can imagine. Because |jedi| has to
follow **every** ``append`` and check wheter it's the right array. However this
works pretty good, because in *slow* cases, the recursion detector and other
settings will stop this process.

It is important to note that:

1. Array modfications work only in the current module.
2. Jedi only checks Array additions; ``list.pop``, etc are ignored.
"""
from jedi import debug
from jedi import settings
from jedi import common
from jedi.common import unite, safe_property
from jedi._compatibility import unicode, zip_longest, is_py3
from jedi.evaluate import compiled
from jedi.evaluate import helpers
from jedi.evaluate import analysis
from jedi.evaluate import pep0484
from jedi.evaluate import context
from jedi.evaluate import precedence
from jedi.evaluate import recursion
from jedi.evaluate.cache import memoize_default
from jedi.evaluate.filters import DictFilter, AbstractNameDefinition, \
    ParserTreeFilter


</t>
<t tx="ekr.20180516071751.142">class AbstractSequence(context.Context):
    builtin_methods = {}
    api_type = 'instance'

    @others
</t>
<t tx="ekr.20180516071751.143">def __init__(self, evaluator):
    super(AbstractSequence, self).__init__(evaluator, evaluator.BUILTINS)

</t>
<t tx="ekr.20180516071751.144">def get_filters(self, search_global, until_position=None, origin_scope=None):
    raise NotImplementedError

</t>
<t tx="ekr.20180516071751.145">@property
def name(self):
    return compiled.CompiledContextName(self, self.array_type)


</t>
<t tx="ekr.20180516071751.146">class BuiltinMethod(object):
    """``Generator.__next__`` ``dict.values`` methods and so on."""
    @others
</t>
<t tx="ekr.20180516071751.147">def __init__(self, builtin_context, method, builtin_func):
    self._builtin_context = builtin_context
    self._method = method
    self._builtin_func = builtin_func

</t>
<t tx="ekr.20180516071751.148">def py__call__(self, params):
    return self._method(self._builtin_context)

</t>
<t tx="ekr.20180516071751.149">def __getattr__(self, name):
    return getattr(self._builtin_func, name)


</t>
<t tx="ekr.20180516071751.15">def _check_isinstance_type(context, element, search_name):
    try:
        assert element.type in ('power', 'atom_expr')
        # this might be removed if we analyze and, etc
        assert len(element.children) == 2
        first, trailer = element.children
        assert first.type == 'name' and first.value == 'isinstance'
        assert trailer.type == 'trailer' and trailer.children[0] == '('
        assert len(trailer.children) == 3

        # arglist stuff
        arglist = trailer.children[1]
        args = param.TreeArguments(context.evaluator, context, arglist, trailer)
        param_list = list(args.unpack())
        # Disallow keyword arguments
        assert len(param_list) == 2
        (key1, lazy_context_object), (key2, lazy_context_cls) = param_list
        assert key1 is None and key2 is None
        call = helpers.call_of_leaf(search_name)
        is_instance_call = helpers.call_of_leaf(lazy_context_object.data)
        # Do a simple get_code comparison. They should just have the same code,
        # and everything will be all right.
        assert is_instance_call.get_code(normalized=True) == call.get_code(normalized=True)
    except AssertionError:
        return None

    result = set()
    for cls_or_tup in lazy_context_cls.infer():
        if isinstance(cls_or_tup, iterable.AbstractSequence) and \
                cls_or_tup.array_type == 'tuple':
            for lazy_context in cls_or_tup.py__iter__():
                for context in lazy_context.infer():
                    result |= context.execute_evaluated()
        else:
            result |= cls_or_tup.execute_evaluated()
    return result


</t>
<t tx="ekr.20180516071751.150">class SpecialMethodFilter(DictFilter):
    """
    A filter for methods that are defined in this module on the corresponding
    classes like Generator (for __next__, etc).
    """
    @others
</t>
<t tx="ekr.20180516071751.151">class SpecialMethodName(AbstractNameDefinition):
    api_type = 'function'

    @others
</t>
<t tx="ekr.20180516071751.152">def __init__(self, parent_context, string_name, callable_, builtin_context):
    self.parent_context = parent_context
    self.string_name = string_name
    self._callable = callable_
    self._builtin_context = builtin_context

</t>
<t tx="ekr.20180516071751.153">def infer(self):
    filter = next(self._builtin_context.get_filters())
    # We can take the first index, because on builtin methods there's
    # always only going to be one name. The same is true for the
    # inferred values.
    builtin_func = next(iter(filter.get(self.string_name)[0].infer()))
    return set([BuiltinMethod(self.parent_context, self._callable, builtin_func)])

</t>
<t tx="ekr.20180516071751.154">def __init__(self, context, dct, builtin_context):
    super(SpecialMethodFilter, self).__init__(dct)
    self.context = context
    self._builtin_context = builtin_context
    """
    This context is what will be used to introspect the name, where as the
    other context will be used to execute the function.

    We distinguish, because we have to.
    """

</t>
<t tx="ekr.20180516071751.155">def _convert(self, name, value):
    return self.SpecialMethodName(self.context, name, value, self._builtin_context)


</t>
<t tx="ekr.20180516071751.156">def has_builtin_methods(cls):
    base_dct = {}
    # Need to care properly about inheritance. Builtin Methods should not get
    # lost, just because they are not mentioned in a class.
    for base_cls in reversed(cls.__bases__):
        try:
            base_dct.update(base_cls.builtin_methods)
        except AttributeError:
            pass

    cls.builtin_methods = base_dct
    for func in cls.__dict__.values():
        try:
            cls.builtin_methods.update(func.registered_builtin_methods)
        except AttributeError:
            pass
    return cls


</t>
<t tx="ekr.20180516071751.157">def register_builtin_method(method_name, python_version_match=None):
    def wrapper(func):
        if python_version_match and python_version_match != 2 + int(is_py3):
            # Some functions do only apply to certain versions.
            return func
        dct = func.__dict__.setdefault('registered_builtin_methods', {})
        dct[method_name] = func
        return func
    return wrapper


</t>
<t tx="ekr.20180516071751.158">@has_builtin_methods
class GeneratorMixin(object):
    array_type = None

    @others
</t>
<t tx="ekr.20180516071751.159">@register_builtin_method('send')
@register_builtin_method('next', python_version_match=2)
@register_builtin_method('__next__', python_version_match=3)
def py__next__(self):
    # TODO add TypeError if params are given.
    return unite(lazy_context.infer() for lazy_context in self.py__iter__())

</t>
<t tx="ekr.20180516071751.16">def check_tuple_assignments(evaluator, contextualized_name, types):
    """
    Checks if tuples are assigned.
    """
    lazy_context = None
    for index, node in contextualized_name.assignment_indexes():
        cn = ContextualizedNode(contextualized_name.context, node)
        iterated = iterable.py__iter__(evaluator, types, cn)
        for _ in range(index + 1):
            try:
                lazy_context = next(iterated)
            except StopIteration:
                # We could do this with the default param in next. But this
                # would allow this loop to run for a very long time if the
                # index number is high. Therefore break if the loop is
                # finished.
                return set()
        types = lazy_context.infer()
    return types
</t>
<t tx="ekr.20180516071751.160">def get_filters(self, search_global, until_position=None, origin_scope=None):
    gen_obj = compiled.get_special_object(self.evaluator, 'GENERATOR_OBJECT')
    yield SpecialMethodFilter(self, self.builtin_methods, gen_obj)
    for filter in gen_obj.get_filters(search_global):
        yield filter

</t>
<t tx="ekr.20180516071751.161">def py__bool__(self):
    return True

</t>
<t tx="ekr.20180516071751.162">def py__class__(self):
    gen_obj = compiled.get_special_object(self.evaluator, 'GENERATOR_OBJECT')
    return gen_obj.py__class__()

</t>
<t tx="ekr.20180516071751.163">@property
def name(self):
    return compiled.CompiledContextName(self, 'generator')


</t>
<t tx="ekr.20180516071751.164">class Generator(GeneratorMixin, context.Context):
    """Handling of `yield` functions."""

    @others
</t>
<t tx="ekr.20180516071751.165">def __init__(self, evaluator, func_execution_context):
    super(Generator, self).__init__(evaluator, parent_context=evaluator.BUILTINS)
    self._func_execution_context = func_execution_context

</t>
<t tx="ekr.20180516071751.166">def py__iter__(self):
    return self._func_execution_context.get_yield_values()

</t>
<t tx="ekr.20180516071751.167">def __repr__(self):
    return "&lt;%s of %s&gt;" % (type(self).__name__, self._func_execution_context)


</t>
<t tx="ekr.20180516071751.168">class CompForContext(context.TreeContext):
    @others
</t>
<t tx="ekr.20180516071751.169">@classmethod
def from_comp_for(cls, parent_context, comp_for):
    return cls(parent_context.evaluator, parent_context, comp_for)

</t>
<t tx="ekr.20180516071751.17">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.170">def __init__(self, evaluator, parent_context, comp_for):
    super(CompForContext, self).__init__(evaluator, parent_context)
    self.tree_node = comp_for

</t>
<t tx="ekr.20180516071751.171">def get_node(self):
    return self.tree_node

</t>
<t tx="ekr.20180516071751.172">def get_filters(self, search_global, until_position=None, origin_scope=None):
    yield ParserTreeFilter(self.evaluator, self)


</t>
<t tx="ekr.20180516071751.173">class Comprehension(AbstractSequence):
    @others
</t>
<t tx="ekr.20180516071751.174">@staticmethod
def from_atom(evaluator, context, atom):
    bracket = atom.children[0]
    if bracket == '{':
        if atom.children[1].children[1] == ':':
            cls = DictComprehension
        else:
            cls = SetComprehension
    elif bracket == '(':
        cls = GeneratorComprehension
    elif bracket == '[':
        cls = ListComprehension
    return cls(evaluator, context, atom)

</t>
<t tx="ekr.20180516071751.175">def __init__(self, evaluator, defining_context, atom):
    super(Comprehension, self).__init__(evaluator)
    self._defining_context = defining_context
    self._atom = atom

</t>
<t tx="ekr.20180516071751.176">def _get_comprehension(self):
    # The atom contains a testlist_comp
    return self._atom.children[1]

</t>
<t tx="ekr.20180516071751.177">def _get_comp_for(self):
    # The atom contains a testlist_comp
    return self._get_comprehension().children[1]

</t>
<t tx="ekr.20180516071751.178">def _eval_node(self, index=0):
    """
    The first part `x + 1` of the list comprehension:

        [x + 1 for x in foo]
    """
    return self._get_comprehension().children[index]

</t>
<t tx="ekr.20180516071751.179">@memoize_default()
def _get_comp_for_context(self, parent_context, comp_for):
    # TODO shouldn't this be part of create_context?
    return CompForContext.from_comp_for(parent_context, comp_for)

</t>
<t tx="ekr.20180516071751.18">class Status(object):
    lookup_table = {}

    @others
REACHABLE = Status(True, 'reachable')
UNREACHABLE = Status(False, 'unreachable')
UNSURE = Status(None, 'unsure')


</t>
<t tx="ekr.20180516071751.180">def _nested(self, comp_fors, parent_context=None):
    evaluator = self.evaluator
    comp_for = comp_fors[0]
    input_node = comp_for.children[3]
    parent_context = parent_context or self._defining_context
    input_types = parent_context.eval_node(input_node)

    cn = context.ContextualizedNode(parent_context, input_node)
    iterated = py__iter__(evaluator, input_types, cn)
    exprlist = comp_for.children[1]
    for i, lazy_context in enumerate(iterated):
        types = lazy_context.infer()
        dct = unpack_tuple_to_dict(parent_context, types, exprlist)
        context_ = self._get_comp_for_context(
            parent_context,
            comp_for,
        )
        with helpers.predefine_names(context_, comp_for, dct):
            try:
                for result in self._nested(comp_fors[1:], context_):
                    yield result
            except IndexError:
                iterated = context_.eval_node(self._eval_node())
                if self.array_type == 'dict':
                    yield iterated, context_.eval_node(self._eval_node(2))
                else:
                    yield iterated

</t>
<t tx="ekr.20180516071751.181">@memoize_default(default=[])
@common.to_list
def _iterate(self):
    comp_fors = tuple(self._get_comp_for().get_comp_fors())
    for result in self._nested(comp_fors):
        yield result

</t>
<t tx="ekr.20180516071751.182">def py__iter__(self):
    for set_ in self._iterate():
        yield context.LazyKnownContexts(set_)

</t>
<t tx="ekr.20180516071751.183">def __repr__(self):
    return "&lt;%s of %s&gt;" % (type(self).__name__, self._atom)


</t>
<t tx="ekr.20180516071751.184">class ArrayMixin(object):
    @others
</t>
<t tx="ekr.20180516071751.185">def get_filters(self, search_global, until_position=None, origin_scope=None):
    # `array.type` is a string with the type, e.g. 'list'.
    compiled_obj = compiled.builtin_from_name(self.evaluator, self.array_type)
    yield SpecialMethodFilter(self, self.builtin_methods, compiled_obj)
    for typ in compiled_obj.execute_evaluated(self):
        for filter in typ.get_filters():
            yield filter

</t>
<t tx="ekr.20180516071751.186">def py__bool__(self):
    return None  # We don't know the length, because of appends.

</t>
<t tx="ekr.20180516071751.187">def py__class__(self):
    return compiled.builtin_from_name(self.evaluator, self.array_type)

</t>
<t tx="ekr.20180516071751.188">@safe_property
def parent(self):
    return self.evaluator.BUILTINS

</t>
<t tx="ekr.20180516071751.189">def dict_values(self):
    return unite(self._defining_context.eval_node(v) for k, v in self._items())


</t>
<t tx="ekr.20180516071751.19">def __init__(self, value, name):
    self._value = value
    self._name = name
    Status.lookup_table[value] = self

</t>
<t tx="ekr.20180516071751.190">class ListComprehension(ArrayMixin, Comprehension):
    array_type = 'list'

    @others
</t>
<t tx="ekr.20180516071751.191">def py__getitem__(self, index):
    if isinstance(index, slice):
        return set([self])

    all_types = list(self.py__iter__())
    return all_types[index].infer()


</t>
<t tx="ekr.20180516071751.192">class SetComprehension(ArrayMixin, Comprehension):
    array_type = 'set'


</t>
<t tx="ekr.20180516071751.193">@has_builtin_methods
class DictComprehension(ArrayMixin, Comprehension):
    array_type = 'dict'

    @others
</t>
<t tx="ekr.20180516071751.194">def _get_comp_for(self):
    return self._get_comprehension().children[3]

</t>
<t tx="ekr.20180516071751.195">def py__iter__(self):
    for keys, values in self._iterate():
        yield context.LazyKnownContexts(keys)

</t>
<t tx="ekr.20180516071751.196">def py__getitem__(self, index):
    for keys, values in self._iterate():
        for k in keys:
            if isinstance(k, compiled.CompiledObject):
                if k.obj == index:
                    return values
    return self.dict_values()

</t>
<t tx="ekr.20180516071751.197">def dict_values(self):
    return unite(values for keys, values in self._iterate())

</t>
<t tx="ekr.20180516071751.198">@register_builtin_method('values')
def _imitate_values(self):
    lazy_context = context.LazyKnownContexts(self.dict_values())
    return set([FakeSequence(self.evaluator, 'list', [lazy_context])])

</t>
<t tx="ekr.20180516071751.199">@register_builtin_method('items')
def _imitate_items(self):
    items = set(
        FakeSequence(
            self.evaluator, 'tuple'
            (context.LazyKnownContexts(keys), context.LazyKnownContexts(values))
        ) for keys, values in self._iterate()
    )

    return create_evaluated_sequence_set(self.evaluator, items, sequence_type='list')


</t>
<t tx="ekr.20180516071751.2">"""
Searching for names with given scope and name. This is very central in Jedi and
Python. The name resolution is quite complicated with descripter,
``__getattribute__``, ``__getattr__``, ``global``, etc.

If you want to understand name resolution, please read the first few chapters
in http://blog.ionelmc.ro/2015/02/09/understanding-python-metaclasses/.

Flow checks
+++++++++++

Flow checks are not really mature. There's only a check for ``isinstance``.  It
would check whether a flow has the form of ``if isinstance(a, type_or_tuple)``.
Unfortunately every other thing is being ignored (e.g. a == '' would be easy to
check for -&gt; a is a string). There's big potential in these checks.
"""

from jedi.parser.python import tree
from jedi import debug
from jedi.common import unite
from jedi import settings
from jedi.evaluate import representation as er
from jedi.evaluate.instance import AbstractInstanceContext
from jedi.evaluate import compiled
from jedi.evaluate import pep0484
from jedi.evaluate import iterable
from jedi.evaluate import imports
from jedi.evaluate import analysis
from jedi.evaluate import flow_analysis
from jedi.evaluate import param
from jedi.evaluate import helpers
from jedi.evaluate.filters import get_global_filters
from jedi.evaluate.context import ContextualizedName, ContextualizedNode


</t>
<t tx="ekr.20180516071751.20">def invert(self):
    if self is REACHABLE:
        return UNREACHABLE
    elif self is UNREACHABLE:
        return REACHABLE
    else:
        return UNSURE

</t>
<t tx="ekr.20180516071751.200">class GeneratorComprehension(GeneratorMixin, Comprehension):
    pass


</t>
<t tx="ekr.20180516071751.201">class SequenceLiteralContext(ArrayMixin, AbstractSequence):
    mapping = {'(': 'tuple',
               '[': 'list',
               '{': 'set'}

    @others
</t>
<t tx="ekr.20180516071751.202">def __init__(self, evaluator, defining_context, atom):
    super(SequenceLiteralContext, self).__init__(evaluator)
    self.atom = atom
    self._defining_context = defining_context

    if self.atom.type in ('testlist_star_expr', 'testlist'):
        self.array_type = 'tuple'
    else:
        self.array_type = SequenceLiteralContext.mapping[atom.children[0]]
        """The builtin name of the array (list, set, tuple or dict)."""

</t>
<t tx="ekr.20180516071751.203">def py__getitem__(self, index):
    """Here the index is an int/str. Raises IndexError/KeyError."""
    if self.array_type == 'dict':
        for key, value in self._items():
            for k in self._defining_context.eval_node(key):
                if isinstance(k, compiled.CompiledObject) \
                        and index == k.obj:
                    return self._defining_context.eval_node(value)
        raise KeyError('No key found in dictionary %s.' % self)

    # Can raise an IndexError
    if isinstance(index, slice):
        return set([self])
    else:
        return self._defining_context.eval_node(self._items()[index])

</t>
<t tx="ekr.20180516071751.204">def py__iter__(self):
    """
    While values returns the possible values for any array field, this
    function returns the value for a certain index.
    """
    if self.array_type == 'dict':
        # Get keys.
        types = set()
        for k, _ in self._items():
            types |= self._defining_context.eval_node(k)
        # We don't know which dict index comes first, therefore always
        # yield all the types.
        for _ in types:
            yield context.LazyKnownContexts(types)
    else:
        for node in self._items():
            yield context.LazyTreeContext(self._defining_context, node)

        for addition in check_array_additions(self._defining_context, self):
            yield addition

</t>
<t tx="ekr.20180516071751.205">def _values(self):
    """Returns a list of a list of node."""
    if self.array_type == 'dict':
        return unite(v for k, v in self._items())
    else:
        return self._items()

</t>
<t tx="ekr.20180516071751.206">def _items(self):
    c = self.atom.children

    if self.atom.type in ('testlist_star_expr', 'testlist'):
        return c[::2]

    array_node = c[1]
    if array_node in (']', '}', ')'):
        return []  # Direct closing bracket, doesn't contain items.

    if array_node.type == 'testlist_comp':
        return array_node.children[::2]
    elif array_node.type == 'dictorsetmaker':
        kv = []
        iterator = iter(array_node.children)
        for key in iterator:
            op = next(iterator, None)
            if op is None or op == ',':
                kv.append(key)  # A set.
            else:
                assert op == ':'  # A dict.
                kv.append((key, next(iterator)))
                next(iterator, None)  # Possible comma.
        return kv
    else:
        return [array_node]

</t>
<t tx="ekr.20180516071751.207">def exact_key_items(self):
    """
    Returns a generator of tuples like dict.items(), where the key is
    resolved (as a string) and the values are still lazy contexts.
    """
    for key_node, value in self._items():
        for key in self._defining_context.eval_node(key_node):
            if precedence.is_string(key):
                yield key.obj, context.LazyTreeContext(self._defining_context, value)

</t>
<t tx="ekr.20180516071751.208">def __repr__(self):
    return "&lt;%s of %s&gt;" % (self.__class__.__name__, self.atom)


</t>
<t tx="ekr.20180516071751.209">@has_builtin_methods
class DictLiteralContext(SequenceLiteralContext):
    array_type = 'dict'

    @others
</t>
<t tx="ekr.20180516071751.21">def __and__(self, other):
    if UNSURE in (self, other):
        return UNSURE
    else:
        return REACHABLE if self._value and other._value else UNREACHABLE

</t>
<t tx="ekr.20180516071751.210">def __init__(self, evaluator, defining_context, atom):
    super(SequenceLiteralContext, self).__init__(evaluator)
    self._defining_context = defining_context
    self.atom = atom

</t>
<t tx="ekr.20180516071751.211">@register_builtin_method('values')
def _imitate_values(self):
    lazy_context = context.LazyKnownContexts(self.dict_values())
    return set([FakeSequence(self.evaluator, 'list', [lazy_context])])

</t>
<t tx="ekr.20180516071751.212">@register_builtin_method('items')
def _imitate_items(self):
    lazy_contexts = [
        context.LazyKnownContext(FakeSequence(
            self.evaluator, 'tuple',
            (context.LazyTreeContext(self._defining_context, key_node),
             context.LazyTreeContext(self._defining_context, value_node))
        )) for key_node, value_node in self._items()
    ]

    return set([FakeSequence(self.evaluator, 'list', lazy_contexts)])


</t>
<t tx="ekr.20180516071751.213">class _FakeArray(SequenceLiteralContext):
    @others
</t>
<t tx="ekr.20180516071751.214">def __init__(self, evaluator, container, type):
    super(SequenceLiteralContext, self).__init__(evaluator)
    self.array_type = type
    self.atom = container
    # TODO is this class really needed?


</t>
<t tx="ekr.20180516071751.215">class FakeSequence(_FakeArray):
    @others
</t>
<t tx="ekr.20180516071751.216">def __init__(self, evaluator, array_type, lazy_context_list):
    """
    type should be one of "tuple", "list"
    """
    super(FakeSequence, self).__init__(evaluator, None, array_type)
    self._lazy_context_list = lazy_context_list

</t>
<t tx="ekr.20180516071751.217">def _items(self):
    raise DeprecationWarning
    return self._context_list

</t>
<t tx="ekr.20180516071751.218">def py__getitem__(self, index):
    return set(self._lazy_context_list[index].infer())

</t>
<t tx="ekr.20180516071751.219">def py__iter__(self):
    return self._lazy_context_list

</t>
<t tx="ekr.20180516071751.22">def __repr__(self):
    return '&lt;%s: %s&gt;' % (type(self).__name__, self._name)


</t>
<t tx="ekr.20180516071751.220">def __repr__(self):
    return "&lt;%s of %s&gt;" % (type(self).__name__, self._lazy_context_list)


</t>
<t tx="ekr.20180516071751.221">class FakeDict(_FakeArray):
    @others
</t>
<t tx="ekr.20180516071751.222">def __init__(self, evaluator, dct):
    super(FakeDict, self).__init__(evaluator, dct, 'dict')
    self._dct = dct

</t>
<t tx="ekr.20180516071751.223">def py__iter__(self):
    for key in self._dct:
        yield context.LazyKnownContext(compiled.create(self.evaluator, key))

</t>
<t tx="ekr.20180516071751.224">def py__getitem__(self, index):
    return self._dct[index].infer()

</t>
<t tx="ekr.20180516071751.225">def dict_values(self):
    return unite(lazy_context.infer() for lazy_context in self._dct.values())

</t>
<t tx="ekr.20180516071751.226">def _items(self):
    raise DeprecationWarning
    for key, values in self._dct.items():
        # TODO this is not proper. The values could be multiple values?!
        yield key, values[0]

</t>
<t tx="ekr.20180516071751.227">def exact_key_items(self):
    return self._dct.items()


</t>
<t tx="ekr.20180516071751.228">class MergedArray(_FakeArray):
    @others
</t>
<t tx="ekr.20180516071751.229">def __init__(self, evaluator, arrays):
    super(MergedArray, self).__init__(evaluator, arrays, arrays[-1].array_type)
    self._arrays = arrays

</t>
<t tx="ekr.20180516071751.23">def _get_flow_scopes(node):
    while True:
        node = node.get_parent_scope(include_flows=True)
        if node is None or node.is_scope():
            return
        yield node


</t>
<t tx="ekr.20180516071751.230">def py__iter__(self):
    for array in self._arrays:
        for lazy_context in array.py__iter__():
            yield lazy_context

</t>
<t tx="ekr.20180516071751.231">def py__getitem__(self, index):
    return unite(lazy_context.infer() for lazy_context in self.py__iter__())

</t>
<t tx="ekr.20180516071751.232">def _items(self):
    for array in self._arrays:
        for a in array._items():
            yield a

</t>
<t tx="ekr.20180516071751.233">def __len__(self):
    return sum(len(a) for a in self._arrays)


</t>
<t tx="ekr.20180516071751.234">def unpack_tuple_to_dict(context, types, exprlist):
    """
    Unpacking tuple assignments in for statements and expr_stmts.
    """
    if exprlist.type == 'name':
        return {exprlist.value: types}
    elif exprlist.type == 'atom' and exprlist.children[0] in '([':
        return unpack_tuple_to_dict(context, types, exprlist.children[1])
    elif exprlist.type in ('testlist', 'testlist_comp', 'exprlist',
                           'testlist_star_expr'):
        dct = {}
        parts = iter(exprlist.children[::2])
        n = 0
        for lazy_context in py__iter__(context.evaluator, types, exprlist):
            n += 1
            try:
                part = next(parts)
            except StopIteration:
                # TODO this context is probably not right.
                analysis.add(context, 'value-error-too-many-values', part,
                             message="ValueError: too many values to unpack (expected %s)" % n)
            else:
                dct.update(unpack_tuple_to_dict(context, lazy_context.infer(), part))
        has_parts = next(parts, None)
        if types and has_parts is not None:
            # TODO this context is probably not right.
            analysis.add(context, 'value-error-too-few-values', has_parts,
                         message="ValueError: need more than %s values to unpack" % n)
        return dct
    elif exprlist.type == 'power' or exprlist.type == 'atom_expr':
        # Something like ``arr[x], var = ...``.
        # This is something that is not yet supported, would also be difficult
        # to write into a dict.
        return {}
    elif exprlist.type == 'star_expr':  # `a, *b, c = x` type unpackings
        # Currently we're not supporting them.
        return {}
    raise NotImplementedError


</t>
<t tx="ekr.20180516071751.235">def py__iter__(evaluator, types, contextualized_node=None):
    debug.dbg('py__iter__')
    type_iters = []
    for typ in types:
        try:
            iter_method = typ.py__iter__
        except AttributeError:
            if contextualized_node is not None:
                analysis.add(
                    contextualized_node.context,
                    'type-error-not-iterable',
                    contextualized_node._node,
                    message="TypeError: '%s' object is not iterable" % typ)
        else:
            type_iters.append(iter_method())

    for lazy_contexts in zip_longest(*type_iters):
        yield context.get_merged_lazy_context(
            [l for l in lazy_contexts if l is not None]
        )


</t>
<t tx="ekr.20180516071751.236">def py__iter__types(evaluator, types, contextualized_node=None):
    """
    Calls `py__iter__`, but ignores the ordering in the end and just returns
    all types that it contains.
    """
    return unite(
        lazy_context.infer()
        for lazy_context in py__iter__(evaluator, types, contextualized_node)
    )


</t>
<t tx="ekr.20180516071751.237">def py__getitem__(evaluator, context, types, trailer):
    from jedi.evaluate.representation import ClassContext
    from jedi.evaluate.instance import TreeInstance
    result = set()

    trailer_op, node, trailer_cl = trailer.children
    assert trailer_op == "["
    assert trailer_cl == "]"

    # special case: PEP0484 typing module, see
    # https://github.com/davidhalter/jedi/issues/663
    for typ in list(types):
        if isinstance(typ, (ClassContext, TreeInstance)):
            typing_module_types = pep0484.py__getitem__(context, typ, node)
            if typing_module_types is not None:
                types.remove(typ)
                result |= typing_module_types

    if not types:
        # all consumed by special cases
        return result

    for index in create_index_types(evaluator, context, node):
        if isinstance(index, (compiled.CompiledObject, Slice)):
            index = index.obj

        if type(index) not in (float, int, str, unicode, slice):
            # If the index is not clearly defined, we have to get all the
            # possiblities.
            for typ in list(types):
                if isinstance(typ, AbstractSequence) and typ.array_type == 'dict':
                    types.remove(typ)
                    result |= typ.dict_values()
            return result | py__iter__types(evaluator, types)

        for typ in types:
            # The actual getitem call.
            try:
                getitem = typ.py__getitem__
            except AttributeError:
                # TODO this context is probably not right.
                analysis.add(context, 'type-error-not-subscriptable', trailer_op,
                             message="TypeError: '%s' object is not subscriptable" % typ)
            else:
                try:
                    result |= getitem(index)
                except IndexError:
                    result |= py__iter__types(evaluator, set([typ]))
                except KeyError:
                    # Must be a dict. Lists don't raise KeyErrors.
                    result |= typ.dict_values()
    return result


</t>
<t tx="ekr.20180516071751.238">def check_array_additions(context, sequence):
    """ Just a mapper function for the internal _check_array_additions """
    if sequence.array_type not in ('list', 'set'):
        # TODO also check for dict updates
        return set()

    return _check_array_additions(context, sequence)


</t>
<t tx="ekr.20180516071751.239">@memoize_default(default=set())
@debug.increase_indent
def _check_array_additions(context, sequence):
    """
    Checks if a `Array` has "add" (append, insert, extend) statements:

    &gt;&gt;&gt; a = [""]
    &gt;&gt;&gt; a.append(1)
    """
    from jedi.evaluate import param

    debug.dbg('Dynamic array search for %s' % sequence, color='MAGENTA')
    module_context = context.get_root_context()
    if not settings.dynamic_array_additions or isinstance(module_context, compiled.CompiledObject):
        debug.dbg('Dynamic array search aborted.', color='MAGENTA')
        return set()

    def find_additions(context, arglist, add_name):
        params = list(param.TreeArguments(context.evaluator, context, arglist).unpack())
        result = set()
        if add_name in ['insert']:
            params = params[1:]
        if add_name in ['append', 'add', 'insert']:
            for key, lazy_context in params:
                result.add(lazy_context)
        elif add_name in ['extend', 'update']:
            for key, lazy_context in params:
                result |= set(py__iter__(context.evaluator, lazy_context.infer()))
        return result

    temp_param_add, settings.dynamic_params_for_other_modules = \
        settings.dynamic_params_for_other_modules, False

    is_list = sequence.name.string_name == 'list'
    search_names = (['append', 'extend', 'insert'] if is_list else ['add', 'update'])

    added_types = set()
    for add_name in search_names:
        try:
            possible_names = module_context.tree_node.used_names[add_name]
        except KeyError:
            continue
        else:
            for name in possible_names:
                context_node = context.tree_node
                if not (context_node.start_pos &lt; name.start_pos &lt; context_node.end_pos):
                    continue
                trailer = name.parent
                power = trailer.parent
                trailer_pos = power.children.index(trailer)
                try:
                    execution_trailer = power.children[trailer_pos + 1]
                except IndexError:
                    continue
                else:
                    if execution_trailer.type != 'trailer' \
                            or execution_trailer.children[0] != '(' \
                            or execution_trailer.children[1] == ')':
                        continue

                random_context = context.create_context(name)

                with recursion.execution_allowed(context.evaluator, power) as allowed:
                    if allowed:
                        found = helpers.evaluate_call_of_leaf(
                            random_context,
                            name,
                            cut_own_trailer=True
                        )
                        if sequence in found:
                            # The arrays match. Now add the results
                            added_types |= find_additions(
                                random_context,
                                execution_trailer.children[1],
                                add_name
                            )

    # reset settings
    settings.dynamic_params_for_other_modules = temp_param_add
    debug.dbg('Dynamic array result %s' % added_types, color='MAGENTA')
    return added_types


</t>
<t tx="ekr.20180516071751.24">def reachability_check(context, context_scope, node, origin_scope=None):
    first_flow_scope = node.get_parent_scope(include_flows=True)
    if origin_scope is not None:
        origin_flow_scopes = list(_get_flow_scopes(origin_scope))
        node_flow_scopes = list(_get_flow_scopes(node))

        branch_matches = True
        for flow_scope in origin_flow_scopes:
            if flow_scope in node_flow_scopes:
                node_keyword = flow_scope.get_branch_keyword(node)
                origin_keyword = flow_scope.get_branch_keyword(origin_scope)
                branch_matches = node_keyword == origin_keyword
                if flow_scope.type == 'if_stmt':
                    if not branch_matches:
                        return UNREACHABLE
                elif flow_scope.type == 'try_stmt':
                    if not branch_matches and origin_keyword == 'else' \
                            and node_keyword == 'except':
                        return UNREACHABLE
                break

        # Direct parents get resolved, we filter scopes that are separate
        # branches.  This makes sense for autocompletion and static analysis.
        # For actual Python it doesn't matter, because we're talking about
        # potentially unreachable code.
        # e.g. `if 0:` would cause all name lookup within the flow make
        # unaccessible. This is not a "problem" in Python, because the code is
        # never called. In Jedi though, we still want to infer types.
        while origin_scope is not None:
            if first_flow_scope == origin_scope and branch_matches:
                return REACHABLE
            origin_scope = origin_scope.parent

    return _break_check(context, context_scope, first_flow_scope, node)


</t>
<t tx="ekr.20180516071751.240">def get_dynamic_array_instance(instance):
    """Used for set() and list() instances."""
    if not settings.dynamic_array_additions:
        return instance.var_args

    ai = _ArrayInstance(instance)
    from jedi.evaluate import param
    return param.ValuesArguments([[ai]])


</t>
<t tx="ekr.20180516071751.241">class _ArrayInstance(object):
    """
    Used for the usage of set() and list().
    This is definitely a hack, but a good one :-)
    It makes it possible to use set/list conversions.

    In contrast to Array, ListComprehension and all other iterable types, this
    is something that is only used inside `evaluate/compiled/fake/builtins.py`
    and therefore doesn't need filters, `py__bool__` and so on, because
    we don't use these operations in `builtins.py`.
    """
    @others
</t>
<t tx="ekr.20180516071751.242">def __init__(self, instance):
    self.instance = instance
    self.var_args = instance.var_args

</t>
<t tx="ekr.20180516071751.243">def py__iter__(self):
    var_args = self.var_args
    try:
        _, lazy_context = next(var_args.unpack())
    except StopIteration:
        pass
    else:
        for lazy in py__iter__(self.instance.evaluator, lazy_context.infer()):
            yield lazy

    from jedi.evaluate import param
    if isinstance(var_args, param.TreeArguments):
        additions = _check_array_additions(var_args.context, self.instance)
        for addition in additions:
            yield addition


</t>
<t tx="ekr.20180516071751.244">class Slice(context.Context):
    @others
</t>
<t tx="ekr.20180516071751.245">def __init__(self, context, start, stop, step):
    super(Slice, self).__init__(
        context.evaluator,
        parent_context=context.evaluator.BUILTINS
    )
    self._context = context
    # all of them are either a Precedence or None.
    self._start = start
    self._stop = stop
    self._step = step

</t>
<t tx="ekr.20180516071751.246">@property
def obj(self):
    """
    Imitate CompiledObject.obj behavior and return a ``builtin.slice()``
    object.
    """
    def get(element):
        if element is None:
            return None

        result = self._context.eval_node(element)
        if len(result) != 1:
            # For simplicity, we want slices to be clear defined with just
            # one type.  Otherwise we will return an empty slice object.
            raise IndexError
        try:
            return list(result)[0].obj
        except AttributeError:
            return None

    try:
        return slice(get(self._start), get(self._stop), get(self._step))
    except IndexError:
        return slice(None, None, None)


</t>
<t tx="ekr.20180516071751.247">def create_index_types(evaluator, context, index):
    """
    Handles slices in subscript nodes.
    """
    if index == ':':
        # Like array[:]
        return set([Slice(context, None, None, None)])
    elif index.type == 'subscript':  # subscript is a slice operation.
        # Like array[:3]
        result = []
        for el in index.children:
            if el == ':':
                if not result:
                    result.append(None)
            elif el.type == 'sliceop':
                if len(el.children) == 2:
                    result.append(el.children[1])
            else:
                result.append(el)
        result += [None] * (3 - len(result))

        return set([Slice(context, *result)])

    # No slices
    return context.eval_node(index)
</t>
<t tx="ekr.20180516071751.248">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.249">"""
This module is not intended to be used in jedi, rather it will be fed to the
jedi-parser to replace classes in the typing module
"""

try:
    from collections import abc
except ImportError:
    # python 2
    import collections as abc


</t>
<t tx="ekr.20180516071751.25">def _break_check(context, context_scope, flow_scope, node):
    reachable = REACHABLE
    if flow_scope.type == 'if_stmt':
        if flow_scope.node_after_else(node):
            for check_node in flow_scope.check_nodes():
                reachable = _check_if(context, check_node)
                if reachable in (REACHABLE, UNSURE):
                    break
            reachable = reachable.invert()
        else:
            flow_node = flow_scope.node_in_which_check_node(node)
            if flow_node is not None:
                reachable = _check_if(context, flow_node)
    elif flow_scope.type in ('try_stmt', 'while_stmt'):
        return UNSURE

    # Only reachable branches need to be examined further.
    if reachable in (UNREACHABLE, UNSURE):
        return reachable

    if context_scope != flow_scope and context_scope != flow_scope.parent:
        flow_scope = flow_scope.get_parent_scope(include_flows=True)
        return reachable &amp; _break_check(context, context_scope, flow_scope, node)
    else:
        return reachable


</t>
<t tx="ekr.20180516071751.250">def factory(typing_name, indextypes):
    class Iterable(abc.Iterable):
        def __iter__(self):
            while True:
                yield indextypes[0]()

    class Iterator(Iterable, abc.Iterator):
        def next(self):
            """ needed for python 2 """
            return self.__next__()

        def __next__(self):
            return indextypes[0]()

    class Sequence(abc.Sequence):
        def __getitem__(self, index):
            return indextypes[0]()

    class MutableSequence(Sequence, abc.MutableSequence):
        pass

    class List(MutableSequence, list):
        pass

    class Tuple(Sequence, tuple):
        def __getitem__(self, index):
            if indextypes[1] == Ellipsis:
                # https://www.python.org/dev/peps/pep-0484/#the-typing-module
                # Tuple[int, ...] means a tuple of ints of indetermined length
                return indextypes[0]()
            else:
                return indextypes[index]()

    class AbstractSet(Iterable, abc.Set):
        pass

    class MutableSet(AbstractSet, abc.MutableSet):
        pass

    class KeysView(Iterable, abc.KeysView):
        pass

    class ValuesView(abc.ValuesView):
        def __iter__(self):
            while True:
                yield indextypes[1]()

    class ItemsView(abc.ItemsView):
        def __iter__(self):
            while True:
                yield (indextypes[0](), indextypes[1]())

    class Mapping(Iterable, abc.Mapping):
        def __getitem__(self, item):
            return indextypes[1]()

        def keys(self):
            return KeysView()

        def values(self):
            return ValuesView()

        def items(self):
            return ItemsView()

    class MutableMapping(Mapping, abc.MutableMapping):
        pass

    class Dict(MutableMapping, dict):
        pass

    dct = {
        "Sequence": Sequence,
        "MutableSequence": MutableSequence,
        "List": List,
        "Iterable": Iterable,
        "Iterator": Iterator,
        "AbstractSet": AbstractSet,
        "MutableSet": MutableSet,
        "Mapping": Mapping,
        "MutableMapping": MutableMapping,
        "Tuple": Tuple,
        "KeysView": KeysView,
        "ItemsView": ItemsView,
        "ValuesView": ValuesView,
        "Dict": Dict,
    }
    return dct[typing_name]
</t>
<t tx="ekr.20180516071751.251">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.252">from collections import defaultdict

from jedi._compatibility import zip_longest
from jedi import debug
from jedi import common
from jedi.parser.python import tree
from jedi.evaluate import iterable
from jedi.evaluate import analysis
from jedi.evaluate import context
from jedi.evaluate import docstrings
from jedi.evaluate import pep0484
from jedi.evaluate.filters import ParamName


</t>
<t tx="ekr.20180516071751.253">def add_argument_issue(parent_context, error_name, lazy_context, message):
    if isinstance(lazy_context, context.LazyTreeContext):
        node = lazy_context.data
        if node.parent.type == 'argument':
            node = node.parent
        analysis.add(parent_context, error_name, node, message)


</t>
<t tx="ekr.20180516071751.254">def try_iter_content(types, depth=0):
    """Helper method for static analysis."""
    if depth &gt; 10:
        # It's possible that a loop has references on itself (especially with
        # CompiledObject). Therefore don't loop infinitely.
        return

    for typ in types:
        try:
            f = typ.py__iter__
        except AttributeError:
            pass
        else:
            for lazy_context in f():
                try_iter_content(lazy_context.infer(), depth + 1)


</t>
<t tx="ekr.20180516071751.255">class AbstractArguments():
    context = None

    @others
</t>
<t tx="ekr.20180516071751.256">def eval_argument_clinic(self, parameters):
    """Uses a list with argument clinic information (see PEP 436)."""
    iterator = self.unpack()
    for i, (name, optional, allow_kwargs) in enumerate(parameters):
        key, argument = next(iterator, (None, None))
        if key is not None:
            raise NotImplementedError
        if argument is None and not optional:
            debug.warning('TypeError: %s expected at least %s arguments, got %s',
                          name, len(parameters), i)
            raise ValueError
        values = set() if argument is None else argument.infer()

        if not values and not optional:
            # For the stdlib we always want values. If we don't get them,
            # that's ok, maybe something is too hard to resolve, however,
            # we will not proceed with the evaluation of that function.
            debug.warning('argument_clinic "%s" not resolvable.', name)
            raise ValueError
        yield values

</t>
<t tx="ekr.20180516071751.257">def eval_all(self, func=None):
    """
    Evaluates all arguments as a support for static analysis
    (normally Jedi).
    """
    for key, lazy_context in self.unpack():
        types = lazy_context.infer()
        try_iter_content(types)


</t>
<t tx="ekr.20180516071751.258">class TreeArguments(AbstractArguments):
    @others
</t>
<t tx="ekr.20180516071751.259">def __init__(self, evaluator, context, argument_node, trailer=None):
    """
    The argument_node is either a parser node or a list of evaluated
    objects. Those evaluated objects may be lists of evaluated objects
    themselves (one list for the first argument, one for the second, etc).

    :param argument_node: May be an argument_node or a list of nodes.
    """
    self.argument_node = argument_node
    self.context = context
    self._evaluator = evaluator
    self.trailer = trailer  # Can be None, e.g. in a class definition.

</t>
<t tx="ekr.20180516071751.26">def _check_if(context, node):
    types = context.eval_node(node)
    values = set(x.py__bool__() for x in types)
    if len(values) == 1:
        return Status.lookup_table[values.pop()]
    else:
        return UNSURE
</t>
<t tx="ekr.20180516071751.260">def _split(self):
    if isinstance(self.argument_node, (tuple, list)):
        for el in self.argument_node:
            yield 0, el
    else:
        if not (self.argument_node.type == 'arglist' or (
                # in python 3.5 **arg is an argument, not arglist
                (self.argument_node.type == 'argument') and
                 self.argument_node.children[0] in ('*', '**'))):
            yield 0, self.argument_node
            return

        iterator = iter(self.argument_node.children)
        for child in iterator:
            if child == ',':
                continue
            elif child in ('*', '**'):
                yield len(child.value), next(iterator)
            elif child.type == 'argument' and \
                    child.children[0] in ('*', '**'):
                assert len(child.children) == 2
                yield len(child.children[0].value), child.children[1]
            else:
                yield 0, child

</t>
<t tx="ekr.20180516071751.261">def unpack(self, func=None):
    named_args = []
    for stars, el in self._split():
        if stars == 1:
            arrays = self.context.eval_node(el)
            iterators = [_iterate_star_args(self.context, a, el, func)
                         for a in arrays]
            iterators = list(iterators)
            for values in list(zip_longest(*iterators)):
                # TODO zip_longest yields None, that means this would raise
                # an exception?
                yield None, context.get_merged_lazy_context(
                    [v for v in values if v is not None]
                )
        elif stars == 2:
            arrays = self._evaluator.eval_element(self.context, el)
            for dct in arrays:
                for key, values in _star_star_dict(self.context, dct, el, func):
                    yield key, values
        else:
            if el.type == 'argument':
                c = el.children
                if len(c) == 3:  # Keyword argument.
                    named_args.append((c[0].value, context.LazyTreeContext(self.context, c[2]),))
                else:  # Generator comprehension.
                    # Include the brackets with the parent.
                    comp = iterable.GeneratorComprehension(
                        self._evaluator, self.context, self.argument_node.parent)
                    yield None, context.LazyKnownContext(comp)
            else:
                yield None, context.LazyTreeContext(self.context, el)

    # Reordering var_args is necessary, because star args sometimes appear
    # after named argument, but in the actual order it's prepended.
    for named_arg in named_args:
        yield named_arg

</t>
<t tx="ekr.20180516071751.262">def as_tree_tuple_objects(self):
    for stars, argument in self._split():
        if argument.type == 'argument':
            argument, default = argument.children[::2]
        else:
            default = None
        yield argument, default, stars

</t>
<t tx="ekr.20180516071751.263">def __repr__(self):
    return '&lt;%s: %s&gt;' % (self.__class__.__name__, self.argument_node)

</t>
<t tx="ekr.20180516071751.264">def get_calling_nodes(self):
    from jedi.evaluate.dynamic import MergedExecutedParams
    old_arguments_list = []
    arguments = self

    while arguments not in old_arguments_list:
        if not isinstance(arguments, TreeArguments):
            break

        old_arguments_list.append(arguments)
        for name, default, stars in reversed(list(arguments.as_tree_tuple_objects())):
            if not stars or not isinstance(name, tree.Name):
                continue

            names = self._evaluator.goto(arguments.context, name)
            if len(names) != 1:
                break
            if not isinstance(names[0], ParamName):
                break
            param = names[0].get_param()
            if isinstance(param, MergedExecutedParams):
                # For dynamic searches we don't even want to see errors.
                return []
            if not isinstance(param, ExecutedParam):
                break
            if param.var_args is None:
                break
            arguments = param.var_args
            break

    return [arguments.argument_node or arguments.trailer]


</t>
<t tx="ekr.20180516071751.265">class ValuesArguments(AbstractArguments):
    @others
</t>
<t tx="ekr.20180516071751.266">def __init__(self, values_list):
    self._values_list = values_list

</t>
<t tx="ekr.20180516071751.267">def unpack(self, func=None):
    for values in self._values_list:
        yield None, context.LazyKnownContexts(values)

</t>
<t tx="ekr.20180516071751.268">def get_calling_nodes(self):
    return []

</t>
<t tx="ekr.20180516071751.269">def __repr__(self):
    return '&lt;%s: %s&gt;' % (self.__class__.__name__, self._values_list)


</t>
<t tx="ekr.20180516071751.27">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.270">class ExecutedParam(object):
    """Fake a param and give it values."""
    @others
</t>
<t tx="ekr.20180516071751.271">def __init__(self, var_args_context, original_param, var_args, lazy_context):
    self._root_context = var_args_context.get_root_context()
    self._original_param = original_param
    self.var_args = var_args
    self._lazy_context = lazy_context
    self.string_name = self._original_param.name.value

</t>
<t tx="ekr.20180516071751.272">def infer(self):
    pep0484_hints = pep0484.follow_param(self._root_context, self._original_param)
    doc_params = docstrings.follow_param(self._root_context, self._original_param)
    if pep0484_hints or doc_params:
        return list(set(pep0484_hints) | set(doc_params))

    return self._lazy_context.infer()

</t>
<t tx="ekr.20180516071751.273">@property
def position_nr(self):
    # Need to use the original logic here, because it uses the parent.
    return self._original_param.position_nr

</t>
<t tx="ekr.20180516071751.274">def __repr__(self):
    return '&lt;%s: %s&gt;' % (self.__class__.__name__, self.string_name)


</t>
<t tx="ekr.20180516071751.275">def get_params(evaluator, parent_context, func, var_args):
    result_params = []
    param_dict = {}
    for param in func.params:
        param_dict[str(param.name)] = param
    unpacked_va = list(var_args.unpack(func))
    var_arg_iterator = common.PushBackIterator(iter(unpacked_va))

    non_matching_keys = defaultdict(lambda: [])
    keys_used = {}
    keys_only = False
    had_multiple_value_error = False
    for param in func.params:
        # The value and key can both be null. There, the defaults apply.
        # args / kwargs will just be empty arrays / dicts, respectively.
        # Wrong value count is just ignored. If you try to test cases that are
        # not allowed in Python, Jedi will maybe not show any completions.
        key, argument = next(var_arg_iterator, (None, None))
        while key is not None:
            keys_only = True
            try:
                key_param = param_dict[key]
            except KeyError:
                non_matching_keys[key] = argument
            else:
                if key in keys_used:
                    had_multiple_value_error = True
                    m = ("TypeError: %s() got multiple values for keyword argument '%s'."
                         % (func.name, key))
                    for node in var_args.get_calling_nodes():
                        analysis.add(parent_context, 'type-error-multiple-values',
                                     node, message=m)
                else:
                    keys_used[key] = ExecutedParam(parent_context, key_param, var_args, argument)
            key, argument = next(var_arg_iterator, (None, None))

        try:
            result_params.append(keys_used[param.name.value])
            continue
        except KeyError:
            pass

        if param.stars == 1:
            # *args param
            lazy_context_list = []
            if argument is not None:
                lazy_context_list.append(argument)
                for key, argument in var_arg_iterator:
                    # Iterate until a key argument is found.
                    if key:
                        var_arg_iterator.push_back((key, argument))
                        break
                    lazy_context_list.append(argument)
            seq = iterable.FakeSequence(evaluator, 'tuple', lazy_context_list)
            result_arg = context.LazyKnownContext(seq)
        elif param.stars == 2:
            # **kwargs param
            dct = iterable.FakeDict(evaluator, dict(non_matching_keys))
            result_arg = context.LazyKnownContext(dct)
            non_matching_keys = {}
        else:
            # normal param
            if argument is None:
                # No value: Return an empty container
                if param.default is None:
                    result_arg = context.LazyUnknownContext()
                    if not keys_only:
                        for node in var_args.get_calling_nodes():
                            m = _error_argument_count(func, len(unpacked_va))
                            analysis.add(parent_context, 'type-error-too-few-arguments',
                                         node, message=m)
                else:
                    result_arg = context.LazyTreeContext(parent_context, param.default)
            else:
                result_arg = argument

        result_params.append(ExecutedParam(parent_context, param, var_args, result_arg))
        if not isinstance(result_arg, context.LazyUnknownContext):
            keys_used[param.name.value] = result_params[-1]

    if keys_only:
        # All arguments should be handed over to the next function. It's not
        # about the values inside, it's about the names. Jedi needs to now that
        # there's nothing to find for certain names.
        for k in set(param_dict) - set(keys_used):
            param = param_dict[k]

            if not (non_matching_keys or had_multiple_value_error or
                    param.stars or param.default):
                # add a warning only if there's not another one.
                for node in var_args.get_calling_nodes():
                    m = _error_argument_count(func, len(unpacked_va))
                    analysis.add(parent_context, 'type-error-too-few-arguments',
                                 node, message=m)

    for key, lazy_context in non_matching_keys.items():
        m = "TypeError: %s() got an unexpected keyword argument '%s'." \
            % (func.name, key)
        add_argument_issue(
            parent_context,
            'type-error-keyword-argument',
            lazy_context,
            message=m
        )

    remaining_arguments = list(var_arg_iterator)
    if remaining_arguments:
        m = _error_argument_count(func, len(unpacked_va))
        # Just report an error for the first param that is not needed (like
        # cPython).
        first_key, lazy_context = remaining_arguments[0]
        if var_args.get_calling_nodes():
            # There might not be a valid calling node so check for that first.
            add_argument_issue(parent_context, 'type-error-too-many-arguments', lazy_context, message=m)
    return result_params


</t>
<t tx="ekr.20180516071751.276">def _iterate_star_args(context, array, input_node, func=None):
    try:
        iter_ = array.py__iter__
    except AttributeError:
        if func is not None:
            # TODO this func should not be needed.
            m = "TypeError: %s() argument after * must be a sequence, not %s" \
                % (func.name.value, array)
            analysis.add(context, 'type-error-star', input_node, message=m)
    else:
        for lazy_context in iter_():
            yield lazy_context


</t>
<t tx="ekr.20180516071751.277">def _star_star_dict(context, array, input_node, func):
    from jedi.evaluate.instance import CompiledInstance
    if isinstance(array, CompiledInstance) and array.name.string_name == 'dict':
        # For now ignore this case. In the future add proper iterators and just
        # make one call without crazy isinstance checks.
        return {}
    elif isinstance(array, iterable.AbstractSequence) and array.array_type == 'dict':
        return array.exact_key_items()
    else:
        if func is not None:
            m = "TypeError: %s argument after ** must be a mapping, not %s" \
                % (func.name.value, array)
            analysis.add(context, 'type-error-star-star', input_node, message=m)
        return {}


</t>
<t tx="ekr.20180516071751.278">def _error_argument_count(func, actual_count):
    default_arguments = sum(1 for p in func.params if p.default or p.stars)

    if default_arguments == 0:
        before = 'exactly '
    else:
        before = 'from %s to ' % (len(func.params) - default_arguments)
    return ('TypeError: %s() takes %s%s arguments (%s given).'
            % (func.name, before, len(func.params), actual_count))


</t>
<t tx="ekr.20180516071751.279">def create_default_param(parent_context, param):
    if param.stars == 1:
        result_arg = context.LazyKnownContext(
            iterable.FakeSequence(parent_context.evaluator, 'tuple', [])
        )
    elif param.stars == 2:
        result_arg = context.LazyKnownContext(
            iterable.FakeDict(parent_context.evaluator, {})
        )
    elif param.default is None:
        result_arg = context.LazyUnknownContext()
    else:
        result_arg = context.LazyTreeContext(parent_context, param.default)
    return ExecutedParam(parent_context, param, None, result_arg)
</t>
<t tx="ekr.20180516071751.28">import copy
from itertools import chain
from contextlib import contextmanager

from jedi.parser.python import tree


</t>
<t tx="ekr.20180516071751.280">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.281">"""
PEP 0484 ( https://www.python.org/dev/peps/pep-0484/ ) describes type hints
through function annotations. There is a strong suggestion in this document
that only the type of type hinting defined in PEP0484 should be allowed
as annotations in future python versions.

The (initial / probably incomplete) implementation todo list for pep-0484:
v Function parameter annotations with builtin/custom type classes
v Function returntype annotations with builtin/custom type classes
v Function parameter annotations with strings (forward reference)
v Function return type annotations with strings (forward reference)
v Local variable type hints
v Assigned types: `Url = str\ndef get(url:Url) -&gt; str:`
v Type hints in `with` statements
x Stub files support
x support `@no_type_check` and `@no_type_check_decorator`
x support for typing.cast() operator
x support for type hint comments for functions, `# type: (int, str) -&gt; int`.
    See comment from Guido https://github.com/davidhalter/jedi/issues/662
"""

import itertools

import os
from jedi.parser import ParserSyntaxError
from jedi.parser.python import parse, tree
from jedi.common import unite
from jedi.evaluate.cache import memoize_default
from jedi.evaluate import compiled
from jedi.evaluate.context import LazyTreeContext
from jedi import debug
from jedi import _compatibility
import re


</t>
<t tx="ekr.20180516071751.282">def _evaluate_for_annotation(context, annotation, index=None):
    """
    Evaluates a string-node, looking for an annotation
    If index is not None, the annotation is expected to be a tuple
    and we're interested in that index
    """
    if annotation is not None:
        definitions = context.eval_node(
            _fix_forward_reference(context, annotation))
        if index is not None:
            definitions = list(itertools.chain.from_iterable(
                definition.py__getitem__(index) for definition in definitions
                if definition.array_type == 'tuple' and
                len(list(definition.py__iter__())) &gt;= index))
        return unite(d.execute_evaluated() for d in definitions)
    else:
        return set()


</t>
<t tx="ekr.20180516071751.283">def _fix_forward_reference(context, node):
    evaled_nodes = context.eval_node(node)
    if len(evaled_nodes) != 1:
        debug.warning("Eval'ed typing index %s should lead to 1 object, "
                      " not %s" % (node, evaled_nodes))
        return node
    evaled_node = list(evaled_nodes)[0]
    if isinstance(evaled_node, compiled.CompiledObject) and \
            isinstance(evaled_node.obj, str):
        try:
            new_node = parse(
                _compatibility.unicode(evaled_node.obj),
                start_symbol='eval_input',
                error_recovery=False
            )
        except ParserSyntaxError:
            debug.warning('Annotation not parsed: %s' % evaled_node.obj)
            return node
        else:
            module = node.get_root_node()
            new_node.move(module.end_pos[0])
            new_node.parent = context.tree_node
            return new_node
    else:
        return node


</t>
<t tx="ekr.20180516071751.284">@memoize_default()
def follow_param(context, param):
    annotation = param.annotation()
    return _evaluate_for_annotation(context, annotation)


</t>
<t tx="ekr.20180516071751.285">def py__annotations__(funcdef):
    return_annotation = funcdef.annotation()
    if return_annotation:
        dct = {'return': return_annotation}
    else:
        dct = {}
    for function_param in funcdef.params:
        param_annotation = function_param.annotation()
        if param_annotation is not None:
            dct[function_param.name.value] = param_annotation
    return dct


</t>
<t tx="ekr.20180516071751.286">@memoize_default()
def find_return_types(context, func):
    annotation = py__annotations__(func).get("return", None)
    return _evaluate_for_annotation(context, annotation)


_typing_module = None


</t>
<t tx="ekr.20180516071751.287">def _get_typing_replacement_module():
    """
    The idea is to return our jedi replacement for the PEP-0484 typing module
    as discussed at https://github.com/davidhalter/jedi/issues/663
    """
    global _typing_module
    if _typing_module is None:
        typing_path = \
            os.path.abspath(os.path.join(__file__, "../jedi_typing.py"))
        with open(typing_path) as f:
            code = _compatibility.unicode(f.read())
        _typing_module = parse(code)
    return _typing_module


</t>
<t tx="ekr.20180516071751.288">def py__getitem__(context, typ, node):
    if not typ.get_root_context().name.string_name == "typing":
        return None
    # we assume that any class using [] in a module called
    # "typing" with a name for which we have a replacement
    # should be replaced by that class. This is not 100%
    # airtight but I don't have a better idea to check that it's
    # actually the PEP-0484 typing module and not some other
    if node.type == "subscriptlist":
        nodes = node.children[::2]  # skip the commas
    else:
        nodes = [node]
    del node

    nodes = [_fix_forward_reference(context, node) for node in nodes]
    type_name = typ.name.string_name

    # hacked in Union and Optional, since it's hard to do nicely in parsed code
    if type_name in ("Union", '_Union'):
        # In Python 3.6 it's still called typing.Union but it's an instance
        # called _Union.
        return unite(context.eval_node(node) for node in nodes)
    if type_name in ("Optional", '_Optional'):
        # Here we have the same issue like in Union. Therefore we also need to
        # check for the instance typing._Optional (Python 3.6).
        return context.eval_node(nodes[0])

    from jedi.evaluate.representation import ModuleContext
    typing = ModuleContext(
        context.evaluator,
        module_node=_get_typing_replacement_module(),
        path=None
    )
    factories = typing.py__getattribute__("factory")
    assert len(factories) == 1
    factory = list(factories)[0]
    assert factory
    function_body_nodes = factory.tree_node.children[4].children
    valid_classnames = set(child.name.value
                           for child in function_body_nodes
                           if isinstance(child, tree.Class))
    if type_name not in valid_classnames:
        return None
    compiled_classname = compiled.create(context.evaluator, type_name)

    from jedi.evaluate.iterable import FakeSequence
    args = FakeSequence(
        context.evaluator,
        "tuple",
        [LazyTreeContext(context, n) for n in nodes]
    )

    result = factory.execute_evaluated(compiled_classname, args)
    return result


</t>
<t tx="ekr.20180516071751.289">def find_type_from_comment_hint_for(context, node, name):
    return _find_type_from_comment_hint(context, node, node.children[1], name)


</t>
<t tx="ekr.20180516071751.29">def deep_ast_copy(obj):
    """
    Much, much faster than copy.deepcopy, but just for parser tree nodes.
    """
    # If it's already in the cache, just return it.
    new_obj = copy.copy(obj)

    # Copy children
    new_children = []
    for child in obj.children:
        if isinstance(child, tree.Leaf):
            new_child = copy.copy(child)
            new_child.parent = new_obj
        else:
            new_child = deep_ast_copy(child)
            new_child.parent = new_obj
        new_children.append(new_child)
    new_obj.children = new_children

    return new_obj


</t>
<t tx="ekr.20180516071751.290">def find_type_from_comment_hint_with(context, node, name):
    assert len(node.children[1].children) == 3, \
        "Can only be here when children[1] is 'foo() as f'"
    varlist = node.children[1].children[2]
    return _find_type_from_comment_hint(context, node, varlist, name)


</t>
<t tx="ekr.20180516071751.291">def find_type_from_comment_hint_assign(context, node, name):
    return _find_type_from_comment_hint(context, node, node.children[0], name)


</t>
<t tx="ekr.20180516071751.292">def _find_type_from_comment_hint(context, node, varlist, name):
    index = None
    if varlist.type in ("testlist_star_expr", "exprlist"):
        # something like "a, b = 1, 2"
        index = 0
        for child in varlist.children:
            if child == name:
                break
            if child.type == "operator":
                continue
            index += 1
        else:
            return []

    comment = node.get_following_comment_same_line()
    if comment is None:
        return []
    match = re.match(r"^#\s*type:\s*([^#]*)", comment)
    if not match:
        return []
    annotation = tree.String(
        repr(str(match.group(1).strip())),
        node.start_pos)
    annotation.parent = node.parent
    return _evaluate_for_annotation(context, annotation, index)
</t>
<t tx="ekr.20180516071751.293">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.294">"""
Handles operator precedence.
"""
import operator as op

from jedi._compatibility import unicode
from jedi.parser.python import tree
from jedi import debug
from jedi.evaluate.compiled import CompiledObject, create, builtin_from_name
from jedi.evaluate import analysis

# Maps Python syntax to the operator module.
COMPARISON_OPERATORS = {
    '==': op.eq,
    '!=': op.ne,
    'is': op.is_,
    'is not': op.is_not,
    '&lt;': op.lt,
    '&lt;=': op.le,
    '&gt;': op.gt,
    '&gt;=': op.ge,
}


</t>
<t tx="ekr.20180516071751.295">def literals_to_types(evaluator, result):
    # Changes literals ('a', 1, 1.0, etc) to its type instances (str(),
    # int(), float(), etc).
    new_result = set()
    for typ in result:
        if is_literal(typ):
            # Literals are only valid as long as the operations are
            # correct. Otherwise add a value-free instance.
            cls = builtin_from_name(evaluator, typ.name.string_name)
            new_result |= cls.execute_evaluated()
        else:
            new_result.add(typ)
    return new_result


</t>
<t tx="ekr.20180516071751.296">def calculate_children(evaluator, context, children):
    """
    Calculate a list of children with operators.
    """
    iterator = iter(children)
    types = context.eval_node(next(iterator))
    for operator in iterator:
        right = next(iterator)
        if operator.type == 'comp_op':  # not in / is not
            operator = ' '.join(str(c.value) for c in operator.children)

        # handle lazy evaluation of and/or here.
        if operator in ('and', 'or'):
            left_bools = set([left.py__bool__() for left in types])
            if left_bools == set([True]):
                if operator == 'and':
                    types = context.eval_node(right)
            elif left_bools == set([False]):
                if operator != 'and':
                    types = context.eval_node(right)
            # Otherwise continue, because of uncertainty.
        else:
            types = calculate(evaluator, context, types, operator,
                              context.eval_node(right))
    debug.dbg('calculate_children types %s', types)
    return types


</t>
<t tx="ekr.20180516071751.297">def calculate(evaluator, context, left_result, operator, right_result):
    result = set()
    if not left_result or not right_result:
        # illegal slices e.g. cause left/right_result to be None
        result = (left_result or set()) | (right_result or set())
        result = literals_to_types(evaluator, result)
    else:
        # I don't think there's a reasonable chance that a string
        # operation is still correct, once we pass something like six
        # objects.
        if len(left_result) * len(right_result) &gt; 6:
            result = literals_to_types(evaluator, left_result | right_result)
        else:
            for left in left_result:
                for right in right_result:
                    result |= _element_calculate(evaluator, context, left, operator, right)
    return result


</t>
<t tx="ekr.20180516071751.298">def factor_calculate(evaluator, types, operator):
    """
    Calculates `+`, `-`, `~` and `not` prefixes.
    """
    for typ in types:
        if operator == '-':
            if _is_number(typ):
                yield create(evaluator, -typ.obj)
        elif operator == 'not':
            value = typ.py__bool__()
            if value is None:  # Uncertainty.
                return
            yield create(evaluator, not value)
        else:
            yield typ


</t>
<t tx="ekr.20180516071751.299">def _is_number(obj):
    return isinstance(obj, CompiledObject) \
        and isinstance(obj.obj, (int, float))


</t>
<t tx="ekr.20180516071751.3">class NameFinder(object):
    @others
</t>
<t tx="ekr.20180516071751.30">def evaluate_call_of_leaf(context, leaf, cut_own_trailer=False):
    """
    Creates a "call" node that consist of all ``trailer`` and ``power``
    objects.  E.g. if you call it with ``append``::

        list([]).append(3) or None

    You would get a node with the content ``list([]).append`` back.

    This generates a copy of the original ast node.

    If you're using the leaf, e.g. the bracket `)` it will return ``list([])``.

    # TODO remove cut_own_trailer option, since its always used with it. Just
    #      ignore it, It's not what we want anyway. Or document it better?
    """
    trailer = leaf.parent
    # The leaf may not be the last or first child, because there exist three
    # different trailers: `( x )`, `[ x ]` and `.x`. In the first two examples
    # we should not match anything more than x.
    if trailer.type != 'trailer' or leaf not in (trailer.children[0], trailer.children[-1]):
        if trailer.type == 'atom':
            return context.eval_node(trailer)
        return context.eval_node(leaf)

    power = trailer.parent
    index = power.children.index(trailer)
    if cut_own_trailer:
        cut = index
    else:
        cut = index + 1

    if power.type == 'error_node':
        start = index
        while True:
            start -= 1
            base = power.children[start]
            if base.type != 'trailer':
                break
        trailers = power.children[start + 1: index + 1]
    else:
        base = power.children[0]
        trailers = power.children[1:cut]

    values = context.eval_node(base)
    for trailer in trailers:
        values = context.eval_trailer(values, trailer)
    return values


</t>
<t tx="ekr.20180516071751.300">def is_string(obj):
    return isinstance(obj, CompiledObject) \
        and isinstance(obj.obj, (str, unicode))


</t>
<t tx="ekr.20180516071751.301">def is_literal(obj):
    return _is_number(obj) or is_string(obj)


</t>
<t tx="ekr.20180516071751.302">def _is_tuple(obj):
    from jedi.evaluate import iterable
    return isinstance(obj, iterable.AbstractSequence) and obj.array_type == 'tuple'


</t>
<t tx="ekr.20180516071751.303">def _is_list(obj):
    from jedi.evaluate import iterable
    return isinstance(obj, iterable.AbstractSequence) and obj.array_type == 'list'


</t>
<t tx="ekr.20180516071751.304">def _element_calculate(evaluator, context, left, operator, right):
    from jedi.evaluate import iterable, instance
    l_is_num = _is_number(left)
    r_is_num = _is_number(right)
    if operator == '*':
        # for iterables, ignore * operations
        if isinstance(left, iterable.AbstractSequence) or is_string(left):
            return set([left])
        elif isinstance(right, iterable.AbstractSequence) or is_string(right):
            return set([right])
    elif operator == '+':
        if l_is_num and r_is_num or is_string(left) and is_string(right):
            return set([create(evaluator, left.obj + right.obj)])
        elif _is_tuple(left) and _is_tuple(right) or _is_list(left) and _is_list(right):
            return set([iterable.MergedArray(evaluator, (left, right))])
    elif operator == '-':
        if l_is_num and r_is_num:
            return set([create(evaluator, left.obj - right.obj)])
    elif operator == '%':
        # With strings and numbers the left type typically remains. Except for
        # `int() % float()`.
        return set([left])
    elif operator in COMPARISON_OPERATORS:
        operation = COMPARISON_OPERATORS[operator]
        if isinstance(left, CompiledObject) and isinstance(right, CompiledObject):
            # Possible, because the return is not an option. Just compare.
            left = left.obj
            right = right.obj

        try:
            result = operation(left, right)
        except TypeError:
            # Could be True or False.
            return set([create(evaluator, True), create(evaluator, False)])
        else:
            return set([create(evaluator, result)])
    elif operator == 'in':
        return set()

    def check(obj):
        """Checks if a Jedi object is either a float or an int."""
        return isinstance(obj, instance.CompiledInstance) and \
            obj.name.string_name in ('int', 'float')

    # Static analysis, one is a number, the other one is not.
    if operator in ('+', '-') and l_is_num != r_is_num \
            and not (check(left) or check(right)):
        message = "TypeError: unsupported operand type(s) for +: %s and %s"
        analysis.add(context, 'type-error-operation', operator,
                     message % (left, right))

    return set([left, right])
</t>
<t tx="ekr.20180516071751.305">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.306">"""
Recursions are the recipe of |jedi| to conquer Python code. However, someone
must stop recursions going mad. Some settings are here to make |jedi| stop at
the right time. You can read more about them :ref:`here &lt;settings-recursion&gt;`.

Next to :mod:`jedi.evaluate.cache` this module also makes |jedi| not
thread-safe. Why?  ``execution_recursion_decorator`` uses class variables to
count the function calls.
"""
from contextlib import contextmanager

from jedi import debug
from jedi import settings


</t>
<t tx="ekr.20180516071751.307">class RecursionDetector(object):
    @others
</t>
<t tx="ekr.20180516071751.308">def __init__(self):
    self.pushed_nodes = []


</t>
<t tx="ekr.20180516071751.309">@contextmanager
def execution_allowed(evaluator, node):
    """
    A decorator to detect recursions in statements. In a recursion a statement
    at the same place, in the same module may not be executed two times.
    """
    pushed_nodes = evaluator.recursion_detector.pushed_nodes

    if node in pushed_nodes:
        debug.warning('catched stmt recursion: %s @%s', node,
                      node.start_pos)
        yield False
    else:
        pushed_nodes.append(node)
        yield True
        pushed_nodes.pop()


</t>
<t tx="ekr.20180516071751.31">def call_of_leaf(leaf):
    """
    Creates a "call" node that consist of all ``trailer`` and ``power``
    objects.  E.g. if you call it with ``append``::

        list([]).append(3) or None

    You would get a node with the content ``list([]).append`` back.

    This generates a copy of the original ast node.

    If you're using the leaf, e.g. the bracket `)` it will return ``list([])``.
    """
    # TODO this is the old version of this call. Try to remove it.
    trailer = leaf.parent
    # The leaf may not be the last or first child, because there exist three
    # different trailers: `( x )`, `[ x ]` and `.x`. In the first two examples
    # we should not match anything more than x.
    if trailer.type != 'trailer' or leaf not in (trailer.children[0], trailer.children[-1]):
        if trailer.type == 'atom':
            return trailer
        return leaf

    power = trailer.parent
    index = power.children.index(trailer)

    new_power = copy.copy(power)
    new_power.children = list(new_power.children)
    new_power.children[index + 1:] = []

    if power.type == 'error_node':
        start = index
        while True:
            start -= 1
            if power.children[start].type != 'trailer':
                break
        transformed = tree.Node('power', power.children[start:])
        transformed.parent = power.parent
        return transformed

    return power


</t>
<t tx="ekr.20180516071751.310">def execution_recursion_decorator(default=set()):
    def decorator(func):
        def wrapper(execution, **kwargs):
            detector = execution.evaluator.execution_recursion_detector
            allowed = detector.push_execution(execution)
            try:
                if allowed:
                    result = default
                else:
                    result = func(execution, **kwargs)
            finally:
                detector.pop_execution()
            return result
        return wrapper
    return decorator


</t>
<t tx="ekr.20180516071751.311">class ExecutionRecursionDetector(object):
    """
    Catches recursions of executions.
    """
    @others
</t>
<t tx="ekr.20180516071751.312">def __init__(self, evaluator):
    self.recursion_level = 0
    self.parent_execution_funcs = []
    self.execution_funcs = set()
    self.execution_count = 0
    self._evaluator = evaluator

</t>
<t tx="ekr.20180516071751.313">def __call__(self, execution):
    debug.dbg('Execution recursions: %s', execution, self.recursion_level,
              self.execution_count, len(self.execution_funcs))
    if self.check_recursion(execution):
        result = set()
    else:
        result = self.func(execution)
    self.pop_execution()
    return result

</t>
<t tx="ekr.20180516071751.314">def pop_execution(self):
    self.parent_execution_funcs.pop()
    self.recursion_level -= 1

</t>
<t tx="ekr.20180516071751.315">def push_execution(self, execution):
    in_par_execution_funcs = execution.tree_node in self.parent_execution_funcs
    in_execution_funcs = execution.tree_node in self.execution_funcs
    self.recursion_level += 1
    self.execution_count += 1
    self.execution_funcs.add(execution.tree_node)
    self.parent_execution_funcs.append(execution.tree_node)

    if self.execution_count &gt; settings.max_executions:
        return True

    module = execution.get_root_context()
    if module == self._evaluator.BUILTINS:
        return False

    if in_par_execution_funcs:
        if self.recursion_level &gt; settings.max_function_recursion_level:
            return True
    if in_execution_funcs and \
            len(self.execution_funcs) &gt; settings.max_until_execution_unique:
        return True
    if self.execution_count &gt; settings.max_executions_without_builtins:
        return True
    return False
</t>
<t tx="ekr.20180516071751.316">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.317">"""
Like described in the :mod:`jedi.parser.python.tree` module,
there's a need for an ast like module to represent the states of parsed
modules.

But now there are also structures in Python that need a little bit more than
that. An ``Instance`` for example is only a ``Class`` before it is
instantiated. This class represents these cases.

So, why is there also a ``Class`` class here? Well, there are decorators and
they change classes in Python 3.

Representation modules also define "magic methods". Those methods look like
``py__foo__`` and are typically mappable to the Python equivalents ``__call__``
and others. Here's a list:

====================================== ========================================
**Method**                             **Description**
-------------------------------------- ----------------------------------------
py__call__(params: Array)              On callable objects, returns types.
py__bool__()                           Returns True/False/None; None means that
                                       there's no certainty.
py__bases__()                          Returns a list of base classes.
py__mro__()                            Returns a list of classes (the mro).
py__iter__()                           Returns a generator of a set of types.
py__class__()                          Returns the class of an instance.
py__getitem__(index: int/str)          Returns a a set of types of the index.
                                       Can raise an IndexError/KeyError.
py__file__()                           Only on modules. Returns None if does
                                       not exist.
py__package__()                        Only on modules. For the import system.
py__path__()                           Only on modules. For the import system.
py__get__(call_object)                 Only on instances. Simulates
                                       descriptors.
====================================== ========================================

"""
import os
import pkgutil
import imp
import re
from itertools import chain

from jedi._compatibility import use_metaclass
from jedi.parser.python import tree
from jedi import debug
from jedi import common
from jedi.evaluate.cache import memoize_default, CachedMetaClass, NO_DEFAULT
from jedi.evaluate import compiled
from jedi.evaluate import recursion
from jedi.evaluate import iterable
from jedi.evaluate import docstrings
from jedi.evaluate import pep0484
from jedi.evaluate import param
from jedi.evaluate import flow_analysis
from jedi.evaluate import imports
from jedi.evaluate import helpers
from jedi.evaluate.filters import ParserTreeFilter, FunctionExecutionFilter, \
    GlobalNameFilter, DictFilter, ContextName, AbstractNameDefinition, \
    ParamName, AnonymousInstanceParamName, TreeNameDefinition, \
    ContextNameMixin
from jedi.evaluate.dynamic import search_params
from jedi.evaluate import context
from jedi.evaluate.context import ContextualizedNode


</t>
<t tx="ekr.20180516071751.318">def apply_py__get__(context, base_context):
    try:
        method = context.py__get__
    except AttributeError:
        yield context
    else:
        for descriptor_context in method(base_context):
            yield descriptor_context


</t>
<t tx="ekr.20180516071751.319">class ClassName(TreeNameDefinition):
    @others
</t>
<t tx="ekr.20180516071751.32">def get_names_of_node(node):
    try:
        children = node.children
    except AttributeError:
        if node.type == 'name':
            return [node]
        else:
            return []
    else:
        return list(chain.from_iterable(get_names_of_node(c) for c in children))


</t>
<t tx="ekr.20180516071751.320">def __init__(self, parent_context, tree_name, name_context):
    super(ClassName, self).__init__(parent_context, tree_name)
    self._name_context = name_context

</t>
<t tx="ekr.20180516071751.321">def infer(self):
    # TODO this _name_to_types might get refactored and be a part of the
    # parent class. Once it is, we can probably just overwrite method to
    # achieve this.
    from jedi.evaluate.finder import _name_to_types
    inferred = _name_to_types(
        self.parent_context.evaluator, self._name_context, self.tree_name)

    for result_context in inferred:
        for c in apply_py__get__(result_context, self.parent_context):
            yield c


</t>
<t tx="ekr.20180516071751.322">class ClassFilter(ParserTreeFilter):
    name_class = ClassName

    @others
</t>
<t tx="ekr.20180516071751.323">def _convert_names(self, names):
    return [self.name_class(self.context, name, self._node_context)
            for name in names]


</t>
<t tx="ekr.20180516071751.324">class ClassContext(use_metaclass(CachedMetaClass, context.TreeContext)):
    """
    This class is not only important to extend `tree.Class`, it is also a
    important for descriptors (if the descriptor methods are evaluated or not).
    """
    api_type = 'class'

    @others
</t>
<t tx="ekr.20180516071751.325">def __init__(self, evaluator, classdef, parent_context):
    super(ClassContext, self).__init__(evaluator, parent_context=parent_context)
    self.tree_node = classdef

</t>
<t tx="ekr.20180516071751.326">@memoize_default(default=())
def py__mro__(self):
    def add(cls):
        if cls not in mro:
            mro.append(cls)

    mro = [self]
    # TODO Do a proper mro resolution. Currently we are just listing
    # classes. However, it's a complicated algorithm.
    for lazy_cls in self.py__bases__():
        # TODO there's multiple different mro paths possible if this yields
        # multiple possibilities. Could be changed to be more correct.
        for cls in lazy_cls.infer():
            # TODO detect for TypeError: duplicate base class str,
            # e.g.  `class X(str, str): pass`
            try:
                mro_method = cls.py__mro__
            except AttributeError:
                # TODO add a TypeError like:
                """
                &gt;&gt;&gt; class Y(lambda: test): pass
                Traceback (most recent call last):
                  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
                TypeError: function() argument 1 must be code, not str
                &gt;&gt;&gt; class Y(1): pass
                Traceback (most recent call last):
                  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
                TypeError: int() takes at most 2 arguments (3 given)
                """
                pass
            else:
                add(cls)
                for cls_new in mro_method():
                    add(cls_new)
    return tuple(mro)

</t>
<t tx="ekr.20180516071751.327">@memoize_default(default=())
def py__bases__(self):
    arglist = self.tree_node.get_super_arglist()
    if arglist:
        args = param.TreeArguments(self.evaluator, self, arglist)
        return [value for key, value in args.unpack() if key is None]
    else:
        return [context.LazyKnownContext(compiled.create(self.evaluator, object))]

</t>
<t tx="ekr.20180516071751.328">def py__call__(self, params):
    from jedi.evaluate.instance import TreeInstance
    return set([TreeInstance(self.evaluator, self.parent_context, self, params)])

</t>
<t tx="ekr.20180516071751.329">def py__class__(self):
    return compiled.create(self.evaluator, type)

</t>
<t tx="ekr.20180516071751.33">def get_module_names(module, all_scopes):
    """
    Returns a dictionary with name parts as keys and their call paths as
    values.
    """
    names = chain.from_iterable(module.used_names.values())
    if not all_scopes:
        # We have to filter all the names that don't have the module as a
        # parent_scope. There's None as a parent, because nodes in the module
        # node have the parent module and not suite as all the others.
        # Therefore it's important to catch that case.
        names = [n for n in names if n.get_parent_scope().parent in (module, None)]
    return names


</t>
<t tx="ekr.20180516071751.330">def get_params(self):
    from jedi.evaluate.instance import AnonymousInstance
    anon = AnonymousInstance(self.evaluator, self.parent_context, self)
    return [AnonymousInstanceParamName(anon, param.name) for param in self.funcdef.params]

</t>
<t tx="ekr.20180516071751.331">def get_filters(self, search_global, until_position=None, origin_scope=None, is_instance=False):
    if search_global:
        yield ParserTreeFilter(
            self.evaluator,
            context=self,
            until_position=until_position,
            origin_scope=origin_scope
        )
    else:
        for cls in self.py__mro__():
            if isinstance(cls, compiled.CompiledObject):
                for filter in cls.get_filters(is_instance=is_instance):
                    yield filter
            else:
                yield ClassFilter(
                    self.evaluator, self, node_context=cls,
                    origin_scope=origin_scope)

</t>
<t tx="ekr.20180516071751.332">def is_class(self):
    return True

</t>
<t tx="ekr.20180516071751.333">def get_subscope_by_name(self, name):
    raise DeprecationWarning
    for s in self.py__mro__():
        for sub in reversed(s.subscopes):
            if sub.name.value == name:
                return sub
    raise KeyError("Couldn't find subscope.")

</t>
<t tx="ekr.20180516071751.334">def get_function_slot_names(self, name):
    for filter in self.get_filters(search_global=False):
        names = filter.get(name)
        if names:
            return names
    return []

</t>
<t tx="ekr.20180516071751.335">def get_param_names(self):
    for name in self.get_function_slot_names('__init__'):
        for context_ in name.infer():
            try:
                method = context_.get_param_names
            except AttributeError:
                pass
            else:
                return list(method())[1:]
    return []

</t>
<t tx="ekr.20180516071751.336">@property
def name(self):
    return ContextName(self, self.tree_node.name)


</t>
<t tx="ekr.20180516071751.337">class FunctionContext(use_metaclass(CachedMetaClass, context.TreeContext)):
    """
    Needed because of decorators. Decorators are evaluated here.
    """
    api_type = 'function'

    @others
</t>
<t tx="ekr.20180516071751.338">def __init__(self, evaluator, parent_context, funcdef):
    """ This should not be called directly """
    super(FunctionContext, self).__init__(evaluator, parent_context)
    self.tree_node = funcdef

</t>
<t tx="ekr.20180516071751.339">def get_filters(self, search_global, until_position=None, origin_scope=None):
    if search_global:
        yield ParserTreeFilter(
            self.evaluator,
            context=self,
            until_position=until_position,
            origin_scope=origin_scope
        )
    else:
        scope = self.py__class__()
        for filter in scope.get_filters(search_global=False, origin_scope=origin_scope):
            yield filter

</t>
<t tx="ekr.20180516071751.34">class FakeName(tree.Name):
    @others
</t>
<t tx="ekr.20180516071751.340">def infer_function_execution(self, function_execution):
    """
    Created to be used by inheritance.
    """
    if self.tree_node.is_generator():
        return set([iterable.Generator(self.evaluator, function_execution)])
    else:
        return function_execution.get_return_values()

</t>
<t tx="ekr.20180516071751.341">def get_function_execution(self, arguments=None):
    e = self.evaluator
    if arguments is None:
        return AnonymousFunctionExecution(e, self.parent_context, self)
    else:
        return FunctionExecutionContext(e, self.parent_context, self, arguments)

</t>
<t tx="ekr.20180516071751.342">def py__call__(self, arguments):
    function_execution = self.get_function_execution(arguments)
    return self.infer_function_execution(function_execution)

</t>
<t tx="ekr.20180516071751.343">def py__class__(self):
    # This differentiation is only necessary for Python2. Python3 does not
    # use a different method class.
    if isinstance(self.tree_node.get_parent_scope(), tree.Class):
        name = 'METHOD_CLASS'
    else:
        name = 'FUNCTION_CLASS'
    return compiled.get_special_object(self.evaluator, name)

</t>
<t tx="ekr.20180516071751.344">@property
def name(self):
    return ContextName(self, self.tree_node.name)

</t>
<t tx="ekr.20180516071751.345">def get_param_names(self):
    function_execution = self.get_function_execution()
    return [ParamName(function_execution, param.name) for param in self.tree_node.params]


</t>
<t tx="ekr.20180516071751.346">class FunctionExecutionContext(context.TreeContext):
    """
    This class is used to evaluate functions and their returns.

    This is the most complicated class, because it contains the logic to
    transfer parameters. It is even more complicated, because there may be
    multiple calls to functions and recursion has to be avoided. But this is
    responsibility of the decorators.
    """
    function_execution_filter = FunctionExecutionFilter

    @others
</t>
<t tx="ekr.20180516071751.347">def __init__(self, evaluator, parent_context, function_context, var_args):
    super(FunctionExecutionContext, self).__init__(evaluator, parent_context)
    self.function_context = function_context
    self.tree_node = function_context.tree_node
    self.var_args = var_args

</t>
<t tx="ekr.20180516071751.348">@memoize_default(default=set())
@recursion.execution_recursion_decorator()
def get_return_values(self, check_yields=False):
    funcdef = self.tree_node
    if funcdef.type == 'lambda':
        return self.evaluator.eval_element(self, funcdef.children[-1])

    if check_yields:
        types = set()
        returns = funcdef.yields
    else:
        returns = funcdef.returns
        types = set(docstrings.find_return_types(self.get_root_context(), funcdef))
        types |= set(pep0484.find_return_types(self.get_root_context(), funcdef))

    for r in returns:
        check = flow_analysis.reachability_check(self, funcdef, r)
        if check is flow_analysis.UNREACHABLE:
            debug.dbg('Return unreachable: %s', r)
        else:
            if check_yields:
                types |= set(self._eval_yield(r))
            else:
                types |= self.eval_node(r.children[1])
        if check is flow_analysis.REACHABLE:
            debug.dbg('Return reachable: %s', r)
            break
    return types

</t>
<t tx="ekr.20180516071751.349">def _eval_yield(self, yield_expr):
    node = yield_expr.children[1]
    if node.type == 'yield_arg':  # It must be a yield from.
        cn = ContextualizedNode(self, node.children[1])
        for lazy_context in iterable.py__iter__(self.evaluator, cn.infer(), cn):
            yield lazy_context
    else:
        yield context.LazyTreeContext(self, node)

</t>
<t tx="ekr.20180516071751.35">def __init__(self, name_str, parent=None, start_pos=(0, 0), is_definition=None):
    """
    In case is_definition is defined (not None), that bool value will be
    returned.
    """
    super(FakeName, self).__init__(name_str, start_pos)
    self.parent = parent
    self._is_definition = is_definition

</t>
<t tx="ekr.20180516071751.350">@recursion.execution_recursion_decorator(default=iter([]))
def get_yield_values(self):
    for_parents = [(y, tree.search_ancestor(y, ('for_stmt', 'funcdef',
                                                'while_stmt', 'if_stmt')))
                   for y in self.tree_node.yields]

    # Calculate if the yields are placed within the same for loop.
    yields_order = []
    last_for_stmt = None
    for yield_, for_stmt in for_parents:
        # For really simple for loops we can predict the order. Otherwise
        # we just ignore it.
        parent = for_stmt.parent
        if parent.type == 'suite':
            parent = parent.parent
        if for_stmt.type == 'for_stmt' and parent == self.tree_node \
                and for_stmt.defines_one_name():  # Simplicity for now.
            if for_stmt == last_for_stmt:
                yields_order[-1][1].append(yield_)
            else:
                yields_order.append((for_stmt, [yield_]))
        elif for_stmt == self.tree_node:
            yields_order.append((None, [yield_]))
        else:
            types = self.get_return_values(check_yields=True)
            if types:
                yield context.get_merged_lazy_context(list(types))
            return
        last_for_stmt = for_stmt

    evaluator = self.evaluator
    for for_stmt, yields in yields_order:
        if for_stmt is None:
            # No for_stmt, just normal yields.
            for yield_ in yields:
                for result in self._eval_yield(yield_):
                    yield result
        else:
            input_node = for_stmt.get_input_node()
            cn = ContextualizedNode(self, input_node)
            ordered = iterable.py__iter__(evaluator, cn.infer(), cn)
            ordered = list(ordered)
            for lazy_context in ordered:
                dct = {str(for_stmt.children[1]): lazy_context.infer()}
                with helpers.predefine_names(self, for_stmt, dct):
                    for yield_in_same_for_stmt in yields:
                        for result in self._eval_yield(yield_in_same_for_stmt):
                            yield result

</t>
<t tx="ekr.20180516071751.351">def get_filters(self, search_global, until_position=None, origin_scope=None):
    yield self.function_execution_filter(self.evaluator, self,
                                         until_position=until_position,
                                         origin_scope=origin_scope)

</t>
<t tx="ekr.20180516071751.352">@memoize_default(default=NO_DEFAULT)
def get_params(self):
    return param.get_params(self.evaluator, self.parent_context, self.tree_node, self.var_args)


</t>
<t tx="ekr.20180516071751.353">class AnonymousFunctionExecution(FunctionExecutionContext):
    @others
</t>
<t tx="ekr.20180516071751.354">def __init__(self, evaluator, parent_context, function_context):
    super(AnonymousFunctionExecution, self).__init__(
        evaluator, parent_context, function_context, var_args=None)

</t>
<t tx="ekr.20180516071751.355">@memoize_default(default=NO_DEFAULT)
def get_params(self):
    # We need to do a dynamic search here.
    return search_params(self.evaluator, self.parent_context, self.tree_node)


</t>
<t tx="ekr.20180516071751.356">class ModuleAttributeName(AbstractNameDefinition):
    """
    For module attributes like __file__, __str__ and so on.
    """
    api_type = 'instance'

    @others
</t>
<t tx="ekr.20180516071751.357">def __init__(self, parent_module, string_name):
    self.parent_context = parent_module
    self.string_name = string_name

</t>
<t tx="ekr.20180516071751.358">def infer(self):
    return compiled.create(self.parent_context.evaluator, str).execute(
        param.ValuesArguments([])
    )


</t>
<t tx="ekr.20180516071751.359">class ModuleName(ContextNameMixin, AbstractNameDefinition):
    start_pos = 1, 0

    @others
</t>
<t tx="ekr.20180516071751.36">def get_definition(self):
    return self.parent

</t>
<t tx="ekr.20180516071751.360">def __init__(self, context, name):
    self._context = context
    self._name = name

</t>
<t tx="ekr.20180516071751.361">@property
def string_name(self):
    return self._name


</t>
<t tx="ekr.20180516071751.362">class ModuleContext(use_metaclass(CachedMetaClass, context.TreeContext)):
    api_type = 'module'
    parent_context = None

    @others
</t>
<t tx="ekr.20180516071751.363">def __init__(self, evaluator, module_node, path):
    super(ModuleContext, self).__init__(evaluator, parent_context=None)
    self.tree_node = module_node
    self._path = path

</t>
<t tx="ekr.20180516071751.364">def get_filters(self, search_global, until_position=None, origin_scope=None):
    yield ParserTreeFilter(
        self.evaluator,
        context=self,
        until_position=until_position,
        origin_scope=origin_scope
    )
    yield GlobalNameFilter(self, self.tree_node)
    yield DictFilter(self._sub_modules_dict())
    yield DictFilter(self._module_attributes_dict())
    for star_module in self.star_imports():
        yield next(star_module.get_filters(search_global))

# I'm not sure if the star import cache is really that effective anymore
# with all the other really fast import caches. Recheck. Also we would need
# to push the star imports into Evaluator.modules, if we reenable this.
</t>
<t tx="ekr.20180516071751.365">@memoize_default([])
def star_imports(self):
    modules = []
    for i in self.tree_node.imports:
        if i.is_star_import():
            name = i.star_import_name()
            new = imports.infer_import(self, name)
            for module in new:
                if isinstance(module, ModuleContext):
                    modules += module.star_imports()
            modules += new
    return modules

</t>
<t tx="ekr.20180516071751.366">@memoize_default()
def _module_attributes_dict(self):
    names = ['__file__', '__package__', '__doc__', '__name__']
    # All the additional module attributes are strings.
    return dict((n, ModuleAttributeName(self, n)) for n in names)

</t>
<t tx="ekr.20180516071751.367">@property
def _string_name(self):
    """ This is used for the goto functions. """
    if self._path is None:
        return ''  # no path -&gt; empty name
    else:
        sep = (re.escape(os.path.sep),) * 2
        r = re.search(r'([^%s]*?)(%s__init__)?(\.py|\.so)?$' % sep, self._path)
        # Remove PEP 3149 names
        return re.sub('\.[a-z]+-\d{2}[mud]{0,3}$', '', r.group(1))

</t>
<t tx="ekr.20180516071751.368">@property
@memoize_default()
def name(self):
    return ModuleName(self, self._string_name)

</t>
<t tx="ekr.20180516071751.369">def _get_init_directory(self):
    """
    :return: The path to the directory of a package. None in case it's not
             a package.
    """
    for suffix, _, _ in imp.get_suffixes():
        ending = '__init__' + suffix
        py__file__ = self.py__file__()
        if py__file__ is not None and py__file__.endswith(ending):
            # Remove the ending, including the separator.
            return self.py__file__()[:-len(ending) - 1]
    return None

</t>
<t tx="ekr.20180516071751.37">def is_definition(self):
    if self._is_definition is None:
        return super(FakeName, self).is_definition()
    else:
        return self._is_definition


</t>
<t tx="ekr.20180516071751.370">def py__name__(self):
    for name, module in self.evaluator.modules.items():
        if module == self and name != '':
            return name

    return '__main__'

</t>
<t tx="ekr.20180516071751.371">def py__file__(self):
    """
    In contrast to Python's __file__ can be None.
    """
    if self._path is None:
        return None

    return os.path.abspath(self._path)

</t>
<t tx="ekr.20180516071751.372">def py__package__(self):
    if self._get_init_directory() is None:
        return re.sub(r'\.?[^\.]+$', '', self.py__name__())
    else:
        return self.py__name__()

</t>
<t tx="ekr.20180516071751.373">def _py__path__(self):
    search_path = self.evaluator.sys_path
    init_path = self.py__file__()
    if os.path.basename(init_path) == '__init__.py':
        with open(init_path, 'rb') as f:
            content = common.source_to_unicode(f.read())
            # these are strings that need to be used for namespace packages,
            # the first one is ``pkgutil``, the second ``pkg_resources``.
            options = ('declare_namespace(__name__)', 'extend_path(__path__')
            if options[0] in content or options[1] in content:
                # It is a namespace, now try to find the rest of the
                # modules on sys_path or whatever the search_path is.
                paths = set()
                for s in search_path:
                    other = os.path.join(s, self.name.string_name)
                    if os.path.isdir(other):
                        paths.add(other)
                if paths:
                    return list(paths)
                # TODO I'm not sure if this is how nested namespace
                # packages work. The tests are not really good enough to
                # show that.
    # Default to this.
    return [self._get_init_directory()]

</t>
<t tx="ekr.20180516071751.374">@property
def py__path__(self):
    """
    Not seen here, since it's a property. The callback actually uses a
    variable, so use it like::

        foo.py__path__(sys_path)

    In case of a package, this returns Python's __path__ attribute, which
    is a list of paths (strings).
    Raises an AttributeError if the module is not a package.
    """
    path = self._get_init_directory()

    if path is None:
        raise AttributeError('Only packages have __path__ attributes.')
    else:
        return self._py__path__

</t>
<t tx="ekr.20180516071751.375">@memoize_default()
def _sub_modules_dict(self):
    """
    Lists modules in the directory of this module (if this module is a
    package).
    """
    path = self._path
    names = {}
    if path is not None and path.endswith(os.path.sep + '__init__.py'):
        mods = pkgutil.iter_modules([os.path.dirname(path)])
        for module_loader, name, is_pkg in mods:
            # It's obviously a relative import to the current module.
            names[name] = imports.SubModuleName(self, name)

    # TODO add something like this in the future, its cleaner than the
    #   import hacks.
    # ``os.path`` is a hardcoded exception, because it's a
    # ``sys.modules`` modification.
    # if str(self.name) == 'os':
    #     names.append(Name('path', parent_context=self))

    return names

</t>
<t tx="ekr.20180516071751.376">def py__class__(self):
    return compiled.get_special_object(self.evaluator, 'MODULE_CLASS')

</t>
<t tx="ekr.20180516071751.377">def __repr__(self):
    return "&lt;%s: %s@%s-%s&gt;" % (
        self.__class__.__name__, self._string_name,
        self.tree_node.start_pos[0], self.tree_node.end_pos[0])


</t>
<t tx="ekr.20180516071751.378">class ImplicitNSName(AbstractNameDefinition):
    """
    Accessing names for implicit namespace packages should infer to nothing.
    This object will prevent Jedi from raising exceptions
    """
    @others
</t>
<t tx="ekr.20180516071751.379">def __init__(self, implicit_ns_context, string_name):
    self.implicit_ns_context = implicit_ns_context
    self.string_name = string_name

</t>
<t tx="ekr.20180516071751.38">@contextmanager
def predefine_names(context, flow_scope, dct):
    predefined = context.predefined_names
    if flow_scope in predefined:
        raise NotImplementedError('Why does this happen?')
    predefined[flow_scope] = dct
    try:
        yield
    finally:
        del predefined[flow_scope]
</t>
<t tx="ekr.20180516071751.380">def infer(self):
    return []

</t>
<t tx="ekr.20180516071751.381">def get_root_context(self):
    return self.implicit_ns_context


</t>
<t tx="ekr.20180516071751.382">class ImplicitNamespaceContext(use_metaclass(CachedMetaClass, context.TreeContext)):
    """
    Provides support for implicit namespace packages
    """
    api_type = 'module'
    parent_context = None

    @others
</t>
<t tx="ekr.20180516071751.383">def __init__(self, evaluator, fullname):
    super(ImplicitNamespaceContext, self).__init__(evaluator, parent_context=None)
    self.evaluator = evaluator
    self.fullname = fullname

</t>
<t tx="ekr.20180516071751.384">def get_filters(self, search_global, until_position=None, origin_scope=None):
    yield DictFilter(self._sub_modules_dict())

</t>
<t tx="ekr.20180516071751.385">@property
@memoize_default()
def name(self):
    string_name = self.py__package__().rpartition('.')[-1]
    return ImplicitNSName(self, string_name)

</t>
<t tx="ekr.20180516071751.386">def py__file__(self):
    return None

</t>
<t tx="ekr.20180516071751.387">def py__package__(self):
    """Return the fullname
    """
    return self.fullname

</t>
<t tx="ekr.20180516071751.388">@property
def py__path__(self):
    return lambda: [self.paths]

</t>
<t tx="ekr.20180516071751.389">@memoize_default()
def _sub_modules_dict(self):
    names = {}

    paths = self.paths
    file_names = chain.from_iterable(os.listdir(path) for path in paths)
    mods = [
        file_name.rpartition('.')[0] if '.' in file_name else file_name
        for file_name in file_names
        if file_name != '__pycache__'
    ]

    for name in mods:
        names[name] = imports.SubModuleName(self, name)
    return names
</t>
<t tx="ekr.20180516071751.39">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.390">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.391">"""An adapted copy of relevant site-packages functionality from Python stdlib.

This file contains some functions related to handling site-packages in Python
with jedi-specific modifications:

- the functions operate on sys_path argument rather than global sys.path

- in .pth files "import ..." lines that allow execution of arbitrary code are
  skipped to prevent code injection into jedi interpreter

"""

# Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010,
# 2011, 2012, 2013, 2014, 2015 Python Software Foundation; All Rights Reserved

from __future__ import print_function

import sys
import os


</t>
<t tx="ekr.20180516071751.392">def makepath(*paths):
    dir = os.path.join(*paths)
    try:
        dir = os.path.abspath(dir)
    except OSError:
        pass
    return dir, os.path.normcase(dir)


</t>
<t tx="ekr.20180516071751.393">def _init_pathinfo(sys_path):
    """Return a set containing all existing directory entries from sys_path"""
    d = set()
    for dir in sys_path:
        try:
            if os.path.isdir(dir):
                dir, dircase = makepath(dir)
                d.add(dircase)
        except TypeError:
            continue
    return d


</t>
<t tx="ekr.20180516071751.394">def addpackage(sys_path, sitedir, name, known_paths):
    """Process a .pth file within the site-packages directory:
       For each line in the file, either combine it with sitedir to a path
       and add that to known_paths, or execute it if it starts with 'import '.
    """
    if known_paths is None:
        known_paths = _init_pathinfo(sys_path)
        reset = 1
    else:
        reset = 0
    fullname = os.path.join(sitedir, name)
    try:
        f = open(fullname, "r")
    except OSError:
        return
    with f:
        for n, line in enumerate(f):
            if line.startswith("#"):
                continue
            try:
                if line.startswith(("import ", "import\t")):
                    # Change by immerrr: don't evaluate import lines to prevent
                    # code injection into jedi through pth files.
                    #
                    # exec(line)
                    continue
                line = line.rstrip()
                dir, dircase = makepath(sitedir, line)
                if not dircase in known_paths and os.path.exists(dir):
                    sys_path.append(dir)
                    known_paths.add(dircase)
            except Exception:
                print("Error processing line {:d} of {}:\n".format(n+1, fullname),
                      file=sys.stderr)
                import traceback
                for record in traceback.format_exception(*sys.exc_info()):
                    for line in record.splitlines():
                        print('  '+line, file=sys.stderr)
                print("\nRemainder of file ignored", file=sys.stderr)
                break
    if reset:
        known_paths = None
    return known_paths


</t>
<t tx="ekr.20180516071751.395">def addsitedir(sys_path, sitedir, known_paths=None):
    """Add 'sitedir' argument to sys_path if missing and handle .pth files in
    'sitedir'"""
    if known_paths is None:
        known_paths = _init_pathinfo(sys_path)
        reset = 1
    else:
        reset = 0
    sitedir, sitedircase = makepath(sitedir)
    if not sitedircase in known_paths:
        sys_path.append(sitedir)        # Add path component
        known_paths.add(sitedircase)
    try:
        names = os.listdir(sitedir)
    except OSError:
        return
    names = [name for name in names if name.endswith(".pth")]
    for name in sorted(names):
        addpackage(sys_path, sitedir, name, known_paths)
    if reset:
        known_paths = None
    return known_paths
</t>
<t tx="ekr.20180516071751.396">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.397">"""
Implementations of standard library functions, because it's not possible to
understand them with Jedi.

To add a new implementation, create a function and add it to the
``_implemented`` dict at the bottom of this module.

Note that this module exists only to implement very specific functionality in
the standard library. The usual way to understand the standard library is the
compiled module that returns the types for C-builtins.
"""
import collections
import re

from jedi.common import unite
from jedi.evaluate import compiled
from jedi.evaluate import representation as er
from jedi.evaluate.instance import InstanceFunctionExecution, \
    AbstractInstanceContext, CompiledInstance, BoundMethod
from jedi.evaluate import iterable
from jedi.parser.python import parse
from jedi import debug
from jedi.evaluate import precedence
from jedi.evaluate import param
from jedi.evaluate import analysis
from jedi.evaluate.context import LazyTreeContext, ContextualizedNode


</t>
<t tx="ekr.20180516071751.398">class NotInStdLib(LookupError):
    pass


</t>
<t tx="ekr.20180516071751.399">def execute(evaluator, obj, arguments):
    if isinstance(obj, BoundMethod):
        raise NotInStdLib()

    try:
        obj_name = obj.name.string_name
    except AttributeError:
        pass
    else:
        if obj.parent_context == evaluator.BUILTINS:
            module_name = 'builtins'
        elif isinstance(obj.parent_context, er.ModuleContext):
            module_name = obj.parent_context.name.string_name
        else:
            module_name = ''

        # for now we just support builtin functions.
        try:
            func = _implemented[module_name][obj_name]
        except KeyError:
            pass
        else:
            return func(evaluator, obj, arguments)
    raise NotInStdLib()


</t>
<t tx="ekr.20180516071751.4">def __init__(self, evaluator, context, name_context, name_or_str, position=None):
    self._evaluator = evaluator
    # Make sure that it's not just a syntax tree node.
    self._context = context
    self._name_context = name_context
    self._name = name_or_str
    if isinstance(name_or_str, tree.Name):
        self._string_name = name_or_str.value
    else:
        self._string_name = name_or_str
    self._position = position
    self._found_predefined_types = None

</t>
<t tx="ekr.20180516071751.40">"""
:mod:`jedi.evaluate.imports` is here to resolve import statements and return
the modules/classes/functions/whatever, which they stand for. However there's
not any actual importing done. This module is about finding modules in the
filesystem. This can be quite tricky sometimes, because Python imports are not
always that simple.

This module uses imp for python up to 3.2 and importlib for python 3.3 on; the
correct implementation is delegated to _compatibility.

This module also supports import autocompletion, which means to complete
statements like ``from datetim`` (curser at the end would return ``datetime``).
"""
import imp
import os
import pkgutil
import sys

from jedi._compatibility import find_module, unicode, ImplicitNSInfo
from jedi import debug
from jedi import settings
from jedi.common import source_to_unicode, unite
from jedi.parser.python import parse
from jedi.parser.python import tree
from jedi.parser.cache import parser_cache
from jedi.evaluate import sys_path
from jedi.evaluate import helpers
from jedi.evaluate import compiled
from jedi.evaluate import analysis
from jedi.evaluate.cache import memoize_default
from jedi.evaluate.filters import AbstractNameDefinition


# This memoization is needed, because otherwise we will infinitely loop on
# certain imports.
</t>
<t tx="ekr.20180516071751.400">def _follow_param(evaluator, arguments, index):
    try:
        key, lazy_context = list(arguments.unpack())[index]
    except IndexError:
        return set()
    else:
        return lazy_context.infer()


</t>
<t tx="ekr.20180516071751.401">def argument_clinic(string, want_obj=False, want_context=False, want_arguments=False):
    """
    Works like Argument Clinic (PEP 436), to validate function params.
    """
    clinic_args = []
    allow_kwargs = False
    optional = False
    while string:
        # Optional arguments have to begin with a bracket. And should always be
        # at the end of the arguments. This is therefore not a proper argument
        # clinic implementation. `range()` for exmple allows an optional start
        # value at the beginning.
        match = re.match('(?:(?:(\[),? ?|, ?|)(\w+)|, ?/)\]*', string)
        string = string[len(match.group(0)):]
        if not match.group(2):  # A slash -&gt; allow named arguments
            allow_kwargs = True
            continue
        optional = optional or bool(match.group(1))
        word = match.group(2)
        clinic_args.append((word, optional, allow_kwargs))

    def f(func):
        def wrapper(evaluator, obj, arguments):
            debug.dbg('builtin start %s' % obj, color='MAGENTA')
            try:
                lst = list(arguments.eval_argument_clinic(clinic_args))
            except ValueError:
                return set()
            else:
                kwargs = {}
                if want_context:
                    kwargs['context'] = arguments.context
                if want_obj:
                    kwargs['obj'] = obj
                if want_arguments:
                    kwargs['arguments'] = arguments
                return func(evaluator, *lst, **kwargs)
            finally:
                debug.dbg('builtin end', color='MAGENTA')

        return wrapper
    return f


</t>
<t tx="ekr.20180516071751.402">@argument_clinic('iterator[, default], /')
def builtins_next(evaluator, iterators, defaults):
    """
    TODO this function is currently not used. It's a stab at implementing next
    in a different way than fake objects. This would be a bit more flexible.
    """
    if evaluator.python_version[0] == 2:
        name = 'next'
    else:
        name = '__next__'

    types = set()
    for iterator in iterators:
        if isinstance(iterator, AbstractInstanceContext):
            for filter in iterator.get_filters(include_self_names=True):
                for n in filter.get(name):
                    for context in n.infer():
                        types |= context.execute_evaluated()
    if types:
        return types
    return defaults


</t>
<t tx="ekr.20180516071751.403">@argument_clinic('object, name[, default], /')
def builtins_getattr(evaluator, objects, names, defaults=None):
    # follow the first param
    for obj in objects:
        for name in names:
            if precedence.is_string(name):
                return obj.py__getattribute__(name.obj)
            else:
                debug.warning('getattr called without str')
                continue
    return set()


</t>
<t tx="ekr.20180516071751.404">@argument_clinic('object[, bases, dict], /')
def builtins_type(evaluator, objects, bases, dicts):
    if bases or dicts:
        # It's a type creation... maybe someday...
        return set()
    else:
        return set([o.py__class__() for o in objects])


</t>
<t tx="ekr.20180516071751.405">class SuperInstance(AbstractInstanceContext):
    """To be used like the object ``super`` returns."""
    @others
</t>
<t tx="ekr.20180516071751.406">def __init__(self, evaluator, cls):
    su = cls.py_mro()[1]
    super().__init__(evaluator, su and su[0] or self)


</t>
<t tx="ekr.20180516071751.407">@argument_clinic('[type[, obj]], /', want_context=True)
def builtins_super(evaluator, types, objects, context):
    # TODO make this able to detect multiple inheritance super
    if isinstance(context, InstanceFunctionExecution):
        su = context.instance.py__class__().py__bases__()
        return unite(context.execute_evaluated() for context in su[0].infer())
    return set()


</t>
<t tx="ekr.20180516071751.408">@argument_clinic('sequence, /', want_obj=True, want_arguments=True)
def builtins_reversed(evaluator, sequences, obj, arguments):
    # While we could do without this variable (just by using sequences), we
    # want static analysis to work well. Therefore we need to generated the
    # values again.
    key, lazy_context = next(arguments.unpack())
    cn = None
    if isinstance(lazy_context, LazyTreeContext):
        # TODO access private
        cn = ContextualizedNode(lazy_context._context, lazy_context.data)
    ordered = list(iterable.py__iter__(evaluator, sequences, cn))

    rev = list(reversed(ordered))
    # Repack iterator values and then run it the normal way. This is
    # necessary, because `reversed` is a function and autocompletion
    # would fail in certain cases like `reversed(x).__iter__` if we
    # just returned the result directly.
    seq = iterable.FakeSequence(evaluator, 'list', rev)
    arguments = param.ValuesArguments([[seq]])
    return set([CompiledInstance(evaluator, evaluator.BUILTINS, obj, arguments)])


</t>
<t tx="ekr.20180516071751.409">@argument_clinic('obj, type, /', want_arguments=True)
def builtins_isinstance(evaluator, objects, types, arguments):
    bool_results = set([])
    for o in objects:
        try:
            mro_func = o.py__class__().py__mro__
        except AttributeError:
            # This is temporary. Everything should have a class attribute in
            # Python?! Maybe we'll leave it here, because some numpy objects or
            # whatever might not.
            return set([compiled.create(True), compiled.create(False)])

        mro = mro_func()

        for cls_or_tup in types:
            if cls_or_tup.is_class():
                bool_results.add(cls_or_tup in mro)
            elif cls_or_tup.name.string_name == 'tuple' \
                    and cls_or_tup.get_root_context() == evaluator.BUILTINS:
                # Check for tuples.
                classes = unite(
                    lazy_context.infer()
                    for lazy_context in cls_or_tup.py__iter__()
                )
                bool_results.add(any(cls in mro for cls in classes))
            else:
                _, lazy_context = list(arguments.unpack())[1]
                if isinstance(lazy_context, LazyTreeContext):
                    node = lazy_context.data
                    message = 'TypeError: isinstance() arg 2 must be a ' \
                              'class, type, or tuple of classes and types, ' \
                              'not %s.' % cls_or_tup
                    analysis.add(lazy_context._context, 'type-error-isinstance', node, message)

    return set(compiled.create(evaluator, x) for x in bool_results)


</t>
<t tx="ekr.20180516071751.41">@memoize_default(default=set())
def infer_import(context, tree_name, is_goto=False):
    module_context = context.get_root_context()
    import_node = tree.search_ancestor(tree_name, ('import_name', 'import_from'))
    import_path = import_node.path_for_name(tree_name)
    from_import_name = None
    evaluator = context.evaluator
    try:
        from_names = import_node.get_from_names()
    except AttributeError:
        # Is an import_name
        pass
    else:
        if len(from_names) + 1 == len(import_path):
            # We have to fetch the from_names part first and then check
            # if from_names exists in the modules.
            from_import_name = import_path[-1]
            import_path = from_names

    importer = Importer(evaluator, tuple(import_path),
                        module_context, import_node.level)

    types = importer.follow()

    #if import_node.is_nested() and not self.nested_resolve:
    #    scopes = [NestedImportModule(module, import_node)]

    if from_import_name is not None:
        types = unite(
            t.py__getattribute__(
                unicode(from_import_name),
                name_context=context,
                is_goto=is_goto
            ) for t in types
        )

        if not types:
            path = import_path + [from_import_name]
            importer = Importer(evaluator, tuple(path),
                                module_context, import_node.level)
            types = importer.follow()
            # goto only accepts `Name`
            if is_goto:
                types = set(s.name for s in types)
    else:
        # goto only accepts `Name`
        if is_goto:
            types = set(s.name for s in types)

    debug.dbg('after import: %s', types)
    return types


</t>
<t tx="ekr.20180516071751.410">def collections_namedtuple(evaluator, obj, arguments):
    """
    Implementation of the namedtuple function.

    This has to be done by processing the namedtuple class template and
    evaluating the result.

    .. note:: |jedi| only supports namedtuples on Python &gt;2.6.

    """
    # Namedtuples are not supported on Python 2.6
    if not hasattr(collections, '_class_template'):
        return set()

    # Process arguments
    # TODO here we only use one of the types, we should use all.
    name = list(_follow_param(evaluator, arguments, 0))[0].obj
    _fields = list(_follow_param(evaluator, arguments, 1))[0]
    if isinstance(_fields, compiled.CompiledObject):
        fields = _fields.obj.replace(',', ' ').split()
    elif isinstance(_fields, iterable.AbstractSequence):
        fields = [
            v.obj
            for lazy_context in _fields.py__iter__()
            for v in lazy_context.infer() if hasattr(v, 'obj')
        ]
    else:
        return set()

    # Build source
    source = collections._class_template.format(
        typename=name,
        field_names=fields,
        num_fields=len(fields),
        arg_list=', '.join(fields),
        repr_fmt=', '.join(collections._repr_template.format(name=name) for name in fields),
        field_defs='\n'.join(collections._field_template.format(index=index, name=name)
                             for index, name in enumerate(fields))
    )

    # Parse source
    generated_class = parse(source, grammar=evaluator.grammar).subscopes[0]
    return set([er.ClassContext(evaluator, generated_class, evaluator.BUILTINS)])


</t>
<t tx="ekr.20180516071751.411">@argument_clinic('first, /')
def _return_first_param(evaluator, firsts):
    return firsts


_implemented = {
    'builtins': {
        'getattr': builtins_getattr,
        'type': builtins_type,
        'super': builtins_super,
        'reversed': builtins_reversed,
        'isinstance': builtins_isinstance,
    },
    'copy': {
        'copy': _return_first_param,
        'deepcopy': _return_first_param,
    },
    'json': {
        'load': lambda *args: set(),
        'loads': lambda *args: set(),
    },
    'collections': {
        'namedtuple': collections_namedtuple,
    },
}
</t>
<t tx="ekr.20180516071751.412">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.413">import glob
import os
import sys
from jedi.evaluate.site import addsitedir

from jedi._compatibility import exec_function, unicode
from jedi.parser.python import tree
from jedi.parser.python import parse
from jedi.evaluate.cache import memoize_default
from jedi import debug
from jedi import common
from jedi.evaluate.compiled import CompiledObject
from jedi.evaluate.context import ContextualizedNode


</t>
<t tx="ekr.20180516071751.414">def get_venv_path(venv):
    """Get sys.path for specified virtual environment."""
    sys_path = _get_venv_path_dirs(venv)
    with common.ignored(ValueError):
        sys_path.remove('')
    sys_path = _get_sys_path_with_egglinks(sys_path)
    # As of now, get_venv_path_dirs does not scan built-in pythonpath and
    # user-local site-packages, let's approximate them using path from Jedi
    # interpreter.
    return sys_path + sys.path


</t>
<t tx="ekr.20180516071751.415">def _get_sys_path_with_egglinks(sys_path):
    """Find all paths including those referenced by egg-links.

    Egg-link-referenced directories are inserted into path immediately before
    the directory on which their links were found.  Such directories are not
    taken into consideration by normal import mechanism, but they are traversed
    when doing pkg_resources.require.
    """
    result = []
    for p in sys_path:
        # pkg_resources does not define a specific order for egg-link files
        # using os.listdir to enumerate them, we're sorting them to have
        # reproducible tests.
        for egg_link in sorted(glob.glob(os.path.join(p, '*.egg-link'))):
            with open(egg_link) as fd:
                for line in fd:
                    line = line.strip()
                    if line:
                        result.append(os.path.join(p, line))
                        # pkg_resources package only interprets the first
                        # non-empty line in egg-link files.
                        break
        result.append(p)
    return result


</t>
<t tx="ekr.20180516071751.416">def _get_venv_path_dirs(venv):
    """Get sys.path for venv without starting up the interpreter."""
    venv = os.path.abspath(venv)
    sitedir = _get_venv_sitepackages(venv)
    sys_path = []
    addsitedir(sys_path, sitedir)
    return sys_path


</t>
<t tx="ekr.20180516071751.417">def _get_venv_sitepackages(venv):
    if os.name == 'nt':
        p = os.path.join(venv, 'lib', 'site-packages')
    else:
        p = os.path.join(venv, 'lib', 'python%d.%d' % sys.version_info[:2],
                         'site-packages')
    return p


</t>
<t tx="ekr.20180516071751.418">def _execute_code(module_path, code):
    c = "import os; from os.path import *; result=%s"
    variables = {'__file__': module_path}
    try:
        exec_function(c % code, variables)
    except Exception:
        debug.warning('sys.path manipulation detected, but failed to evaluate.')
    else:
        try:
            res = variables['result']
            if isinstance(res, str):
                return [os.path.abspath(res)]
        except KeyError:
            pass
    return []


</t>
<t tx="ekr.20180516071751.419">def _paths_from_assignment(module_context, expr_stmt):
    """
    Extracts the assigned strings from an assignment that looks as follows::

    &gt;&gt;&gt; sys.path[0:0] = ['module/path', 'another/module/path']

    This function is in general pretty tolerant (and therefore 'buggy').
    However, it's not a big issue usually to add more paths to Jedi's sys_path,
    because it will only affect Jedi in very random situations and by adding
    more paths than necessary, it usually benefits the general user.
    """
    for assignee, operator in zip(expr_stmt.children[::2], expr_stmt.children[1::2]):
        try:
            assert operator in ['=', '+=']
            assert assignee.type in ('power', 'atom_expr') and \
                len(assignee.children) &gt; 1
            c = assignee.children
            assert c[0].type == 'name' and c[0].value == 'sys'
            trailer = c[1]
            assert trailer.children[0] == '.' and trailer.children[1].value == 'path'
            # TODO Essentially we're not checking details on sys.path
            # manipulation. Both assigment of the sys.path and changing/adding
            # parts of the sys.path are the same: They get added to the current
            # sys.path.
            """
            execution = c[2]
            assert execution.children[0] == '['
            subscript = execution.children[1]
            assert subscript.type == 'subscript'
            assert ':' in subscript.children
            """
        except AssertionError:
            continue

        from jedi.evaluate.iterable import py__iter__
        from jedi.evaluate.precedence import is_string
        cn = ContextualizedNode(module_context.create_context(expr_stmt), expr_stmt)
        for lazy_context in py__iter__(module_context.evaluator, cn.infer(), cn):
            for context in lazy_context.infer():
                if is_string(context):
                    yield context.obj


</t>
<t tx="ekr.20180516071751.42">class NestedImportModule(tree.Module):
    """
    TODO while there's no use case for nested import module right now, we might
        be able to use them for static analysis checks later on.
    """
    @others
</t>
<t tx="ekr.20180516071751.420">def _paths_from_list_modifications(module_path, trailer1, trailer2):
    """ extract the path from either "sys.path.append" or "sys.path.insert" """
    # Guarantee that both are trailers, the first one a name and the second one
    # a function execution with at least one param.
    if not (trailer1.type == 'trailer' and trailer1.children[0] == '.'
            and trailer2.type == 'trailer' and trailer2.children[0] == '('
            and len(trailer2.children) == 3):
        return []

    name = trailer1.children[1].value
    if name not in ['insert', 'append']:
        return []
    arg = trailer2.children[1]
    if name == 'insert' and len(arg.children) in (3, 4):  # Possible trailing comma.
        arg = arg.children[2]
    return _execute_code(module_path, arg.get_code())


</t>
<t tx="ekr.20180516071751.421">def _check_module(module_context):
    """
    Detect sys.path modifications within module.
    """
    def get_sys_path_powers(names):
        for name in names:
            power = name.parent.parent
            if power.type in ('power', 'atom_expr'):
                c = power.children
                if isinstance(c[0], tree.Name) and c[0].value == 'sys' \
                        and c[1].type == 'trailer':
                    n = c[1].children[1]
                    if isinstance(n, tree.Name) and n.value == 'path':
                        yield name, power

    sys_path = list(module_context.evaluator.sys_path)  # copy
    if isinstance(module_context, CompiledObject):
        return sys_path

    try:
        possible_names = module_context.tree_node.used_names['path']
    except KeyError:
        # module.used_names is MergedNamesDict whose getitem never throws
        # keyerror, this is superfluous.
        pass
    else:
        for name, power in get_sys_path_powers(possible_names):
            stmt = name.get_definition()
            if len(power.children) &gt;= 4:
                sys_path.extend(
                    _paths_from_list_modifications(
                        module_context.py__file__(), *power.children[2:4]
                    )
                )
            elif name.get_definition().type == 'expr_stmt':
                sys_path.extend(_paths_from_assignment(module_context, stmt))
    return sys_path


</t>
<t tx="ekr.20180516071751.422">@memoize_default(evaluator_is_first_arg=True, default=[])
def sys_path_with_modifications(evaluator, module_context):
    path = module_context.py__file__()
    if path is None:
        # Support for modules without a path is bad, therefore return the
        # normal path.
        return list(evaluator.sys_path)

    curdir = os.path.abspath(os.curdir)
    #TODO why do we need a chdir?
    with common.ignored(OSError):
        os.chdir(os.path.dirname(path))

    buildout_script_paths = set()

    result = _check_module(module_context)
    result += _detect_django_path(path)
    for buildout_script_path in _get_buildout_script_paths(path):
        for path in _get_paths_from_buildout_script(evaluator, buildout_script_path):
            buildout_script_paths.add(path)
    # cleanup, back to old directory
    os.chdir(curdir)
    return list(result) + list(buildout_script_paths)


</t>
<t tx="ekr.20180516071751.423">def _get_paths_from_buildout_script(evaluator, buildout_script_path):
    try:
        module_node = parse(
            path=buildout_script_path,
            grammar=evaluator.grammar,
            cache=True
        )
    except IOError:
        debug.warning('Error trying to read buildout_script: %s', buildout_script_path)
        return

    from jedi.evaluate.representation import ModuleContext
    for path in _check_module(ModuleContext(evaluator, module_node, buildout_script_path)):
        yield path


</t>
<t tx="ekr.20180516071751.424">def traverse_parents(path):
    while True:
        new = os.path.dirname(path)
        if new == path:
            return
        path = new
        yield path


</t>
<t tx="ekr.20180516071751.425">def _get_parent_dir_with_file(path, filename):
    for parent in traverse_parents(path):
        if os.path.isfile(os.path.join(parent, filename)):
            return parent
    return None


</t>
<t tx="ekr.20180516071751.426">def _detect_django_path(module_path):
    """ Detects the path of the very well known Django library (if used) """
    result = []

    for parent in traverse_parents(module_path):
        with common.ignored(IOError):
            with open(parent + os.path.sep + 'manage.py'):
                debug.dbg('Found django path: %s', module_path)
                result.append(parent)
    return result


</t>
<t tx="ekr.20180516071751.427">def _get_buildout_script_paths(module_path):
    """
    if there is a 'buildout.cfg' file in one of the parent directories of the
    given module it will return a list of all files in the buildout bin
    directory that look like python files.

    :param module_path: absolute path to the module.
    :type module_path: str
    """
    project_root = _get_parent_dir_with_file(module_path, 'buildout.cfg')
    if not project_root:
        return []
    bin_path = os.path.join(project_root, 'bin')
    if not os.path.exists(bin_path):
        return []
    extra_module_paths = []
    for filename in os.listdir(bin_path):
        try:
            filepath = os.path.join(bin_path, filename)
            with open(filepath, 'r') as f:
                firstline = f.readline()
                if firstline.startswith('#!') and 'python' in firstline:
                    extra_module_paths.append(filepath)
        except (UnicodeDecodeError, IOError) as e:
            # Probably a binary file; permission error or race cond. because file got deleted
            # ignore
            debug.warning(unicode(e))
            continue
    return extra_module_paths
</t>
<t tx="ekr.20180516071751.428">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.429">"""
Evaluation of Python code in |jedi| is based on three assumptions:

* The code uses as least side effects as possible. Jedi understands certain
  list/tuple/set modifications, but there's no guarantee that Jedi detects
  everything (list.append in different modules for example).
* No magic is being used:

  - metaclasses
  - ``setattr()`` / ``__import__()``
  - writing to ``globals()``, ``locals()``, ``object.__dict__``
* The programmer is not a total dick, e.g. like `this
  &lt;https://github.com/davidhalter/jedi/issues/24&gt;`_ :-)

The actual algorithm is based on a principle called lazy evaluation. If you
don't know about it, google it.  That said, the typical entry point for static
analysis is calling ``eval_statement``. There's separate logic for
autocompletion in the API, the evaluator is all about evaluating an expression.

Now you need to understand what follows after ``eval_statement``. Let's
make an example::

    import datetime
    datetime.date.toda# &lt;-- cursor here

First of all, this module doesn't care about completion. It really just cares
about ``datetime.date``. At the end of the procedure ``eval_statement`` will
return the ``date`` class.

To *visualize* this (simplified):

- ``Evaluator.eval_statement`` doesn't do much, because there's no assignment.
- ``Evaluator.eval_element`` cares for resolving the dotted path
- ``Evaluator.find_types`` searches for global definitions of datetime, which
  it finds in the definition of an import, by scanning the syntax tree.
- Using the import logic, the datetime module is found.
- Now ``find_types`` is called again by ``eval_element`` to find ``date``
  inside the datetime module.

Now what would happen if we wanted ``datetime.date.foo.bar``? Two more
calls to ``find_types``. However the second call would be ignored, because the
first one would return nothing (there's no foo attribute in ``date``).

What if the import would contain another ``ExprStmt`` like this::

    from foo import bar
    Date = bar.baz

Well... You get it. Just another ``eval_statement`` recursion. It's really
easy. Python can obviously get way more complicated then this. To understand
tuple assignments, list comprehensions and everything else, a lot more code had
to be written.

Jedi has been tested very well, so you can just start modifying code. It's best
to write your own test first for your "new" feature. Don't be scared of
breaking stuff. As long as the tests pass, you're most likely to be fine.

I need to mention now that lazy evaluation is really good because it
only *evaluates* what needs to be *evaluated*. All the statements and modules
that are not used are just being ignored.
"""

import copy
import sys

from jedi.parser.python import tree
from jedi import debug
from jedi.common import unite
from jedi.evaluate import representation as er
from jedi.evaluate import imports
from jedi.evaluate import recursion
from jedi.evaluate import iterable
from jedi.evaluate.cache import memoize_default
from jedi.evaluate import stdlib
from jedi.evaluate import finder
from jedi.evaluate import compiled
from jedi.evaluate import precedence
from jedi.evaluate import param
from jedi.evaluate import helpers
from jedi.evaluate import pep0484
from jedi.evaluate.filters import TreeNameDefinition, ParamName
from jedi.evaluate.instance import AnonymousInstance, BoundMethod
from jedi.evaluate.context import ContextualizedName, ContextualizedNode


</t>
<t tx="ekr.20180516071751.43">def __init__(self, module, nested_import):
    self._module = module
    self._nested_import = nested_import

</t>
<t tx="ekr.20180516071751.430">class Evaluator(object):
    @others
</t>
<t tx="ekr.20180516071751.431">def __init__(self, grammar, sys_path=None):
    self.grammar = grammar
    self.memoize_cache = {}  # for memoize decorators
    # To memorize modules -&gt; equals `sys.modules`.
    self.modules = {}  # like `sys.modules`.
    self.compiled_cache = {}  # see `evaluate.compiled.create()`
    self.mixed_cache = {}  # see `evaluate.compiled.mixed.create()`
    self.analysis = []
    self.dynamic_params_depth = 0
    self.is_analysis = False
    self.python_version = sys.version_info[:2]

    if sys_path is None:
        sys_path = sys.path
    self.sys_path = copy.copy(sys_path)
    try:
        self.sys_path.remove('')
    except ValueError:
        pass

    self.reset_recursion_limitations()

    # Constants
    self.BUILTINS = compiled.get_special_object(self, 'BUILTINS')

</t>
<t tx="ekr.20180516071751.432">def reset_recursion_limitations(self):
    self.recursion_detector = recursion.RecursionDetector()
    self.execution_recursion_detector = recursion.ExecutionRecursionDetector(self)

</t>
<t tx="ekr.20180516071751.433">def find_types(self, context, name_or_str, name_context, position=None,
               search_global=False, is_goto=False):
    """
    This is the search function. The most important part to debug.
    `remove_statements` and `filter_statements` really are the core part of
    this completion.

    :param position: Position of the last statement -&gt; tuple of line, column
    :return: List of Names. Their parents are the types.
    """
    f = finder.NameFinder(self, context, name_context, name_or_str, position)
    filters = f.get_filters(search_global)
    if is_goto:
        return f.filter_name(filters)
    return f.find(filters, attribute_lookup=not search_global)

</t>
<t tx="ekr.20180516071751.434">def eval_statement(self, context, stmt, seek_name=None):
    with recursion.execution_allowed(self, stmt) as allowed:
        if allowed or context.get_root_context() == self.BUILTINS:
            return self._eval_stmt(context, stmt, seek_name)
    return set()

#@memoize_default(default=[], evaluator_is_first_arg=True)
</t>
<t tx="ekr.20180516071751.435">@debug.increase_indent
def _eval_stmt(self, context, stmt, seek_name=None):
    """
    The starting point of the completion. A statement always owns a call
    list, which are the calls, that a statement does. In case multiple
    names are defined in the statement, `seek_name` returns the result for
    this name.

    :param stmt: A `tree.ExprStmt`.
    """
    debug.dbg('eval_statement %s (%s)', stmt, seek_name)
    rhs = stmt.get_rhs()
    types = self.eval_element(context, rhs)

    if seek_name:
        c_node = ContextualizedName(context, seek_name)
        types = finder.check_tuple_assignments(self, c_node, types)

    first_operation = stmt.first_operation()
    if first_operation not in ('=', None) and first_operation.type == 'operator':
        # `=` is always the last character in aug assignments -&gt; -1
        operator = copy.copy(first_operation)
        operator.value = operator.value[:-1]
        name = str(stmt.get_defined_names()[0])
        left = context.py__getattribute__(
            name, position=stmt.start_pos, search_global=True)

        for_stmt = tree.search_ancestor(stmt, 'for_stmt')
        if for_stmt is not None and for_stmt.type == 'for_stmt' and types \
                and for_stmt.defines_one_name():
            # Iterate through result and add the values, that's possible
            # only in for loops without clutter, because they are
            # predictable. Also only do it, if the variable is not a tuple.
            node = for_stmt.get_input_node()
            cn = ContextualizedNode(context, node)
            ordered = list(iterable.py__iter__(self, cn.infer(), cn))

            for lazy_context in ordered:
                dct = {str(for_stmt.children[1]): lazy_context.infer()}
                with helpers.predefine_names(context, for_stmt, dct):
                    t = self.eval_element(context, rhs)
                    left = precedence.calculate(self, context, left, operator, t)
            types = left
        else:
            types = precedence.calculate(self, context, left, operator, types)
    debug.dbg('eval_statement result %s', types)
    return types

</t>
<t tx="ekr.20180516071751.436">def eval_element(self, context, element):
    if isinstance(context, iterable.CompForContext):
        return self._eval_element_not_cached(context, element)

    if_stmt = element
    while if_stmt is not None:
        if_stmt = if_stmt.parent
        if if_stmt.type in ('if_stmt', 'for_stmt'):
            break
        if if_stmt.is_scope():
            if_stmt = None
            break
    predefined_if_name_dict = context.predefined_names.get(if_stmt)
    if predefined_if_name_dict is None and if_stmt and if_stmt.type == 'if_stmt':
        if_stmt_test = if_stmt.children[1]
        name_dicts = [{}]
        # If we already did a check, we don't want to do it again -&gt; If
        # context.predefined_names is filled, we stop.
        # We don't want to check the if stmt itself, it's just about
        # the content.
        if element.start_pos &gt; if_stmt_test.end_pos:
            # Now we need to check if the names in the if_stmt match the
            # names in the suite.
            if_names = helpers.get_names_of_node(if_stmt_test)
            element_names = helpers.get_names_of_node(element)
            str_element_names = [str(e) for e in element_names]
            if any(str(i) in str_element_names for i in if_names):
                for if_name in if_names:
                    definitions = self.goto_definitions(context, if_name)
                    # Every name that has multiple different definitions
                    # causes the complexity to rise. The complexity should
                    # never fall below 1.
                    if len(definitions) &gt; 1:
                        if len(name_dicts) * len(definitions) &gt; 16:
                            debug.dbg('Too many options for if branch evaluation %s.', if_stmt)
                            # There's only a certain amount of branches
                            # Jedi can evaluate, otherwise it will take to
                            # long.
                            name_dicts = [{}]
                            break

                        original_name_dicts = list(name_dicts)
                        name_dicts = []
                        for definition in definitions:
                            new_name_dicts = list(original_name_dicts)
                            for i, name_dict in enumerate(new_name_dicts):
                                new_name_dicts[i] = name_dict.copy()
                                new_name_dicts[i][str(if_name)] = set([definition])

                            name_dicts += new_name_dicts
                    else:
                        for name_dict in name_dicts:
                            name_dict[str(if_name)] = definitions
        if len(name_dicts) &gt; 1:
            result = set()
            for name_dict in name_dicts:
                with helpers.predefine_names(context, if_stmt, name_dict):
                    result |= self._eval_element_not_cached(context, element)
            return result
        else:
            return self._eval_element_if_evaluated(context, element)
    else:
        if predefined_if_name_dict:
            return self._eval_element_not_cached(context, element)
        else:
            return self._eval_element_if_evaluated(context, element)

</t>
<t tx="ekr.20180516071751.437">def _eval_element_if_evaluated(self, context, element):
    """
    TODO This function is temporary: Merge with eval_element.
    """
    parent = element
    while parent is not None:
        parent = parent.parent
        predefined_if_name_dict = context.predefined_names.get(parent)
        if predefined_if_name_dict is not None:
            return self._eval_element_not_cached(context, element)
    return self._eval_element_cached(context, element)

</t>
<t tx="ekr.20180516071751.438">@memoize_default(default=set(), evaluator_is_first_arg=True)
def _eval_element_cached(self, context, element):
    return self._eval_element_not_cached(context, element)

</t>
<t tx="ekr.20180516071751.439">@debug.increase_indent
def _eval_element_not_cached(self, context, element):
    debug.dbg('eval_element %s@%s', element, element.start_pos)
    types = set()
    typ = element.type
    if typ in ('name', 'number', 'string', 'atom'):
        types = self.eval_atom(context, element)
    elif typ == 'keyword':
        # For False/True/None
        if element.value in ('False', 'True', 'None'):
            types.add(compiled.builtin_from_name(self, element.value))
        # else: print e.g. could be evaluated like this in Python 2.7
    elif typ == 'lambda':
        types = set([er.FunctionContext(self, context, element)])
    elif typ == 'expr_stmt':
        types = self.eval_statement(context, element)
    elif typ in ('power', 'atom_expr'):
        first_child = element.children[0]
        if not (first_child.type == 'keyword' and first_child.value == 'await'):
            types = self.eval_atom(context, first_child)
            for trailer in element.children[1:]:
                if trailer == '**':  # has a power operation.
                    right = self.eval_element(context, element.children[2])
                    types = set(precedence.calculate(self, context, types, trailer, right))
                    break
                types = self.eval_trailer(context, types, trailer)
    elif typ in ('testlist_star_expr', 'testlist',):
        # The implicit tuple in statements.
        types = set([iterable.SequenceLiteralContext(self, context, element)])
    elif typ in ('not_test', 'factor'):
        types = self.eval_element(context, element.children[-1])
        for operator in element.children[:-1]:
            types = set(precedence.factor_calculate(self, types, operator))
    elif typ == 'test':
        # `x if foo else y` case.
        types = (self.eval_element(context, element.children[0]) |
                 self.eval_element(context, element.children[-1]))
    elif typ == 'operator':
        # Must be an ellipsis, other operators are not evaluated.
        assert element.value == '...'
        types = set([compiled.create(self, Ellipsis)])
    elif typ == 'dotted_name':
        types = self.eval_atom(context, element.children[0])
        for next_name in element.children[2::2]:
            # TODO add search_global=True?
            types = unite(
                typ.py__getattribute__(next_name, name_context=context)
                for typ in types
            )
        types = types
    elif typ == 'eval_input':
        types = self._eval_element_not_cached(context, element.children[0])
    elif typ == 'annassign':
        types = pep0484._evaluate_for_annotation(context, element.children[1])
    else:
        types = precedence.calculate_children(self, context, element.children)
    debug.dbg('eval_element result %s', types)
    return types

</t>
<t tx="ekr.20180516071751.44">def _get_nested_import_name(self):
    """
    Generates an Import statement, that can be used to fake nested imports.
    """
    i = self._nested_import
    # This is not an existing Import statement. Therefore, set position to
    # 0 (0 is not a valid line number).
    zero = (0, 0)
    names = [unicode(name) for name in i.namespace_names[1:]]
    name = helpers.FakeName(names, self._nested_import)
    new = tree.Import(i._sub_module, zero, zero, name)
    new.parent = self._module
    debug.dbg('Generated a nested import: %s', new)
    return helpers.FakeName(str(i.namespace_names[1]), new)

</t>
<t tx="ekr.20180516071751.440">def eval_atom(self, context, atom):
    """
    Basically to process ``atom`` nodes. The parser sometimes doesn't
    generate the node (because it has just one child). In that case an atom
    might be a name or a literal as well.
    """
    if atom.type == 'name':
        # This is the first global lookup.
        stmt = atom.get_definition()
        if stmt.type == 'comp_for':
            stmt = tree.search_ancestor(stmt, ('expr_stmt', 'lambda', 'funcdef', 'classdef'))
        if stmt is None or stmt.type != 'expr_stmt':
            # We only need to adjust the start_pos for statements, because
            # there the name cannot be used.
            stmt = atom
        return context.py__getattribute__(
            name_or_str=atom,
            position=stmt.start_pos,
            search_global=True
        )
    elif isinstance(atom, tree.Literal):
        return set([compiled.create(self, atom.eval())])
    else:
        c = atom.children
        if c[0].type == 'string':
            # Will be one string.
            types = self.eval_atom(context, c[0])
            for string in c[1:]:
                right = self.eval_atom(context, string)
                types = precedence.calculate(self, context, types, '+', right)
            return types
        # Parentheses without commas are not tuples.
        elif c[0] == '(' and not len(c) == 2 \
                and not(c[1].type == 'testlist_comp' and
                        len(c[1].children) &gt; 1):
            return self.eval_element(context, c[1])

        try:
            comp_for = c[1].children[1]
        except (IndexError, AttributeError):
            pass
        else:
            if comp_for == ':':
                # Dict comprehensions have a colon at the 3rd index.
                try:
                    comp_for = c[1].children[3]
                except IndexError:
                    pass

            if comp_for.type == 'comp_for':
                return set([iterable.Comprehension.from_atom(self, context, atom)])

        # It's a dict/list/tuple literal.
        array_node = c[1]
        try:
            array_node_c = array_node.children
        except AttributeError:
            array_node_c = []
        if c[0] == '{' and (array_node == '}' or ':' in array_node_c):
            context = iterable.DictLiteralContext(self, context, atom)
        else:
            context = iterable.SequenceLiteralContext(self, context, atom)
        return set([context])

</t>
<t tx="ekr.20180516071751.441">def eval_trailer(self, context, types, trailer):
    trailer_op, node = trailer.children[:2]
    if node == ')':  # `arglist` is optional.
        node = ()

    new_types = set()
    if trailer_op == '[':
        new_types |= iterable.py__getitem__(self, context, types, trailer)
    else:
        for typ in types:
            debug.dbg('eval_trailer: %s in scope %s', trailer, typ)
            if trailer_op == '.':
                new_types |= typ.py__getattribute__(
                    name_context=context,
                    name_or_str=node
                )
            elif trailer_op == '(':
                arguments = param.TreeArguments(self, context, node, trailer)
                new_types |= self.execute(typ, arguments)
    return new_types

</t>
<t tx="ekr.20180516071751.442">@debug.increase_indent
def execute(self, obj, arguments):
    if not isinstance(arguments, param.AbstractArguments):
        raise NotImplementedError
        arguments = param.Arguments(self, arguments)

    if self.is_analysis:
        arguments.eval_all()

    debug.dbg('execute: %s %s', obj, arguments)
    try:
        # Some stdlib functions like super(), namedtuple(), etc. have been
        # hard-coded in Jedi to support them.
        return stdlib.execute(self, obj, arguments)
    except stdlib.NotInStdLib:
        pass

    try:
        func = obj.py__call__
    except AttributeError:
        debug.warning("no execution possible %s", obj)
        return set()
    else:
        types = func(arguments)
        debug.dbg('execute result: %s in %s', types, obj)
        return types

</t>
<t tx="ekr.20180516071751.443">def goto_definitions(self, context, name):
    def_ = name.get_definition()
    is_simple_name = name.parent.type not in ('power', 'trailer')
    if is_simple_name:
        if name.parent.type == 'classdef' and name.parent.name == name:
            return [er.ClassContext(self, name.parent, context)]
        elif name.parent.type == 'funcdef':
            return [er.FunctionContext(self, context, name.parent)]
        elif name.parent.type == 'file_input':
            raise NotImplementedError
        if def_.type == 'expr_stmt' and name in def_.get_defined_names():
            return self.eval_statement(context, def_, name)
        elif def_.type == 'for_stmt':
            container_types = self.eval_element(context, def_.children[3])
            cn = ContextualizedNode(context, def_.children[3])
            for_types = iterable.py__iter__types(self, container_types, cn)
            c_node = ContextualizedName(context, name)
            return finder.check_tuple_assignments(self, c_node, for_types)
        elif def_.type in ('import_from', 'import_name'):
            return imports.infer_import(context, name)

    return helpers.evaluate_call_of_leaf(context, name)

</t>
<t tx="ekr.20180516071751.444">def goto(self, context, name):
    stmt = name.get_definition()
    par = name.parent
    if par.type == 'argument' and par.children[1] == '=' and par.children[0] == name:
        # Named param goto.
        trailer = par.parent
        if trailer.type == 'arglist':
            trailer = trailer.parent
        if trailer.type != 'classdef':
            if trailer.type == 'decorator':
                types = self.eval_element(context, trailer.children[1])
            else:
                i = trailer.parent.children.index(trailer)
                to_evaluate = trailer.parent.children[:i]
                types = self.eval_element(context, to_evaluate[0])
                for trailer in to_evaluate[1:]:
                    types = self.eval_trailer(context, types, trailer)
            param_names = []
            for typ in types:
                try:
                    get_param_names = typ.get_param_names
                except AttributeError:
                    pass
                else:
                    for param_name in get_param_names():
                        if param_name.string_name == name.value:
                            param_names.append(param_name)
            return param_names
    elif par.type == 'expr_stmt' and name in par.get_defined_names():
        # Only take the parent, because if it's more complicated than just
        # a name it's something you can "goto" again.
        return [TreeNameDefinition(context, name)]
    elif par.type == 'param' and par.name:
        return [ParamName(context, name)]
    elif isinstance(par, (tree.Param, tree.Function, tree.Class)) and par.name is name:
        return [TreeNameDefinition(context, name)]
    elif isinstance(stmt, tree.Import):
        module_names = imports.infer_import(context, name, is_goto=True)
        return module_names
    elif par.type == 'dotted_name':  # Is a decorator.
        index = par.children.index(name)
        if index &gt; 0:
            new_dotted = helpers.deep_ast_copy(par)
            new_dotted.children[index - 1:] = []
            values = self.eval_element(context, new_dotted)
            return unite(
                value.py__getattribute__(name, name_context=context, is_goto=True)
                for value in values
            )

    if par.type == 'trailer' and par.children[0] == '.':
        values = helpers.evaluate_call_of_leaf(context, name, cut_own_trailer=True)
        return unite(
            value.py__getattribute__(name, name_context=context, is_goto=True)
            for value in values
        )
    else:
        if stmt.type != 'expr_stmt':
            # We only need to adjust the start_pos for statements, because
            # there the name cannot be used.
            stmt = name
        return context.py__getattribute__(
            name,
            position=stmt.start_pos,
            search_global=True, is_goto=True
        )

</t>
<t tx="ekr.20180516071751.445">def create_context(self, base_context, node, node_is_context=False, node_is_object=False):
    def parent_scope(node):
        while True:
            node = node.parent

            if node.is_scope():
                return node
            elif node.type in ('argument', 'testlist_comp'):
                if node.children[1].type == 'comp_for':
                    return node.children[1]
            elif node.type == 'dictorsetmaker':
                for n in node.children[1:4]:
                    # In dictionaries it can be pretty much anything.
                    if n.type == 'comp_for':
                        return n

    def from_scope_node(scope_node, child_is_funcdef=None, is_nested=True, node_is_object=False):
        if scope_node == base_node:
            return base_context

        is_funcdef = scope_node.type in ('funcdef', 'lambda')
        parent_scope = scope_node.get_parent_scope()
        parent_context = from_scope_node(parent_scope, child_is_funcdef=is_funcdef)

        if is_funcdef:
            if isinstance(parent_context, AnonymousInstance):
                func = BoundMethod(
                    self, parent_context, parent_context.class_context,
                    parent_context.parent_context, scope_node
                )
            else:
                func = er.FunctionContext(
                    self,
                    parent_context,
                    scope_node
                )
            if is_nested and not node_is_object:
                return func.get_function_execution()
            return func
        elif scope_node.type == 'classdef':
            class_context = er.ClassContext(self, scope_node, parent_context)
            if child_is_funcdef:
                # anonymous instance
                return AnonymousInstance(self, parent_context, class_context)
            else:
                return class_context
        elif scope_node.type == 'comp_for':
            if node.start_pos &gt;= scope_node.children[-1].start_pos:
                return parent_context
            return iterable.CompForContext.from_comp_for(parent_context, scope_node)
        raise Exception("There's a scope that was not managed.")

    base_node = base_context.tree_node

    if node_is_context and node.is_scope():
        scope_node = node
    else:
        if node.parent.type in ('funcdef', 'classdef'):
            # When we're on class/function names/leafs that define the
            # object itself and not its contents.
            node = node.parent
        scope_node = parent_scope(node)
    return from_scope_node(scope_node, is_nested=True, node_is_object=node_is_object)
</t>
<t tx="ekr.20180516071751.447"></t>
<t tx="ekr.20180516071751.448">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/compiled/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.449">"""
Loads functions that are mixed in to the standard library. E.g. builtins are
written in C (binaries), but my autocompletion only understands Python code. By
mixing in Python code, the autocompletion should work much better for builtins.
"""

import os
import inspect
import types

from jedi._compatibility import is_py3, builtins, unicode, is_py34
from jedi.parser.python import parse
from jedi.parser.python import tree

modules = {}


MethodDescriptorType = type(str.replace)
# These are not considered classes and access is granted even though they have
# a __class__ attribute.
NOT_CLASS_TYPES = (
    types.BuiltinFunctionType,
    types.CodeType,
    types.FrameType,
    types.FunctionType,
    types.GeneratorType,
    types.GetSetDescriptorType,
    types.LambdaType,
    types.MemberDescriptorType,
    types.MethodType,
    types.ModuleType,
    types.TracebackType,
    MethodDescriptorType
)

if is_py3:
    NOT_CLASS_TYPES += (
        types.MappingProxyType,
        types.SimpleNamespace
    )
    if is_py34:
        NOT_CLASS_TYPES += (types.DynamicClassAttribute,)


</t>
<t tx="ekr.20180516071751.45">def __getattr__(self, name):
    return getattr(self._module, name)

</t>
<t tx="ekr.20180516071751.450">class FakeDoesNotExist(Exception):
    pass


</t>
<t tx="ekr.20180516071751.451">def _load_faked_module(module):
    module_name = module.__name__
    if module_name == '__builtin__' and not is_py3:
        module_name = 'builtins'

    try:
        return modules[module_name]
    except KeyError:
        path = os.path.dirname(os.path.abspath(__file__))
        try:
            with open(os.path.join(path, 'fake', module_name) + '.pym') as f:
                source = f.read()
        except IOError:
            modules[module_name] = None
            return
        modules[module_name] = m = parse(unicode(source))

        if module_name == 'builtins' and not is_py3:
            # There are two implementations of `open` for either python 2/3.
            # -&gt; Rename the python2 version (`look at fake/builtins.pym`).
            open_func = _search_scope(m, 'open')
            open_func.children[1].value = 'open_python3'
            open_func = _search_scope(m, 'open_python2')
            open_func.children[1].value = 'open'
        return m


</t>
<t tx="ekr.20180516071751.452">def _search_scope(scope, obj_name):
    for s in scope.subscopes:
        if s.name.value == obj_name:
            return s


</t>
<t tx="ekr.20180516071751.453">def get_module(obj):
    if inspect.ismodule(obj):
        return obj
    try:
        obj = obj.__objclass__
    except AttributeError:
        pass

    try:
        imp_plz = obj.__module__
    except AttributeError:
        # Unfortunately in some cases like `int` there's no __module__
        return builtins
    else:
        if imp_plz is None:
            # Happens for example in `(_ for _ in []).send.__module__`.
            return builtins
        else:
            try:
                return __import__(imp_plz)
            except ImportError:
                # __module__ can be something arbitrary that doesn't exist.
                return builtins


</t>
<t tx="ekr.20180516071751.454">def _faked(module, obj, name):
    # Crazy underscore actions to try to escape all the internal madness.
    if module is None:
        module = get_module(obj)

    faked_mod = _load_faked_module(module)
    if faked_mod is None:
        return None, None

    # Having the module as a `parser.python.tree.Module`, we need to scan
    # for methods.
    if name is None:
        if inspect.isbuiltin(obj) or inspect.isclass(obj):
            return _search_scope(faked_mod, obj.__name__), faked_mod
        elif not inspect.isclass(obj):
            # object is a method or descriptor
            try:
                objclass = obj.__objclass__
            except AttributeError:
                return None, None
            else:
                cls = _search_scope(faked_mod, objclass.__name__)
                if cls is None:
                    return None, None
                return _search_scope(cls, obj.__name__), faked_mod
    else:
        if obj is module:
            return _search_scope(faked_mod, name), faked_mod
        else:
            try:
                cls_name = obj.__name__
            except AttributeError:
                return None, None
            cls = _search_scope(faked_mod, cls_name)
            if cls is None:
                return None, None
            return _search_scope(cls, name), faked_mod
    return None, None


</t>
<t tx="ekr.20180516071751.455">def memoize_faked(obj):
    """
    A typical memoize function that ignores issues with non hashable results.
    """
    cache = obj.cache = {}

    def memoizer(*args, **kwargs):
        key = (obj, args, frozenset(kwargs.items()))
        try:
            result = cache[key]
        except (TypeError, ValueError):
            return obj(*args, **kwargs)
        except KeyError:
            result = obj(*args, **kwargs)
            if result is not None:
                cache[key] = obj(*args, **kwargs)
            return result
        else:
            return result
    return memoizer


</t>
<t tx="ekr.20180516071751.456">@memoize_faked
def _get_faked(module, obj, name=None):
    result, fake_module = _faked(module, obj, name)
    if result is None:
        # We're not interested in classes. What we want is functions.
        raise FakeDoesNotExist
    elif result.type == 'classdef':
        return result, fake_module
    else:
        # Set the docstr which was previously not set (faked modules don't
        # contain it).
        assert result.type == 'funcdef'
        doc = '"""%s"""' % obj.__doc__  # TODO need escapes.
        suite = result.children[-1]
        string = tree.String(doc, (0, 0), '')
        new_line = tree.Newline('\n', (0, 0))
        docstr_node = tree.PythonNode('simple_stmt', [string, new_line])
        suite.children.insert(1, docstr_node)
        return result, fake_module


</t>
<t tx="ekr.20180516071751.457">def get_faked(evaluator, module, obj, name=None, parent_context=None):
    if parent_context and parent_context.tree_node is not None:
        # Try to search in already clearly defined stuff.
        found = _search_scope(parent_context.tree_node, name)
        if found is not None:
            return found
        else:
            raise FakeDoesNotExist

    faked, fake_module = _get_faked(module and module.obj, obj, name)
    if module is not None:
        module.used_names = fake_module.used_names
    return faked


</t>
<t tx="ekr.20180516071751.458">def is_class_instance(obj):
    """Like inspect.* methods."""
    try:
        cls = obj.__class__
    except AttributeError:
        return False
    else:
        return cls != type and not issubclass(cls, NOT_CLASS_TYPES)
</t>
<t tx="ekr.20180516071751.459">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/compiled/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.46">def __repr__(self):
    return "&lt;%s: %s of %s&gt;" % (self.__class__.__name__, self._module,
                               self._nested_import)


</t>
<t tx="ekr.20180516071751.460">"""
Used only for REPL Completion.
"""

import inspect
import os

from jedi.parser.python import parse
from jedi.evaluate import compiled
from jedi.cache import underscore_memoization
from jedi.evaluate import imports
from jedi.evaluate.context import Context
from jedi.evaluate.cache import memoize_default


</t>
<t tx="ekr.20180516071751.461">class MixedObject(object):
    """
    A ``MixedObject`` is used in two ways:

    1. It uses the default logic of ``parser.python.tree`` objects,
    2. except for getattr calls. The names dicts are generated in a fashion
       like ``CompiledObject``.

    This combined logic makes it possible to provide more powerful REPL
    completion. It allows side effects that are not noticable with the default
    parser structure to still be completeable.

    The biggest difference from CompiledObject to MixedObject is that we are
    generally dealing with Python code and not with C code. This will generate
    fewer special cases, because we in Python you don't have the same freedoms
    to modify the runtime.
    """
    @others
</t>
<t tx="ekr.20180516071751.462">def __init__(self, evaluator, parent_context, compiled_object, tree_context):
    self.evaluator = evaluator
    self.parent_context = parent_context
    self.compiled_object = compiled_object
    self._context = tree_context
    self.obj = compiled_object.obj

# We have to overwrite everything that has to do with trailers, name
# lookups and filters to make it possible to route name lookups towards
# compiled objects and the rest towards tree node contexts.
</t>
<t tx="ekr.20180516071751.463">def eval_trailer(*args, **kwags):
    return Context.eval_trailer(*args, **kwags)

</t>
<t tx="ekr.20180516071751.464">def py__getattribute__(*args, **kwargs):
    return Context.py__getattribute__(*args, **kwargs)

</t>
<t tx="ekr.20180516071751.465">def get_filters(self, *args, **kwargs):
    yield MixedObjectFilter(self.evaluator, self)

</t>
<t tx="ekr.20180516071751.466">def __repr__(self):
    return '&lt;%s: %s&gt;' % (type(self).__name__, repr(self.obj))

</t>
<t tx="ekr.20180516071751.467">def __getattr__(self, name):
    return getattr(self._context, name)


</t>
<t tx="ekr.20180516071751.468">class MixedName(compiled.CompiledName):
    """
    The ``CompiledName._compiled_object`` is our MixedObject.
    """
    @others
</t>
<t tx="ekr.20180516071751.469">@property
def start_pos(self):
    contexts = list(self.infer())
    if not contexts:
        # This means a start_pos that doesn't exist (compiled objects).
        return (0, 0)
    return contexts[0].name.start_pos

</t>
<t tx="ekr.20180516071751.47">def _add_error(context, name, message=None):
    # Should be a name, not a string!
    if hasattr(name, 'parent'):
        analysis.add(context, 'import-error', name, message)


</t>
<t tx="ekr.20180516071751.470">@start_pos.setter
def start_pos(self, value):
    # Ignore the __init__'s start_pos setter call.
    pass

</t>
<t tx="ekr.20180516071751.471">@underscore_memoization
def infer(self):
    obj = self.parent_context.obj
    try:
        obj = getattr(obj, self.string_name)
    except AttributeError:
        # Happens e.g. in properties of
        # PyQt4.QtGui.QStyleOptionComboBox.currentText
        # -&gt; just set it to None
        obj = None
    return [create(self._evaluator, obj, parent_context=self.parent_context)]

</t>
<t tx="ekr.20180516071751.472">@property
def api_type(self):
    return next(iter(self.infer())).api_type


</t>
<t tx="ekr.20180516071751.473">class MixedObjectFilter(compiled.CompiledObjectFilter):
    name_class = MixedName

    @others
</t>
<t tx="ekr.20180516071751.474">def __init__(self, evaluator, mixed_object, is_instance=False):
    super(MixedObjectFilter, self).__init__(
        evaluator, mixed_object, is_instance)
    self._mixed_object = mixed_object

#def _create(self, name):
    #return MixedName(self._evaluator, self._compiled_object, name)


</t>
<t tx="ekr.20180516071751.475">@memoize_default(evaluator_is_first_arg=True)
def _load_module(evaluator, path, python_object):
    module = parse(
        grammar=evaluator.grammar,
        path=path,
        cache=True,
        diff_cache=True
    ).get_root_node()
    python_module = inspect.getmodule(python_object)

    evaluator.modules[python_module.__name__] = module
    return module


</t>
<t tx="ekr.20180516071751.476">def find_syntax_node_name(evaluator, python_object):
    try:
        path = inspect.getsourcefile(python_object)
    except TypeError:
        # The type might not be known (e.g. class_with_dict.__weakref__)
        return None, None
    if path is None or not os.path.exists(path):
        # The path might not exist or be e.g. &lt;stdin&gt;.
        return None, None

    module = _load_module(evaluator, path, python_object)

    if inspect.ismodule(python_object):
        # We don't need to check names for modules, because there's not really
        # a way to write a module in a module in Python (and also __name__ can
        # be something like ``email.utils``).
        return module, path

    try:
        name_str = python_object.__name__
    except AttributeError:
        # Stuff like python_function.__code__.
        return None, None

    if name_str == '&lt;lambda&gt;':
        return None, None  # It's too hard to find lambdas.

    # Doesn't always work (e.g. os.stat_result)
    try:
        names = module.used_names[name_str]
    except KeyError:
        return None, None
    names = [n for n in names if n.is_definition()]

    try:
        code = python_object.__code__
        # By using the line number of a code object we make the lookup in a
        # file pretty easy. There's still a possibility of people defining
        # stuff like ``a = 3; foo(a); a = 4`` on the same line, but if people
        # do so we just don't care.
        line_nr = code.co_firstlineno
    except AttributeError:
        pass
    else:
        line_names = [name for name in names if name.start_pos[0] == line_nr]
        # There's a chance that the object is not available anymore, because
        # the code has changed in the background.
        if line_names:
            return line_names[-1].parent, path

    # It's really hard to actually get the right definition, here as a last
    # resort we just return the last one. This chance might lead to odd
    # completions at some points but will lead to mostly correct type
    # inference, because people tend to define a public name in a module only
    # once.
    return names[-1].parent, path


</t>
<t tx="ekr.20180516071751.477">@compiled.compiled_objects_cache('mixed_cache')
def create(evaluator, obj, parent_context=None, *args):
    tree_node, path = find_syntax_node_name(evaluator, obj)

    compiled_object = compiled.create(
        evaluator, obj, parent_context=parent_context.compiled_object)
    if tree_node is None:
        return compiled_object

    module_node = tree_node.get_root_node()
    if parent_context.tree_node.get_root_node() == module_node:
        module_context = parent_context.get_root_context()
    else:
        from jedi.evaluate.representation import ModuleContext
        module_context = ModuleContext(evaluator, module_node, path=path)
        name = compiled_object.get_root_context().py__name__()
        imports.add_module(evaluator, name, module_context)

    tree_context = module_context.create_context(
        tree_node,
        node_is_context=True,
        node_is_object=True
    )

    return MixedObject(
        evaluator,
        parent_context,
        compiled_object,
        tree_context=tree_context
    )

</t>
<t tx="ekr.20180516071751.478">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/compiled/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.479">"""
Imitate the parser representation.
"""
import inspect
import re
import sys
import os
from functools import partial

from jedi._compatibility import builtins as _builtins, unicode
from jedi import debug
from jedi.cache import underscore_memoization, memoize_method
from jedi.parser.python.tree import Param, Operator
from jedi.evaluate.helpers import FakeName
from jedi.evaluate.filters import AbstractFilter, AbstractNameDefinition, \
    ContextNameMixin
from jedi.evaluate.context import Context, LazyKnownContext
from . import fake


_sep = os.path.sep
if os.path.altsep is not None:
    _sep += os.path.altsep
_path_re = re.compile('(?:\.[^{0}]+|[{0}]__init__\.py)$'.format(re.escape(_sep)))
del _sep


</t>
<t tx="ekr.20180516071751.48">def get_init_path(directory_path):
    """
    The __init__ file can be searched in a directory. If found return it, else
    None.
    """
    for suffix, _, _ in imp.get_suffixes():
        path = os.path.join(directory_path, '__init__' + suffix)
        if os.path.exists(path):
            return path
    return None


</t>
<t tx="ekr.20180516071751.480">class CheckAttribute(object):
    """Raises an AttributeError if the attribute X isn't available."""
    @others
</t>
<t tx="ekr.20180516071751.481">def __init__(self, func):
    self.func = func
    # Remove the py in front of e.g. py__call__.
    self.check_name = func.__name__[2:]

</t>
<t tx="ekr.20180516071751.482">def __get__(self, instance, owner):
    # This might raise an AttributeError. That's wanted.
    if self.check_name == '__iter__':
        # Python iterators are a bit strange, because there's no need for
        # the __iter__ function as long as __getitem__ is defined (it will
        # just start with __getitem__(0). This is especially true for
        # Python 2 strings, where `str.__iter__` is not even defined.
        try:
            iter(instance.obj)
        except TypeError:
            raise AttributeError
    else:
        getattr(instance.obj, self.check_name)
    return partial(self.func, instance)


</t>
<t tx="ekr.20180516071751.483">class CompiledObject(Context):
    path = None  # modules have this attribute - set it to None.
    used_names = {}  # To be consistent with modules.

    @others
</t>
<t tx="ekr.20180516071751.484">def __init__(self, evaluator, obj, parent_context=None, faked_class=None):
    super(CompiledObject, self).__init__(evaluator, parent_context)
    self.obj = obj
    # This attribute will not be set for most classes, except for fakes.
    self.tree_node = faked_class

</t>
<t tx="ekr.20180516071751.485">def get_root_node(self):
    # To make things a bit easier with filters we add this method here.
    return self.get_root_context()

</t>
<t tx="ekr.20180516071751.486">@CheckAttribute
def py__call__(self, params):
    if inspect.isclass(self.obj):
        from jedi.evaluate.instance import CompiledInstance
        return set([CompiledInstance(self.evaluator, self.parent_context, self, params)])
    else:
        return set(self._execute_function(params))

</t>
<t tx="ekr.20180516071751.487">@CheckAttribute
def py__class__(self):
    return create(self.evaluator, self.obj.__class__)

</t>
<t tx="ekr.20180516071751.488">@CheckAttribute
def py__mro__(self):
    return (self,) + tuple(create(self.evaluator, cls) for cls in self.obj.__mro__[1:])

</t>
<t tx="ekr.20180516071751.489">@CheckAttribute
def py__bases__(self):
    return tuple(create(self.evaluator, cls) for cls in self.obj.__bases__)

</t>
<t tx="ekr.20180516071751.49">class ImportName(AbstractNameDefinition):
    start_pos = (1, 0)

    @others
</t>
<t tx="ekr.20180516071751.490">def py__bool__(self):
    return bool(self.obj)

</t>
<t tx="ekr.20180516071751.491">def py__file__(self):
    try:
        return self.obj.__file__
    except AttributeError:
        return None

</t>
<t tx="ekr.20180516071751.492">def is_class(self):
    return inspect.isclass(self.obj)

</t>
<t tx="ekr.20180516071751.493">@property
def doc(self):
    return inspect.getdoc(self.obj) or ''

</t>
<t tx="ekr.20180516071751.494">@property
def get_params(self):
    return []  # TODO Fix me.
    params_str, ret = self._parse_function_doc()
    tokens = params_str.split(',')
    if inspect.ismethoddescriptor(self.obj):
        tokens.insert(0, 'self')
    params = []
    for p in tokens:
        parts = [FakeName(part) for part in p.strip().split('=')]
        if len(parts) &gt; 1:
            parts.insert(1, Operator('=', (0, 0)))
        params.append(Param(parts, self))
    return params

</t>
<t tx="ekr.20180516071751.495">def get_param_names(self):
    params_str, ret = self._parse_function_doc()
    tokens = params_str.split(',')
    if inspect.ismethoddescriptor(self.obj):
        tokens.insert(0, 'self')
    for p in tokens:
        parts = p.strip().split('=')
        if len(parts) &gt; 1:
            parts.insert(1, Operator('=', (0, 0)))
        yield UnresolvableParamName(self, parts[0])

</t>
<t tx="ekr.20180516071751.496">def __repr__(self):
    return '&lt;%s: %s&gt;' % (self.__class__.__name__, repr(self.obj))

</t>
<t tx="ekr.20180516071751.497">@underscore_memoization
def _parse_function_doc(self):
    if self.doc is None:
        return '', ''

    return _parse_function_doc(self.doc)

</t>
<t tx="ekr.20180516071751.498">@property
def api_type(self):
    obj = self.obj
    if inspect.isclass(obj):
        return 'class'
    elif inspect.ismodule(obj):
        return 'module'
    elif inspect.isbuiltin(obj) or inspect.ismethod(obj) \
            or inspect.ismethoddescriptor(obj) or inspect.isfunction(obj):
        return 'function'
    # Everything else...
    return 'instance'

</t>
<t tx="ekr.20180516071751.499">@property
def type(self):
    """Imitate the tree.Node.type values."""
    cls = self._get_class()
    if inspect.isclass(cls):
        return 'classdef'
    elif inspect.ismodule(cls):
        return 'file_input'
    elif inspect.isbuiltin(cls) or inspect.ismethod(cls) or \
            inspect.ismethoddescriptor(cls):
        return 'funcdef'

</t>
<t tx="ekr.20180516071751.5">@debug.increase_indent
def find(self, filters, attribute_lookup):
    """
    :params bool attribute_lookup: Tell to logic if we're accessing the
        attribute or the contents of e.g. a function.
    """
    names = self.filter_name(filters)
    if self._found_predefined_types is not None and names:
        check = flow_analysis.reachability_check(
            self._context, self._context.tree_node, self._name)
        if check is flow_analysis.UNREACHABLE:
            return set()
        return self._found_predefined_types

    types = self._names_to_types(names, attribute_lookup)

    if not names and not types \
            and not (isinstance(self._name, tree.Name) and
                     isinstance(self._name.parent.parent, tree.Param)):
        if isinstance(self._name, tree.Name):
            if attribute_lookup:
                analysis.add_attribute_error(
                    self._name_context, self._context, self._name)
            else:
                message = ("NameError: name '%s' is not defined."
                           % self._string_name)
                analysis.add(self._name_context, 'name-error', self._name, message)

    return types

</t>
<t tx="ekr.20180516071751.50">def __init__(self, parent_context, string_name):
    self.parent_context = parent_context
    self.string_name = string_name

</t>
<t tx="ekr.20180516071751.500">@underscore_memoization
def _cls(self):
    """
    We used to limit the lookups for instantiated objects like list(), but
    this is not the case anymore. Python itself
    """
    # Ensures that a CompiledObject is returned that is not an instance (like list)
    return self

</t>
<t tx="ekr.20180516071751.501">def _get_class(self):
    if not fake.is_class_instance(self.obj) or \
            inspect.ismethoddescriptor(self.obj):  # slots
        return self.obj

    try:
        return self.obj.__class__
    except AttributeError:
        # happens with numpy.core.umath._UFUNC_API (you get it
        # automatically by doing `import numpy`.
        return type

</t>
<t tx="ekr.20180516071751.502">def get_filters(self, search_global=False, is_instance=False,
                until_position=None, origin_scope=None):
    yield self._ensure_one_filter(is_instance)

</t>
<t tx="ekr.20180516071751.503">@memoize_method
def _ensure_one_filter(self, is_instance):
    """
    search_global shouldn't change the fact that there's one dict, this way
    there's only one `object`.
    """
    return CompiledObjectFilter(self.evaluator, self, is_instance)

</t>
<t tx="ekr.20180516071751.504">def get_subscope_by_name(self, name):
    if name in dir(self.obj):
        return CompiledName(self.evaluator, self, name).parent
    else:
        raise KeyError("CompiledObject doesn't have an attribute '%s'." % name)

</t>
<t tx="ekr.20180516071751.505">@CheckAttribute
def py__getitem__(self, index):
    if type(self.obj) not in (str, list, tuple, unicode, bytes, bytearray, dict):
        # Get rid of side effects, we won't call custom `__getitem__`s.
        return set()

    return set([create(self.evaluator, self.obj[index])])

</t>
<t tx="ekr.20180516071751.506">@CheckAttribute
def py__iter__(self):
    if type(self.obj) not in (str, list, tuple, unicode, bytes, bytearray, dict):
        # Get rid of side effects, we won't call custom `__getitem__`s.
        return

    for part in self.obj:
        yield LazyKnownContext(create(self.evaluator, part))

</t>
<t tx="ekr.20180516071751.507">def py__name__(self):
    try:
        return self._get_class().__name__
    except AttributeError:
        return None

</t>
<t tx="ekr.20180516071751.508">@property
def name(self):
    try:
        name = self._get_class().__name__
    except AttributeError:
        name = repr(self.obj)
    return CompiledContextName(self, name)

</t>
<t tx="ekr.20180516071751.509">def _execute_function(self, params):
    if self.type != 'funcdef':
        return

    for name in self._parse_function_doc()[1].split():
        try:
            bltn_obj = getattr(_builtins, name)
        except AttributeError:
            continue
        else:
            if bltn_obj is None:
                # We want to evaluate everything except None.
                # TODO do we?
                continue
            bltn_obj = create(self.evaluator, bltn_obj)
            for result in self.evaluator.execute(bltn_obj, params):
                yield result

</t>
<t tx="ekr.20180516071751.51">def infer(self):
    return Importer(
        self.parent_context.evaluator,
        [self.string_name],
        self.parent_context,
    ).follow()

</t>
<t tx="ekr.20180516071751.510">def is_scope(self):
    return True

</t>
<t tx="ekr.20180516071751.511">def get_self_attributes(self):
    return []  # Instance compatibility

</t>
<t tx="ekr.20180516071751.512">def get_imports(self):
    return []  # Builtins don't have imports


</t>
<t tx="ekr.20180516071751.513">class CompiledName(AbstractNameDefinition):
    @others
</t>
<t tx="ekr.20180516071751.514">def __init__(self, evaluator, parent_context, name):
    self._evaluator = evaluator
    self.parent_context = parent_context
    self.string_name = name

</t>
<t tx="ekr.20180516071751.515">def __repr__(self):
    try:
        name = self.parent_context.name  # __name__ is not defined all the time
    except AttributeError:
        name = None
    return '&lt;%s: (%s).%s&gt;' % (self.__class__.__name__, name, self.string_name)

</t>
<t tx="ekr.20180516071751.516">@property
def api_type(self):
    return next(iter(self.infer())).api_type

</t>
<t tx="ekr.20180516071751.517">@underscore_memoization
def infer(self):
    module = self.parent_context.get_root_context()
    return [_create_from_name(self._evaluator, module, self.parent_context, self.string_name)]


</t>
<t tx="ekr.20180516071751.518">class UnresolvableParamName(AbstractNameDefinition):
    api_type = 'param'

    @others
</t>
<t tx="ekr.20180516071751.519">def __init__(self, compiled_obj, name):
    self.parent_context = compiled_obj.parent_context
    self.string_name = name

</t>
<t tx="ekr.20180516071751.52">def get_root_context(self):
    # Not sure if this is correct.
    return self.parent_context.get_root_context()

</t>
<t tx="ekr.20180516071751.520">def infer(self):
    return set()


</t>
<t tx="ekr.20180516071751.521">class CompiledContextName(ContextNameMixin, AbstractNameDefinition):
    @others
</t>
<t tx="ekr.20180516071751.522">def __init__(self, context, name):
    self.string_name = name
    self._context = context
    self.parent_context = context.parent_context


</t>
<t tx="ekr.20180516071751.523">class EmptyCompiledName(AbstractNameDefinition):
    """
    Accessing some names will raise an exception. To avoid not having any
    completions, just give Jedi the option to return this object. It infers to
    nothing.
    """
    @others
</t>
<t tx="ekr.20180516071751.524">def __init__(self, evaluator, name):
    self.parent_context = evaluator.BUILTINS
    self.string_name = name

</t>
<t tx="ekr.20180516071751.525">def infer(self):
    return []


</t>
<t tx="ekr.20180516071751.526">class CompiledObjectFilter(AbstractFilter):
    name_class = CompiledName

    @others
</t>
<t tx="ekr.20180516071751.527">def __init__(self, evaluator, compiled_object, is_instance=False):
    self._evaluator = evaluator
    self._compiled_object = compiled_object
    self._is_instance = is_instance

</t>
<t tx="ekr.20180516071751.528">@memoize_method
def get(self, name):
    name = str(name)
    obj = self._compiled_object.obj
    try:
        getattr(obj, name)
        if self._is_instance and name not in dir(obj):
            return []
    except AttributeError:
        return []
    except Exception:
        # This is a bit ugly. We're basically returning this to make
        # lookups possible without having the actual attribute. However
        # this makes proper completion possible.
        return [EmptyCompiledName(self._evaluator, name)]
    return [self._create_name(name)]

</t>
<t tx="ekr.20180516071751.529">def values(self):
    obj = self._compiled_object.obj

    names = []
    for name in dir(obj):
        names += self.get(name)

    is_instance = self._is_instance or fake.is_class_instance(obj)
    # ``dir`` doesn't include the type names.
    if not inspect.ismodule(obj) and (obj is not type) and not is_instance:
        for filter in create(self._evaluator, type).get_filters():
            names += filter.values()
    return names

</t>
<t tx="ekr.20180516071751.53">@property
def api_type(self):
    return 'module'


</t>
<t tx="ekr.20180516071751.530">def _create_name(self, name):
    return self.name_class(self._evaluator, self._compiled_object, name)


</t>
<t tx="ekr.20180516071751.531">def dotted_from_fs_path(fs_path, sys_path):
    """
    Changes `/usr/lib/python3.4/email/utils.py` to `email.utils`.  I.e.
    compares the path with sys.path and then returns the dotted_path. If the
    path is not in the sys.path, just returns None.
    """
    if os.path.basename(fs_path).startswith('__init__.'):
        # We are calculating the path. __init__ files are not interesting.
        fs_path = os.path.dirname(fs_path)

    # prefer
    #   - UNIX
    #     /path/to/pythonX.Y/lib-dynload
    #     /path/to/pythonX.Y/site-packages
    #   - Windows
    #     C:\path\to\DLLs
    #     C:\path\to\Lib\site-packages
    # over
    #   - UNIX
    #     /path/to/pythonX.Y
    #   - Windows
    #     C:\path\to\Lib
    path = ''
    for s in sys_path:
        if (fs_path.startswith(s) and len(path) &lt; len(s)):
            path = s

    # - Window
    # X:\path\to\lib-dynload/datetime.pyd =&gt; datetime
    module_path = fs_path[len(path):].lstrip(os.path.sep).lstrip('/')
    # - Window
    # Replace like X:\path\to\something/foo/bar.py
    return _path_re.sub('', module_path).replace(os.path.sep, '.').replace('/', '.')


</t>
<t tx="ekr.20180516071751.532">def load_module(evaluator, path=None, name=None):
    sys_path = evaluator.sys_path
    if path is not None:
        dotted_path = dotted_from_fs_path(path, sys_path=sys_path)
    else:
        dotted_path = name

    if dotted_path is None:
        p, _, dotted_path = path.partition(os.path.sep)
        sys_path.insert(0, p)

    temp, sys.path = sys.path, sys_path
    try:
        __import__(dotted_path)
    except RuntimeError:
        if 'PySide' in dotted_path or 'PyQt' in dotted_path:
            # RuntimeError: the PyQt4.QtCore and PyQt5.QtCore modules both wrap
            # the QObject class.
            # See https://github.com/davidhalter/jedi/pull/483
            return None
        raise
    except ImportError:
        # If a module is "corrupt" or not really a Python module or whatever.
        debug.warning('Module %s not importable in path %s.', dotted_path, path)
        return None
    finally:
        sys.path = temp

    # Just access the cache after import, because of #59 as well as the very
    # complicated import structure of Python.
    module = sys.modules[dotted_path]

    return create(evaluator, module)


docstr_defaults = {
    'floating point number': 'float',
    'character': 'str',
    'integer': 'int',
    'dictionary': 'dict',
    'string': 'str',
}


</t>
<t tx="ekr.20180516071751.533">def _parse_function_doc(doc):
    """
    Takes a function and returns the params and return value as a tuple.
    This is nothing more than a docstring parser.

    TODO docstrings like utime(path, (atime, mtime)) and a(b [, b]) -&gt; None
    TODO docstrings like 'tuple of integers'
    """
    # parse round parentheses: def func(a, (b,c))
    try:
        count = 0
        start = doc.index('(')
        for i, s in enumerate(doc[start:]):
            if s == '(':
                count += 1
            elif s == ')':
                count -= 1
            if count == 0:
                end = start + i
                break
        param_str = doc[start + 1:end]
    except (ValueError, UnboundLocalError):
        # ValueError for doc.index
        # UnboundLocalError for undefined end in last line
        debug.dbg('no brackets found - no param')
        end = 0
        param_str = ''
    else:
        # remove square brackets, that show an optional param ( = None)
        def change_options(m):
            args = m.group(1).split(',')
            for i, a in enumerate(args):
                if a and '=' not in a:
                    args[i] += '=None'
            return ','.join(args)

        while True:
            param_str, changes = re.subn(r' ?\[([^\[\]]+)\]',
                                         change_options, param_str)
            if changes == 0:
                break
    param_str = param_str.replace('-', '_')  # see: isinstance.__doc__

    # parse return value
    r = re.search('-[&gt;-]* ', doc[end:end + 7])
    if r is None:
        ret = ''
    else:
        index = end + r.end()
        # get result type, which can contain newlines
        pattern = re.compile(r'(,\n|[^\n-])+')
        ret_str = pattern.match(doc, index).group(0).strip()
        # New object -&gt; object()
        ret_str = re.sub(r'[nN]ew (.*)', r'\1()', ret_str)

        ret = docstr_defaults.get(ret_str, ret_str)

    return param_str, ret


</t>
<t tx="ekr.20180516071751.534">def _create_from_name(evaluator, module, compiled_object, name):
    obj = compiled_object.obj
    faked = None
    try:
        faked = fake.get_faked(evaluator, module, obj, parent_context=compiled_object, name=name)
        if faked.type == 'funcdef':
            from jedi.evaluate.representation import FunctionContext
            return FunctionContext(evaluator, compiled_object, faked)
    except fake.FakeDoesNotExist:
        pass

    try:
        obj = getattr(obj, name)
    except AttributeError:
        # Happens e.g. in properties of
        # PyQt4.QtGui.QStyleOptionComboBox.currentText
        # -&gt; just set it to None
        obj = None
    return create(evaluator, obj, parent_context=compiled_object, faked=faked)


</t>
<t tx="ekr.20180516071751.535">def builtin_from_name(evaluator, string):
    bltn_obj = getattr(_builtins, string)
    return create(evaluator, bltn_obj)


</t>
<t tx="ekr.20180516071751.536">def _a_generator(foo):
    """Used to have an object to return for generators."""
    yield 42
    yield foo


_SPECIAL_OBJECTS = {
    'FUNCTION_CLASS': type(load_module),
    'METHOD_CLASS': type(CompiledObject.is_class),
    'MODULE_CLASS': type(os),
    'GENERATOR_OBJECT': _a_generator(1.0),
    'BUILTINS': _builtins,
}


</t>
<t tx="ekr.20180516071751.537">def get_special_object(evaluator, identifier):
    obj = _SPECIAL_OBJECTS[identifier]
    return create(evaluator, obj, parent_context=create(evaluator, _builtins))


</t>
<t tx="ekr.20180516071751.538">def compiled_objects_cache(attribute_name):
    def decorator(func):
        """
        This decorator caches just the ids, oopposed to caching the object itself.
        Caching the id has the advantage that an object doesn't need to be
        hashable.
        """
        def wrapper(evaluator, obj, parent_context=None, module=None, faked=None):
            cache = getattr(evaluator, attribute_name)
            # Do a very cheap form of caching here.
            key = id(obj), id(parent_context)
            try:
                return cache[key][0]
            except KeyError:
                # TODO this whole decorator is way too ugly
                result = func(evaluator, obj, parent_context, module, faked)
                # Need to cache all of them, otherwise the id could be overwritten.
                cache[key] = result, obj, parent_context, module, faked
                return result
        return wrapper

    return decorator


</t>
<t tx="ekr.20180516071751.539">@compiled_objects_cache('compiled_cache')
def create(evaluator, obj, parent_context=None, module=None, faked=None):
    """
    A very weird interface class to this module. The more options provided the
    more acurate loading compiled objects is.
    """
    if inspect.ismodule(obj):
        if parent_context is not None:
            # Modules don't have parents, be careful with caching: recurse.
            return create(evaluator, obj)
    else:
        if parent_context is None and obj != _builtins:
            return create(evaluator, obj, create(evaluator, _builtins))

        try:
            faked = fake.get_faked(evaluator, module, obj, parent_context=parent_context)
            if faked.type == 'funcdef':
                from jedi.evaluate.representation import FunctionContext
                return FunctionContext(evaluator, parent_context, faked)
        except fake.FakeDoesNotExist:
            pass

    return CompiledObject(evaluator, obj, parent_context, faked)
</t>
<t tx="ekr.20180516071751.54">class SubModuleName(ImportName):
    @others
</t>
<t tx="ekr.20180516071751.542"></t>
<t tx="ekr.20180516071751.543">@path C:/Anaconda3/Lib/site-packages/jedi/parser/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.544">import time
import os
import sys
import hashlib
import gc
import shutil
import pickle
import platform
import errno

from jedi import settings
from jedi import debug
from jedi._compatibility import FileNotFoundError


_PICKLE_VERSION = 30
"""
Version number (integer) for file system cache.

Increment this number when there are any incompatible changes in
the parser tree classes.  For example, the following changes
are regarded as incompatible.

- A class name is changed.
- A class is moved to another module.
- A __slot__ of a class is changed.
"""

_VERSION_TAG = '%s-%s%s-%s' % (
    platform.python_implementation(),
    sys.version_info[0],
    sys.version_info[1],
    _PICKLE_VERSION
)
"""
Short name for distinguish Python implementations and versions.

It's like `sys.implementation.cache_tag` but for Python &lt; 3.3
we generate something similar.  See:
http://docs.python.org/3/library/sys.html#sys.implementation
"""

# for fast_parser, should not be deleted
parser_cache = {}



</t>
<t tx="ekr.20180516071751.545">class _NodeCacheItem(object):
    @others
</t>
<t tx="ekr.20180516071751.546">def __init__(self, node, lines, change_time=None):
    self.node = node
    self.lines = lines
    if change_time is None:
        change_time = time.time()
    self.change_time = change_time


</t>
<t tx="ekr.20180516071751.547">def load_module(grammar, path):
    """
    Returns a module or None, if it fails.
    """
    try:
        p_time = os.path.getmtime(path)
    except FileNotFoundError:
        return None

    try:
        # TODO Add grammar sha256
        module_cache_item = parser_cache[path]
        if p_time &lt;= module_cache_item.change_time:
            return module_cache_item.node
    except KeyError:
        if not settings.use_filesystem_cache:
            return None

        return _load_from_file_system(grammar, path, p_time)


</t>
<t tx="ekr.20180516071751.548">def _load_from_file_system(grammar, path, p_time):
    cache_path = _get_hashed_path(grammar, path)
    try:
        try:
            if p_time &gt; os.path.getmtime(cache_path):
                # Cache is outdated
                return None
        except OSError as e:
            if e.errno == errno.ENOENT:
                # In Python 2 instead of an IOError here we get an OSError.
                raise FileNotFoundError
            else:
                raise

        with open(cache_path, 'rb') as f:
            gc.disable()
            try:
                module_cache_item = pickle.load(f)
            finally:
                gc.enable()
    except FileNotFoundError:
        return None
    else:
        parser_cache[path] = module_cache_item
        debug.dbg('pickle loaded: %s', path)
        return module_cache_item.node


</t>
<t tx="ekr.20180516071751.549">def save_module(grammar, path, module, lines, pickling=True):
    try:
        p_time = None if path is None else os.path.getmtime(path)
    except OSError:
        p_time = None
        pickling = False

    item = _NodeCacheItem(module, lines, p_time)
    parser_cache[path] = item
    if settings.use_filesystem_cache and pickling and path is not None:
        _save_to_file_system(grammar, path, item)


</t>
<t tx="ekr.20180516071751.55">def infer(self):
    return Importer(
        self.parent_context.evaluator,
        [self.string_name],
        self.parent_context,
        level=1
    ).follow()


</t>
<t tx="ekr.20180516071751.550">def _save_to_file_system(grammar, path, item):
    with open(_get_hashed_path(grammar, path), 'wb') as f:
        pickle.dump(item, f, pickle.HIGHEST_PROTOCOL)


</t>
<t tx="ekr.20180516071751.551">def remove_old_modules(self):
    """
    # TODO Might want to use such a function to clean up the cache (if it's
    # too old). We could potentially also scan for old files in the
    # directory and delete those.
    """


</t>
<t tx="ekr.20180516071751.552">def clear_cache(self):
    shutil.rmtree(settings.cache_directory)
    parser_cache.clear()


</t>
<t tx="ekr.20180516071751.553">def _get_hashed_path(grammar, path):
    file_hash = hashlib.sha256(path.encode("utf-8")).hexdigest()
    directory = _get_cache_directory_path()
    return os.path.join(directory, '%s-%s.pkl' % (grammar.sha256, file_hash))


</t>
<t tx="ekr.20180516071751.554">def _get_cache_directory_path():
    directory = os.path.join(settings.cache_directory, _VERSION_TAG)
    if not os.path.exists(directory):
        os.makedirs(directory)
    return directory
</t>
<t tx="ekr.20180516071751.555">@path C:/Anaconda3/Lib/site-packages/jedi/parser/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.556">"""
The ``Parser`` tries to convert the available Python code in an easy to read
format, something like an abstract syntax tree. The classes who represent this
tree, are sitting in the :mod:`jedi.parser.tree` module.

The Python module ``tokenize`` is a very important part in the ``Parser``,
because it splits the code into different words (tokens).  Sometimes it looks a
bit messy. Sorry for that! You might ask now: "Why didn't you use the ``ast``
module for this? Well, ``ast`` does a very good job understanding proper Python
code, but fails to work as soon as there's a single line of broken code.

There's one important optimization that needs to be known: Statements are not
being parsed completely. ``Statement`` is just a representation of the tokens
within the statement. This lowers memory usage and cpu time and reduces the
complexity of the ``Parser`` (there's another parser sitting inside
``Statement``, which produces ``Array`` and ``Call``).
"""
from jedi.parser import tree
from jedi.parser.pgen2.parse import PgenParser


</t>
<t tx="ekr.20180516071751.557">class ParserSyntaxError(Exception):
    """
    Contains error information about the parser tree.

    May be raised as an exception.
    """
    @others
</t>
<t tx="ekr.20180516071751.558">def __init__(self, message, position):
    self.message = message
    self.position = position


</t>
<t tx="ekr.20180516071751.559">class BaseParser(object):
    node_map = {}
    default_node = tree.Node

    leaf_map = {
    }
    default_leaf = tree.Leaf

    @others
</t>
<t tx="ekr.20180516071751.56">class Importer(object):
    @others
</t>
<t tx="ekr.20180516071751.560">def __init__(self, grammar, start_symbol='file_input', error_recovery=False):
    self._grammar = grammar
    self._start_symbol = start_symbol
    self._error_recovery = error_recovery

</t>
<t tx="ekr.20180516071751.561">def parse(self, tokens):
    start_number = self._grammar.symbol2number[self._start_symbol]
    self.pgen_parser = PgenParser(
        self._grammar, self.convert_node, self.convert_leaf,
        self.error_recovery, start_number
    )

    node = self.pgen_parser.parse(tokens)
    # The stack is empty now, we don't need it anymore.
    del self.pgen_parser
    return node

</t>
<t tx="ekr.20180516071751.562">def error_recovery(self, grammar, stack, arcs, typ, value, start_pos, prefix,
                   add_token_callback):
    if self._error_recovery:
        raise NotImplementedError("Error Recovery is not implemented")
    else:
        raise ParserSyntaxError('SyntaxError: invalid syntax', start_pos)

</t>
<t tx="ekr.20180516071751.563">def convert_node(self, grammar, type_, children):
    # TODO REMOVE symbol, we don't want type here.
    symbol = grammar.number2symbol[type_]
    try:
        return self.node_map[symbol](children)
    except KeyError:
        return self.default_node(symbol, children)

</t>
<t tx="ekr.20180516071751.564">def convert_leaf(self, grammar, type_, value, prefix, start_pos):
    try:
        return self.leaf_map[type_](value, start_pos, prefix)
    except KeyError:
        return self.default_leaf(value, start_pos, prefix)
</t>
<t tx="ekr.20180516071751.565">@path C:/Anaconda3/Lib/site-packages/jedi/parser/
@others
@language python
@tabwidth -4

opmap = {}
for line in opmap_raw.splitlines():
    op, name = line.split()
    opmap[op] = globals()[name]
</t>
<t tx="ekr.20180516071751.566">from __future__ import absolute_import

from jedi._compatibility import is_py3, is_py35
from token import *


COMMENT = N_TOKENS
tok_name[COMMENT] = 'COMMENT'
N_TOKENS += 1

NL = N_TOKENS
tok_name[NL] = 'NL'
N_TOKENS += 1

if is_py3:
    BACKQUOTE = N_TOKENS
    tok_name[BACKQUOTE] = 'BACKQUOTE'
    N_TOKENS += 1
else:
    RARROW = N_TOKENS
    tok_name[RARROW] = 'RARROW'
    N_TOKENS += 1
    ELLIPSIS = N_TOKENS
    tok_name[ELLIPSIS] = 'ELLIPSIS'
    N_TOKENS += 1

if not is_py35:
    ATEQUAL = N_TOKENS
    tok_name[ATEQUAL] = 'ATEQUAL'
    N_TOKENS += 1



# Map from operator to number (since tokenize doesn't do this)

opmap_raw = """\
( LPAR
) RPAR
[ LSQB
] RSQB
: COLON
, COMMA
; SEMI
+ PLUS
- MINUS
* STAR
/ SLASH
| VBAR
&amp; AMPER
&lt; LESS
&gt; GREATER
= EQUAL
. DOT
% PERCENT
` BACKQUOTE
{ LBRACE
} RBRACE
@ AT
== EQEQUAL
!= NOTEQUAL
&lt;&gt; NOTEQUAL
&lt;= LESSEQUAL
&gt;= GREATEREQUAL
~ TILDE
^ CIRCUMFLEX
&lt;&lt; LEFTSHIFT
&gt;&gt; RIGHTSHIFT
** DOUBLESTAR
+= PLUSEQUAL
-= MINEQUAL
*= STAREQUAL
/= SLASHEQUAL
%= PERCENTEQUAL
&amp;= AMPEREQUAL
|= VBAREQUAL
@= ATEQUAL
^= CIRCUMFLEXEQUAL
&lt;&lt;= LEFTSHIFTEQUAL
&gt;&gt;= RIGHTSHIFTEQUAL
**= DOUBLESTAREQUAL
// DOUBLESLASH
//= DOUBLESLASHEQUAL
-&gt; RARROW
... ELLIPSIS
"""
</t>
<t tx="ekr.20180516071751.567">@path C:/Anaconda3/Lib/site-packages/jedi/parser/
# -*- coding: utf-8 -*-
@others
if __name__ == "__main__":
    import sys
    if len(sys.argv) &gt;= 2:
        path = sys.argv[1]
        with open(path) as f:
            code = u(f.read())
    else:
        code = u(sys.stdin.read())
    for token in source_tokens(code, use_exact_op_types=True):
        print(token)
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.568">"""
This tokenizer has been copied from the ``tokenize.py`` standard library
tokenizer. The reason was simple: The standard library tokenizer fails
if the indentation is not right. The fast parser of jedi however requires
"wrong" indentation.

Basically this is a stripped down version of the standard library module, so
you can read the documentation there. Additionally we included some speed and
memory optimizations here.
"""
from __future__ import absolute_import

import string
import re
from collections import namedtuple
import itertools as _itertools

from jedi.parser.token import (tok_name, N_TOKENS, ENDMARKER, STRING, NUMBER, opmap,
                               NAME, OP, ERRORTOKEN, NEWLINE, INDENT, DEDENT)
from jedi._compatibility import is_py3, py_version, u
from jedi.common import splitlines


cookie_re = re.compile("coding[:=]\s*([-\w.]+)")


if is_py3:
    # Python 3 has str.isidentifier() to check if a char is a valid identifier
    is_identifier = str.isidentifier
else:
    namechars = string.ascii_letters + '_'
    is_identifier = lambda s: s in namechars


COMMENT = N_TOKENS
tok_name[COMMENT] = 'COMMENT'


</t>
<t tx="ekr.20180516071751.569">def group(*choices, **kwargs):
    capture = kwargs.pop('capture', False)  # Python 2, arrghhhhh :(
    assert not kwargs

    start = '('
    if not capture:
        start += '?:'
    return start + '|'.join(choices) + ')'

</t>
<t tx="ekr.20180516071751.57">def __init__(self, evaluator, import_path, module_context, level=0):
    """
    An implementation similar to ``__import__``. Use `follow`
    to actually follow the imports.

    *level* specifies whether to use absolute or relative imports. 0 (the
    default) means only perform absolute imports. Positive values for level
    indicate the number of parent directories to search relative to the
    directory of the module calling ``__import__()`` (see PEP 328 for the
    details).

    :param import_path: List of namespaces (strings or Names).
    """
    debug.speed('import %s' % (import_path,))
    self._evaluator = evaluator
    self.level = level
    self.module_context = module_context
    try:
        self.file_path = module_context.py__file__()
    except AttributeError:
        # Can be None for certain compiled modules like 'builtins'.
        self.file_path = None

    if level:
        base = module_context.py__package__().split('.')
        if base == ['']:
            base = []
        if level &gt; len(base):
            path = module_context.py__file__()
            if path is not None:
                import_path = list(import_path)
                p = path
                for i in range(level):
                    p = os.path.dirname(p)
                dir_name = os.path.basename(p)
                # This is not the proper way to do relative imports. However, since
                # Jedi cannot be sure about the entry point, we just calculate an
                # absolute path here.
                if dir_name:
                    # TODO those sys.modules modifications are getting
                    # really stupid. this is the 3rd time that we're using
                    # this. We should probably refactor.
                    if path.endswith(os.path.sep + 'os.py'):
                        import_path.insert(0, 'os')
                    else:
                        import_path.insert(0, dir_name)
                else:
                    _add_error(module_context, import_path[-1])
                    import_path = []
                    # TODO add import error.
                    debug.warning('Attempted relative import beyond top-level package.')
        else:
            # Here we basically rewrite the level to 0.
            import_path = tuple(base) + tuple(import_path)
    self.import_path = import_path

</t>
<t tx="ekr.20180516071751.570">def any(*choices):
    return group(*choices) + '*'

</t>
<t tx="ekr.20180516071751.571">def maybe(*choices):
    return group(*choices) + '?'

# Note: we use unicode matching for names ("\w") but ascii matching for
# number literals.
Whitespace = r'[ \f\t]*'
Comment = r'#[^\r\n]*'
Name = r'\w+'

if py_version &gt;= 36:
    Hexnumber = r'0[xX](?:_?[0-9a-fA-F])+'
    Binnumber = r'0[bB](?:_?[01])+'
    Octnumber = r'0[oO](?:_?[0-7])+'
    Decnumber = r'(?:0(?:_?0)*|[1-9](?:_?[0-9])*)'
    Intnumber = group(Hexnumber, Binnumber, Octnumber, Decnumber)
    Exponent = r'[eE][-+]?[0-9](?:_?[0-9])*'
    Pointfloat = group(r'[0-9](?:_?[0-9])*\.(?:[0-9](?:_?[0-9])*)?',
                       r'\.[0-9](?:_?[0-9])*') + maybe(Exponent)
    Expfloat = r'[0-9](?:_?[0-9])*' + Exponent
    Floatnumber = group(Pointfloat, Expfloat)
    Imagnumber = group(r'[0-9](?:_?[0-9])*[jJ]', Floatnumber + r'[jJ]')
else:
    Hexnumber = r'0[xX][0-9a-fA-F]+'
    Binnumber = r'0[bB][01]+'
    if is_py3:
        Octnumber = r'0[oO][0-7]+'
    else:
        Octnumber = '0[0-7]+'
    Decnumber = r'(?:0+|[1-9][0-9]*)'
    Intnumber = group(Hexnumber, Binnumber, Octnumber, Decnumber)
    Exponent = r'[eE][-+]?[0-9]+'
    Pointfloat = group(r'[0-9]+\.[0-9]*', r'\.[0-9]+') + maybe(Exponent)
    Expfloat = r'[0-9]+' + Exponent
    Floatnumber = group(Pointfloat, Expfloat)
    Imagnumber = group(r'[0-9]+[jJ]', Floatnumber + r'[jJ]')
Number = group(Imagnumber, Floatnumber, Intnumber)

# Return the empty string, plus all of the valid string prefixes.
</t>
<t tx="ekr.20180516071751.572">def _all_string_prefixes():
    # The valid string prefixes. Only contain the lower case versions,
    #  and don't contain any permuations (include 'fr', but not
    #  'rf'). The various permutations will be generated.
    _valid_string_prefixes = ['b', 'r', 'u', 'br']
    if py_version &gt;= 36:
        _valid_string_prefixes += ['f', 'fr']
    if py_version &lt;= 27:
        # TODO this is actually not 100% valid. ur is valid in Python 2.7,
        # while ru is not.
        _valid_string_prefixes.append('ur')

    # if we add binary f-strings, add: ['fb', 'fbr']
    result = set([''])
    for prefix in _valid_string_prefixes:
        for t in _itertools.permutations(prefix):
            # create a list with upper and lower versions of each
            #  character
            for u in _itertools.product(*[(c, c.upper()) for c in t]):
                result.add(''.join(u))
    return result

</t>
<t tx="ekr.20180516071751.573">def _compile(expr):
    return re.compile(expr, re.UNICODE)

# Note that since _all_string_prefixes includes the empty string,
#  StringPrefix can be the empty string (making it optional).
StringPrefix = group(*_all_string_prefixes())

# Tail end of ' string.
Single = r"[^'\\]*(?:\\.[^'\\]*)*'"
# Tail end of " string.
Double = r'[^"\\]*(?:\\.[^"\\]*)*"'
# Tail end of ''' string.
Single3 = r"[^'\\]*(?:(?:\\.|'(?!''))[^'\\]*)*'''"
# Tail end of """ string.
Double3 = r'[^"\\]*(?:(?:\\.|"(?!""))[^"\\]*)*"""'
Triple = group(StringPrefix + "'''", StringPrefix + '"""')

# Because of leftmost-then-longest match semantics, be sure to put the
# longest operators first (e.g., if = came before ==, == would get
# recognized as two instances of =).
Operator = group(r"\*\*=?", r"&gt;&gt;=?", r"&lt;&lt;=?", r"!=",
                 r"//=?", r"-&gt;",
                 r"[+\-*/%&amp;@|^=&lt;&gt;]=?",
                 r"~")

Bracket = '[][(){}]'
Special = group(r'\r?\n', r'\.\.\.', r'[:;.,@]')
Funny = group(Operator, Bracket, Special)

PlainToken = group(Number, Funny, Name, capture=True)

# First (or only) line of ' or " string.
ContStr = group(StringPrefix + r"'[^\n'\\]*(?:\\.[^\n'\\]*)*" +
                group("'", r'\\\r?\n'),
                StringPrefix + r'"[^\n"\\]*(?:\\.[^\n"\\]*)*' +
                group('"', r'\\\r?\n'))
PseudoExtras = group(r'\\\r?\n|\Z', Comment, Triple)
PseudoToken = group(Whitespace, capture=True) + \
    group(PseudoExtras, Number, Funny, ContStr, Name, capture=True)

# For a given string prefix plus quotes, endpats maps it to a regex
#  to match the remainder of that string. _prefix can be empty, for
#  a normal single or triple quoted string (with no prefix).
endpats = {}
for _prefix in _all_string_prefixes():
    endpats[_prefix + "'"] = _compile(Single)
    endpats[_prefix + '"'] = _compile(Double)
    endpats[_prefix + "'''"] = _compile(Single3)
    endpats[_prefix + '"""'] = _compile(Double3)

# A set of all of the single and triple quoted string prefixes,
#  including the opening quotes.
single_quoted = set()
triple_quoted = set()
for t in _all_string_prefixes():
    for p in (t + '"', t + "'"):
        single_quoted.add(p)
    for p in (t + '"""', t + "'''"):
        triple_quoted.add(p)


# TODO add with?
ALWAYS_BREAK_TOKENS = (';', 'import', 'class', 'def', 'try', 'except',
                       'finally', 'while', 'return')
pseudo_token_compiled = _compile(PseudoToken)


</t>
<t tx="ekr.20180516071751.574">class TokenInfo(namedtuple('Token', ['type', 'string', 'start_pos', 'prefix'])):
    @others
</t>
<t tx="ekr.20180516071751.575">def __repr__(self):
    return ('TokenInfo(type=%s, string=%r, start=%r, prefix=%r)' %
            self._replace(type=self.get_type_name()))

</t>
<t tx="ekr.20180516071751.576">def get_type_name(self, exact=True):
    if exact:
        typ = self.exact_type
    else:
        typ = self.type
    return tok_name[typ]

</t>
<t tx="ekr.20180516071751.577">@property
def exact_type(self):
    if self.type == OP and self.string in opmap:
        return opmap[self.string]
    else:
        return self.type

</t>
<t tx="ekr.20180516071751.578">@property
def end_pos(self):
    lines = splitlines(self.string)
    if len(lines) &gt; 1:
        return self.start_pos[0] + len(lines) - 1, 0
    else:
        return self.start_pos[0], self.start_pos[1] + len(self.string)


</t>
<t tx="ekr.20180516071751.579">def source_tokens(source, use_exact_op_types=False):
    """Generate tokens from a the source code (string)."""
    lines = splitlines(source, keepends=True)
    return generate_tokens(lines, use_exact_op_types)


</t>
<t tx="ekr.20180516071751.58">@property
def str_import_path(self):
    """Returns the import path as pure strings instead of `Name`."""
    return tuple(str(name) for name in self.import_path)

</t>
<t tx="ekr.20180516071751.580">def generate_tokens(lines, use_exact_op_types=False):
    """
    A heavily modified Python standard library tokenizer.

    Additionally to the default information, yields also the prefix of each
    token. This idea comes from lib2to3. The prefix contains all information
    that is irrelevant for the parser like newlines in parentheses or comments.
    """
    paren_level = 0  # count parentheses
    indents = [0]
    max = 0
    numchars = '0123456789'
    contstr = ''
    contline = None
    # We start with a newline. This makes indent at the first position
    # possible. It's not valid Python, but still better than an INDENT in the
    # second line (and not in the first). This makes quite a few things in
    # Jedi's fast parser possible.
    new_line = True
    prefix = ''  # Should never be required, but here for safety
    additional_prefix = ''
    for lnum, line in enumerate(lines, 1):  # loop over lines in stream
        pos, max = 0, len(line)

        if contstr:                                         # continued string
            endmatch = endprog.match(line)
            if endmatch:
                pos = endmatch.end(0)
                yield TokenInfo(STRING, contstr + line[:pos], contstr_start, prefix)
                contstr = ''
                contline = None
            else:
                contstr = contstr + line
                contline = contline + line
                continue

        while pos &lt; max:
            pseudomatch = pseudo_token_compiled.match(line, pos)
            if not pseudomatch:                             # scan for tokens
                txt = line[pos:]
                if txt.endswith('\n'):
                    new_line = True
                yield TokenInfo(ERRORTOKEN, txt, (lnum, pos), prefix)
                break

            prefix = additional_prefix + pseudomatch.group(1)
            additional_prefix = ''
            start, pos = pseudomatch.span(2)
            spos = (lnum, start)
            token = pseudomatch.group(2)
            initial = token[0]

            if new_line and initial not in '\r\n#':
                new_line = False
                if paren_level == 0:
                    i = 0
                    while line[i] == '\f':
                        i += 1
                        start -= 1
                    if start &gt; indents[-1]:
                        yield TokenInfo(INDENT, '', spos, '')
                        indents.append(start)
                    while start &lt; indents[-1]:
                        yield TokenInfo(DEDENT, '', spos, '')
                        indents.pop()

            if (initial in numchars or                      # ordinary number
                    (initial == '.' and token != '.' and token != '...')):
                yield TokenInfo(NUMBER, token, spos, prefix)
            elif initial in '\r\n':
                if not new_line and paren_level == 0:
                    yield TokenInfo(NEWLINE, token, spos, prefix)
                else:
                    additional_prefix = prefix + token
                new_line = True
            elif initial == '#':  # Comments
                assert not token.endswith("\n")
                additional_prefix = prefix + token
            elif token in triple_quoted:
                endprog = endpats[token]
                endmatch = endprog.match(line, pos)
                if endmatch:                                # all on one line
                    pos = endmatch.end(0)
                    token = line[start:pos]
                    yield TokenInfo(STRING, token, spos, prefix)
                else:
                    contstr_start = (lnum, start)           # multiple lines
                    contstr = line[start:]
                    contline = line
                    break
            elif initial in single_quoted or \
                    token[:2] in single_quoted or \
                    token[:3] in single_quoted:
                if token[-1] == '\n':                       # continued string
                    contstr_start = lnum, start
                    endprog = (endpats.get(initial) or endpats.get(token[1])
                               or endpats.get(token[2]))
                    contstr = line[start:]
                    contline = line
                    break
                else:                                       # ordinary string
                    yield TokenInfo(STRING, token, spos, prefix)
            elif is_identifier(initial):                      # ordinary name
                if token in ALWAYS_BREAK_TOKENS:
                    paren_level = 0
                    while True:
                        indent = indents.pop()
                        if indent &gt; start:
                            yield TokenInfo(DEDENT, '', spos, '')
                        else:
                            indents.append(indent)
                            break
                yield TokenInfo(NAME, token, spos, prefix)
            elif initial == '\\' and line[start:] in ('\\\n', '\\\r\n'):  # continued stmt
                additional_prefix += prefix + line[start:]
                break
            else:
                if token in '([{':
                    paren_level += 1
                elif token in ')]}':
                    paren_level -= 1

                try:
                    # This check is needed in any case to check if it's a valid
                    # operator or just some random unicode character.
                    exact_type = opmap[token]
                except KeyError:
                    exact_type = typ = ERRORTOKEN
                if use_exact_op_types:
                    typ = exact_type
                else:
                    typ = OP
                yield TokenInfo(typ, token, spos, prefix)

    if contstr:
        yield TokenInfo(ERRORTOKEN, contstr, contstr_start, prefix)
        if contstr.endswith('\n'):
            new_line = True

    end_pos = lnum, max
    # As the last position we just take the maximally possible position. We
    # remove -1 for the last new line.
    for indent in indents[1:]:
        yield TokenInfo(DEDENT, '', end_pos, '')
    yield TokenInfo(ENDMARKER, '', end_pos, additional_prefix)


</t>
<t tx="ekr.20180516071751.581">@path C:/Anaconda3/Lib/site-packages/jedi/parser/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.582">from jedi._compatibility import utf8_repr, encoding, is_py3


</t>
<t tx="ekr.20180516071751.583">class _NodeOrLeaf(object):
    """
    This is just here to have an isinstance check, which is also used on
    evaluate classes. But since they have sometimes a special type of
    delegation, it is important for those classes to override this method.

    I know that there is a chance to do such things with __instancecheck__, but
    since Python 2.5 doesn't support it, I decided to do it this way.
    """
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071751.584">def get_root_node(self):
    scope = self
    while scope.parent is not None:
        scope = scope.parent
    return scope

</t>
<t tx="ekr.20180516071751.585">def get_next_sibling(self):
    """
    The node immediately following the invocant in their parent's children
    list. If the invocant does not have a next sibling, it is None
    """
    # Can't use index(); we need to test by identity
    for i, child in enumerate(self.parent.children):
        if child is self:
            try:
                return self.parent.children[i + 1]
            except IndexError:
                return None

</t>
<t tx="ekr.20180516071751.586">def get_previous_sibling(self):
    """
    The node/leaf immediately preceding the invocant in their parent's
    children list. If the invocant does not have a previous sibling, it is
    None.
    """
    # Can't use index(); we need to test by identity
    for i, child in enumerate(self.parent.children):
        if child is self:
            if i == 0:
                return None
            return self.parent.children[i - 1]

</t>
<t tx="ekr.20180516071751.587">def get_previous_leaf(self):
    """
    Returns the previous leaf in the parser tree.
    Raises an IndexError if it's the first element.
    """
    node = self
    while True:
        c = node.parent.children
        i = c.index(node)
        if i == 0:
            node = node.parent
            if node.parent is None:
                return None
        else:
            node = c[i - 1]
            break

    while True:
        try:
            node = node.children[-1]
        except AttributeError:  # A Leaf doesn't have children.
            return node

</t>
<t tx="ekr.20180516071751.588">def get_next_leaf(self):
    """
    Returns the previous leaf in the parser tree.
    Raises an IndexError if it's the last element.
    """
    node = self
    while True:
        c = node.parent.children
        i = c.index(node)
        if i == len(c) - 1:
            node = node.parent
            if node.parent is None:
                return None
        else:
            node = c[i + 1]
            break

    while True:
        try:
            node = node.children[0]
        except AttributeError:  # A Leaf doesn't have children.
            return node


</t>
<t tx="ekr.20180516071751.589">class Leaf(_NodeOrLeaf):
    __slots__ = ('value', 'parent', 'line', 'indent', 'prefix')

    @others
</t>
<t tx="ekr.20180516071751.59">def sys_path_with_modifications(self):
    in_path = []
    sys_path_mod = list(sys_path.sys_path_with_modifications(
        self._evaluator,
        self.module_context
    ))
    if self.file_path is not None:
        # If you edit e.g. gunicorn, there will be imports like this:
        # `from gunicorn import something`. But gunicorn is not in the
        # sys.path. Therefore look if gunicorn is a parent directory, #56.
        if self.import_path:  # TODO is this check really needed?
            for path in sys_path.traverse_parents(self.file_path):
                if os.path.basename(path) == self.str_import_path[0]:
                    in_path.append(os.path.dirname(path))

        # Since we know nothing about the call location of the sys.path,
        # it's a possibility that the current directory is the origin of
        # the Python execution.
        sys_path_mod.insert(0, os.path.dirname(self.file_path))

    return in_path + sys_path_mod

</t>
<t tx="ekr.20180516071751.590">def __init__(self, value, start_pos, prefix=''):
    self.value = value
    self.start_pos = start_pos
    self.prefix = prefix
    self.parent = None

</t>
<t tx="ekr.20180516071751.591">@property
def start_pos(self):
    return self.line, self.indent

</t>
<t tx="ekr.20180516071751.592">@start_pos.setter
def start_pos(self, value):
    self.line = value[0]
    self.indent = value[1]

</t>
<t tx="ekr.20180516071751.593">def get_start_pos_of_prefix(self):
    previous_leaf = self.get_previous_leaf()
    if previous_leaf is None:
        return self.line - self.prefix.count('\n'), 0  # It's the first leaf.
    return previous_leaf.end_pos

</t>
<t tx="ekr.20180516071751.594">def move(self, line_offset):
    self.line += line_offset

</t>
<t tx="ekr.20180516071751.595">def get_first_leaf(self):
    return self

</t>
<t tx="ekr.20180516071751.596">def get_last_leaf(self):
    return self

</t>
<t tx="ekr.20180516071751.597">def get_code(self, normalized=False, include_prefix=True):
    if normalized:
        return self.value
    if include_prefix:
        return self.prefix + self.value
    else:
        return self.value

</t>
<t tx="ekr.20180516071751.598">def nodes_to_execute(self, last_added=False):
    return []

</t>
<t tx="ekr.20180516071751.599">@property
def end_pos(self):
    """
    Literals and whitespace end_pos are more complicated than normal
    end_pos, because the containing newlines may change the indexes.
    """
    lines = self.value.split('\n')
    end_pos_line = self.line + len(lines) - 1
    # Check for multiline token
    if self.line == end_pos_line:
        end_pos_indent = self.indent + len(lines[-1])
    else:
        end_pos_indent = len(lines[-1])
    return end_pos_line, end_pos_indent

</t>
<t tx="ekr.20180516071751.6">def _get_origin_scope(self):
    if isinstance(self._name, tree.Name):
        scope = self._name
        while scope.parent is not None:
            # TODO why if classes?
            if not isinstance(scope, tree.Scope):
                break
            scope = scope.parent
        return scope
    else:
        return None

</t>
<t tx="ekr.20180516071751.60">def follow(self):
    if not self.import_path:
        return set()
    return self._do_import(self.import_path, self.sys_path_with_modifications())

</t>
<t tx="ekr.20180516071751.600">@utf8_repr
def __repr__(self):
    return "&lt;%s: %s start=%s&gt;" % (type(self).__name__, self.value, self.start_pos)


</t>
<t tx="ekr.20180516071751.601">class BaseNode(_NodeOrLeaf):
    """
    The super class for all nodes.

    If you create custom nodes, you will probably want to inherit from this
    ``BaseNode``.
    """
    __slots__ = ('children', 'parent')
    type = None

    @others
</t>
<t tx="ekr.20180516071751.602">def __init__(self, children):
    """
    Initialize :class:`BaseNode`.

    :param children: The module in which this Python object locates.
    """
    for c in children:
        c.parent = self
    self.children = children
    self.parent = None

</t>
<t tx="ekr.20180516071751.603">def move(self, line_offset):
    """
    Move the Node's start_pos.
    """
    for c in self.children:
        c.move(line_offset)

</t>
<t tx="ekr.20180516071751.604">@property
def start_pos(self):
    return self.children[0].start_pos

</t>
<t tx="ekr.20180516071751.605">def get_start_pos_of_prefix(self):
    return self.children[0].get_start_pos_of_prefix()

</t>
<t tx="ekr.20180516071751.606">@property
def end_pos(self):
    return self.children[-1].end_pos

</t>
<t tx="ekr.20180516071751.607">def _get_code_for_children(self, children, normalized, include_prefix):
    # TODO implement normalized (depending on context).
    if include_prefix:
        return "".join(c.get_code(normalized) for c in children)
    else:
        first = children[0].get_code(include_prefix=False)
        return first + "".join(c.get_code(normalized) for c in children[1:])

</t>
<t tx="ekr.20180516071751.608">def get_code(self, normalized=False, include_prefix=True):
    return self._get_code_for_children(self.children, normalized, include_prefix)

</t>
<t tx="ekr.20180516071751.609">def get_leaf_for_position(self, position, include_prefixes=False):
    def binary_search(lower, upper):
        if lower == upper:
            element = self.children[lower]
            if not include_prefixes and position &lt; element.start_pos:
                # We're on a prefix.
                return None
            # In case we have prefixes, a leaf always matches
            try:
                return element.get_leaf_for_position(position, include_prefixes)
            except AttributeError:
                return element


        index = int((lower + upper) / 2)
        element = self.children[index]
        if position &lt;= element.end_pos:
            return binary_search(lower, index)
        else:
            return binary_search(index + 1, upper)

    if not ((1, 0) &lt;= position &lt;= self.children[-1].end_pos):
        raise ValueError('Please provide a position that exists within this node.')
    return binary_search(0, len(self.children) - 1)

</t>
<t tx="ekr.20180516071751.61">def _do_import(self, import_path, sys_path):
    """
    This method is very similar to importlib's `_gcd_import`.
    """
    import_parts = [str(i) for i in import_path]

    # Handle "magic" Flask extension imports:
    # ``flask.ext.foo`` is really ``flask_foo`` or ``flaskext.foo``.
    if len(import_path) &gt; 2 and import_parts[:2] == ['flask', 'ext']:
        # New style.
        ipath = ('flask_' + str(import_parts[2]),) + import_path[3:]
        modules = self._do_import(ipath, sys_path)
        if modules:
            return modules
        else:
            # Old style
            return self._do_import(('flaskext',) + import_path[2:], sys_path)

    module_name = '.'.join(import_parts)
    try:
        return set([self._evaluator.modules[module_name]])
    except KeyError:
        pass

    if len(import_path) &gt; 1:
        # This is a recursive way of importing that works great with
        # the module cache.
        bases = self._do_import(import_path[:-1], sys_path)
        if not bases:
            return set()
        # We can take the first element, because only the os special
        # case yields multiple modules, which is not important for
        # further imports.
        parent_module = list(bases)[0]

        # This is a huge exception, we follow a nested import
        # ``os.path``, because it's a very important one in Python
        # that is being achieved by messing with ``sys.modules`` in
        # ``os``.
        if [str(i) for i in import_path] == ['os', 'path']:
            return parent_module.py__getattribute__('path')

        try:
            method = parent_module.py__path__
        except AttributeError:
            # The module is not a package.
            _add_error(self.module_context, import_path[-1])
            return set()
        else:
            paths = method()
            debug.dbg('search_module %s in paths %s', module_name, paths)
            for path in paths:
                # At the moment we are only using one path. So this is
                # not important to be correct.
                try:
                    if not isinstance(path, list):
                        path = [path]
                    module_file, module_path, is_pkg = \
                        find_module(import_parts[-1], path, fullname=module_name)
                    break
                except ImportError:
                    module_path = None
            if module_path is None:
                _add_error(self.module_context, import_path[-1])
                return set()
    else:
        parent_module = None
        try:
            debug.dbg('search_module %s in %s', import_parts[-1], self.file_path)
            # Override the sys.path. It works only good that way.
            # Injecting the path directly into `find_module` did not work.
            sys.path, temp = sys_path, sys.path
            try:
                module_file, module_path, is_pkg = \
                    find_module(import_parts[-1], fullname=module_name)
            finally:
                sys.path = temp
        except ImportError:
            # The module is not a package.
            _add_error(self.module_context, import_path[-1])
            return set()

    code = None
    if is_pkg:
        # In this case, we don't have a file yet. Search for the
        # __init__ file.
        if module_path.endswith(('.zip', '.egg')):
            code = module_file.loader.get_source(module_name)
        else:
            module_path = get_init_path(module_path)
    elif module_file:
        code = module_file.read()
        module_file.close()

    if isinstance(module_path, ImplicitNSInfo):
        from jedi.evaluate.representation import ImplicitNamespaceContext
        fullname, paths = module_path.name, module_path.paths
        module = ImplicitNamespaceContext(self._evaluator, fullname=fullname)
        module.paths = paths
    elif module_file is None and not module_path.endswith(('.py', '.zip', '.egg')):
        module = compiled.load_module(self._evaluator, module_path)
    else:
        module = _load_module(self._evaluator, module_path, code, sys_path, parent_module)

    if module is None:
        # The file might raise an ImportError e.g. and therefore not be
        # importable.
        return set()

    self._evaluator.modules[module_name] = module
    return set([module])

</t>
<t tx="ekr.20180516071751.610">def get_first_leaf(self):
    return self.children[0].get_first_leaf()

</t>
<t tx="ekr.20180516071751.611">def get_last_leaf(self):
    return self.children[-1].get_last_leaf()

</t>
<t tx="ekr.20180516071751.612">def get_following_comment_same_line(self):
    """
    returns (as string) any comment that appears on the same line,
    after the node, including the #
    """
    try:
        if self.type == 'for_stmt':
            whitespace = self.children[5].get_first_leaf().prefix
        elif self.type == 'with_stmt':
            whitespace = self.children[3].get_first_leaf().prefix
        else:
            whitespace = self.get_last_leaf().get_next_leaf().prefix
    except AttributeError:
        return None
    except ValueError:
        # TODO in some particular cases, the tree doesn't seem to be linked
        # correctly
        return None
    if "#" not in whitespace:
        return None
    comment = whitespace[whitespace.index("#"):]
    if "\r" in comment:
        comment = comment[:comment.index("\r")]
    if "\n" in comment:
        comment = comment[:comment.index("\n")]
    return comment

</t>
<t tx="ekr.20180516071751.613">@utf8_repr
def __repr__(self):
    code = self.get_code().replace('\n', ' ').strip()
    if not is_py3:
        code = code.encode(encoding, 'replace')
    return "&lt;%s: %s@%s,%s&gt;" % \
        (type(self).__name__, code, self.start_pos[0], self.start_pos[1])


</t>
<t tx="ekr.20180516071751.614">class Node(BaseNode):
    """Concrete implementation for interior nodes."""
    __slots__ = ('type',)

    _IGNORE_EXECUTE_NODES = set([
        'suite', 'subscriptlist', 'subscript', 'simple_stmt', 'sliceop',
        'testlist_comp', 'dictorsetmaker', 'trailer', 'decorators',
        'decorated', 'arglist', 'argument', 'exprlist', 'testlist',
        'testlist_safe', 'testlist1'
    ])

    @others
</t>
<t tx="ekr.20180516071751.615">def __init__(self, type, children):
    """
    Initializer.

    Takes a type constant (a symbol number &gt;= 256), a sequence of
    child nodes, and an optional context keyword argument.

    As a side effect, the parent pointers of the children are updated.
    """
    super(Node, self).__init__(children)
    self.type = type

</t>
<t tx="ekr.20180516071751.616">def nodes_to_execute(self, last_added=False):
    """
    For static analysis.
    """
    result = []
    if self.type not in Node._IGNORE_EXECUTE_NODES and not last_added:
        result.append(self)
        last_added = True

    for child in self.children:
        result += child.nodes_to_execute(last_added)
    return result

</t>
<t tx="ekr.20180516071751.617">def __repr__(self):
    return "%s(%s, %r)" % (self.__class__.__name__, self.type, self.children)


</t>
<t tx="ekr.20180516071751.618">class ErrorNode(BaseNode):
    """
    TODO doc
    """
    __slots__ = ()
    type = 'error_node'

    @others
</t>
<t tx="ekr.20180516071751.619">def nodes_to_execute(self, last_added=False):
    return []


</t>
<t tx="ekr.20180516071751.62">def _generate_name(self, name, in_module=None):
    # Create a pseudo import to be able to follow them.
    if in_module is None:
        return ImportName(self.module_context, name)
    return SubModuleName(in_module, name)

</t>
<t tx="ekr.20180516071751.620">class ErrorLeaf(Leaf):
    """
    TODO doc
    """
    __slots__ = ('original_type')
    type = 'error_leaf'

    @others
</t>
<t tx="ekr.20180516071751.621">def __init__(self, original_type, value, start_pos, prefix=''):
    super(ErrorLeaf, self).__init__(value, start_pos, prefix)
    self.original_type = original_type

</t>
<t tx="ekr.20180516071751.622">def __repr__(self):
    return "&lt;%s: %s:%s, %s)&gt;" % \
        (type(self).__name__, self.original_type, repr(self.value), self.start_pos)


</t>
<t tx="ekr.20180516071751.623">@path C:/Anaconda3/Lib/site-packages/jedi/parser/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.624">from jedi.parser.parser import ParserSyntaxError
from jedi.parser.pgen2.pgen import generate_grammar
from jedi.parser import python


</t>
<t tx="ekr.20180516071751.625">def parse(grammar, code):
    raise NotImplementedError
    Parser(grammar, code)
</t>
<t tx="ekr.20180516071751.63">def _get_module_names(self, search_path=None, in_module=None):
    """
    Get the names of all modules in the search_path. This means file names
    and not names defined in the files.
    """

    names = []
    # add builtin module names
    if search_path is None and in_module is None:
        names += [self._generate_name(name) for name in sys.builtin_module_names]

    if search_path is None:
        search_path = self.sys_path_with_modifications()
    for module_loader, name, is_pkg in pkgutil.iter_modules(search_path):
        names.append(self._generate_name(name, in_module=in_module))
    return names

</t>
<t tx="ekr.20180516071751.64">def completion_names(self, evaluator, only_modules=False):
    """
    :param only_modules: Indicates wheter it's possible to import a
        definition that is not defined in a module.
    """
    from jedi.evaluate.representation import ModuleContext, ImplicitNamespaceContext
    names = []
    if self.import_path:
        # flask
        if self.str_import_path == ('flask', 'ext'):
            # List Flask extensions like ``flask_foo``
            for mod in self._get_module_names():
                modname = mod.string_name
                if modname.startswith('flask_'):
                    extname = modname[len('flask_'):]
                    names.append(self._generate_name(extname))
            # Now the old style: ``flaskext.foo``
            for dir in self.sys_path_with_modifications():
                flaskext = os.path.join(dir, 'flaskext')
                if os.path.isdir(flaskext):
                    names += self._get_module_names([flaskext])

        for context in self.follow():
            # Non-modules are not completable.
            if context.api_type != 'module':  # not a module
                continue
            # namespace packages
            if isinstance(context, ModuleContext) and context.py__file__().endswith('__init__.py'):
                paths = context.py__path__()
                names += self._get_module_names(paths, in_module=context)

            # implicit namespace packages
            elif isinstance(context, ImplicitNamespaceContext):
                paths = context.paths
                names += self._get_module_names(paths)

            if only_modules:
                # In the case of an import like `from x.` we don't need to
                # add all the variables.
                if ('os',) == self.str_import_path and not self.level:
                    # os.path is a hardcoded exception, because it's a
                    # ``sys.modules`` modification.
                    names.append(self._generate_name('path', context))

                continue

            for filter in context.get_filters(search_global=False):
                names += filter.values()
    else:
        # Empty import path=completion after import
        if not self.level:
            names += self._get_module_names()

        if self.file_path is not None:
            path = os.path.abspath(self.file_path)
            for i in range(self.level - 1):
                path = os.path.dirname(path)
            names += self._get_module_names([path])

    return names


</t>
<t tx="ekr.20180516071751.65">def _load_module(evaluator, path=None, code=None, sys_path=None, parent_module=None):
    if sys_path is None:
        sys_path = evaluator.sys_path

    dotted_path = path and compiled.dotted_from_fs_path(path, sys_path)
    if path is not None and path.endswith(('.py', '.zip', '.egg')) \
            and dotted_path not in settings.auto_import_modules:

        module_node = parse(code=code, path=path, cache=True, diff_cache=True)

        from jedi.evaluate.representation import ModuleContext
        return ModuleContext(evaluator, module_node, path=path)
    else:
        return compiled.load_module(evaluator, path)


</t>
<t tx="ekr.20180516071751.66">def add_module(evaluator, module_name, module):
    if '.' not in module_name:
        # We cannot add paths with dots, because that would collide with
        # the sepatator dots for nested packages. Therefore we return
        # `__main__` in ModuleWrapper.py__name__(), which is similar to
        # Python behavior.
        evaluator.modules[module_name] = module


</t>
<t tx="ekr.20180516071751.67">def get_modules_containing_name(evaluator, modules, name):
    """
    Search a name in the directories of modules.
    """
    from jedi.evaluate import representation as er

    def check_python_file(path):
        try:
            node_cache_item = parser_cache[path]
        except KeyError:
            try:
                return check_fs(path)
            except IOError:
                return None
        else:
            module_node = node_cache_item.node
            return er.ModuleContext(evaluator, module_node, path=path)

    def check_fs(path):
        with open(path, 'rb') as f:
            code = source_to_unicode(f.read())
            if name in code:
                module_name = os.path.basename(path)[:-3]  # Remove `.py`.
                module = _load_module(evaluator, path, code)
                add_module(evaluator, module_name, module)
                return module

    # skip non python modules
    used_mod_paths = set()
    for m in modules:
        try:
            path = m.py__file__()
        except AttributeError:
            pass
        else:
            used_mod_paths.add(path)
        yield m

    if not settings.dynamic_params_for_other_modules:
        return

    paths = set(settings.additional_dynamic_modules)
    for p in used_mod_paths:
        if p is not None:
            # We need abspath, because the seetings paths might not already
            # have been converted to absolute paths.
            d = os.path.dirname(os.path.abspath(p))
            for file_name in os.listdir(d):
                path = os.path.join(d, file_name)
                if path not in used_mod_paths and path not in paths:
                    if file_name.endswith('.py'):
                        paths.add(path)

    # Sort here to make issues less random.
    for p in sorted(paths):
        # make testing easier, sort it - same results on every interpreter
        m = check_python_file(p)
        if m is not None and not isinstance(m, compiled.CompiledObject):
            yield m
</t>
<t tx="ekr.20180516071751.68">@path C:/Anaconda3/Lib/site-packages/jedi/evaluate/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071751.69">from abc import abstractproperty

from jedi._compatibility import is_py3
from jedi.common import unite
from jedi import debug
from jedi.evaluate import compiled
from jedi.evaluate import filters
from jedi.evaluate.context import Context, LazyKnownContext, LazyKnownContexts
from jedi.evaluate.cache import memoize_default
from jedi.cache import memoize_method
from jedi.evaluate import representation as er
from jedi.evaluate.dynamic import search_params
from jedi.evaluate import iterable


</t>
<t tx="ekr.20180516071751.7">def get_filters(self, search_global=False):
    origin_scope = self._get_origin_scope()
    if search_global:
        return get_global_filters(self._evaluator, self._context, self._position, origin_scope)
    else:
        return self._context.get_filters(search_global, self._position, origin_scope=origin_scope)

</t>
<t tx="ekr.20180516071751.70">class AbstractInstanceContext(Context):
    """
    This class is used to evaluate instances.
    """
    api_type = 'instance'

    @others
</t>
<t tx="ekr.20180516071751.71">def __init__(self, evaluator, parent_context, class_context, var_args):
    super(AbstractInstanceContext, self).__init__(evaluator, parent_context)
    # Generated instances are classes that are just generated by self
    # (No var_args) used.
    self.class_context = class_context
    self.var_args = var_args

</t>
<t tx="ekr.20180516071751.72">def is_class(self):
    return False

</t>
<t tx="ekr.20180516071751.73">@property
def py__call__(self):
    names = self.get_function_slot_names('__call__')
    if not names:
        # Means the Instance is not callable.
        raise AttributeError

    def execute(arguments):
        return unite(name.execute(arguments) for name in names)

    return execute

</t>
<t tx="ekr.20180516071751.74">def py__class__(self):
    return self.class_context

</t>
<t tx="ekr.20180516071751.75">def py__bool__(self):
    # Signalize that we don't know about the bool type.
    return None

</t>
<t tx="ekr.20180516071751.76">def get_function_slot_names(self, name):
    # Python classes don't look at the dictionary of the instance when
    # looking up `__call__`. This is something that has to do with Python's
    # internal slot system (note: not __slots__, but C slots).
    for filter in self.get_filters(include_self_names=False):
        names = filter.get(name)
        if names:
            return names
    return []

</t>
<t tx="ekr.20180516071751.77">def execute_function_slots(self, names, *evaluated_args):
    return unite(
        name.execute_evaluated(*evaluated_args)
        for name in names
    )

</t>
<t tx="ekr.20180516071751.78">def py__get__(self, obj):
    # Arguments in __get__ descriptors are obj, class.
    # `method` is the new parent of the array, don't know if that's good.
    names = self.get_function_slot_names('__get__')
    if names:
        if isinstance(obj, AbstractInstanceContext):
            return self.execute_function_slots(names, obj, obj.class_context)
        else:
            none_obj = compiled.create(self.evaluator, None)
            return self.execute_function_slots(names, none_obj, obj)
    else:
        return set([self])

</t>
<t tx="ekr.20180516071751.79">def get_filters(self, search_global=None, until_position=None,
                origin_scope=None, include_self_names=True):
    if include_self_names:
        for cls in self.class_context.py__mro__():
            if isinstance(cls, compiled.CompiledObject):
                if cls.tree_node is not None:
                    # In this case we're talking about a fake object, it
                    # doesn't make sense for normal compiled objects to
                    # search for self variables.
                    yield SelfNameFilter(self.evaluator, self, cls, origin_scope)
            else:
                yield SelfNameFilter(self.evaluator, self, cls, origin_scope)

    for cls in self.class_context.py__mro__():
        if isinstance(cls, compiled.CompiledObject):
            yield CompiledInstanceClassFilter(self.evaluator, self, cls)
        else:
            yield InstanceClassFilter(self.evaluator, self, cls, origin_scope)

</t>
<t tx="ekr.20180516071751.8">def filter_name(self, filters):
    """
    Searches names that are defined in a scope (the different
    ``filters``), until a name fits.
    """
    names = []
    if self._context.predefined_names:
        # TODO is this ok? node might not always be a tree.Name
        node = self._name
        while node is not None and not node.is_scope():
            node = node.parent
            if node.type in ("if_stmt", "for_stmt", "comp_for"):
                try:
                    name_dict = self._context.predefined_names[node]
                    types = name_dict[self._string_name]
                except KeyError:
                    continue
                else:
                    self._found_predefined_types = types
                    break

    for filter in filters:
        names = filter.get(self._name)
        if names:
            break
    debug.dbg('finder.filter_name "%s" in (%s): %s@%s', self._string_name,
              self._context, names, self._position)
    return list(names)

</t>
<t tx="ekr.20180516071751.80">def py__getitem__(self, index):
    try:
        names = self.get_function_slot_names('__getitem__')
    except KeyError:
        debug.warning('No __getitem__, cannot access the array.')
        return set()
    else:
        index_obj = compiled.create(self.evaluator, index)
        return self.execute_function_slots(names, index_obj)

</t>
<t tx="ekr.20180516071751.81">def py__iter__(self):
    iter_slot_names = self.get_function_slot_names('__iter__')
    if not iter_slot_names:
        debug.warning('No __iter__ on %s.' % self)
        return

    for generator in self.execute_function_slots(iter_slot_names):
        if isinstance(generator, AbstractInstanceContext):
            # `__next__` logic.
            name = '__next__' if is_py3 else 'next'
            iter_slot_names = generator.get_function_slot_names(name)
            if iter_slot_names:
                yield LazyKnownContexts(
                    generator.execute_function_slots(iter_slot_names)
                )
            else:
                debug.warning('Instance has no __next__ function in %s.', generator)
        else:
            for lazy_context in generator.py__iter__():
                yield lazy_context

</t>
<t tx="ekr.20180516071751.82">@abstractproperty
def name(self):
    pass

</t>
<t tx="ekr.20180516071751.83">def _create_init_execution(self, class_context, func_node):
    bound_method = BoundMethod(
        self.evaluator, self, class_context, self.parent_context, func_node
    )
    return InstanceFunctionExecution(
        self,
        class_context.parent_context,
        bound_method,
        self.var_args
    )

</t>
<t tx="ekr.20180516071751.84">def create_init_executions(self):
    for name in self.get_function_slot_names('__init__'):
        if isinstance(name, LazyInstanceName):
            yield self._create_init_execution(name.class_context, name.tree_name.parent)

</t>
<t tx="ekr.20180516071751.85">@memoize_default()
def create_instance_context(self, class_context, node):
    if node.parent.type in ('funcdef', 'classdef'):
        node = node.parent
    scope = node.get_parent_scope()
    if scope == class_context.tree_node:
        return class_context
    else:
        parent_context = self.create_instance_context(class_context, scope)
        if scope.type == 'funcdef':
            if scope.name.value == '__init__' and parent_context == class_context:
                return self._create_init_execution(class_context, scope)
            else:
                bound_method = BoundMethod(
                    self.evaluator, self, class_context,
                    self.parent_context, scope
                )
                return bound_method.get_function_execution()
        else:
            raise NotImplementedError
    return class_context

</t>
<t tx="ekr.20180516071751.86">def __repr__(self):
    return "&lt;%s of %s(%s)&gt;" % (self.__class__.__name__, self.class_context,
                               self.var_args)


</t>
<t tx="ekr.20180516071751.87">class CompiledInstance(AbstractInstanceContext):
    @others
</t>
<t tx="ekr.20180516071751.88">def __init__(self, *args, **kwargs):
    super(CompiledInstance, self).__init__(*args, **kwargs)
    # I don't think that dynamic append lookups should happen here. That
    # sounds more like something that should go to py__iter__.
    if self.class_context.name.string_name in ['list', 'set'] \
            and self.parent_context.get_root_context() == self.evaluator.BUILTINS:
        # compare the module path with the builtin name.
        self.var_args = iterable.get_dynamic_array_instance(self)

</t>
<t tx="ekr.20180516071751.89">@property
def name(self):
    return compiled.CompiledContextName(self, self.class_context.name.string_name)

</t>
<t tx="ekr.20180516071751.9">def _check_getattr(self, inst):
    """Checks for both __getattr__ and __getattribute__ methods"""
    # str is important, because it shouldn't be `Name`!
    name = compiled.create(self._evaluator, self._string_name)

    # This is a little bit special. `__getattribute__` is in Python
    # executed before `__getattr__`. But: I know no use case, where
    # this could be practical and where Jedi would return wrong types.
    # If you ever find something, let me know!
    # We are inversing this, because a hand-crafted `__getattribute__`
    # could still call another hand-crafted `__getattr__`, but not the
    # other way around.
    names = (inst.get_function_slot_names('__getattr__') or
             inst.get_function_slot_names('__getattribute__'))
    return inst.execute_function_slots(names, name)

</t>
<t tx="ekr.20180516071751.90">def create_instance_context(self, class_context, node):
    if node.get_parent_scope().type == 'classdef':
        return class_context
    else:
        return super(CompiledInstance, self).create_instance_context(class_context, node)


</t>
<t tx="ekr.20180516071751.91">class TreeInstance(AbstractInstanceContext):
    @others
</t>
<t tx="ekr.20180516071751.92">@property
def name(self):
    return filters.ContextName(self, self.class_context.name.tree_name)


</t>
<t tx="ekr.20180516071751.93">class AnonymousInstance(TreeInstance):
    @others
</t>
<t tx="ekr.20180516071751.94">def __init__(self, evaluator, parent_context, class_context):
    super(AnonymousInstance, self).__init__(
        evaluator,
        parent_context,
        class_context,
        var_args=None
    )


</t>
<t tx="ekr.20180516071751.95">class CompiledInstanceName(compiled.CompiledName):
    @others
</t>
<t tx="ekr.20180516071751.96">def __init__(self, evaluator, instance, parent_context, name):
    super(CompiledInstanceName, self).__init__(evaluator, parent_context, name)
    self._instance = instance

</t>
<t tx="ekr.20180516071751.97">def infer(self):
    for result_context in super(CompiledInstanceName, self).infer():
        if isinstance(result_context, er.FunctionContext):
            parent_context = result_context.parent_context
            while parent_context.is_class():
                parent_context = parent_context.parent_context

            yield BoundMethod(
                result_context.evaluator, self._instance, self.parent_context,
                parent_context, result_context.tree_node
            )
        else:
            if result_context.api_type == 'function':
                yield CompiledBoundMethod(result_context)
            else:
                yield result_context


</t>
<t tx="ekr.20180516071751.98">class CompiledInstanceClassFilter(compiled.CompiledObjectFilter):
    name_class = CompiledInstanceName

    @others
</t>
<t tx="ekr.20180516071751.99">def __init__(self, evaluator, instance, compiled_object):
    super(CompiledInstanceClassFilter, self).__init__(
        evaluator,
        compiled_object,
        is_instance=True,
    )
    self._instance = instance

</t>
<t tx="ekr.20180516071752.10">def report(self):
    """Dump the grammar tables to standard output, for debugging."""
    from pprint import pprint
    print("s2n")
    pprint(self.symbol2number)
    print("n2s")
    pprint(self.number2symbol)
    print("states")
    pprint(self.states)
    print("dfas")
    pprint(self.dfas)
    print("labels")
    pprint(self.labels)
    print("start", self.start)
</t>
<t tx="ekr.20180516071752.100">def convert_leaf(self, grammar, type, value, prefix, start_pos):
    # print('leaf', repr(value), token.tok_name[type])
    if type == tokenize.NAME:
        if value in grammar.keywords:
            return tree.Keyword(value, start_pos, prefix)
        else:
            return tree.Name(value, start_pos, prefix)
    elif type == STRING:
        return tree.String(value, start_pos, prefix)
    elif type == NUMBER:
        return tree.Number(value, start_pos, prefix)
    elif type == NEWLINE:
        return tree.Newline(value, start_pos, prefix)
    elif type == ENDMARKER:
        return tree.EndMarker(value, start_pos, prefix)
    else:
        return tree.Operator(value, start_pos, prefix)

</t>
<t tx="ekr.20180516071752.101">def error_recovery(self, grammar, stack, arcs, typ, value, start_pos, prefix,
                   add_token_callback):
    """
    This parser is written in a dynamic way, meaning that this parser
    allows using different grammars (even non-Python). However, error
    recovery is purely written for Python.
    """
    if not self._error_recovery:
        return super(Parser, self).error_recovery(
            grammar, stack, arcs, typ, value, start_pos, prefix,
            add_token_callback)

    def current_suite(stack):
        # For now just discard everything that is not a suite or
        # file_input, if we detect an error.
        for index, (dfa, state, (type_, nodes)) in reversed(list(enumerate(stack))):
            # `suite` can sometimes be only simple_stmt, not stmt.
            symbol = grammar.number2symbol[type_]
            if symbol == 'file_input':
                break
            elif symbol == 'suite' and len(nodes) &gt; 1:
                # suites without an indent in them get discarded.
                break
            elif symbol == 'simple_stmt' and len(nodes) &gt; 1:
                # simple_stmt can just be turned into a PythonNode, if
                # there are enough statements. Ignore the rest after that.
                break
        return index, symbol, nodes

    index, symbol, nodes = current_suite(stack)
    if symbol == 'simple_stmt':
        index -= 2
        (_, _, (type_, suite_nodes)) = stack[index]
        symbol = grammar.number2symbol[type_]
        suite_nodes.append(tree.PythonNode(symbol, list(nodes)))
        # Remove
        nodes[:] = []
        nodes = suite_nodes
        stack[index]

    # print('err', token.tok_name[typ], repr(value), start_pos, len(stack), index)
    if self._stack_removal(grammar, stack, arcs, index + 1, value, start_pos):
        add_token_callback(typ, value, start_pos, prefix)
    else:
        if typ == INDENT:
            # For every deleted INDENT we have to delete a DEDENT as well.
            # Otherwise the parser will get into trouble and DEDENT too early.
            self._omit_dedent_list.append(self._indent_counter)
        else:
            error_leaf = tree.PythonErrorLeaf(tok_name[typ].lower(), value, start_pos, prefix)
            stack[-1][2][1].append(error_leaf)

</t>
<t tx="ekr.20180516071752.102">def _stack_removal(self, grammar, stack, arcs, start_index, value, start_pos):
    failed_stack = []
    found = False
    all_nodes = []
    for dfa, state, (typ, nodes) in stack[start_index:]:
        if nodes:
            found = True
        if found:
            symbol = grammar.number2symbol[typ]
            failed_stack.append((symbol, nodes))
            all_nodes += nodes
    if failed_stack:
        stack[start_index - 1][2][1].append(tree.PythonErrorNode(all_nodes))

    stack[start_index:] = []
    return failed_stack

</t>
<t tx="ekr.20180516071752.103">def _recovery_tokenize(self, tokens):
    for typ, value, start_pos, prefix in tokens:
        # print(tokenize.tok_name[typ], repr(value), start_pos, repr(prefix))
        if typ == DEDENT:
            # We need to count indents, because if we just omit any DEDENT,
            # we might omit them in the wrong place.
            o = self._omit_dedent_list
            if o and o[-1] == self._indent_counter:
                o.pop()
                continue

            self._indent_counter -= 1
        elif typ == INDENT:
            self._indent_counter += 1

        yield typ, value, start_pos, prefix


</t>
<t tx="ekr.20180516071752.104">def _remove_last_newline(node):
    endmarker = node.children[-1]
    # The newline is either in the endmarker as a prefix or the previous
    # leaf as a newline token.
    prefix = endmarker.prefix
    leaf = endmarker.get_previous_leaf()
    if prefix:
        text = prefix
    else:
        if leaf is None:
            raise ValueError("You're trying to remove a newline from an empty module.")

        text = leaf.value

    if not text.endswith('\n'):
        raise ValueError("There's no newline at the end, cannot remove it.")

    text = text[:-1]
    if prefix:
        endmarker.prefix = text

        if leaf is None:
            end_pos = (1, 0)
        else:
            end_pos = leaf.end_pos

        lines = splitlines(text, keepends=True)
        if len(lines) == 1:
            end_pos = end_pos[0], end_pos[1] + len(lines[0])
        else:
            end_pos = end_pos[0] + len(lines) - 1,  len(lines[-1])
        endmarker.start_pos = end_pos
    else:
        leaf.value = text
        endmarker.start_pos = leaf.end_pos
</t>
<t tx="ekr.20180516071752.105">@path C:/Anaconda3/Lib/site-packages/jedi/parser/python/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071752.106">"""
If you know what an syntax tree is, you'll see that this module is pretty much
that. The classes represent syntax elements like functions and imports.

This is the "business logic" part of the parser. There's a lot of logic here
that makes it easier for Jedi (and other libraries) to deal with a Python syntax
tree.

By using `get_code` on a module, you can get back the 1-to-1 representation of
the input given to the parser. This is important if you are using refactoring.

The easiest way to play with this module is to use :class:`parsing.Parser`.
:attr:`parsing.Parser.module` holds an instance of :class:`Module`:

&gt;&gt;&gt; from jedi.parser.python import parse
&gt;&gt;&gt; parser = parse('import os')
&gt;&gt;&gt; module = parser.get_root_node()
&gt;&gt;&gt; module
&lt;Module: @1-1&gt;

Any subclasses of :class:`Scope`, including :class:`Module` has an attribute
:attr:`imports &lt;Scope.imports&gt;`:

&gt;&gt;&gt; module.imports
[&lt;ImportName: import os@1,0&gt;]

See also :attr:`Scope.subscopes` and :attr:`Scope.statements`.

For static analysis purposes there exists a method called
``nodes_to_execute`` on all nodes and leaves. It's documented in the static
anaylsis documentation.
"""

from inspect import cleandoc
from itertools import chain
import textwrap
import abc

from jedi._compatibility import (Python3Method, is_py3, utf8_repr,
                                 literal_eval, unicode)
from jedi.parser.tree import Node, BaseNode, Leaf, ErrorNode, ErrorLeaf


</t>
<t tx="ekr.20180516071752.107">def _safe_literal_eval(value):
    first_two = value[:2].lower()
    if first_two[0] == 'f' or first_two in ('fr', 'rf'):
        # literal_eval is not able to resovle f literals. We have to do that
        # manually in a later stage
        return ''

    try:
        return literal_eval(value)
    except SyntaxError:
        # It's possible to create syntax errors with literals like rb'' in
        # Python 2. This should not be possible and in that case just return an
        # empty string.
        # Before Python 3.3 there was a more strict definition in which order
        # you could define literals.
        return ''


</t>
<t tx="ekr.20180516071752.108">def search_ancestor(node, node_type_or_types):
    if not isinstance(node_type_or_types, (list, tuple)):
        node_type_or_types = (node_type_or_types,)

    while True:
        node = node.parent
        if node is None or node.type in node_type_or_types:
            return node


</t>
<t tx="ekr.20180516071752.109">class DocstringMixin(object):
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.11">@path C:/Anaconda3/Lib/site-packages/jedi/parser/pgen2/
# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
# Licensed to PSF under a Contributor Agreement.

# Modifications:
# Copyright 2014 David Halter. Integration into Jedi.
# Modifications are dual-licensed: MIT and PSF.

@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071752.110">@property
def raw_doc(self):
    """ Returns a cleaned version of the docstring token. """
    if self.type == 'file_input':
        node = self.children[0]
    elif isinstance(self, ClassOrFunc):
        node = self.children[self.children.index(':') + 1]
        if node.type == 'suite':  # Normally a suite
            node = node.children[1]  # -&gt; NEWLINE stmt
    else:  # ExprStmt
        simple_stmt = self.parent
        c = simple_stmt.parent.children
        index = c.index(simple_stmt)
        if not index:
            return ''
        node = c[index - 1]

    if node.type == 'simple_stmt':
        node = node.children[0]

    if node.type == 'string':
        # TODO We have to check next leaves until there are no new
        # leaves anymore that might be part of the docstring. A
        # docstring can also look like this: ``'foo' 'bar'
        # Returns a literal cleaned version of the ``Token``.
        cleaned = cleandoc(_safe_literal_eval(node.value))
        # Since we want the docstr output to be always unicode, just
        # force it.
        if is_py3 or isinstance(cleaned, unicode):
            return cleaned
        else:
            return unicode(cleaned, 'UTF-8', 'replace')
    return ''


</t>
<t tx="ekr.20180516071752.111">class PythonMixin():
    @others
</t>
<t tx="ekr.20180516071752.112">def get_parent_scope(self, include_flows=False):
    """
    Returns the underlying scope.
    """
    scope = self.parent
    while scope is not None:
        if include_flows and isinstance(scope, Flow):
            return scope
        if scope.is_scope():
            break
        scope = scope.parent
    return scope

</t>
<t tx="ekr.20180516071752.113">def get_definition(self):
    if self.type in ('newline', 'endmarker'):
        raise ValueError('Cannot get the indentation of whitespace or indentation.')
    scope = self
    while scope.parent is not None:
        parent = scope.parent
        if isinstance(scope, (PythonNode, PythonLeaf)) and parent.type != 'simple_stmt':
            if scope.type == 'testlist_comp':
                try:
                    if scope.children[1].type == 'comp_for':
                        return scope.children[1]
                except IndexError:
                    pass
            scope = parent
        else:
            break
    return scope

</t>
<t tx="ekr.20180516071752.114">def is_scope(self):
    # Default is not being a scope. Just inherit from Scope.
    return False

</t>
<t tx="ekr.20180516071752.115">@abc.abstractmethod
def nodes_to_execute(self, last_added=False):
    raise NotImplementedError()

</t>
<t tx="ekr.20180516071752.116">@Python3Method
def name_for_position(self, position):
    for c in self.children:
        if isinstance(c, Leaf):
            if isinstance(c, Name) and c.start_pos &lt;= position &lt;= c.end_pos:
                return c
        else:
            result = c.name_for_position(position)
            if result is not None:
                return result
    return None

</t>
<t tx="ekr.20180516071752.117">@Python3Method
def get_statement_for_position(self, pos):
    for c in self.children:
        if c.start_pos &lt;= pos &lt;= c.end_pos:
            if c.type not in ('decorated', 'simple_stmt', 'suite') \
                    and not isinstance(c, (Flow, ClassOrFunc)):
                return c
            else:
                try:
                    return c.get_statement_for_position(pos)
                except AttributeError:
                    pass  # Must be a non-scope
    return None


</t>
<t tx="ekr.20180516071752.118">class PythonLeaf(Leaf, PythonMixin):
    __slots__ = ()


</t>
<t tx="ekr.20180516071752.119">class _LeafWithoutNewlines(PythonLeaf):
    """
    Simply here to optimize performance.
    """
    __slots__ = ()

    @others
# Python base classes
</t>
<t tx="ekr.20180516071752.12">"""
Parser engine for the grammar tables generated by pgen.

The grammar table must be loaded first.

See Parser/parser.c in the Python distribution for additional info on
how this parsing engine works.
"""

# Local imports
from jedi.parser import tokenize


</t>
<t tx="ekr.20180516071752.120">@property
def end_pos(self):
    return self.line, self.indent + len(self.value)


</t>
<t tx="ekr.20180516071752.121">class PythonBaseNode(BaseNode, PythonMixin):
    __slots__ = ()


</t>
<t tx="ekr.20180516071752.122">class PythonNode(Node, PythonMixin):
    __slots__ = ()


</t>
<t tx="ekr.20180516071752.123">class PythonErrorNode(ErrorNode, PythonMixin):
    __slots__ = ()


</t>
<t tx="ekr.20180516071752.124">class PythonErrorLeaf(ErrorLeaf, PythonMixin):
    __slots__ = ()


</t>
<t tx="ekr.20180516071752.125">class EndMarker(_LeafWithoutNewlines):
    __slots__ = ()
    type = 'endmarker'


</t>
<t tx="ekr.20180516071752.126">class Newline(PythonLeaf):
    """Contains NEWLINE and ENDMARKER tokens."""
    __slots__ = ()
    type = 'newline'

    @others
</t>
<t tx="ekr.20180516071752.127">@utf8_repr
def __repr__(self):
    return "&lt;%s: %s&gt;" % (type(self).__name__, repr(self.value))


</t>
<t tx="ekr.20180516071752.128">class Name(_LeafWithoutNewlines):
    """
    A string. Sometimes it is important to know if the string belongs to a name
    or not.
    """
    type = 'name'
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.129">def __str__(self):
    return self.value

</t>
<t tx="ekr.20180516071752.13">class InternalParseError(Exception):
    """
    Exception to signal the parser is stuck and error recovery didn't help.
    Basically this shouldn't happen. It's a sign that something is really
    wrong.
    """

    @others
</t>
<t tx="ekr.20180516071752.130">def __unicode__(self):
    return self.value

</t>
<t tx="ekr.20180516071752.131">def __repr__(self):
    return "&lt;%s: %s@%s,%s&gt;" % (type(self).__name__, self.value,
                               self.line, self.indent)

</t>
<t tx="ekr.20180516071752.132">def is_definition(self):
    if self.parent.type in ('power', 'atom_expr'):
        # In `self.x = 3` self is not a definition, but x is.
        return False

    stmt = self.get_definition()
    if stmt.type in ('funcdef', 'classdef', 'param'):
        return self == stmt.name
    elif stmt.type == 'for_stmt':
        return self.start_pos &lt; stmt.children[2].start_pos
    elif stmt.type == 'try_stmt':
        return self.get_previous_sibling() == 'as'
    else:
        return stmt.type in ('expr_stmt', 'import_name', 'import_from',
                             'comp_for', 'with_stmt') \
            and self in stmt.get_defined_names()

</t>
<t tx="ekr.20180516071752.133">def nodes_to_execute(self, last_added=False):
    if last_added is False:
        yield self


</t>
<t tx="ekr.20180516071752.134">class Literal(PythonLeaf):
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.135">def eval(self):
    return _safe_literal_eval(self.value)


</t>
<t tx="ekr.20180516071752.136">class Number(Literal):
    type = 'number'
    __slots__ = ()


</t>
<t tx="ekr.20180516071752.137">class String(Literal):
    type = 'string'
    __slots__ = ()


</t>
<t tx="ekr.20180516071752.138">class Operator(_LeafWithoutNewlines):
    type = 'operator'
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.139">def __str__(self):
    return self.value

</t>
<t tx="ekr.20180516071752.14">def __init__(self, msg, type, value, start_pos):
    Exception.__init__(self, "%s: type=%r, value=%r, start_pos=%r" %
                       (msg, tokenize.tok_name[type], value, start_pos))
    self.msg = msg
    self.type = type
    self.value = value
    self.start_pos = start_pos


</t>
<t tx="ekr.20180516071752.140">def __eq__(self, other):
    """
    Make comparisons with strings easy.
    Improves the readability of the parser.
    """
    if isinstance(other, Operator):
        return self is other
    else:
        return self.value == other

</t>
<t tx="ekr.20180516071752.141">def __ne__(self, other):
    """Python 2 compatibility."""
    return self.value != other

</t>
<t tx="ekr.20180516071752.142">def __hash__(self):
    return hash(self.value)


</t>
<t tx="ekr.20180516071752.143">class Keyword(_LeafWithoutNewlines):
    type = 'keyword'
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.144">def __eq__(self, other):
    """
    Make comparisons with strings easy.
    Improves the readability of the parser.
    """
    if isinstance(other, Keyword):
        return self is other
    return self.value == other

</t>
<t tx="ekr.20180516071752.145">def __ne__(self, other):
    """Python 2 compatibility."""
    return not self.__eq__(other)

</t>
<t tx="ekr.20180516071752.146">def __hash__(self):
    return hash(self.value)


</t>
<t tx="ekr.20180516071752.147">class Scope(PythonBaseNode, DocstringMixin):
    """
    Super class for the parser tree, which represents the state of a python
    text file.
    A Scope manages and owns its subscopes, which are classes and functions, as
    well as variables and imports. It is used to access the structure of python
    files.

    :param start_pos: The position (line and column) of the scope.
    :type start_pos: tuple(int, int)
    """
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.148">def __init__(self, children):
    super(Scope, self).__init__(children)

</t>
<t tx="ekr.20180516071752.149">@property
def returns(self):
    # Needed here for fast_parser, because the fast_parser splits and
    # returns will be in "normal" modules.
    return self._search_in_scope(ReturnStmt)

</t>
<t tx="ekr.20180516071752.15">def token_to_ilabel(grammar, type_, value):
    # Map from token to label
    if type_ == tokenize.NAME:
        # Check for reserved words (keywords)
        try:
            return grammar.keywords[value]
        except KeyError:
            pass

    try:
        return grammar.tokens[type_]
    except KeyError:
        return None


</t>
<t tx="ekr.20180516071752.150">@property
def subscopes(self):
    return self._search_in_scope(Scope)

</t>
<t tx="ekr.20180516071752.151">@property
def flows(self):
    return self._search_in_scope(Flow)

</t>
<t tx="ekr.20180516071752.152">@property
def imports(self):
    return self._search_in_scope(Import)

</t>
<t tx="ekr.20180516071752.153">@Python3Method
def _search_in_scope(self, typ):
    def scan(children):
        elements = []
        for element in children:
            if isinstance(element, typ):
                elements.append(element)
            if element.type in ('suite', 'simple_stmt', 'decorated') \
                    or isinstance(element, Flow):
                elements += scan(element.children)
        return elements

    return scan(self.children)

</t>
<t tx="ekr.20180516071752.154">@property
def statements(self):
    return self._search_in_scope((ExprStmt, KeywordStatement))

</t>
<t tx="ekr.20180516071752.155">def is_scope(self):
    return True

</t>
<t tx="ekr.20180516071752.156">def __repr__(self):
    try:
        name = self.name
    except AttributeError:
        name = ''

    return "&lt;%s: %s@%s-%s&gt;" % (type(self).__name__, name,
                               self.start_pos[0], self.end_pos[0])

</t>
<t tx="ekr.20180516071752.157">def walk(self):
    yield self
    for s in self.subscopes:
        for scope in s.walk():
            yield scope

    for r in self.statements:
        while isinstance(r, Flow):
            for scope in r.walk():
                yield scope
            r = r.next


</t>
<t tx="ekr.20180516071752.158">class Module(Scope):
    """
    The top scope, which is always a module.
    Depending on the underlying parser this may be a full module or just a part
    of a module.
    """
    __slots__ = ('_used_names',)
    type = 'file_input'

    @others
</t>
<t tx="ekr.20180516071752.159">def __init__(self, children):
    super(Module, self).__init__(children)
    self._used_names = None

</t>
<t tx="ekr.20180516071752.16">class PgenParser(object):
    """Parser engine.

    The proper usage sequence is:

    p = Parser(grammar, [converter])  # create instance
    p.setup([start])                  # prepare for parsing
    &lt;for each input token&gt;:
        if p.addtoken(...):           # parse a token
            break
    root = p.rootnode                 # root of abstract syntax tree

    A Parser instance may be reused by calling setup() repeatedly.

    A Parser instance contains state pertaining to the current token
    sequence, and should not be used concurrently by different threads
    to parse separate token sequences.

    See driver.py for how to get input tokens by tokenizing a file or
    string.

    Parsing is complete when addtoken() returns True; the root of the
    abstract syntax tree can then be retrieved from the rootnode
    instance variable.  When a syntax error occurs, error_recovery()
    is called. There is no error recovery; the parser cannot be used
    after a syntax error was reported (but it can be reinitialized by
    calling setup()).

    """

    @others
</t>
<t tx="ekr.20180516071752.160">@property
def has_explicit_absolute_import(self):
    """
    Checks if imports in this module are explicitly absolute, i.e. there
    is a ``__future__`` import.
    """
    # TODO this is a strange scan and not fully correct. I think Python's
    # parser does it in a different way and scans for the first
    # statement/import with a tokenizer (to check for syntax changes like
    # the future print statement).
    for imp in self.imports:
        if imp.type == 'import_from' and imp.level == 0:
            for path in imp.paths():
                if [str(name) for name in path] == ['__future__', 'absolute_import']:
                    return True
    return False

</t>
<t tx="ekr.20180516071752.161">def nodes_to_execute(self, last_added=False):
    # Yield itself, class needs to be executed for decorator checks.
    result = []
    for child in self.children:
        result += child.nodes_to_execute()
    return result

</t>
<t tx="ekr.20180516071752.162">@property
def used_names(self):
    if self._used_names is None:
        # Don't directly use self._used_names to eliminate a lookup.
        dct = {}

        def recurse(node):
            try:
                children = node.children
            except AttributeError:
                if node.type == 'name':
                    arr = dct.setdefault(node.value, [])
                    arr.append(node)
            else:
                for child in children:
                    recurse(child)

        recurse(self)
        self._used_names = dct
    return self._used_names


</t>
<t tx="ekr.20180516071752.163">class Decorator(PythonBaseNode):
    type = 'decorator'
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.164">def nodes_to_execute(self, last_added=False):
    if self.children[-2] == ')':
        node = self.children[-3]
        if node != '(':
            return node.nodes_to_execute()
    return []


</t>
<t tx="ekr.20180516071752.165">class ClassOrFunc(Scope):
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.166">@property
def name(self):
    return self.children[1]

</t>
<t tx="ekr.20180516071752.167">def get_decorators(self):
    decorated = self.parent
    if decorated.type == 'decorated':
        if decorated.children[0].type == 'decorators':
            return decorated.children[0].children
        else:
            return decorated.children[:1]
    else:
        return []


</t>
<t tx="ekr.20180516071752.168">class Class(ClassOrFunc):
    """
    Used to store the parsed contents of a python class.

    :param name: The Class name.
    :type name: str
    :param supers: The super classes of a Class.
    :type supers: list
    :param start_pos: The start position (line, column) of the class.
    :type start_pos: tuple(int, int)
    """
    type = 'classdef'
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.169">def __init__(self, children):
    super(Class, self).__init__(children)

</t>
<t tx="ekr.20180516071752.17">def __init__(self, grammar, convert_node, convert_leaf, error_recovery, start):
    """Constructor.

    The grammar argument is a grammar.Grammar instance; see the
    grammar module for more information.

    The parser is not ready yet for parsing; you must call the
    setup() method to get it started.

    The optional convert argument is a function mapping concrete
    syntax tree nodes to abstract syntax tree nodes.  If not
    given, no conversion is done and the syntax tree produced is
    the concrete syntax tree.  If given, it must be a function of
    two arguments, the first being the grammar (a grammar.Grammar
    instance), and the second being the concrete syntax tree node
    to be converted.  The syntax tree is converted from the bottom
    up.

    A concrete syntax tree node is a (type, nodes) tuple, where
    type is the node type (a token or symbol number) and nodes
    is a list of children for symbols, and None for tokens.

    An abstract syntax tree node may be anything; this is entirely
    up to the converter function.

    """
    self.grammar = grammar
    self.convert_node = convert_node
    self.convert_leaf = convert_leaf

    # Each stack entry is a tuple: (dfa, state, node).
    # A node is a tuple: (type, children),
    # where children is a list of nodes or None
    newnode = (start, [])
    stackentry = (self.grammar.dfas[start], 0, newnode)
    self.stack = [stackentry]
    self.rootnode = None
    self.error_recovery = error_recovery

</t>
<t tx="ekr.20180516071752.170">def get_super_arglist(self):
    if self.children[2] != '(':  # Has no parentheses
        return None
    else:
        if self.children[3] == ')':  # Empty parentheses
            return None
        else:
            return self.children[3]

</t>
<t tx="ekr.20180516071752.171">@property
def doc(self):
    """
    Return a document string including call signature of __init__.
    """
    docstr = self.raw_doc
    for sub in self.subscopes:
        if str(sub.name) == '__init__':
            return '%s\n\n%s' % (
                sub.get_call_signature(func_name=self.name), docstr)
    return docstr

</t>
<t tx="ekr.20180516071752.172">def nodes_to_execute(self, last_added=False):
    # Yield itself, class needs to be executed for decorator checks.
    yield self
    # Super arguments.
    arglist = self.get_super_arglist()
    try:
        children = arglist.children
    except AttributeError:
        if arglist is not None:
            for node_to_execute in arglist.nodes_to_execute():
                yield node_to_execute
    else:
        for argument in children:
            if argument.type == 'argument':
                # metaclass= or list comprehension or */**
                raise NotImplementedError('Metaclasses not implemented')
            else:
                for node_to_execute in argument.nodes_to_execute():
                    yield node_to_execute

    # care for the class suite:
    for node in self.children[self.children.index(':'):]:
        # This could be easier without the fast parser. But we need to find
        # the position of the colon, because everything after it can be a
        # part of the class, not just its suite.
        for node_to_execute in node.nodes_to_execute():
            yield node_to_execute


</t>
<t tx="ekr.20180516071752.173">def _create_params(parent, argslist_list):
    """
    `argslist_list` is a list that can contain an argslist as a first item, but
    most not. It's basically the items between the parameter brackets (which is
    at most one item).
    This function modifies the parser structure. It generates `Param` objects
    from the normal ast. Those param objects do not exist in a normal ast, but
    make the evaluation of the ast tree so much easier.
    You could also say that this function replaces the argslist node with a
    list of Param objects.
    """
    def check_python2_nested_param(node):
        """
        Python 2 allows params to look like ``def x(a, (b, c))``, which is
        basically a way of unpacking tuples in params. Python 3 has ditched
        this behavior. Jedi currently just ignores those constructs.
        """
        return node.type == 'tfpdef' and node.children[0] == '('

    try:
        first = argslist_list[0]
    except IndexError:
        return []

    if first.type in ('name', 'tfpdef'):
        if check_python2_nested_param(first):
            return [first]
        else:
            return [Param([first], parent)]
    elif first == '*':
        return [first]
    else:  # argslist is a `typedargslist` or a `varargslist`.
        children = first.children
        new_children = []
        start = 0
        # Start with offset 1, because the end is higher.
        for end, child in enumerate(children + [None], 1):
            if child is None or child == ',':
                param_children = children[start:end]
                if param_children:  # Could as well be comma and then end.
                    if check_python2_nested_param(param_children[0]):
                        new_children += param_children
                    elif param_children[0] == '*' and param_children[1] == ',':
                        new_children += param_children
                    else:
                        new_children.append(Param(param_children, parent))
                    start = end
        return new_children


</t>
<t tx="ekr.20180516071752.174">class Function(ClassOrFunc):
    """
    Used to store the parsed contents of a python function.

    Children:
      0) &lt;Keyword: def&gt;
      1) &lt;Name&gt;
      2) parameter list (including open-paren and close-paren &lt;Operator&gt;s)
      3 or 5) &lt;Operator: :&gt;
      4 or 6) Node() representing function body
      3) -&gt; (if annotation is also present)
      4) annotation (if present)
    """
    type = 'funcdef'

    @others
</t>
<t tx="ekr.20180516071752.175">def __init__(self, children):
    super(Function, self).__init__(children)
    parameters = self.children[2]  # After `def foo`
    parameters.children[1:-1] = _create_params(parameters, parameters.children[1:-1])

</t>
<t tx="ekr.20180516071752.176">@property
def params(self):
    return [p for p in self.children[2].children if p.type == 'param']

</t>
<t tx="ekr.20180516071752.177">@property
def name(self):
    return self.children[1]  # First token after `def`

</t>
<t tx="ekr.20180516071752.178">@property
def yields(self):
    # TODO This is incorrect, yields are also possible in a statement.
    return self._search_in_scope(YieldExpr)

</t>
<t tx="ekr.20180516071752.179">def is_generator(self):
    return bool(self.yields)

</t>
<t tx="ekr.20180516071752.18">def parse(self, tokens):
    for type_, value, start_pos, prefix in tokens:
        if self.addtoken(type_, value, start_pos, prefix):
            break
    else:
        # We never broke out -- EOF is too soon -- Unfinished statement.
        # However, the error recovery might have added the token again, if
        # the stack is empty, we're fine.
        if self.stack:
            raise InternalParseError("incomplete input", type_, value, start_pos)
    return self.rootnode

</t>
<t tx="ekr.20180516071752.180">def annotation(self):
    try:
        if self.children[3] == "-&gt;":
            return self.children[4]
        assert self.children[3] == ":"
        return None
    except IndexError:
        return None

</t>
<t tx="ekr.20180516071752.181">def get_call_signature(self, width=72, func_name=None):
    """
    Generate call signature of this function.

    :param width: Fold lines if a line is longer than this value.
    :type width: int
    :arg func_name: Override function name when given.
    :type func_name: str

    :rtype: str
    """
    func_name = func_name or self.name
    code = unicode(func_name) + self._get_paramlist_code()
    return '\n'.join(textwrap.wrap(code, width))

</t>
<t tx="ekr.20180516071752.182">def _get_paramlist_code(self):
    return self.children[2].get_code()

</t>
<t tx="ekr.20180516071752.183">@property
def doc(self):
    """ Return a document string including call signature. """
    docstr = self.raw_doc
    return '%s\n\n%s' % (self.get_call_signature(), docstr)

</t>
<t tx="ekr.20180516071752.184">def nodes_to_execute(self, last_added=False):
    # Yield itself, functions needs to be executed for decorator checks.
    yield self
    for param in self.params:
        if param.default is not None:
            yield param.default
    # care for the function suite:
    for node in self.children[4:]:
        # This could be easier without the fast parser. The fast parser
        # allows that the 4th position is empty or that there's even a
        # fifth element (another function/class). So just scan everything
        # after colon.
        for node_to_execute in node.nodes_to_execute():
            yield node_to_execute


</t>
<t tx="ekr.20180516071752.185">class Lambda(Function):
    """
    Lambdas are basically trimmed functions, so give it the same interface.

    Children:
       0) &lt;Keyword: lambda&gt;
       *) &lt;Param x&gt; for each argument x
      -2) &lt;Operator: :&gt;
      -1) Node() representing body
    """
    type = 'lambda'
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.186">def __init__(self, children):
    # We don't want to call the Function constructor, call its parent.
    super(Function, self).__init__(children)
    lst = self.children[1:-2]  # Everything between `lambda` and the `:` operator is a parameter.
    self.children[1:-2] = _create_params(self, lst)

</t>
<t tx="ekr.20180516071752.187">@property
def name(self):
    # Borrow the position of the &lt;Keyword: lambda&gt; AST node.
    return Name('&lt;lambda&gt;', self.children[0].start_pos)

</t>
<t tx="ekr.20180516071752.188">def _get_paramlist_code(self):
    return '(' + ''.join(param.get_code() for param in self.params).strip() + ')'

</t>
<t tx="ekr.20180516071752.189">@property
def params(self):
    return self.children[1:-2]

</t>
<t tx="ekr.20180516071752.19">def addtoken(self, type_, value, start_pos, prefix):
    """Add a token; return True if this is the end of the program."""
    ilabel = token_to_ilabel(self.grammar, type_, value)

    # Loop until the token is shifted; may raise exceptions
    _gram = self.grammar
    _labels = _gram.labels
    _push = self._push
    _pop = self._pop
    _shift = self._shift
    while True:
        dfa, state, node = self.stack[-1]
        states, first = dfa
        arcs = states[state]
        # Look for a state with this label
        for i, newstate in arcs:
            t, v = _labels[i]
            if ilabel == i:
                # Look it up in the list of labels
                assert t &lt; 256
                # Shift a token; we're done with it
                _shift(type_, value, newstate, prefix, start_pos)
                # Pop while we are in an accept-only state
                state = newstate
                while states[state] == [(0, state)]:
                    _pop()
                    if not self.stack:
                        # Done parsing!
                        return True
                    dfa, state, node = self.stack[-1]
                    states, first = dfa
                # Done with this token
                return False
            elif t &gt;= 256:
                # See if it's a symbol and if we're in its first set
                itsdfa = _gram.dfas[t]
                itsstates, itsfirst = itsdfa
                if ilabel in itsfirst:
                    # Push a symbol
                    _push(t, itsdfa, newstate)
                    break  # To continue the outer while loop
        else:
            if (0, state) in arcs:
                # An accepting state, pop it and try something else
                _pop()
                if not self.stack:
                    # Done parsing, but another token is input
                    raise InternalParseError("too much input", type_, value, start_pos)
            else:
                self.error_recovery(self.grammar, self.stack, arcs, type_,
                                    value, start_pos, prefix, self.addtoken)
                break

</t>
<t tx="ekr.20180516071752.190">def is_generator(self):
    return False

</t>
<t tx="ekr.20180516071752.191">def annotation(self):
    # lambda functions do not support annotations
    return None

</t>
<t tx="ekr.20180516071752.192">@property
def yields(self):
    return []

</t>
<t tx="ekr.20180516071752.193">def nodes_to_execute(self, last_added=False):
    for param in self.params:
        if param.default is not None:
            yield param.default
    # Care for the lambda test (last child):
    for node_to_execute in self.children[-1].nodes_to_execute():
        yield node_to_execute

</t>
<t tx="ekr.20180516071752.194">def __repr__(self):
    return "&lt;%s@%s&gt;" % (self.__class__.__name__, self.start_pos)


</t>
<t tx="ekr.20180516071752.195">class Flow(PythonBaseNode):
    __slots__ = ()
    FLOW_KEYWORDS = (
        'try', 'except', 'finally', 'else', 'if', 'elif', 'with', 'for', 'while'
    )

    @others
</t>
<t tx="ekr.20180516071752.196">def nodes_to_execute(self, last_added=False):
    for child in self.children:
        for node_to_execute in child.nodes_to_execute():
            yield node_to_execute

</t>
<t tx="ekr.20180516071752.197">def get_branch_keyword(self, node):
    start_pos = node.start_pos
    if not (self.start_pos &lt; start_pos &lt;= self.end_pos):
        raise ValueError('The node is not part of the flow.')

    keyword = None
    for i, child in enumerate(self.children):
        if start_pos &lt; child.start_pos:
            return keyword
        first_leaf = child.get_first_leaf()
        if first_leaf in self.FLOW_KEYWORDS:
            keyword = first_leaf
    return 0


</t>
<t tx="ekr.20180516071752.198">class IfStmt(Flow):
    type = 'if_stmt'
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.199">def check_nodes(self):
    """
    Returns all the `test` nodes that are defined as x, here:

        if x:
            pass
        elif x:
            pass
    """
    for i, c in enumerate(self.children):
        if c in ('elif', 'if'):
            yield self.children[i + 1]

</t>
<t tx="ekr.20180516071752.2"></t>
<t tx="ekr.20180516071752.20">def _shift(self, type_, value, newstate, prefix, start_pos):
    """Shift a token.  (Internal)"""
    dfa, state, node = self.stack[-1]
    newnode = self.convert_leaf(self.grammar, type_, value, prefix, start_pos)
    node[-1].append(newnode)
    self.stack[-1] = (dfa, newstate, node)

</t>
<t tx="ekr.20180516071752.200">def node_in_which_check_node(self, node):
    """
    Returns the check node (see function above) that a node is contained
    in. However if it the node is in the check node itself and not in the
    suite return None.
    """
    start_pos = node.start_pos
    for check_node in reversed(list(self.check_nodes())):
        if check_node.start_pos &lt; start_pos:
            if start_pos &lt; check_node.end_pos:
                return None
                # In this case the node is within the check_node itself,
                # not in the suite
            else:
                return check_node

</t>
<t tx="ekr.20180516071752.201">def node_after_else(self, node):
    """
    Checks if a node is defined after `else`.
    """
    for c in self.children:
        if c == 'else':
            if node.start_pos &gt; c.start_pos:
                return True
    else:
        return False


</t>
<t tx="ekr.20180516071752.202">class WhileStmt(Flow):
    type = 'while_stmt'
    __slots__ = ()


</t>
<t tx="ekr.20180516071752.203">class ForStmt(Flow):
    type = 'for_stmt'
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.204">def get_input_node(self):
    """
    Returns the input node ``y`` from: ``for x in y:``.
    """
    return self.children[3]

</t>
<t tx="ekr.20180516071752.205">def defines_one_name(self):
    """
    Returns True if only one name is returned: ``for x in y``.
    Returns False if the for loop is more complicated: ``for x, z in y``.

    :returns: bool
    """
    return self.children[1].type == 'name'


</t>
<t tx="ekr.20180516071752.206">class TryStmt(Flow):
    type = 'try_stmt'
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.207">def except_clauses(self):
    """
    Returns the ``test`` nodes found in ``except_clause`` nodes.
    Returns ``[None]`` for except clauses without an exception given.
    """
    for node in self.children:
        if node.type == 'except_clause':
            yield node.children[1]
        elif node == 'except':
            yield None

</t>
<t tx="ekr.20180516071752.208">def nodes_to_execute(self, last_added=False):
    result = []
    for child in self.children[2::3]:
        result += child.nodes_to_execute()
    for child in self.children[0::3]:
        if child.type == 'except_clause':
            # Add the test node and ignore the `as NAME` definition.
            result += child.children[1].nodes_to_execute()
    return result


</t>
<t tx="ekr.20180516071752.209">class WithStmt(Flow):
    type = 'with_stmt'
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.21">def _push(self, type_, newdfa, newstate):
    """Push a nonterminal.  (Internal)"""
    dfa, state, node = self.stack[-1]
    newnode = (type_, [])
    self.stack[-1] = (dfa, newstate, node)
    self.stack.append((newdfa, 0, newnode))

</t>
<t tx="ekr.20180516071752.210">def get_defined_names(self):
    names = []
    for with_item in self.children[1:-2:2]:
        # Check with items for 'as' names.
        if with_item.type == 'with_item':
            names += _defined_names(with_item.children[2])
    return names

</t>
<t tx="ekr.20180516071752.211">def node_from_name(self, name):
    node = name
    while True:
        node = node.parent
        if node.type == 'with_item':
            return node.children[0]

</t>
<t tx="ekr.20180516071752.212">def nodes_to_execute(self, last_added=False):
    result = []
    for child in self.children[1::2]:
        if child.type == 'with_item':
            # Just ignore the `as EXPR` part - at least for now, because
            # most times it's just a name.
            child = child.children[0]
        result += child.nodes_to_execute()
    return result


</t>
<t tx="ekr.20180516071752.213">class Import(PythonBaseNode):
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.214">def path_for_name(self, name):
    try:
        # The name may be an alias. If it is, just map it back to the name.
        name = self.aliases()[name]
    except KeyError:
        pass

    for path in self.paths():
        if name in path:
            return path[:path.index(name) + 1]
    raise ValueError('Name should be defined in the import itself')

</t>
<t tx="ekr.20180516071752.215">def is_nested(self):
    return False  # By default, sub classes may overwrite this behavior

</t>
<t tx="ekr.20180516071752.216">def is_star_import(self):
    return self.children[-1] == '*'

</t>
<t tx="ekr.20180516071752.217">def nodes_to_execute(self, last_added=False):
    """
    `nodes_to_execute` works a bit different for imports, because the names
    itself cannot directly get resolved (except on itself).
    """
    # TODO couldn't we return the names? Would be nicer.
    return [self]


</t>
<t tx="ekr.20180516071752.218">class ImportFrom(Import):
    type = 'import_from'
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.219">def get_defined_names(self):
    return [alias or name for name, alias in self._as_name_tuples()]

</t>
<t tx="ekr.20180516071752.22">def _pop(self):
    """Pop a nonterminal.  (Internal)"""
    popdfa, popstate, (type_, children) = self.stack.pop()
    # If there's exactly one child, return that child instead of creating a
    # new node.  We still create expr_stmt and file_input though, because a
    # lot of Jedi depends on its logic.
    if len(children) == 1:
        newnode = children[0]
    else:
        newnode = self.convert_node(self.grammar, type_, children)

    try:
        # Equal to:
        # dfa, state, node = self.stack[-1]
        # symbol, children = node
        self.stack[-1][2][1].append(newnode)
    except IndexError:
        # Stack is empty, set the rootnode.
        self.rootnode = newnode
</t>
<t tx="ekr.20180516071752.220">def aliases(self):
    """Mapping from alias to its corresponding name."""
    return dict((alias, name) for name, alias in self._as_name_tuples()
                if alias is not None)

</t>
<t tx="ekr.20180516071752.221">def get_from_names(self):
    for n in self.children[1:]:
        if n not in ('.', '...'):
            break
    if n.type == 'dotted_name':  # from x.y import
        return n.children[::2]
    elif n == 'import':  # from . import
        return []
    else:  # from x import
        return [n]

</t>
<t tx="ekr.20180516071752.222">@property
def level(self):
    """The level parameter of ``__import__``."""
    level = 0
    for n in self.children[1:]:
        if n in ('.', '...'):
            level += len(n.value)
        else:
            break
    return level

</t>
<t tx="ekr.20180516071752.223">def _as_name_tuples(self):
    last = self.children[-1]
    if last == ')':
        last = self.children[-2]
    elif last == '*':
        return  # No names defined directly.

    if last.type == 'import_as_names':
        as_names = last.children[::2]
    else:
        as_names = [last]
    for as_name in as_names:
        if as_name.type == 'name':
            yield as_name, None
        else:
            yield as_name.children[::2]  # yields x, y -&gt; ``x as y``

</t>
<t tx="ekr.20180516071752.224">def star_import_name(self):
    """
    The last name defined in a star import.
    """
    return self.paths()[-1][-1]

</t>
<t tx="ekr.20180516071752.225">def paths(self):
    """
    The import paths defined in an import statement. Typically an array
    like this: ``[&lt;Name: datetime&gt;, &lt;Name: date&gt;]``.
    """
    dotted = self.get_from_names()

    if self.children[-1] == '*':
        return [dotted]
    return [dotted + [name] for name, alias in self._as_name_tuples()]


</t>
<t tx="ekr.20180516071752.226">class ImportName(Import):
    """For ``import_name`` nodes. Covers normal imports without ``from``."""
    type = 'import_name'
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.227">def get_defined_names(self):
    return [alias or path[0] for path, alias in self._dotted_as_names()]

</t>
<t tx="ekr.20180516071752.228">@property
def level(self):
    """The level parameter of ``__import__``."""
    return 0  # Obviously 0 for imports without from.

</t>
<t tx="ekr.20180516071752.229">def paths(self):
    return [path for path, alias in self._dotted_as_names()]

</t>
<t tx="ekr.20180516071752.23">@path C:/Anaconda3/Lib/site-packages/jedi/parser/pgen2/
# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
# Licensed to PSF under a Contributor Agreement.

# Modifications:
# Copyright 2014 David Halter. Integration into Jedi.
# Modifications are dual-licensed: MIT and PSF.

@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071752.230">def _dotted_as_names(self):
    """Generator of (list(path), alias) where alias may be None."""
    dotted_as_names = self.children[1]
    if dotted_as_names.type == 'dotted_as_names':
        as_names = dotted_as_names.children[::2]
    else:
        as_names = [dotted_as_names]

    for as_name in as_names:
        if as_name.type == 'dotted_as_name':
            alias = as_name.children[2]
            as_name = as_name.children[0]
        else:
            alias = None
        if as_name.type == 'name':
            yield [as_name], alias
        else:
            # dotted_names
            yield as_name.children[::2], alias

</t>
<t tx="ekr.20180516071752.231">def is_nested(self):
    """
    This checks for the special case of nested imports, without aliases and
    from statement::

        import foo.bar
    """
    return [1 for path, alias in self._dotted_as_names()
            if alias is None and len(path) &gt; 1]

</t>
<t tx="ekr.20180516071752.232">def aliases(self):
    return dict((alias, path[-1]) for path, alias in self._dotted_as_names()
                if alias is not None)


</t>
<t tx="ekr.20180516071752.233">class KeywordStatement(PythonBaseNode):
    """
    For the following statements: `assert`, `del`, `global`, `nonlocal`,
    `raise`, `return`, `yield`, `return`, `yield`.

    `pass`, `continue` and `break` are not in there, because they are just
    simple keywords and the parser reduces it to a keyword.
    """
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.234">@property
def type(self):
    """
    Keyword statements start with the keyword and end with `_stmt`. You can
    crosscheck this with the Python grammar.
    """
    return '%s_stmt' % self.keyword

</t>
<t tx="ekr.20180516071752.235">@property
def keyword(self):
    return self.children[0].value

</t>
<t tx="ekr.20180516071752.236">def nodes_to_execute(self, last_added=False):
    result = []
    for child in self.children:
        result += child.nodes_to_execute()
    return result


</t>
<t tx="ekr.20180516071752.237">class AssertStmt(KeywordStatement):
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.238">def assertion(self):
    return self.children[1]


</t>
<t tx="ekr.20180516071752.239">class GlobalStmt(KeywordStatement):
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.24">from . import grammar
from jedi.parser import token
from jedi.parser import tokenize


</t>
<t tx="ekr.20180516071752.240">def get_defined_names(self):
    return []

</t>
<t tx="ekr.20180516071752.241">def get_global_names(self):
    return self.children[1::2]

</t>
<t tx="ekr.20180516071752.242">def nodes_to_execute(self, last_added=False):
    """
    The global keyword allows to define any name. Even if it doesn't
    exist.
    """
    return []


</t>
<t tx="ekr.20180516071752.243">class ReturnStmt(KeywordStatement):
    __slots__ = ()


</t>
<t tx="ekr.20180516071752.244">class YieldExpr(PythonBaseNode):
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.245">@property
def type(self):
    return 'yield_expr'

</t>
<t tx="ekr.20180516071752.246">def nodes_to_execute(self, last_added=False):
    if len(self.children) &gt; 1:
        return self.children[1].nodes_to_execute()
    else:
        return []


</t>
<t tx="ekr.20180516071752.247">def _defined_names(current):
    """
    A helper function to find the defined names in statements, for loops and
    list comprehensions.
    """
    names = []
    if current.type in ('testlist_star_expr', 'testlist_comp', 'exprlist'):
        for child in current.children[::2]:
            names += _defined_names(child)
    elif current.type in ('atom', 'star_expr'):
        names += _defined_names(current.children[1])
    elif current.type in ('power', 'atom_expr'):
        if current.children[-2] != '**':  # Just if there's no operation
            trailer = current.children[-1]
            if trailer.children[0] == '.':
                names.append(trailer.children[1])
    else:
        names.append(current)
    return names


</t>
<t tx="ekr.20180516071752.248">class ExprStmt(PythonBaseNode, DocstringMixin):
    type = 'expr_stmt'
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.249">def get_defined_names(self):
    names = []
    if self.children[1].type == 'annassign':
        names = _defined_names(self.children[0])
    return list(chain.from_iterable(
        _defined_names(self.children[i])
        for i in range(0, len(self.children) - 2, 2)
        if '=' in self.children[i + 1].value)
    ) + names

</t>
<t tx="ekr.20180516071752.25">class ParserGenerator(object):
    @others
</t>
<t tx="ekr.20180516071752.250">def get_rhs(self):
    """Returns the right-hand-side of the equals."""
    return self.children[-1]

</t>
<t tx="ekr.20180516071752.251">def first_operation(self):
    """
    Returns `+=`, `=`, etc or None if there is no operation.
    """
    try:
        return self.children[1]
    except IndexError:
        return None

</t>
<t tx="ekr.20180516071752.252">def nodes_to_execute(self, last_added=False):
    # I think evaluating the statement (and possibly returned arrays),
    # should be enough for static analysis.
    result = [self]
    for child in self.children:
        result += child.nodes_to_execute(last_added=True)
    return result


</t>
<t tx="ekr.20180516071752.253">class Param(PythonBaseNode):
    """
    It's a helper class that makes business logic with params much easier. The
    Python grammar defines no ``param`` node. It defines it in a different way
    that is not really suited to working with parameters.
    """
    type = 'param'

    @others
</t>
<t tx="ekr.20180516071752.254">def __init__(self, children, parent):
    super(Param, self).__init__(children)
    self.parent = parent
    for child in children:
        child.parent = self

</t>
<t tx="ekr.20180516071752.255">@property
def stars(self):
    first = self.children[0]
    if first in ('*', '**'):
        return len(first.value)
    return 0

</t>
<t tx="ekr.20180516071752.256">@property
def default(self):
    try:
        return self.children[int(self.children[0] in ('*', '**')) + 2]
    except IndexError:
        return None

</t>
<t tx="ekr.20180516071752.257">def annotation(self):
    tfpdef = self._tfpdef()
    if tfpdef.type == 'tfpdef':
        assert tfpdef.children[1] == ":"
        assert len(tfpdef.children) == 3
        annotation = tfpdef.children[2]
        return annotation
    else:
        return None

</t>
<t tx="ekr.20180516071752.258">def _tfpdef(self):
    """
    tfpdef: see grammar.txt.
    """
    offset = int(self.children[0] in ('*', '**'))
    return self.children[offset]

</t>
<t tx="ekr.20180516071752.259">@property
def name(self):
    if self._tfpdef().type == 'tfpdef':
        return self._tfpdef().children[0]
    else:
        return self._tfpdef()

</t>
<t tx="ekr.20180516071752.26">def __init__(self, bnf_text):
    self._bnf_text = bnf_text
    self.generator = tokenize.source_tokens(bnf_text)
    self.gettoken()  # Initialize lookahead
    self.dfas, self.startsymbol = self.parse()
    self.first = {}  # map from symbol name to set of tokens
    self.addfirstsets()

</t>
<t tx="ekr.20180516071752.260">@property
def position_nr(self):
    index = self.parent.children.index(self)
    try:
        keyword_only_index = self.parent.children.index('*')
        if index &gt; keyword_only_index:
            # Skip the ` *, `
            index -= 2
    except ValueError:
        pass
    return index - 1

</t>
<t tx="ekr.20180516071752.261">def get_parent_function(self):
    return search_ancestor(self, ('funcdef', 'lambda'))

</t>
<t tx="ekr.20180516071752.262">def __repr__(self):
    default = '' if self.default is None else '=%s' % self.default.get_code()
    return '&lt;%s: %s&gt;' % (type(self).__name__, str(self._tfpdef()) + default)

</t>
<t tx="ekr.20180516071752.263">def get_description(self):
    children = self.children
    if children[-1] == ',':
        children = children[:-1]
    return self._get_code_for_children(children, False, False)


</t>
<t tx="ekr.20180516071752.264">class CompFor(PythonBaseNode):
    type = 'comp_for'
    __slots__ = ()

    @others
</t>
<t tx="ekr.20180516071752.265">def get_comp_fors(self):
    yield self
    last = self.children[-1]
    while True:
        if isinstance(last, CompFor):
            yield last
        elif not last.type == 'comp_if':
            break
        last = last.children[-1]

</t>
<t tx="ekr.20180516071752.266">def is_scope(self):
    return True

</t>
<t tx="ekr.20180516071752.267">def get_defined_names(self):
    return _defined_names(self.children[1])

</t>
<t tx="ekr.20180516071752.268">def nodes_to_execute(self, last_added=False):
    last = self.children[-1]
    if last.type == 'comp_if':
        for node in last.children[-1].nodes_to_execute():
            yield node
        last = self.children[-2]
    elif last.type == 'comp_for':
        for node in last.nodes_to_execute():
            yield node
        last = self.children[-2]
    for node in last.nodes_to_execute():
        yield node
</t>
<t tx="ekr.20180516071752.269">@path C:/Anaconda3/Lib/site-packages/jedi/parser/python/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071752.27">def make_grammar(self):
    c = grammar.Grammar(self._bnf_text)
    names = list(self.dfas.keys())
    names.sort()
    names.remove(self.startsymbol)
    names.insert(0, self.startsymbol)
    for name in names:
        i = 256 + len(c.symbol2number)
        c.symbol2number[name] = i
        c.number2symbol[i] = name
    for name in names:
        dfa = self.dfas[name]
        states = []
        for state in dfa:
            arcs = []
            for label, next in state.arcs.items():
                arcs.append((self.make_label(c, label), dfa.index(next)))
            if state.isfinal:
                arcs.append((0, dfa.index(state)))
            states.append(arcs)
        c.states.append(states)
        c.dfas[c.symbol2number[name]] = (states, self.make_first(c, name))
    c.start = c.symbol2number[self.startsymbol]
    return c

</t>
<t tx="ekr.20180516071752.270">"""
Parsers for Python
"""
import os

from jedi import settings
from jedi._compatibility import FileNotFoundError
from jedi.parser.pgen2.pgen import generate_grammar
from jedi.parser.python.parser import Parser, _remove_last_newline
from jedi.parser.python.diff import DiffParser
from jedi.parser.tokenize import generate_tokens
from jedi.parser.cache import parser_cache, load_module, save_module
from jedi.common import splitlines, source_to_unicode


_loaded_grammars = {}


</t>
<t tx="ekr.20180516071752.271">def load_grammar(version=None):
    """
    Loads a Python grammar. The default version is always the latest.

    If you need support for a specific version, please use e.g.
    `version='3.3'`.
    """
    if version is None:
        version = '3.6'

    if version in ('3.2', '3.3'):
        version = '3.4'
    elif version == '2.6':
        version = '2.7'

    file = 'grammar' + version + '.txt'

    global _loaded_grammars
    path = os.path.join(os.path.dirname(__file__), file)
    try:
        return _loaded_grammars[path]
    except KeyError:
        try:
            with open(path) as f:
                bnf_text = f.read()
            grammar = generate_grammar(bnf_text)
            return _loaded_grammars.setdefault(path, grammar)
        except FileNotFoundError:
            # Just load the default if the file does not exist.
            return load_grammar()


</t>
<t tx="ekr.20180516071752.272">def parse(code=None, path=None, grammar=None, error_recovery=True,
          start_symbol='file_input', cache=False, diff_cache=False):
    """
    If you want to parse a Python file you want to start here, most likely.

    If you need finer grained control over the parsed instance, there will be
    other ways to access it.

    :param code: A unicode string that contains Python code.
    :param path: The path to the file you want to open. Only needed for caching.
    :param grammar: A Python grammar file, created with load_grammar.
    :param error_recovery: If enabled, any code will be returned. If it is
        invalid, it will be returned as an error node. If disabled, you will
        get a ParseError when encountering syntax errors in your code.
    :param start_symbol: The grammar symbol that you want to parse. Only
        allowed to be used when error_recovery is disabled.

    :return: A syntax tree node. Typically the module.
    """
    if code is None and path is None:
        raise TypeError("Please provide either code or a path.")

    if grammar is None:
        grammar = load_grammar()

    if path is not None:
        path = os.path.expanduser(path)

    if cache and not code and path is not None:
        # In this case we do actual caching. We just try to load it.
        module_node = load_module(grammar, path)
        if module_node is not None:
            return module_node

    if code is None:
        with open(path, 'rb') as f:
            code = source_to_unicode(f.read())

    if diff_cache and settings.fast_parser:
        try:
            module_cache_item = parser_cache[path]
        except KeyError:
            pass
        else:
            lines = splitlines(code, keepends=True)
            module_node = module_cache_item.node
            old_lines = module_cache_item.lines
            if old_lines == lines:
                save_module(grammar, path, module_node, lines, pickling=False)
                return module_node

            new_node = DiffParser(grammar, module_node).update(
                old_lines=old_lines,
                new_lines=lines
            )
            save_module(grammar, path, new_node, lines, pickling=cache)
            return new_node

    added_newline = not code.endswith('\n')
    lines = tokenize_lines = splitlines(code, keepends=True)
    if added_newline:
        code += '\n'
        tokenize_lines = list(tokenize_lines)
        tokenize_lines[-1] += '\n'
        tokenize_lines.append('')

    tokens = generate_tokens(tokenize_lines, use_exact_op_types=True)

    p = Parser(grammar, error_recovery=error_recovery, start_symbol=start_symbol)
    root_node = p.parse(tokens=tokens)
    if added_newline:
        _remove_last_newline(root_node)

    if cache or diff_cache:
        save_module(grammar, path, root_node, lines, pickling=cache)
    return root_node
</t>
<t tx="ekr.20180516071752.28">def make_first(self, c, name):
    rawfirst = self.first[name]
    first = {}
    for label in rawfirst:
        ilabel = self.make_label(c, label)
        ##assert ilabel not in first # XXX failed on &lt;&gt; ... !=
        first[ilabel] = 1
    return first

</t>
<t tx="ekr.20180516071752.29">def make_label(self, c, label):
    # XXX Maybe this should be a method on a subclass of converter?
    ilabel = len(c.labels)
    if label[0].isalpha():
        # Either a symbol name or a named token
        if label in c.symbol2number:
            # A symbol name (a non-terminal)
            if label in c.symbol2label:
                return c.symbol2label[label]
            else:
                c.labels.append((c.symbol2number[label], None))
                c.symbol2label[label] = ilabel
                return ilabel
        else:
            # A named token (NAME, NUMBER, STRING)
            itoken = getattr(token, label, None)
            assert isinstance(itoken, int), label
            assert itoken in token.tok_name, label
            if itoken in c.tokens:
                return c.tokens[itoken]
            else:
                c.labels.append((itoken, None))
                c.tokens[itoken] = ilabel
                return ilabel
    else:
        # Either a keyword or an operator
        assert label[0] in ('"', "'"), label
        value = eval(label)
        if value[0].isalpha():
            # A keyword
            if value in c.keywords:
                return c.keywords[value]
            else:
                c.labels.append((token.NAME, value))
                c.keywords[value] = ilabel
                return ilabel
        else:
            # An operator (any non-numeric token)
            itoken = token.opmap[value]  # Fails if unknown token
            if itoken in c.tokens:
                return c.tokens[itoken]
            else:
                c.labels.append((itoken, None))
                c.tokens[itoken] = ilabel
                return ilabel

</t>
<t tx="ekr.20180516071752.3">@path C:/Anaconda3/Lib/site-packages/jedi/parser/pgen2/
# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
# Licensed to PSF under a Contributor Agreement.

# Modifications:
# Copyright 2014 David Halter. Integration into Jedi.
# Modifications are dual-licensed: MIT and PSF.

@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071752.30">def addfirstsets(self):
    names = list(self.dfas.keys())
    names.sort()
    for name in names:
        if name not in self.first:
            self.calcfirst(name)
        #print name, self.first[name].keys()

</t>
<t tx="ekr.20180516071752.31">def calcfirst(self, name):
    dfa = self.dfas[name]
    self.first[name] = None  # dummy to detect left recursion
    state = dfa[0]
    totalset = {}
    overlapcheck = {}
    for label, next in state.arcs.items():
        if label in self.dfas:
            if label in self.first:
                fset = self.first[label]
                if fset is None:
                    raise ValueError("recursion for rule %r" % name)
            else:
                self.calcfirst(label)
                fset = self.first[label]
            totalset.update(fset)
            overlapcheck[label] = fset
        else:
            totalset[label] = 1
            overlapcheck[label] = {label: 1}
    inverse = {}
    for label, itsfirst in overlapcheck.items():
        for symbol in itsfirst:
            if symbol in inverse:
                raise ValueError("rule %s is ambiguous; %s is in the"
                                 " first sets of %s as well as %s" %
                                 (name, symbol, label, inverse[symbol]))
            inverse[symbol] = label
    self.first[name] = totalset

</t>
<t tx="ekr.20180516071752.32">def parse(self):
    dfas = {}
    startsymbol = None
    # MSTART: (NEWLINE | RULE)* ENDMARKER
    while self.type != token.ENDMARKER:
        while self.type == token.NEWLINE:
            self.gettoken()
        # RULE: NAME ':' RHS NEWLINE
        name = self.expect(token.NAME)
        self.expect(token.OP, ":")
        a, z = self.parse_rhs()
        self.expect(token.NEWLINE)
        #self.dump_nfa(name, a, z)
        dfa = self.make_dfa(a, z)
        #self.dump_dfa(name, dfa)
        # oldlen = len(dfa)
        self.simplify_dfa(dfa)
        # newlen = len(dfa)
        dfas[name] = dfa
        #print name, oldlen, newlen
        if startsymbol is None:
            startsymbol = name
    return dfas, startsymbol

</t>
<t tx="ekr.20180516071752.33">def make_dfa(self, start, finish):
    # To turn an NFA into a DFA, we define the states of the DFA
    # to correspond to *sets* of states of the NFA.  Then do some
    # state reduction.  Let's represent sets as dicts with 1 for
    # values.
    assert isinstance(start, NFAState)
    assert isinstance(finish, NFAState)

    def closure(state):
        base = {}
        addclosure(state, base)
        return base

    def addclosure(state, base):
        assert isinstance(state, NFAState)
        if state in base:
            return
        base[state] = 1
        for label, next in state.arcs:
            if label is None:
                addclosure(next, base)

    states = [DFAState(closure(start), finish)]
    for state in states:  # NB states grows while we're iterating
        arcs = {}
        for nfastate in state.nfaset:
            for label, next in nfastate.arcs:
                if label is not None:
                    addclosure(next, arcs.setdefault(label, {}))
        for label, nfaset in arcs.items():
            for st in states:
                if st.nfaset == nfaset:
                    break
            else:
                st = DFAState(nfaset, finish)
                states.append(st)
            state.addarc(st, label)
    return states  # List of DFAState instances; first one is start

</t>
<t tx="ekr.20180516071752.34">def dump_nfa(self, name, start, finish):
    print("Dump of NFA for", name)
    todo = [start]
    for i, state in enumerate(todo):
        print("  State", i, state is finish and "(final)" or "")
        for label, next in state.arcs:
            if next in todo:
                j = todo.index(next)
            else:
                j = len(todo)
                todo.append(next)
            if label is None:
                print("    -&gt; %d" % j)
            else:
                print("    %s -&gt; %d" % (label, j))

</t>
<t tx="ekr.20180516071752.35">def dump_dfa(self, name, dfa):
    print("Dump of DFA for", name)
    for i, state in enumerate(dfa):
        print("  State", i, state.isfinal and "(final)" or "")
        for label, next in state.arcs.items():
            print("    %s -&gt; %d" % (label, dfa.index(next)))

</t>
<t tx="ekr.20180516071752.36">def simplify_dfa(self, dfa):
    # This is not theoretically optimal, but works well enough.
    # Algorithm: repeatedly look for two states that have the same
    # set of arcs (same labels pointing to the same nodes) and
    # unify them, until things stop changing.

    # dfa is a list of DFAState instances
    changes = True
    while changes:
        changes = False
        for i, state_i in enumerate(dfa):
            for j in range(i + 1, len(dfa)):
                state_j = dfa[j]
                if state_i == state_j:
                    #print "  unify", i, j
                    del dfa[j]
                    for state in dfa:
                        state.unifystate(state_j, state_i)
                    changes = True
                    break

</t>
<t tx="ekr.20180516071752.37">def parse_rhs(self):
    # RHS: ALT ('|' ALT)*
    a, z = self.parse_alt()
    if self.value != "|":
        return a, z
    else:
        aa = NFAState()
        zz = NFAState()
        aa.addarc(a)
        z.addarc(zz)
        while self.value == "|":
            self.gettoken()
            a, z = self.parse_alt()
            aa.addarc(a)
            z.addarc(zz)
        return aa, zz

</t>
<t tx="ekr.20180516071752.38">def parse_alt(self):
    # ALT: ITEM+
    a, b = self.parse_item()
    while (self.value in ("(", "[") or
           self.type in (token.NAME, token.STRING)):
        c, d = self.parse_item()
        b.addarc(c)
        b = d
    return a, b

</t>
<t tx="ekr.20180516071752.39">def parse_item(self):
    # ITEM: '[' RHS ']' | ATOM ['+' | '*']
    if self.value == "[":
        self.gettoken()
        a, z = self.parse_rhs()
        self.expect(token.OP, "]")
        a.addarc(z)
        return a, z
    else:
        a, z = self.parse_atom()
        value = self.value
        if value not in ("+", "*"):
            return a, z
        self.gettoken()
        z.addarc(a)
        if value == "+":
            return a, z
        else:
            return a, a

</t>
<t tx="ekr.20180516071752.4">"""This module defines the data structures used to represent a grammar.

These are a bit arcane because they are derived from the data
structures used by Python's 'pgen' parser generator.

There's also a table here mapping operators to their names in the
token module; the Python tokenize module reports all operators as the
fallback token code OP, but the parser needs the actual token code.

"""

import pickle
import hashlib



</t>
<t tx="ekr.20180516071752.40">def parse_atom(self):
    # ATOM: '(' RHS ')' | NAME | STRING
    if self.value == "(":
        self.gettoken()
        a, z = self.parse_rhs()
        self.expect(token.OP, ")")
        return a, z
    elif self.type in (token.NAME, token.STRING):
        a = NFAState()
        z = NFAState()
        a.addarc(z, self.value)
        self.gettoken()
        return a, z
    else:
        self.raise_error("expected (...) or NAME or STRING, got %s/%s",
                         self.type, self.value)

</t>
<t tx="ekr.20180516071752.41">def expect(self, type, value=None):
    if self.type != type or (value is not None and self.value != value):
        self.raise_error("expected %s/%s, got %s/%s",
                         type, value, self.type, self.value)
    value = self.value
    self.gettoken()
    return value

</t>
<t tx="ekr.20180516071752.42">def gettoken(self):
    tup = next(self.generator)
    while tup[0] in (token.COMMENT, token.NL):
        tup = next(self.generator)
    self.type, self.value, self.begin, prefix = tup
    #print tokenize.tok_name[self.type], repr(self.value)

</t>
<t tx="ekr.20180516071752.43">def raise_error(self, msg, *args):
    if args:
        try:
            msg = msg % args
        except:
            msg = " ".join([msg] + list(map(str, args)))
    line = open(self.filename).readlines()[self.begin[0]]
    raise SyntaxError(msg, (self.filename, self.begin[0],
                            self.begin[1], line))


</t>
<t tx="ekr.20180516071752.44">class NFAState(object):
    @others
</t>
<t tx="ekr.20180516071752.45">def __init__(self):
    self.arcs = []  # list of (label, NFAState) pairs

</t>
<t tx="ekr.20180516071752.46">def addarc(self, next, label=None):
    assert label is None or isinstance(label, str)
    assert isinstance(next, NFAState)
    self.arcs.append((label, next))


</t>
<t tx="ekr.20180516071752.47">class DFAState(object):
    @others
</t>
<t tx="ekr.20180516071752.48">def __init__(self, nfaset, final):
    assert isinstance(nfaset, dict)
    assert isinstance(next(iter(nfaset)), NFAState)
    assert isinstance(final, NFAState)
    self.nfaset = nfaset
    self.isfinal = final in nfaset
    self.arcs = {}  # map from label to DFAState

</t>
<t tx="ekr.20180516071752.49">def addarc(self, next, label):
    assert isinstance(label, str)
    assert label not in self.arcs
    assert isinstance(next, DFAState)
    self.arcs[label] = next

</t>
<t tx="ekr.20180516071752.5">class Grammar(object):
    """Pgen parsing tables conversion class.

    Once initialized, this class supplies the grammar tables for the
    parsing engine implemented by parse.py.  The parsing engine
    accesses the instance variables directly.  The class here does not
    provide initialization of the tables; several subclasses exist to
    do this (see the conv and pgen modules).

    The load() method reads the tables from a pickle file, which is
    much faster than the other ways offered by subclasses.  The pickle
    file is written by calling dump() (after loading the grammar
    tables using a subclass).  The report() method prints a readable
    representation of the tables to stdout, for debugging.

    The instance variables are as follows:

    symbol2number -- a dict mapping symbol names to numbers.  Symbol
                     numbers are always 256 or higher, to distinguish
                     them from token numbers, which are between 0 and
                     255 (inclusive).

    number2symbol -- a dict mapping numbers to symbol names;
                     these two are each other's inverse.

    states        -- a list of DFAs, where each DFA is a list of
                     states, each state is a list of arcs, and each
                     arc is a (i, j) pair where i is a label and j is
                     a state number.  The DFA number is the index into
                     this list.  (This name is slightly confusing.)
                     Final states are represented by a special arc of
                     the form (0, j) where j is its own state number.

    dfas          -- a dict mapping symbol numbers to (DFA, first)
                     pairs, where DFA is an item from the states list
                     above, and first is a set of tokens that can
                     begin this grammar rule (represented by a dict
                     whose values are always 1).

    labels        -- a list of (x, y) pairs where x is either a token
                     number or a symbol number, and y is either None
                     or a string; the strings are keywords.  The label
                     number is the index in this list; label numbers
                     are used to mark state transitions (arcs) in the
                     DFAs.

    start         -- the number of the grammar's start symbol.

    keywords      -- a dict mapping keyword strings to arc labels.

    tokens        -- a dict mapping token numbers to arc labels.

    """

    @others
</t>
<t tx="ekr.20180516071752.50">def unifystate(self, old, new):
    for label, next in self.arcs.items():
        if next is old:
            self.arcs[label] = new

</t>
<t tx="ekr.20180516071752.51">def __eq__(self, other):
    # Equality test -- ignore the nfaset instance variable
    assert isinstance(other, DFAState)
    if self.isfinal != other.isfinal:
        return False
    # Can't just return self.arcs == other.arcs, because that
    # would invoke this method recursively, with cycles...
    if len(self.arcs) != len(other.arcs):
        return False
    for label, next in self.arcs.items():
        if next is not other.arcs.get(label):
            return False
    return True

__hash__ = None  # For Py3 compatibility.


</t>
<t tx="ekr.20180516071752.52">def generate_grammar(bnf_text):
    """
    ``bnf_text`` is a grammar in extended BNF (using * for repetition, + for
    at-least-once repetition, [] for optional parts, | for alternatives and ()
    for grouping).

    It's not EBNF according to ISO/IEC 14977. It's a dialect Python uses in its
    own parser.
    """
    p = ParserGenerator(bnf_text)
    return p.make_grammar()
</t>
<t tx="ekr.20180516071752.53">@path C:/Anaconda3/Lib/site-packages/jedi/parser/pgen2/
# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
# Licensed to PSF under a Contributor Agreement.

# Modifications:
# Copyright 2006 Google, Inc. All Rights Reserved.
# Licensed to PSF under a Contributor Agreement.
# Copyright 2014 David Halter. Integration into Jedi.
# Modifications are dual-licensed: MIT and PSF.
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071752.55"></t>
<t tx="ekr.20180516071752.56">@path C:/Anaconda3/Lib/site-packages/jedi/parser/python/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071752.57">"""
Basically a contains parser that is faster, because it tries to parse only
parts and if anything changes, it only reparses the changed parts.

It works with a simple diff in the beginning and will try to reuse old parser
fragments.
"""
import re
import difflib
from collections import namedtuple

from jedi.common import splitlines
from jedi.parser.python.parser import Parser, _remove_last_newline
from jedi.parser.python.tree import EndMarker
from jedi import debug
from jedi.parser.tokenize import (generate_tokens, NEWLINE, TokenInfo,
                                  ENDMARKER, INDENT, DEDENT)


</t>
<t tx="ekr.20180516071752.58">def _get_last_line(node_or_leaf):
    last_leaf = node_or_leaf.get_last_leaf()
    if _ends_with_newline(last_leaf):
        return last_leaf.start_pos[0]
    else:
        return last_leaf.end_pos[0]


</t>
<t tx="ekr.20180516071752.59">def _ends_with_newline(leaf, suffix=''):
    if leaf.type == 'error_leaf':
        typ = leaf.original_type
    else:
        typ = leaf.type

    return typ == 'newline' or suffix.endswith('\n')


</t>
<t tx="ekr.20180516071752.6">def __init__(self, bnf_text):
    self.symbol2number = {}
    self.number2symbol = {}
    self.states = []
    self.dfas = {}
    self.labels = [(0, "EMPTY")]
    self.keywords = {}
    self.tokens = {}
    self.symbol2label = {}
    self.start = 256
    self.sha256 = hashlib.sha256(bnf_text.encode("utf-8")).hexdigest()

</t>
<t tx="ekr.20180516071752.60">def _flows_finished(grammar, stack):
    """
    if, while, for and try might not be finished, because another part might
    still be parsed.
    """
    for dfa, newstate, (symbol_number, nodes) in stack:
        if grammar.number2symbol[symbol_number] in ('if_stmt', 'while_stmt',
                                                    'for_stmt', 'try_stmt'):
            return False
    return True


</t>
<t tx="ekr.20180516071752.61">def suite_or_file_input_is_valid(grammar, stack):
    if not _flows_finished(grammar, stack):
        return False

    for dfa, newstate, (symbol_number, nodes) in reversed(stack):
        if grammar.number2symbol[symbol_number] == 'suite':
            # If only newline is in the suite, the suite is not valid, yet.
            return len(nodes) &gt; 1
    # Not reaching a suite means that we're dealing with file_input levels
    # where there's no need for a valid statement in it. It can also be empty.
    return True


</t>
<t tx="ekr.20180516071752.62">def _is_flow_node(node):
    try:
        value = node.children[0].value
    except AttributeError:
        return False
    return value in ('if', 'for', 'while', 'try')


</t>
<t tx="ekr.20180516071752.63">class _PositionUpdatingFinished(Exception):
    pass


</t>
<t tx="ekr.20180516071752.64">def _update_positions(nodes, line_offset, last_leaf):
    for node in nodes:
        try:
            children = node.children
        except AttributeError:
            # Is a leaf
            node.line += line_offset
            if node is last_leaf:
                raise _PositionUpdatingFinished
        else:
            _update_positions(children, line_offset, last_leaf)


</t>
<t tx="ekr.20180516071752.65">class DiffParser(object):
    """
    An advanced form of parsing a file faster. Unfortunately comes with huge
    side effects. It changes the given module.
    """
    @others
</t>
<t tx="ekr.20180516071752.66">def __init__(self, grammar, module):
    self._grammar = grammar
    self._module = module

</t>
<t tx="ekr.20180516071752.67">def _reset(self):
    self._copy_count = 0
    self._parser_count = 0

    self._nodes_stack = _NodesStack(self._module)

</t>
<t tx="ekr.20180516071752.68">def update(self, old_lines, new_lines):
    '''
    The algorithm works as follows:

    Equal:
        - Assure that the start is a newline, otherwise parse until we get
          one.
        - Copy from parsed_until_line + 1 to max(i2 + 1)
        - Make sure that the indentation is correct (e.g. add DEDENT)
        - Add old and change positions
    Insert:
        - Parse from parsed_until_line + 1 to min(j2 + 1), hopefully not
          much more.

    Returns the new module node.
    '''
    debug.speed('diff parser start')
    # Reset the used names cache so they get regenerated.
    self._module._used_names = None

    self._parser_lines_new = new_lines
    self._added_newline = False
    if new_lines[-1] != '':
        # The Python grammar needs a newline at the end of a file, but for
        # everything else we keep working with new_lines here.
        self._parser_lines_new = list(new_lines)
        self._parser_lines_new[-1] += '\n'
        self._parser_lines_new.append('')
        self._added_newline = True

    self._reset()

    line_length = len(new_lines)
    sm = difflib.SequenceMatcher(None, old_lines, self._parser_lines_new)
    opcodes = sm.get_opcodes()
    debug.speed('diff parser calculated')
    debug.dbg('diff: line_lengths old: %s, new: %s' % (len(old_lines), line_length))

    for operation, i1, i2, j1, j2 in opcodes:
        debug.dbg('diff %s old[%s:%s] new[%s:%s]',
                  operation, i1 + 1, i2, j1 + 1, j2)

        if j2 == line_length + int(self._added_newline):
            # The empty part after the last newline is not relevant.
            j2 -= 1

        if operation == 'equal':
            line_offset = j1 - i1
            self._copy_from_old_parser(line_offset, i2, j2)
        elif operation == 'replace':
            self._parse(until_line=j2)
        elif operation == 'insert':
            self._parse(until_line=j2)
        else:
            assert operation == 'delete'

    # With this action all change will finally be applied and we have a
    # changed module.
    self._nodes_stack.close()

    if self._added_newline:
        _remove_last_newline(self._module)

    # Good for debugging.
    if debug.debug_function:
        self._enabled_debugging(old_lines, new_lines)
    last_pos = self._module.end_pos[0]
    if last_pos != line_length:
        current_lines = splitlines(self._module.get_code(), keepends=True)
        diff = difflib.unified_diff(current_lines, new_lines)
        raise Exception(
            "There's an issue (%s != %s) with the diff parser. Please report:\n%s"
            % (last_pos, line_length, ''.join(diff))
        )

    debug.speed('diff parser end')
    return self._module

</t>
<t tx="ekr.20180516071752.69">def _enabled_debugging(self, old_lines, lines_new):
    if self._module.get_code() != ''.join(lines_new):
        debug.warning('parser issue:\n%s\n%s', ''.join(old_lines),
                      ''.join(lines_new))

</t>
<t tx="ekr.20180516071752.7">def dump(self, filename):
    """Dump the grammar tables to a pickle file."""
    with open(filename, "wb") as f:
        pickle.dump(self.__dict__, f, 2)

</t>
<t tx="ekr.20180516071752.70">def _copy_from_old_parser(self, line_offset, until_line_old, until_line_new):
    copied_nodes = [None]

    last_until_line = -1
    while until_line_new &gt; self._nodes_stack.parsed_until_line:
        parsed_until_line_old = self._nodes_stack.parsed_until_line - line_offset
        line_stmt = self._get_old_line_stmt(parsed_until_line_old + 1)
        if line_stmt is None:
            # Parse 1 line at least. We don't need more, because we just
            # want to get into a state where the old parser has statements
            # again that can be copied (e.g. not lines within parentheses).
            self._parse(self._nodes_stack.parsed_until_line + 1)
        elif not copied_nodes:
            # We have copied as much as possible (but definitely not too
            # much). Therefore we just parse the rest.
            # We might not reach the end, because there's a statement
            # that is not finished.
            self._parse(until_line_new)
        else:
            p_children = line_stmt.parent.children
            index = p_children.index(line_stmt)

            copied_nodes = self._nodes_stack.copy_nodes(
                p_children[index:],
                until_line_old,
                line_offset
            )
            # Match all the nodes that are in the wanted range.
            if copied_nodes:
                self._copy_count += 1

                from_ = copied_nodes[0].get_start_pos_of_prefix()[0] + line_offset
                to = self._nodes_stack.parsed_until_line

                debug.dbg('diff actually copy %s to %s', from_, to)
        # Since there are potential bugs that might loop here endlessly, we
        # just stop here.
        assert last_until_line != self._nodes_stack.parsed_until_line \
            or not copied_nodes, last_until_line
        last_until_line = self._nodes_stack.parsed_until_line

</t>
<t tx="ekr.20180516071752.71">def _get_old_line_stmt(self, old_line):
    leaf = self._module.get_leaf_for_position((old_line, 0), include_prefixes=True)

    if _ends_with_newline(leaf):
        leaf = leaf.get_next_leaf()
    if leaf.get_start_pos_of_prefix()[0] == old_line:
        node = leaf
        while node.parent.type not in ('file_input', 'suite'):
            node = node.parent
        return node
    # Must be on the same line. Otherwise we need to parse that bit.
    return None

</t>
<t tx="ekr.20180516071752.72">def _get_before_insertion_node(self):
    if self._nodes_stack.is_empty():
        return None

    line = self._nodes_stack.parsed_until_line + 1
    node = self._new_module.get_last_leaf()
    while True:
        parent = node.parent
        if parent.type in ('suite', 'file_input'):
            assert node.end_pos[0] &lt;= line
            assert node.end_pos[1] == 0 or '\n' in self._prefix
            return node
        node = parent

</t>
<t tx="ekr.20180516071752.73">def _parse(self, until_line):
    """
    Parses at least until the given line, but might just parse more until a
    valid state is reached.
    """
    last_until_line = 0
    while until_line &gt; self._nodes_stack.parsed_until_line:
        node = self._try_parse_part(until_line)
        nodes = self._get_children_nodes(node)
        #self._insert_nodes(nodes)

        self._nodes_stack.add_parsed_nodes(nodes)
        debug.dbg(
            'parse part %s to %s (to %s in parser)',
            nodes[0].get_start_pos_of_prefix()[0],
            self._nodes_stack.parsed_until_line,
            node.end_pos[0] - 1
        )
        # Since the tokenizer sometimes has bugs, we cannot be sure that
        # this loop terminates. Therefore assert that there's always a
        # change.
        assert last_until_line != self._nodes_stack.parsed_until_line, last_until_line
        last_until_line = self._nodes_stack.parsed_until_line

</t>
<t tx="ekr.20180516071752.74">def _get_children_nodes(self, node):
    nodes = node.children
    first_element = nodes[0]
    # TODO this looks very strange...
    if first_element.type == 'error_leaf' and \
            first_element.original_type == 'indent':
        assert False, str(nodes)

    return nodes

</t>
<t tx="ekr.20180516071752.75">def _try_parse_part(self, until_line):
    """
    Sets up a normal parser that uses a spezialized tokenizer to only parse
    until a certain position (or a bit longer if the statement hasn't
    ended.
    """
    self._parser_count += 1
    # TODO speed up, shouldn't copy the whole list all the time.
    # memoryview?
    parsed_until_line = self._nodes_stack.parsed_until_line
    lines_after = self._parser_lines_new[parsed_until_line:]
    #print('parse_content', parsed_until_line, lines_after, until_line)
    tokens = self._diff_tokenize(
        lines_after,
        until_line,
        line_offset=parsed_until_line
    )
    self._active_parser = Parser(
        self._grammar,
        error_recovery=True
    )
    return self._active_parser.parse(tokens=tokens)

</t>
<t tx="ekr.20180516071752.76">def _diff_tokenize(self, lines, until_line, line_offset=0):
    is_first_token = True
    omitted_first_indent = False
    indents = []
    tokens = generate_tokens(lines, use_exact_op_types=True)
    stack = self._active_parser.pgen_parser.stack
    for typ, string, start_pos, prefix in tokens:
        start_pos = start_pos[0] + line_offset, start_pos[1]
        if typ == INDENT:
            indents.append(start_pos[1])
            if is_first_token:
                omitted_first_indent = True
                # We want to get rid of indents that are only here because
                # we only parse part of the file. These indents would only
                # get parsed as error leafs, which doesn't make any sense.
                is_first_token = False
                continue
        is_first_token = False

        if typ == DEDENT:
            indents.pop()
            if omitted_first_indent and not indents:
                # We are done here, only thing that can come now is an
                # endmarker or another dedented code block.
                typ, string, start_pos, prefix = next(tokens)
                if '\n' in prefix:
                    prefix = re.sub(r'(&lt;=\n)[^\n]+$', '', prefix)
                else:
                    prefix = ''
                yield TokenInfo(ENDMARKER, '', (start_pos[0] + line_offset, 0), prefix)
                break
        elif typ == NEWLINE and start_pos[0] &gt;= until_line:
            yield TokenInfo(typ, string, start_pos, prefix)
            # Check if the parser is actually in a valid suite state.
            if suite_or_file_input_is_valid(self._grammar, stack):
                start_pos = start_pos[0] + 1, 0
                while len(indents) &gt; int(omitted_first_indent):
                    indents.pop()
                    yield TokenInfo(DEDENT, '', start_pos, '')

                yield TokenInfo(ENDMARKER, '', start_pos, '')
                break
            else:
                continue

        yield TokenInfo(typ, string, start_pos, prefix)


</t>
<t tx="ekr.20180516071752.77">class _NodesStackNode(object):
    ChildrenGroup = namedtuple('ChildrenGroup', 'children line_offset last_line_offset_leaf')

    @others
</t>
<t tx="ekr.20180516071752.78">def __init__(self, tree_node, parent=None):
    self.tree_node = tree_node
    self.children_groups = []
    self.parent = parent

</t>
<t tx="ekr.20180516071752.79">def close(self):
    children = []
    for children_part, line_offset, last_line_offset_leaf in self.children_groups:
        if line_offset != 0:
            try:
                _update_positions(
                    children_part, line_offset, last_line_offset_leaf)
            except _PositionUpdatingFinished:
                pass
        children += children_part
    self.tree_node.children = children
    # Reset the parents
    for node in children:
        node.parent = self.tree_node

</t>
<t tx="ekr.20180516071752.8">def load(self, filename):
    """Load the grammar tables from a pickle file."""
    with open(filename, "rb") as f:
        d = pickle.load(f)
    self.__dict__.update(d)

</t>
<t tx="ekr.20180516071752.80">def add(self, children, line_offset=0, last_line_offset_leaf=None):
    group = self.ChildrenGroup(children, line_offset, last_line_offset_leaf)
    self.children_groups.append(group)

</t>
<t tx="ekr.20180516071752.81">def get_last_line(self, suffix):
    line = 0
    if self.children_groups:
        children_group = self.children_groups[-1]
        last_leaf = children_group.children[-1].get_last_leaf()
        line = last_leaf.end_pos[0]

        # Calculate the line offsets
        offset = children_group.line_offset
        if offset:
            # In case the line_offset is not applied to this specific leaf,
            # just ignore it.
            if last_leaf.line &lt;= children_group.last_line_offset_leaf.line:
                line += children_group.line_offset

        # Newlines end on the next line, which means that they would cover
        # the next line. That line is not fully parsed at this point.
        if _ends_with_newline(last_leaf, suffix):
            line -= 1
    line += suffix.count('\n')
    return line


</t>
<t tx="ekr.20180516071752.82">class _NodesStack(object):
    endmarker_type = 'endmarker'

    @others
</t>
<t tx="ekr.20180516071752.83">def __init__(self, module):
    # Top of stack
    self._tos = self._base_node = _NodesStackNode(module)
    self._module = module
    self._last_prefix = ''
    self.prefix = ''

</t>
<t tx="ekr.20180516071752.84">def is_empty(self):
    return not self._base_node.children

</t>
<t tx="ekr.20180516071752.85">@property
def parsed_until_line(self):
    return self._tos.get_last_line(self.prefix)

</t>
<t tx="ekr.20180516071752.86">def _get_insertion_node(self, indentation_node):
    indentation = indentation_node.start_pos[1]

    # find insertion node
    node = self._tos
    while True:
        tree_node = node.tree_node
        if tree_node.type == 'suite':
            # A suite starts with NEWLINE, ...
            node_indentation = tree_node.children[1].start_pos[1]

            if indentation &gt;= node_indentation:  # Not a Dedent
                # We might be at the most outer layer: modules. We
                # don't want to depend on the first statement
                # having the right indentation.
                return node

        elif tree_node.type == 'file_input':
            return node

        node = self._close_tos()

</t>
<t tx="ekr.20180516071752.87">def _close_tos(self):
    self._tos.close()
    self._tos = self._tos.parent
    return self._tos

</t>
<t tx="ekr.20180516071752.88">def add_parsed_nodes(self, tree_nodes):
    tree_nodes = self._remove_endmarker(tree_nodes)
    if not tree_nodes:
        return

    assert tree_nodes[0].type != 'newline'

    node = self._get_insertion_node(tree_nodes[0])
    assert node.tree_node.type in ('suite', 'file_input')
    node.add(tree_nodes)
    self._update_tos(tree_nodes[-1])

</t>
<t tx="ekr.20180516071752.89">def _remove_endmarker(self, tree_nodes):
    """
    Helps cleaning up the tree nodes that get inserted.
    """
    last_leaf = tree_nodes[-1].get_last_leaf()
    is_endmarker = last_leaf.type == self.endmarker_type
    self._last_prefix = ''
    if is_endmarker:
        try:
            separation = last_leaf.prefix.rindex('\n')
        except ValueError:
            pass
        else:
            # Remove the whitespace part of the prefix after a newline.
            # That is not relevant if parentheses were opened. Always parse
            # until the end of a line.
            last_leaf.prefix, self._last_prefix = \
                last_leaf.prefix[:separation + 1], last_leaf.prefix[separation + 1:]

    first_leaf = tree_nodes[0].get_first_leaf()
    first_leaf.prefix = self.prefix + first_leaf.prefix
    self.prefix = ''

    if is_endmarker:
        self.prefix = last_leaf.prefix

        tree_nodes = tree_nodes[:-1]

    return tree_nodes

</t>
<t tx="ekr.20180516071752.9">def copy(self):
    """
    Copy the grammar.
    """
    new = self.__class__()
    for dict_attr in ("symbol2number", "number2symbol", "dfas", "keywords",
                      "tokens", "symbol2label"):
        setattr(new, dict_attr, getattr(self, dict_attr).copy())
    new.labels = self.labels[:]
    new.states = self.states[:]
    new.start = self.start
    return new

</t>
<t tx="ekr.20180516071752.90">def copy_nodes(self, tree_nodes, until_line, line_offset):
    """
    Copies tree nodes from the old parser tree.

    Returns the number of tree nodes that were copied.
    """
    tos = self._get_insertion_node(tree_nodes[0])

    new_nodes, self._tos = self._copy_nodes(tos, tree_nodes, until_line, line_offset)
    return new_nodes

</t>
<t tx="ekr.20180516071752.91">def _copy_nodes(self, tos, nodes, until_line, line_offset):
    new_nodes = []

    new_tos = tos
    for node in nodes:
        if node.type == 'endmarker':
            # Endmarkers just distort all the checks below. Remove them.
            break

        if node.start_pos[0] &gt; until_line:
            break
        # TODO this check might take a bit of time for large files. We
        # might want to change this to do more intelligent guessing or
        # binary search.
        if _get_last_line(node) &gt; until_line:
            # We can split up functions and classes later.
            if node.type in ('classdef', 'funcdef') and node.children[-1].type == 'suite':
                new_nodes.append(node)
            break

        new_nodes.append(node)

    if not new_nodes:
        return [], tos

    last_node = new_nodes[-1]
    line_offset_index = -1
    if last_node.type in ('classdef', 'funcdef'):
        suite = last_node.children[-1]
        if suite.type == 'suite':
            suite_tos = _NodesStackNode(suite)
            # Don't need to pass line_offset here, it's already done by the
            # parent.
            suite_nodes, recursive_tos = self._copy_nodes(
                suite_tos, suite.children, until_line, line_offset)
            if len(suite_nodes) &lt; 2:
                # A suite only with newline is not valid.
                new_nodes.pop()
            else:
                suite_tos.parent = tos
                new_tos = recursive_tos
                line_offset_index = -2

    elif (new_nodes[-1].type in ('error_leaf', 'error_node') or
                      _is_flow_node(new_nodes[-1])):
        # Error leafs/nodes don't have a defined start/end. Error
        # nodes might not end with a newline (e.g. if there's an
        # open `(`). Therefore ignore all of them unless they are
        # succeeded with valid parser state.
        # If we copy flows at the end, they might be continued
        # after the copy limit (in the new parser).
        # In this while loop we try to remove until we find a newline.
        new_nodes.pop()
        while new_nodes:
            last_node = new_nodes[-1]
            if last_node.get_last_leaf().type == 'newline':
                break
            new_nodes.pop()

    if new_nodes:
        try:
            last_line_offset_leaf = new_nodes[line_offset_index].get_last_leaf()
        except IndexError:
            line_offset = 0
            # In this case we don't have to calculate an offset, because
            # there's no children to be managed.
            last_line_offset_leaf = None
        tos.add(new_nodes, line_offset, last_line_offset_leaf)
    return new_nodes, new_tos

</t>
<t tx="ekr.20180516071752.92">def _update_tos(self, tree_node):
    if tree_node.type in ('suite', 'file_input'):
        self._tos = _NodesStackNode(tree_node, self._tos)
        self._tos.add(list(tree_node.children))
        self._update_tos(tree_node.children[-1])
    elif tree_node.type in ('classdef', 'funcdef'):
        self._update_tos(tree_node.children[-1])

</t>
<t tx="ekr.20180516071752.93">def close(self):
    while self._tos is not None:
        self._close_tos()

    # Add an endmarker.
    try:
        last_leaf = self._module.get_last_leaf()
        end_pos = list(last_leaf.end_pos)
    except IndexError:
        end_pos = [1, 0]
    lines = splitlines(self.prefix)
    assert len(lines) &gt; 0
    if len(lines) == 1:
        end_pos[1] += len(lines[0])
    else:
        end_pos[0] += len(lines) - 1
        end_pos[1] = len(lines[-1])

    endmarker = EndMarker('', tuple(end_pos), self.prefix + self._last_prefix)
    endmarker.parent = self._module
    self._module.children.append(endmarker)
</t>
<t tx="ekr.20180516071752.94">@path C:/Anaconda3/Lib/site-packages/jedi/parser/python/
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20180516071752.95">from jedi.parser.python import tree
from jedi.parser import tokenize
from jedi.parser.token import (DEDENT, INDENT, ENDMARKER, NEWLINE, NUMBER,
                               STRING, tok_name)
from jedi.parser.parser import BaseParser
from jedi.common import splitlines


</t>
<t tx="ekr.20180516071752.96">class Parser(BaseParser):
    """
    This class is used to parse a Python file, it then divides them into a
    class structure of different scopes.

    :param grammar: The grammar object of pgen2. Loaded by load_grammar.
    """

    node_map = {
        'expr_stmt': tree.ExprStmt,
        'classdef': tree.Class,
        'funcdef': tree.Function,
        'file_input': tree.Module,
        'import_name': tree.ImportName,
        'import_from': tree.ImportFrom,
        'break_stmt': tree.KeywordStatement,
        'continue_stmt': tree.KeywordStatement,
        'return_stmt': tree.ReturnStmt,
        'raise_stmt': tree.KeywordStatement,
        'yield_expr': tree.YieldExpr,
        'del_stmt': tree.KeywordStatement,
        'pass_stmt': tree.KeywordStatement,
        'global_stmt': tree.GlobalStmt,
        'nonlocal_stmt': tree.KeywordStatement,
        'print_stmt': tree.KeywordStatement,
        'assert_stmt': tree.AssertStmt,
        'if_stmt': tree.IfStmt,
        'with_stmt': tree.WithStmt,
        'for_stmt': tree.ForStmt,
        'while_stmt': tree.WhileStmt,
        'try_stmt': tree.TryStmt,
        'comp_for': tree.CompFor,
        'decorator': tree.Decorator,
        'lambdef': tree.Lambda,
        'old_lambdef': tree.Lambda,
        'lambdef_nocond': tree.Lambda,
    }
    default_node = tree.PythonNode

    @others
</t>
<t tx="ekr.20180516071752.97">def __init__(self, grammar, error_recovery=True, start_symbol='file_input'):
    super(Parser, self).__init__(grammar, start_symbol, error_recovery=error_recovery)

    self.syntax_errors = []
    self._omit_dedent_list = []
    self._indent_counter = 0

    # TODO do print absolute import detection here.
    # try:
    #     del python_grammar_no_print_statement.keywords["print"]
    # except KeyError:
    #     pass  # Doesn't exist in the Python 3 grammar.

    # if self.options["print_function"]:
    #     python_grammar = pygram.python_grammar_no_print_statement
    # else:

</t>
<t tx="ekr.20180516071752.98">def parse(self, tokens):
    if self._error_recovery:
        if self._start_symbol != 'file_input':
            raise NotImplementedError

        tokens = self._recovery_tokenize(tokens)

    node = super(Parser, self).parse(tokens)

    if self._start_symbol == 'file_input' != node.type:
        # If there's only one statement, we get back a non-module. That's
        # not what we want, we want a module, so we add it here:
        node = self.convert_node(
            self._grammar,
            self._grammar.symbol2number['file_input'],
            [node]
        )

    return node

</t>
<t tx="ekr.20180516071752.99">def convert_node(self, grammar, type, children):
    """
    Convert raw node information to a PythonBaseNode instance.

    This is passed to the parser driver which calls it whenever a reduction of a
    grammar rule produces a new complete node, so that the tree is build
    strictly bottom-up.
    """
    # TODO REMOVE symbol, we don't want type here.
    symbol = grammar.number2symbol[type]
    try:
        return self.node_map[symbol](children)
    except KeyError:
        if symbol == 'suite':
            # We don't want the INDENT/DEDENT in our parser tree. Those
            # leaves are just cancer. They are virtual leaves and not real
            # ones and therefore have pseudo start/end positions and no
            # prefixes. Just ignore them.
            children = [children[0]] + children[2:-1]
        return self.default_node(symbol, children)

</t>
<t tx="ekr.20180516071825.1">
importing directory: C:\Anaconda3\Lib\site-packages\jedi

@auto failed: common.py
inserting @ignore

importing directory: C:/Anaconda3/Lib/site-packages/jedi/parser

@auto failed: token.py
inserting @ignore

imported 1260 nodes in 56 files in 2.55 seconds
</t>
<t tx="ekr.20180516072403.1">saved: jedi.leo

debug.py:25: redefinition of unused '_lazy_colorama_init' from line 6
ERROR: pyflakes: 1 error

refactoring.py:19: 'jedi.parser.python.tree as pt' imported but unused
refactoring.py:185: undefined name 'pr'

_compatibility.py:247: '__builtin__ as builtins' imported but unused
_compatibility.py:265: 'itertools.izip_longest as zip_longest' imported but unused

__init__.py:41: 'jedi.api.Script' imported but unused
__init__.py:41: 'jedi.api.Interpreter' imported but unused
__init__.py:41: 'jedi.api.NotFoundError' imported but unused
__init__.py:41: 'jedi.api.set_debug_function' imported but unused
__init__.py:42: 'jedi.api.preload_module' imported but unused
__init__.py:42: 'jedi.api.defined_names' imported but unused
__init__.py:42: 'jedi.api.names' imported but unused
__init__.py:43: 'jedi.settings' imported but unused

analysis.py:150: local variable 'colon' is assigned to but never used

iterable.py:353: undefined name 'create_evaluated_sequence_set'

precedence.py:7: 'jedi.parser.python.tree' imported but unused

tokenize.py:108: import 'u' from line 21 shadowed by loop variable
tokenize.py:238: undefined name 'endprog'
tokenize.py:241: undefined name 'contstr_start'

unchanged: __init__.py
__init__.py:1: 'jedi.parser.parser.ParserSyntaxError' imported but unused
__init__.py:2: 'jedi.parser.pgen2.pgen.generate_grammar' imported but unused
__init__.py:3: 'jedi.parser.python' imported but unused
__init__.py:8: undefined name 'Parser'
</t>
<t tx="ekr.20180516072650.1">Syntax error in: @clean common.py
    58:  @functools.wraps(func)
    59:  class PushBackIterator(object):
    60:* def __init__(self, iterator):
           ^
    61:      self.pushes = []

token.py:4: 'from token import *' used; unable to detect undefined names
token.py:7: 'N_TOKENS' may be undefined, or defined from star imports: token
token.py:8: 'tok_name' may be undefined, or defined from star imports: token
token.py:9: 'N_TOKENS' may be undefined, or defined from star imports: token
token.py:12: 'tok_name' may be undefined, or defined from star imports: token
token.py:17: 'tok_name' may be undefined, or defined from star imports: token
token.py:21: 'tok_name' may be undefined, or defined from star imports: token
token.py:24: 'tok_name' may be undefined, or defined from star imports: token
token.py:29: 'tok_name' may be undefined, or defined from star imports: token
ERROR: pyflakes: 9 errors</t>
<t tx="ekr.20180516073042.1"></t>
<t tx="ekr.20180516073049.1"></t>
<t tx="ekr.20180516073122.1"></t>
</tnodes>
</leo_file>
