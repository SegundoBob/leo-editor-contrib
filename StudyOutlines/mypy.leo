<?xml version="1.0" encoding="utf-8"?>
<!-- Created by Leo: http://leoeditor.com/leo_toc.html -->
<leo_file xmlns:leo="http://leoeditor.com/namespaces/leo-python-editor/1.1" >
<leo_header file_format="2"/>
<globals/>
<preferences/>
<find_panel_settings/>
<vnodes>
<v t="ekr.20220525082746.1"><vh> Recursive import script</vh></v>
<v t="ekr.20220526080725.1"><vh>--- Notes</vh></v>
<v t="ekr.20220526075331.1"><vh>Files</vh>
<v t="ekr.20220224065603.3"><vh>@edit C:\Repos\leo-editor\.mypy.ini</vh></v>
<v t="ekr.20220525082932.2"><vh>@path C:/Repos/mypy</vh>
<v t="ekr.20220525082932.3"><vh>@clean conftest.py</vh>
<v t="ekr.20220525082933.1"><vh>pytest_configure</vh></v>
<v t="ekr.20220525082933.2"><vh>pytest_addoption</vh></v>
</v>
<v t="ekr.20220525082933.3"><vh>@clean runtests.py</vh>
<v t="ekr.20220525082933.4"><vh>run_cmd</vh></v>
<v t="ekr.20220525082933.5"><vh>start_background_cmd</vh></v>
<v t="ekr.20220525082933.6"><vh>wait_background_cmd</vh></v>
<v t="ekr.20220525082933.7"><vh>main</vh></v>
</v>
<v t="ekr.20220525082933.17"><vh>@path misc</vh>
<v t="ekr.20220525082933.18"><vh>@clean actions_stubs.py</vh>
<v t="ekr.20220525082933.19"><vh>apply_all</vh></v>
<v t="ekr.20220525082933.20"><vh>confirm</vh></v>
<v t="ekr.20220525082933.21"><vh>actions = ['cp', 'mv', 'rm']</vh></v>
<v t="ekr.20220525082933.22"><vh>main</vh></v>
</v>
<v t="ekr.20220525082933.23"><vh>@clean analyze_cache.py</vh>
<v t="ekr.20220525082933.24"><vh>class CacheData</vh>
<v t="ekr.20220525082933.25"><vh>CacheData.__init__</vh></v>
<v t="ekr.20220525082933.26"><vh>CacheData.total_size</vh></v>
</v>
<v t="ekr.20220525082933.27"><vh>extract_classes</vh>
<v t="ekr.20220525082933.28"><vh>extract</vh></v>
</v>
<v t="ekr.20220525082933.29"><vh>load_json</vh></v>
<v t="ekr.20220525082933.30"><vh>get_files</vh></v>
<v t="ekr.20220525082933.31"><vh>pluck</vh></v>
<v t="ekr.20220525082933.32"><vh>report_counter</vh></v>
<v t="ekr.20220525082933.33"><vh>report_most_common</vh></v>
<v t="ekr.20220525082933.34"><vh>compress</vh>
<v t="ekr.20220525082933.35"><vh>helper</vh></v>
</v>
<v t="ekr.20220525082933.36"><vh>decompress</vh>
<v t="ekr.20220525082933.37"><vh>helper</vh></v>
</v>
<v t="ekr.20220525082933.38"><vh>main</vh></v>
</v>
<v t="ekr.20220525082933.39"><vh>@clean apply-cache-diff.py</vh>
<v t="ekr.20220525082933.40"><vh>make_cache</vh></v>
<v t="ekr.20220525082933.41"><vh>apply_diff</vh></v>
<v t="ekr.20220525082933.42"><vh>main</vh></v>
</v>
<v t="ekr.20220525082933.43"><vh>@clean async_matrix.py</vh>
<v t="ekr.20220525082933.44"><vh>plain_generator</vh></v>
<v t="ekr.20220525082933.45"><vh>plain_coroutine</vh></v>
<v t="ekr.20220525082933.46"><vh>decorated_generator</vh></v>
<v t="ekr.20220525082933.47"><vh>decorated_coroutine</vh></v>
<v t="ekr.20220525082933.48"><vh>class It</vh>
<v t="ekr.20220525082933.49"><vh>It.__iter__</vh></v>
<v t="ekr.20220525082933.50"><vh>It.__next__</vh></v>
</v>
<v t="ekr.20220525082933.51"><vh>other_iterator</vh></v>
<v t="ekr.20220525082933.52"><vh>class Aw</vh></v>
<v t="ekr.20220525082933.53"><vh>other_coroutine</vh></v>
<v t="ekr.20220525082933.54"><vh>The various contexts in which `await` or `yield from` might occur.</vh></v>
<v t="ekr.20220525082933.55"><vh>plain_host_generator</vh></v>
<v t="ekr.20220525082933.56"><vh>plain_host_coroutine</vh></v>
<v t="ekr.20220525082933.57"><vh>decorated_host_generator</vh></v>
<v t="ekr.20220525082933.58"><vh>decorated_host_coroutine</vh></v>
<v t="ekr.20220525082933.59"><vh>Main driver.</vh></v>
<v t="ekr.20220525082933.60"><vh>main</vh></v>
</v>
<v t="ekr.20220525082933.61"><vh>@clean build_wheel.py</vh>
<v t="ekr.20220525082933.62"><vh>create_environ</vh></v>
<v t="ekr.20220525082933.63"><vh>main</vh></v>
</v>
<v t="ekr.20220525082933.64"><vh>@clean cherry-pick-typeshed.py</vh>
<v t="ekr.20220525082933.65"><vh>parse_commit_title</vh></v>
<v t="ekr.20220525082933.66"><vh>main</vh></v>
</v>
<v t="ekr.20220525082933.67"><vh>@clean convert-cache.py</vh>
<v t="ekr.20220525082933.68"><vh>main</vh></v>
</v>
<v t="ekr.20220525082933.69"><vh>@clean diff-cache.py</vh>
<v t="ekr.20220525082933.70"><vh>make_cache</vh></v>
<v t="ekr.20220525082933.71"><vh>merge_deps</vh></v>
<v t="ekr.20220525082933.72"><vh>load</vh></v>
<v t="ekr.20220525082933.73"><vh>main</vh></v>
</v>
<v t="ekr.20220525082933.74"><vh>@clean dump-ast.py</vh>
<v t="ekr.20220525082933.75"><vh>dump</vh></v>
<v t="ekr.20220525082933.76"><vh>main</vh></v>
</v>
<v t="ekr.20220525082933.77"><vh>@clean fix_annotate.py</vh>
<v t="ekr.20220525082933.78"><vh>class FixAnnotate</vh>
<v t="ekr.20220525082933.79"><vh>FixAnnotate.transform</vh></v>
<v t="ekr.20220525082933.80"><vh>FixAnnotate.make_annotation</vh></v>
<v t="ekr.20220525082933.81"><vh>FixAnnotate.The parse tree has a different shape when there is a single</vh></v>
<v t="ekr.20220525082933.82"><vh>FixAnnotate.get_decorators</vh></v>
<v t="ekr.20220525082933.83"><vh>FixAnnotate.is_method</vh></v>
<v t="ekr.20220525082933.84"><vh>FixAnnotate.RETURN_EXPR = "return_stmt&lt; 'return' any &gt;"</vh></v>
<v t="ekr.20220525082933.85"><vh>FixAnnotate.has_return_exprs</vh></v>
</v>
</v>
<v t="ekr.20220525082933.86"><vh>@clean incremental_checker.py</vh>
<v t="ekr.20220525082933.87"><vh>print_offset</vh></v>
<v t="ekr.20220525082933.88"><vh>delete_folder</vh></v>
<v t="ekr.20220525082933.89"><vh>execute</vh></v>
<v t="ekr.20220525082933.90"><vh>ensure_environment_is_ready</vh></v>
<v t="ekr.20220525082933.91"><vh>initialize_repo</vh></v>
<v t="ekr.20220525082933.92"><vh>get_commits</vh></v>
<v t="ekr.20220525082933.93"><vh>get_commits_starting_at</vh></v>
<v t="ekr.20220525082933.94"><vh>get_nth_commit</vh></v>
<v t="ekr.20220525082933.95"><vh>run_mypy</vh></v>
<v t="ekr.20220525082933.96"><vh>filter_daemon_stats</vh></v>
<v t="ekr.20220525082933.97"><vh>start_daemon</vh></v>
<v t="ekr.20220525082933.98"><vh>stop_daemon</vh></v>
<v t="ekr.20220525082933.99"><vh>load_cache</vh></v>
<v t="ekr.20220525082933.100"><vh>save_cache</vh></v>
<v t="ekr.20220525082933.101"><vh>set_expected</vh></v>
<v t="ekr.20220525082933.102"><vh>test_incremental</vh></v>
<v t="ekr.20220525082933.103"><vh>combine_stats</vh></v>
<v t="ekr.20220525082933.104"><vh>cleanup</vh></v>
<v t="ekr.20220525082933.105"><vh>test_repo</vh></v>
<v t="ekr.20220525082933.106"><vh>main</vh></v>
</v>
<v t="ekr.20220525082933.107"><vh>@clean perf_checker.py</vh>
<v t="ekr.20220525082933.108"><vh>class Command</vh></v>
<v t="ekr.20220525082933.109"><vh>print_offset</vh></v>
<v t="ekr.20220525082933.110"><vh>delete_folder</vh></v>
<v t="ekr.20220525082933.111"><vh>execute</vh></v>
<v t="ekr.20220525082933.112"><vh>trial</vh></v>
<v t="ekr.20220525082933.113"><vh>report</vh></v>
<v t="ekr.20220525082933.114"><vh>main</vh></v>
</v>
<v t="ekr.20220525082933.115"><vh>@clean proper_plugin.py</vh>
<v t="ekr.20220525082933.116"><vh>class ProperTypePlugin</vh>
<v t="ekr.20220525082933.117"><vh>ProperTypePlugin.get_function_hook</vh></v>
</v>
<v t="ekr.20220525082933.118"><vh>isinstance_proper_hook</vh></v>
<v t="ekr.20220525082933.119"><vh>is_special_target</vh></v>
<v t="ekr.20220525082933.120"><vh>is_improper_type</vh></v>
<v t="ekr.20220525082933.121"><vh>is_dangerous_target</vh></v>
<v t="ekr.20220525082933.122"><vh>proper_type_hook</vh></v>
<v t="ekr.20220525082933.123"><vh>proper_types_hook</vh></v>
<v t="ekr.20220525082933.124"><vh>get_proper_type_instance</vh></v>
<v t="ekr.20220525082933.125"><vh>plugin</vh></v>
</v>
<v t="ekr.20220525082933.126"><vh>@clean sync-typeshed.py</vh>
<v t="ekr.20220525082933.127"><vh>check_state</vh></v>
<v t="ekr.20220525082933.128"><vh>update_typeshed</vh></v>
<v t="ekr.20220525082933.129"><vh>git_head_commit</vh></v>
<v t="ekr.20220525082933.130"><vh>main</vh></v>
</v>
<v t="ekr.20220525082933.131"><vh>@clean test_case_to_actual.py</vh>
<v t="ekr.20220525082933.132"><vh>class Chunk</vh></v>
<v t="ekr.20220525082933.133"><vh>is_header</vh></v>
<v t="ekr.20220525082933.134"><vh>normalize</vh></v>
<v t="ekr.20220525082933.135"><vh>produce_chunks</vh></v>
<v t="ekr.20220525082933.136"><vh>write_out</vh></v>
<v t="ekr.20220525082933.137"><vh>write_tree</vh></v>
<v t="ekr.20220525082933.138"><vh>help</vh></v>
<v t="ekr.20220525082933.139"><vh>main</vh></v>
</v>
<v t="ekr.20220525082933.140"><vh>@clean touch_checker.py</vh>
<v t="ekr.20220525082933.141"><vh>print_offset</vh></v>
<v t="ekr.20220525082933.142"><vh>delete_folder</vh></v>
<v t="ekr.20220525082933.143"><vh>execute</vh></v>
<v t="ekr.20220525082933.144"><vh>Command = Callable[[], None]</vh></v>
<v t="ekr.20220525082933.145"><vh>test</vh></v>
<v t="ekr.20220525082933.146"><vh>make_touch_wrappers</vh></v>
<v t="ekr.20220525082933.147"><vh>make_change_wrappers</vh>
<v t="ekr.20220525082933.148"><vh>setup</vh></v>
<v t="ekr.20220525082933.149"><vh>teardown</vh></v>
</v>
<v t="ekr.20220525082933.150"><vh>main</vh></v>
</v>
<v t="ekr.20220525082933.151"><vh>@clean upload-pypi.py</vh>
<v t="ekr.20220525082933.152"><vh>is_whl_or_tar</vh></v>
<v t="ekr.20220525082933.153"><vh>get_release_for_tag</vh></v>
<v t="ekr.20220525082933.154"><vh>download_asset</vh></v>
<v t="ekr.20220525082933.155"><vh>download_all_release_assets</vh></v>
<v t="ekr.20220525082933.156"><vh>check_sdist</vh></v>
<v t="ekr.20220525082933.157"><vh>spot_check_dist</vh></v>
<v t="ekr.20220525082933.158"><vh>tmp_twine</vh></v>
<v t="ekr.20220525082933.159"><vh>upload_dist</vh></v>
<v t="ekr.20220525082933.160"><vh>upload_to_pypi</vh></v>
<v t="ekr.20220525082933.161"><vh>main</vh></v>
</v>
<v t="ekr.20220525082933.162"><vh>@clean variadics.py</vh>
<v t="ekr.20220525082933.163"><vh>prelude</vh></v>
<v t="ekr.20220525082933.164"><vh>expand_template</vh></v>
<v t="ekr.20220525082933.165"><vh>main</vh></v>
</v>
</v>
<v t="ekr.20220525082933.166"><vh>@path mypy</vh>
<v t="ekr.20220525082933.167"><vh>@clean api.py</vh>
<v t="ekr.20220525082933.168"><vh>_run</vh></v>
<v t="ekr.20220525082933.169"><vh>run</vh></v>
<v t="ekr.20220525082933.170"><vh>run_dmypy</vh>
<v t="ekr.20220525082933.171"><vh>f</vh></v>
</v>
</v>
<v t="ekr.20220525082933.172"><vh>@clean applytype.py</vh>
<v t="ekr.20220525082933.173"><vh>get_target_type</vh></v>
<v t="ekr.20220525082933.174"><vh>apply_generic_arguments</vh></v>
</v>
<v t="ekr.20220525082933.175"><vh>@clean argmap.py</vh>
<v t="ekr.20220525082933.176"><vh>map_actuals_to_formals</vh></v>
<v t="ekr.20220525082933.177"><vh>map_formals_to_actuals</vh></v>
<v t="ekr.20220525082933.178"><vh>class ArgTypeExpander</vh>
<v t="ekr.20220525082933.179"><vh>ArgTypeExpander.__init__</vh></v>
<v t="ekr.20220525082933.180"><vh>ArgTypeExpander.expand_actual_type</vh></v>
</v>
</v>
<v t="ekr.20220525082933.181"><vh>@clean backports.py</vh></v>
<v t="ekr.20220525082933.182"><vh>@clean binder.py</vh>
<v t="ekr.20220525082933.183"><vh>class Frame</vh>
<v t="ekr.20220525082933.184"><vh>Frame.__init__</vh></v>
</v>
<v t="ekr.20220525082933.185"><vh>Assigns = DefaultDict[Expression, List[Tuple[Type, Optional[Type]]]]</vh></v>
<v t="ekr.20220525082933.186"><vh>class ConditionalTypeBinder</vh>
<v t="ekr.20220525082933.187"><vh>ConditionalTypeBinder.__init__</vh></v>
<v t="ekr.20220525082933.188"><vh>ConditionalTypeBinder._get_id</vh></v>
<v t="ekr.20220525082933.189"><vh>ConditionalTypeBinder._add_dependencies</vh></v>
<v t="ekr.20220525082933.190"><vh>ConditionalTypeBinder.push_frame</vh></v>
<v t="ekr.20220525082933.191"><vh>ConditionalTypeBinder._put</vh></v>
<v t="ekr.20220525082933.192"><vh>ConditionalTypeBinder._get</vh></v>
<v t="ekr.20220525082933.193"><vh>ConditionalTypeBinder.put</vh></v>
<v t="ekr.20220525082933.194"><vh>ConditionalTypeBinder.unreachable</vh></v>
<v t="ekr.20220525082933.195"><vh>ConditionalTypeBinder.suppress_unreachable_warnings</vh></v>
<v t="ekr.20220525082933.196"><vh>ConditionalTypeBinder.get</vh></v>
<v t="ekr.20220525082933.197"><vh>ConditionalTypeBinder.is_unreachable</vh></v>
<v t="ekr.20220525082933.198"><vh>ConditionalTypeBinder.is_unreachable_warning_suppressed</vh></v>
<v t="ekr.20220525082933.199"><vh>ConditionalTypeBinder.cleanse</vh></v>
<v t="ekr.20220525082933.200"><vh>ConditionalTypeBinder._cleanse_key</vh></v>
<v t="ekr.20220525082933.201"><vh>ConditionalTypeBinder.update_from_options</vh></v>
<v t="ekr.20220525082933.202"><vh>ConditionalTypeBinder.pop_frame</vh></v>
<v t="ekr.20220525082933.203"><vh>ConditionalTypeBinder.accumulate_type_assignments</vh></v>
<v t="ekr.20220525082933.204"><vh>ConditionalTypeBinder.assign_type</vh></v>
<v t="ekr.20220525082933.205"><vh>ConditionalTypeBinder.invalidate_dependencies</vh></v>
<v t="ekr.20220525082933.206"><vh>ConditionalTypeBinder.most_recent_enclosing_type</vh></v>
<v t="ekr.20220525082933.207"><vh>ConditionalTypeBinder.allow_jump</vh></v>
<v t="ekr.20220525082933.208"><vh>ConditionalTypeBinder.handle_break</vh></v>
<v t="ekr.20220525082933.209"><vh>ConditionalTypeBinder.handle_continue</vh></v>
<v t="ekr.20220525082933.210"><vh>ConditionalTypeBinder.frame_context</vh></v>
<v t="ekr.20220525082933.211"><vh>ConditionalTypeBinder.top_frame_context</vh></v>
</v>
<v t="ekr.20220525082933.212"><vh>get_declaration</vh></v>
</v>
<v t="ekr.20220525082933.213"><vh>@clean bogus_type.py</vh></v>
<v t="ekr.20220525082933.214"><vh>@clean build.py</vh>
<v t="ekr.20220525082933.215"><vh>class BuildResult</vh>
<v t="ekr.20220525082933.216"><vh>BuildResult.__init__</vh></v>
</v>
<v t="ekr.20220525082933.217"><vh>build</vh>
<v t="ekr.20220525082933.218"><vh>default_flush_errors</vh></v>
</v>
<v t="ekr.20220525082933.219"><vh>_build</vh></v>
<v t="ekr.20220525082933.220"><vh>default_data_dir</vh></v>
<v t="ekr.20220525082933.221"><vh>normpath</vh></v>
<v t="ekr.20220525082933.222"><vh>class CacheMeta</vh></v>
<v t="ekr.20220525082933.223"><vh>NOTE: dependencies + suppressed == all reachable imports;</vh></v>
<v t="ekr.20220525082933.224"><vh>cache_meta_from_dict</vh></v>
<v t="ekr.20220525082933.225"><vh>Priorities used for imports.  (Here, top-level includes inside a class.)</vh></v>
<v t="ekr.20220525082933.226"><vh>import_priority</vh></v>
<v t="ekr.20220525082933.227"><vh>load_plugins_from_config</vh>
<v t="ekr.20220525082933.228"><vh>plugin_error</vh></v>
</v>
<v t="ekr.20220525082933.229"><vh>load_plugins</vh></v>
<v t="ekr.20220525082933.230"><vh>take_module_snapshot</vh></v>
<v t="ekr.20220525082933.231"><vh>find_config_file_line_number</vh></v>
<v t="ekr.20220525082933.232"><vh>class BuildManager</vh>
<v t="ekr.20220525082933.233"><vh>BuildManager.__init__</vh></v>
<v t="ekr.20220525082933.234"><vh>BuildManager.dump_stats</vh></v>
<v t="ekr.20220525082933.235"><vh>BuildManager.use_fine_grained_cache</vh></v>
<v t="ekr.20220525082933.236"><vh>BuildManager.maybe_swap_for_shadow_path</vh></v>
<v t="ekr.20220525082933.237"><vh>BuildManager.get_stat</vh></v>
<v t="ekr.20220525082933.238"><vh>BuildManager.getmtime</vh></v>
<v t="ekr.20220525082933.239"><vh>BuildManager.all_imported_modules_in_file</vh></v>
<v t="ekr.20220525082933.240"><vh>BuildManager.is_module</vh></v>
<v t="ekr.20220525082933.241"><vh>BuildManager.parse_file</vh></v>
<v t="ekr.20220525082933.242"><vh>BuildManager.load_fine_grained_deps</vh></v>
<v t="ekr.20220525082933.243"><vh>BuildManager.report_file</vh></v>
<v t="ekr.20220525082933.244"><vh>BuildManager.verbosity</vh></v>
<v t="ekr.20220525082933.245"><vh>BuildManager.log</vh></v>
<v t="ekr.20220525082933.246"><vh>BuildManager.log_fine_grained</vh></v>
<v t="ekr.20220525082933.247"><vh>BuildManager.trace</vh></v>
<v t="ekr.20220525082933.248"><vh>BuildManager.add_stats</vh></v>
<v t="ekr.20220525082933.249"><vh>BuildManager.stats_summary</vh></v>
</v>
<v t="ekr.20220525082933.250"><vh>deps_to_json</vh></v>
<v t="ekr.20220525082933.251"><vh>File for storing metadata about all the fine-grained dependency caches</vh></v>
<v t="ekr.20220525082933.252"><vh>write_deps_cache</vh></v>
<v t="ekr.20220525082933.253"><vh>invert_deps</vh></v>
<v t="ekr.20220525082933.254"><vh>generate_deps_for_cache</vh></v>
<v t="ekr.20220525082933.255"><vh>PLUGIN_SNAPSHOT_FILE: Final = "@plugins_snapshot.json"</vh></v>
<v t="ekr.20220525082933.256"><vh>write_plugins_snapshot</vh></v>
<v t="ekr.20220525082933.257"><vh>read_plugins_snapshot</vh></v>
<v t="ekr.20220525082933.258"><vh>read_quickstart_file</vh></v>
<v t="ekr.20220525082933.259"><vh>read_deps_cache</vh></v>
<v t="ekr.20220525082933.260"><vh>_load_json_file</vh></v>
<v t="ekr.20220525082933.261"><vh>_cache_dir_prefix</vh></v>
<v t="ekr.20220525082933.262"><vh>add_catch_all_gitignore</vh></v>
<v t="ekr.20220525082933.263"><vh>exclude_from_backups</vh></v>
<v t="ekr.20220525082933.264"><vh>create_metastore</vh></v>
<v t="ekr.20220525082933.265"><vh>get_cache_names</vh></v>
<v t="ekr.20220525082933.266"><vh>find_cache_meta</vh></v>
<v t="ekr.20220525082933.267"><vh>validate_meta</vh></v>
<v t="ekr.20220525082933.268"><vh>compute_hash</vh></v>
<v t="ekr.20220525082933.269"><vh>json_dumps</vh></v>
<v t="ekr.20220525082933.270"><vh>write_cache</vh></v>
<v t="ekr.20220525082933.271"><vh>delete_cache</vh></v>
<v t="ekr.20220525082933.272"><vh>"""Dependency manager.</vh></v>
<v t="ekr.20220525082933.273"><vh>class ModuleNotFound</vh></v>
<v t="ekr.20220525082933.274"><vh>class State</vh>
<v t="ekr.20220525082933.275"><vh>State.__init__</vh></v>
<v t="ekr.20220525082933.276"><vh>State.xmeta</vh></v>
<v t="ekr.20220525082933.277"><vh>State.add_ancestors</vh></v>
<v t="ekr.20220525082933.278"><vh>State.is_fresh</vh></v>
<v t="ekr.20220525082933.279"><vh>State.is_interface_fresh</vh></v>
<v t="ekr.20220525082933.280"><vh>State.mark_as_rechecked</vh></v>
<v t="ekr.20220525082933.281"><vh>State.mark_interface_stale</vh></v>
<v t="ekr.20220525082933.282"><vh>State.check_blockers</vh></v>
<v t="ekr.20220525082933.283"><vh>State.wrap_context</vh></v>
<v t="ekr.20220525082933.284"><vh>State.load_fine_grained_deps</vh></v>
<v t="ekr.20220525082933.285"><vh>State.load_tree</vh></v>
<v t="ekr.20220525082933.286"><vh>State.fix_cross_refs</vh></v>
<v t="ekr.20220525082933.287"><vh>State.Methods for processing modules from source code.</vh></v>
<v t="ekr.20220525082933.288"><vh>State.parse_file</vh></v>
<v t="ekr.20220525082933.289"><vh>State.parse_inline_configuration</vh></v>
<v t="ekr.20220525082933.290"><vh>State.semantic_analysis_pass1</vh></v>
<v t="ekr.20220525082933.291"><vh>State.add_dependency</vh></v>
<v t="ekr.20220525082933.292"><vh>State.suppress_dependency</vh></v>
<v t="ekr.20220525082933.293"><vh>State.compute_dependencies</vh></v>
<v t="ekr.20220525082933.294"><vh>State.type_check_first_pass</vh></v>
<v t="ekr.20220525082933.295"><vh>State.type_checker</vh></v>
<v t="ekr.20220525082933.296"><vh>State.type_map</vh></v>
<v t="ekr.20220525082933.297"><vh>State.type_check_second_pass</vh></v>
<v t="ekr.20220525082933.298"><vh>State.finish_passes</vh></v>
<v t="ekr.20220525082933.299"><vh>State.free_state</vh></v>
<v t="ekr.20220525082933.300"><vh>State._patch_indirect_dependencies</vh></v>
<v t="ekr.20220525082933.301"><vh>State.compute_fine_grained_deps</vh></v>
<v t="ekr.20220525082933.302"><vh>State.update_fine_grained_deps</vh></v>
<v t="ekr.20220525082933.303"><vh>State.valid_references</vh></v>
<v t="ekr.20220525082933.304"><vh>State.write_cache</vh></v>
<v t="ekr.20220525082933.305"><vh>State.verify_dependencies</vh></v>
<v t="ekr.20220525082933.306"><vh>State.dependency_priorities</vh></v>
<v t="ekr.20220525082933.307"><vh>State.dependency_lines</vh></v>
<v t="ekr.20220525082933.308"><vh>State.generate_unused_ignore_notes</vh></v>
<v t="ekr.20220525082933.309"><vh>State.generate_ignore_without_code_notes</vh></v>
</v>
<v t="ekr.20220525082933.310"><vh>Module import and diagnostic glue</vh></v>
<v t="ekr.20220525082933.311"><vh>find_module_and_diagnose</vh></v>
<v t="ekr.20220525082933.312"><vh>exist_added_packages</vh></v>
<v t="ekr.20220525082933.313"><vh>find_module_simple</vh></v>
<v t="ekr.20220525082933.314"><vh>find_module_with_reason</vh></v>
<v t="ekr.20220525082933.315"><vh>in_partial_package</vh></v>
<v t="ekr.20220525082933.316"><vh>module_not_found</vh></v>
<v t="ekr.20220525082933.317"><vh>skipping_module</vh></v>
<v t="ekr.20220525082933.318"><vh>skipping_ancestor</vh></v>
<v t="ekr.20220525082933.319"><vh>log_configuration</vh></v>
<v t="ekr.20220525082933.320"><vh>The driver</vh></v>
<v t="ekr.20220525082933.321"><vh>dispatch</vh></v>
<v t="ekr.20220525082933.322"><vh>class NodeInfo</vh>
<v t="ekr.20220525082933.323"><vh>NodeInfo.__init__</vh></v>
<v t="ekr.20220525082933.324"><vh>NodeInfo.dumps</vh></v>
</v>
<v t="ekr.20220525082933.325"><vh>dump_timing_stats</vh></v>
<v t="ekr.20220525082933.326"><vh>dump_graph</vh></v>
<v t="ekr.20220525082933.327"><vh>load_graph</vh></v>
<v t="ekr.20220525082933.328"><vh>process_graph</vh></v>
<v t="ekr.20220525082933.329"><vh>order_ascc</vh></v>
<v t="ekr.20220525082933.330"><vh>process_fresh_modules</vh></v>
<v t="ekr.20220525082933.331"><vh>process_stale_scc</vh></v>
<v t="ekr.20220525082933.332"><vh>sorted_components</vh></v>
<v t="ekr.20220525082933.333"><vh>deps_filtered</vh></v>
<v t="ekr.20220525082933.334"><vh>strongly_connected_components</vh>
<v t="ekr.20220525082933.335"><vh>dfs</vh></v>
</v>
<v t="ekr.20220525082933.336"><vh>T = TypeVar("T")</vh></v>
<v t="ekr.20220525082933.337"><vh>topsort</vh></v>
<v t="ekr.20220525082933.338"><vh>missing_stubs_file</vh></v>
<v t="ekr.20220525082933.339"><vh>record_missing_stub_packages</vh></v>
</v>
<v t="ekr.20220525082933.340"><vh>@clean checker.py</vh>
<v t="ekr.20220525082933.341"><vh>class DeferredNode</vh></v>
<v t="ekr.20220525082933.342"><vh>class FineGrainedDeferredNode</vh></v>
<v t="ekr.20220525082933.343"><vh>Data structure returned by find_isinstance_check representing</vh></v>
<v t="ekr.20220525082933.344"><vh>class TypeRange</vh></v>
<v t="ekr.20220525082933.345"><vh>class PartialTypeScope</vh></v>
<v t="ekr.20220525082933.346"><vh>class TypeChecker</vh>
<v t="ekr.20220525082933.347"><vh>TypeChecker.__init__</vh></v>
<v t="ekr.20220525082933.348"><vh>TypeChecker.type_context</vh></v>
<v t="ekr.20220525082933.349"><vh>TypeChecker.reset</vh></v>
<v t="ekr.20220525082933.350"><vh>TypeChecker.check_first_pass</vh></v>
<v t="ekr.20220525082933.351"><vh>TypeChecker.check_second_pass</vh></v>
<v t="ekr.20220525082933.352"><vh>TypeChecker.check_partial</vh></v>
<v t="ekr.20220525082933.353"><vh>TypeChecker.check_top_level</vh></v>
<v t="ekr.20220525082933.354"><vh>TypeChecker.defer_node</vh></v>
<v t="ekr.20220525082933.355"><vh>TypeChecker.handle_cannot_determine_type</vh></v>
<v t="ekr.20220525082933.356"><vh>TypeChecker.accept</vh></v>
<v t="ekr.20220525082933.357"><vh>TypeChecker.accept_loop</vh></v>
<v t="ekr.20220525082933.358"><vh>TypeChecker.Definitions</vh></v>
<v t="ekr.20220525082933.359"><vh>TypeChecker.visit_overloaded_func_def</vh></v>
<v t="ekr.20220525082933.360"><vh>TypeChecker._visit_overloaded_func_def</vh></v>
<v t="ekr.20220525082933.361"><vh>TypeChecker.check_overlapping_overloads</vh></v>
<v t="ekr.20220525082933.362"><vh>TypeChecker.Here's the scoop about generators and coroutines.</vh></v>
<v t="ekr.20220525082933.363"><vh>TypeChecker.is_generator_return_type</vh></v>
<v t="ekr.20220525082933.364"><vh>TypeChecker.is_async_generator_return_type</vh></v>
<v t="ekr.20220525082933.365"><vh>TypeChecker.get_generator_yield_type</vh></v>
<v t="ekr.20220525082933.366"><vh>TypeChecker.get_generator_receive_type</vh></v>
<v t="ekr.20220525082933.367"><vh>TypeChecker.get_coroutine_return_type</vh></v>
<v t="ekr.20220525082933.368"><vh>TypeChecker.get_generator_return_type</vh></v>
<v t="ekr.20220525082933.369"><vh>TypeChecker.visit_func_def</vh></v>
<v t="ekr.20220525082933.370"><vh>TypeChecker._visit_func_def</vh></v>
<v t="ekr.20220525082933.371"><vh>TypeChecker.check_func_item</vh></v>
<v t="ekr.20220525082933.372"><vh>TypeChecker.enter_attribute_inference_context</vh></v>
<v t="ekr.20220525082933.373"><vh>TypeChecker.check_func_def</vh></v>
<v t="ekr.20220525082933.374"><vh>TypeChecker.check_default_args</vh></v>
<v t="ekr.20220525082933.375"><vh>TypeChecker.is_forward_op_method</vh></v>
<v t="ekr.20220525082933.376"><vh>TypeChecker.is_reverse_op_method</vh></v>
<v t="ekr.20220525082933.377"><vh>TypeChecker.check_for_missing_annotations</vh></v>
<v t="ekr.20220525082933.378"><vh>TypeChecker.check___new___signature</vh></v>
<v t="ekr.20220525082933.379"><vh>TypeChecker.is_trivial_body</vh></v>
<v t="ekr.20220525082933.380"><vh>TypeChecker.check_reverse_op_method</vh></v>
<v t="ekr.20220525082933.381"><vh>TypeChecker.check_overlapping_op_methods</vh></v>
<v t="ekr.20220525082933.382"><vh>TypeChecker.is_unsafe_overlapping_op</vh></v>
<v t="ekr.20220525082933.383"><vh>TypeChecker.check_inplace_operator_method</vh></v>
<v t="ekr.20220525082933.384"><vh>TypeChecker.check_getattr_method</vh></v>
<v t="ekr.20220525082933.385"><vh>TypeChecker.check_setattr_method</vh></v>
<v t="ekr.20220525082933.386"><vh>TypeChecker.check_slots_definition</vh></v>
<v t="ekr.20220525082933.387"><vh>TypeChecker.check_match_args</vh></v>
<v t="ekr.20220525082933.388"><vh>TypeChecker.expand_typevars</vh></v>
<v t="ekr.20220525082933.389"><vh>TypeChecker.check_method_override</vh></v>
<v t="ekr.20220525082933.390"><vh>TypeChecker.check_method_or_accessor_override_for_base</vh></v>
<v t="ekr.20220525082933.391"><vh>TypeChecker.check_method_override_for_base_with_name</vh></v>
<v t="ekr.20220525082933.392"><vh>TypeChecker.bind_and_map_method</vh></v>
<v t="ekr.20220525082933.393"><vh>TypeChecker.get_op_other_domain</vh></v>
<v t="ekr.20220525082933.394"><vh>TypeChecker.check_override</vh></v>
<v t="ekr.20220525082933.395"><vh>TypeChecker.check__exit__return_type</vh></v>
<v t="ekr.20220525082933.396"><vh>TypeChecker.visit_class_def</vh></v>
<v t="ekr.20220525082933.397"><vh>TypeChecker.check_final_deletable</vh></v>
<v t="ekr.20220525082933.398"><vh>TypeChecker.check_init_subclass</vh></v>
<v t="ekr.20220525082933.399"><vh>TypeChecker.check_enum</vh></v>
<v t="ekr.20220525082933.400"><vh>TypeChecker.check_final_enum</vh></v>
<v t="ekr.20220525082933.401"><vh>TypeChecker.is_final_enum_value</vh></v>
<v t="ekr.20220525082933.402"><vh>TypeChecker.check_enum_bases</vh></v>
<v t="ekr.20220525082933.403"><vh>TypeChecker.check_enum_new</vh></v>
<v t="ekr.20220525082933.404"><vh>TypeChecker.check_protocol_variance</vh></v>
<v t="ekr.20220525082933.405"><vh>TypeChecker.check_multiple_inheritance</vh></v>
<v t="ekr.20220525082933.406"><vh>TypeChecker.determine_type_of_class_member</vh></v>
<v t="ekr.20220525082933.407"><vh>TypeChecker.check_compatibility</vh></v>
<v t="ekr.20220525082933.408"><vh>TypeChecker.visit_import_from</vh></v>
<v t="ekr.20220525082933.409"><vh>TypeChecker.visit_import_all</vh></v>
<v t="ekr.20220525082933.410"><vh>TypeChecker.visit_import</vh></v>
<v t="ekr.20220525082933.411"><vh>TypeChecker.check_import</vh></v>
<v t="ekr.20220525082933.412"><vh>TypeChecker.Statements</vh></v>
<v t="ekr.20220525082933.413"><vh>TypeChecker.visit_block</vh></v>
<v t="ekr.20220525082933.414"><vh>TypeChecker.should_report_unreachable_issues</vh></v>
<v t="ekr.20220525082933.415"><vh>TypeChecker.is_raising_or_empty</vh></v>
<v t="ekr.20220525082933.416"><vh>TypeChecker.visit_assignment_stmt</vh></v>
<v t="ekr.20220525082933.417"><vh>TypeChecker.check_type_alias_rvalue</vh>
<v t="ekr.20220525082933.418"><vh>TypeChecker.accept_items</vh></v>
</v>
<v t="ekr.20220525082933.419"><vh>TypeChecker.check_assignment</vh></v>
<v t="ekr.20220525082933.420"><vh>TypeChecker.(type, operator) tuples for augmented assignments supported with partial types</vh></v>
<v t="ekr.20220525082933.421"><vh>TypeChecker.try_infer_partial_generic_type_from_assignment</vh></v>
<v t="ekr.20220525082933.422"><vh>TypeChecker.check_compatibility_all_supers</vh></v>
<v t="ekr.20220525082933.423"><vh>TypeChecker.check_compatibility_super</vh></v>
<v t="ekr.20220525082933.424"><vh>TypeChecker.lvalue_type_from_base</vh></v>
<v t="ekr.20220525082933.425"><vh>TypeChecker.check_compatibility_classvar_super</vh></v>
<v t="ekr.20220525082933.426"><vh>TypeChecker.check_compatibility_final_super</vh></v>
<v t="ekr.20220525082933.427"><vh>TypeChecker.check_if_final_var_override_writable</vh></v>
<v t="ekr.20220525082933.428"><vh>TypeChecker.get_final_context</vh></v>
<v t="ekr.20220525082933.429"><vh>TypeChecker.enter_final_context</vh></v>
<v t="ekr.20220525082933.430"><vh>TypeChecker.check_final</vh></v>
<v t="ekr.20220525082933.431"><vh>TypeChecker.check_assignment_to_slots</vh></v>
<v t="ekr.20220525082933.432"><vh>TypeChecker.is_assignable_slot</vh></v>
<v t="ekr.20220525082933.433"><vh>TypeChecker.check_assignment_to_multiple_lvalues</vh></v>
<v t="ekr.20220525082933.434"><vh>TypeChecker.check_rvalue_count_in_assignment</vh></v>
<v t="ekr.20220525082933.435"><vh>TypeChecker.check_multi_assignment</vh></v>
<v t="ekr.20220525082933.436"><vh>TypeChecker.check_multi_assignment_from_union</vh></v>
<v t="ekr.20220525082933.437"><vh>TypeChecker.flatten_lvalues</vh></v>
<v t="ekr.20220525082933.438"><vh>TypeChecker.check_multi_assignment_from_tuple</vh></v>
<v t="ekr.20220525082933.439"><vh>TypeChecker.lvalue_type_for_inference</vh></v>
<v t="ekr.20220525082933.440"><vh>TypeChecker.split_around_star</vh></v>
<v t="ekr.20220525082933.441"><vh>TypeChecker.type_is_iterable</vh></v>
<v t="ekr.20220525082933.442"><vh>TypeChecker.check_multi_assignment_from_iterable</vh></v>
<v t="ekr.20220525082933.443"><vh>TypeChecker.check_lvalue</vh></v>
<v t="ekr.20220525082933.444"><vh>TypeChecker.is_definition</vh></v>
<v t="ekr.20220525082933.445"><vh>TypeChecker.infer_variable_type</vh></v>
<v t="ekr.20220525082933.446"><vh>TypeChecker.infer_partial_type</vh></v>
<v t="ekr.20220525082933.447"><vh>TypeChecker.is_valid_defaultdict_partial_value_type</vh></v>
<v t="ekr.20220525082933.448"><vh>TypeChecker.set_inferred_type</vh></v>
<v t="ekr.20220525082933.449"><vh>TypeChecker.set_inference_error_fallback_type</vh></v>
<v t="ekr.20220525082933.450"><vh>TypeChecker.inference_error_fallback_type</vh></v>
<v t="ekr.20220525082933.451"><vh>TypeChecker.check_simple_assignment</vh></v>
<v t="ekr.20220525082933.452"><vh>TypeChecker.check_member_assignment</vh></v>
<v t="ekr.20220525082933.453"><vh>TypeChecker.check_indexed_assignment</vh></v>
<v t="ekr.20220525082933.454"><vh>TypeChecker.try_infer_partial_type_from_indexed_assignment</vh></v>
<v t="ekr.20220525082933.455"><vh>TypeChecker.type_requires_usage</vh></v>
<v t="ekr.20220525082933.456"><vh>TypeChecker.visit_expression_stmt</vh></v>
<v t="ekr.20220525082933.457"><vh>TypeChecker.visit_return_stmt</vh></v>
<v t="ekr.20220525082933.458"><vh>TypeChecker.check_return_stmt</vh></v>
<v t="ekr.20220525082933.459"><vh>TypeChecker.visit_if_stmt</vh></v>
<v t="ekr.20220525082933.460"><vh>TypeChecker.visit_while_stmt</vh></v>
<v t="ekr.20220525082933.461"><vh>TypeChecker.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20220525082933.462"><vh>TypeChecker.visit_assert_stmt</vh></v>
<v t="ekr.20220525082933.463"><vh>TypeChecker.visit_raise_stmt</vh></v>
<v t="ekr.20220525082933.464"><vh>TypeChecker.type_check_raise</vh></v>
<v t="ekr.20220525082933.465"><vh>TypeChecker._type_check_raise_python2</vh></v>
<v t="ekr.20220525082933.466"><vh>TypeChecker.visit_try_stmt</vh></v>
<v t="ekr.20220525082933.467"><vh>TypeChecker.visit_try_without_finally</vh></v>
<v t="ekr.20220525082933.468"><vh>TypeChecker.check_except_handler_test</vh></v>
<v t="ekr.20220525082933.469"><vh>TypeChecker.get_types_from_except_handler</vh></v>
<v t="ekr.20220525082933.470"><vh>TypeChecker.visit_for_stmt</vh></v>
<v t="ekr.20220525082933.471"><vh>TypeChecker.analyze_async_iterable_item_type</vh></v>
<v t="ekr.20220525082933.472"><vh>TypeChecker.analyze_iterable_item_type</vh></v>
<v t="ekr.20220525082933.473"><vh>TypeChecker.analyze_container_item_type</vh></v>
<v t="ekr.20220525082933.474"><vh>TypeChecker.analyze_index_variables</vh></v>
<v t="ekr.20220525082933.475"><vh>TypeChecker.visit_del_stmt</vh></v>
<v t="ekr.20220525082933.476"><vh>TypeChecker.visit_decorator</vh></v>
<v t="ekr.20220525082933.477"><vh>TypeChecker.check_for_untyped_decorator</vh></v>
<v t="ekr.20220525082933.478"><vh>TypeChecker.check_incompatible_property_override</vh></v>
<v t="ekr.20220525082933.479"><vh>TypeChecker.visit_with_stmt</vh></v>
<v t="ekr.20220525082933.480"><vh>TypeChecker.check_untyped_after_decorator</vh></v>
<v t="ekr.20220525082933.481"><vh>TypeChecker.check_async_with_item</vh></v>
<v t="ekr.20220525082933.482"><vh>TypeChecker.check_with_item</vh></v>
<v t="ekr.20220525082933.483"><vh>TypeChecker.visit_print_stmt</vh></v>
<v t="ekr.20220525082933.484"><vh>TypeChecker.visit_break_stmt</vh></v>
<v t="ekr.20220525082933.485"><vh>TypeChecker.visit_continue_stmt</vh></v>
<v t="ekr.20220525082933.486"><vh>TypeChecker.visit_match_stmt</vh></v>
<v t="ekr.20220525082933.487"><vh>TypeChecker.infer_variable_types_from_type_maps</vh></v>
<v t="ekr.20220525082933.488"><vh>TypeChecker.remove_capture_conflicts</vh></v>
<v t="ekr.20220525082933.489"><vh>TypeChecker.make_fake_typeinfo</vh></v>
<v t="ekr.20220525082933.490"><vh>TypeChecker.intersect_instances</vh></v>
<v t="ekr.20220525082933.491"><vh>TypeChecker.intersect_instance_callable</vh></v>
<v t="ekr.20220525082933.492"><vh>TypeChecker.make_fake_callable</vh></v>
<v t="ekr.20220525082933.493"><vh>TypeChecker.partition_by_callable</vh></v>
<v t="ekr.20220525082933.494"><vh>TypeChecker.conditional_callable_type_map</vh></v>
<v t="ekr.20220525082933.495"><vh>TypeChecker._is_truthy_type</vh></v>
<v t="ekr.20220525082933.496"><vh>TypeChecker._check_for_truthy_type</vh></v>
<v t="ekr.20220525082933.497"><vh>TypeChecker.find_type_equals_check</vh></v>
<v t="ekr.20220525082933.498"><vh>TypeChecker.find_isinstance_check</vh></v>
<v t="ekr.20220525082933.499"><vh>TypeChecker.find_isinstance_check_helper</vh></v>
<v t="ekr.20220525082933.500"><vh>TypeChecker.propagate_up_typemap_info</vh></v>
<v t="ekr.20220525082933.501"><vh>TypeChecker.refine_parent_types</vh></v>
<v t="ekr.20220525082933.502"><vh>TypeChecker.refine_identity_comparison_expression</vh></v>
<v t="ekr.20220525082933.503"><vh>TypeChecker.refine_away_none_in_comparison</vh></v>
<v t="ekr.20220525082933.504"><vh>TypeChecker.Helpers</vh></v>
<v t="ekr.20220525082933.505"><vh>TypeChecker.check_subtype</vh></v>
<v t="ekr.20220525082933.506"><vh>TypeChecker.contains_none</vh></v>
<v t="ekr.20220525082933.507"><vh>TypeChecker.should_suppress_optional_error</vh></v>
<v t="ekr.20220525082933.508"><vh>TypeChecker.named_type</vh></v>
<v t="ekr.20220525082933.509"><vh>TypeChecker.named_generic_type</vh></v>
<v t="ekr.20220525082933.510"><vh>TypeChecker.lookup_typeinfo</vh></v>
<v t="ekr.20220525082933.511"><vh>TypeChecker.type_type</vh></v>
<v t="ekr.20220525082933.512"><vh>TypeChecker.str_type</vh></v>
<v t="ekr.20220525082933.513"><vh>TypeChecker.store_type</vh></v>
<v t="ekr.20220525082933.514"><vh>TypeChecker.has_type</vh></v>
<v t="ekr.20220525082933.515"><vh>TypeChecker.lookup_type_or_none</vh></v>
<v t="ekr.20220525082933.516"><vh>TypeChecker.lookup_type</vh></v>
<v t="ekr.20220525082933.517"><vh>TypeChecker.store_types</vh></v>
<v t="ekr.20220525082933.518"><vh>TypeChecker.local_type_map</vh></v>
<v t="ekr.20220525082933.519"><vh>TypeChecker.in_checked_function</vh></v>
<v t="ekr.20220525082933.520"><vh>TypeChecker.lookup</vh></v>
<v t="ekr.20220525082933.521"><vh>TypeChecker.lookup_qualified</vh></v>
<v t="ekr.20220525082933.522"><vh>TypeChecker.enter_partial_types</vh></v>
<v t="ekr.20220525082933.523"><vh>TypeChecker.handle_partial_var_type</vh></v>
<v t="ekr.20220525082933.524"><vh>TypeChecker.fixup_partial_type</vh></v>
<v t="ekr.20220525082933.525"><vh>TypeChecker.is_defined_in_base_class</vh></v>
<v t="ekr.20220525082933.526"><vh>TypeChecker.find_partial_types</vh></v>
<v t="ekr.20220525082933.527"><vh>TypeChecker.find_partial_types_in_all_scopes</vh></v>
<v t="ekr.20220525082933.528"><vh>TypeChecker.temp_node</vh></v>
<v t="ekr.20220525082933.529"><vh>TypeChecker.fail</vh></v>
<v t="ekr.20220525082933.530"><vh>TypeChecker.note</vh></v>
<v t="ekr.20220525082933.531"><vh>TypeChecker.iterable_item_type</vh></v>
<v t="ekr.20220525082933.532"><vh>TypeChecker.function_type</vh></v>
<v t="ekr.20220525082933.533"><vh>TypeChecker.push_type_map</vh></v>
<v t="ekr.20220525082933.534"><vh>TypeChecker.infer_issubclass_maps</vh></v>
<v t="ekr.20220525082933.535"><vh>TypeChecker.conditional_types_with_intersection</vh></v>
<v t="ekr.20220525082933.536"><vh>TypeChecker.conditional_types_with_intersection</vh></v>
<v t="ekr.20220525082933.537"><vh>TypeChecker.conditional_types_with_intersection</vh></v>
<v t="ekr.20220525082933.538"><vh>TypeChecker.is_writable_attribute</vh></v>
<v t="ekr.20220525082933.539"><vh>TypeChecker.get_isinstance_type</vh></v>
<v t="ekr.20220525082933.540"><vh>TypeChecker.is_literal_enum</vh></v>
</v>
<v t="ekr.20220525082933.541"><vh>conditional_types</vh></v>
<v t="ekr.20220525082933.542"><vh>conditional_types</vh></v>
<v t="ekr.20220525082933.543"><vh>conditional_types</vh></v>
<v t="ekr.20220525082933.544"><vh>conditional_types_to_typemaps</vh></v>
<v t="ekr.20220525082933.545"><vh>gen_unique_name</vh></v>
<v t="ekr.20220525082933.546"><vh>is_true_literal</vh></v>
<v t="ekr.20220525082933.547"><vh>is_false_literal</vh></v>
<v t="ekr.20220525082933.548"><vh>is_literal_none</vh></v>
<v t="ekr.20220525082933.549"><vh>is_literal_not_implemented</vh></v>
<v t="ekr.20220525082933.550"><vh>builtin_item_type</vh></v>
<v t="ekr.20220525082933.551"><vh>and_conditional_maps</vh></v>
<v t="ekr.20220525082933.552"><vh>or_conditional_maps</vh></v>
<v t="ekr.20220525082933.553"><vh>reduce_conditional_maps</vh></v>
<v t="ekr.20220525082933.554"><vh>convert_to_typetype</vh></v>
<v t="ekr.20220525082933.555"><vh>flatten</vh></v>
<v t="ekr.20220525082933.556"><vh>flatten_types</vh></v>
<v t="ekr.20220525082933.557"><vh>expand_func</vh></v>
<v t="ekr.20220525082933.558"><vh>class TypeTransformVisitor</vh></v>
<v t="ekr.20220525082933.559"><vh>are_argument_counts_overlapping</vh></v>
<v t="ekr.20220525082933.560"><vh>is_unsafe_overlapping_overload_signatures</vh></v>
<v t="ekr.20220525082933.561"><vh>detach_callable</vh></v>
<v t="ekr.20220525082933.562"><vh>overload_can_never_match</vh></v>
<v t="ekr.20220525082933.563"><vh>is_more_general_arg_prefix</vh></v>
<v t="ekr.20220525082933.564"><vh>is_same_arg_prefix</vh></v>
<v t="ekr.20220525082933.565"><vh>infer_operator_assignment_method</vh></v>
<v t="ekr.20220525082933.566"><vh>is_valid_inferred_type</vh></v>
<v t="ekr.20220525082933.567"><vh>class NothingSeeker</vh>
<v t="ekr.20220525082933.568"><vh>NothingSeeker.__init__</vh></v>
<v t="ekr.20220525082933.569"><vh>NothingSeeker.visit_uninhabited_type</vh></v>
</v>
<v t="ekr.20220525082933.570"><vh>class SetNothingToAny</vh>
<v t="ekr.20220525082933.571"><vh>SetNothingToAny.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082933.572"><vh>SetNothingToAny.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20220525082933.573"><vh>is_node_static</vh></v>
<v t="ekr.20220525082933.574"><vh>class CheckerScope</vh>
<v t="ekr.20220525082933.575"><vh>CheckerScope.__init__</vh></v>
<v t="ekr.20220525082933.576"><vh>CheckerScope.top_function</vh></v>
<v t="ekr.20220525082933.577"><vh>CheckerScope.top_non_lambda_function</vh></v>
<v t="ekr.20220525082933.578"><vh>CheckerScope.active_class</vh></v>
<v t="ekr.20220525082933.579"><vh>CheckerScope.enclosing_class</vh></v>
<v t="ekr.20220525082933.580"><vh>CheckerScope.active_self_type</vh></v>
<v t="ekr.20220525082933.581"><vh>CheckerScope.push_function</vh></v>
<v t="ekr.20220525082933.582"><vh>CheckerScope.push_class</vh></v>
</v>
<v t="ekr.20220525082933.583"><vh>TKey = TypeVar('TKey')</vh></v>
<v t="ekr.20220525082933.584"><vh>class DisjointDict</vh>
<v t="ekr.20220525082933.585"><vh>DisjointDict.__init__</vh></v>
<v t="ekr.20220525082933.586"><vh>DisjointDict.add_mapping</vh></v>
<v t="ekr.20220525082933.587"><vh>DisjointDict.items</vh></v>
<v t="ekr.20220525082933.588"><vh>DisjointDict._lookup_or_make_root_id</vh></v>
<v t="ekr.20220525082933.589"><vh>DisjointDict._lookup_root_id</vh></v>
</v>
<v t="ekr.20220525082933.590"><vh>group_comparison_operands</vh></v>
<v t="ekr.20220525082933.591"><vh>is_typed_callable</vh></v>
<v t="ekr.20220525082933.592"><vh>is_untyped_decorator</vh></v>
<v t="ekr.20220525082933.593"><vh>is_static</vh></v>
<v t="ekr.20220525082933.594"><vh>is_subtype_no_promote</vh></v>
<v t="ekr.20220525082933.595"><vh>is_overlapping_types_no_promote</vh></v>
<v t="ekr.20220525082933.596"><vh>is_private</vh></v>
<v t="ekr.20220525082933.597"><vh>is_string_literal</vh></v>
<v t="ekr.20220525082933.598"><vh>has_bool_item</vh></v>
<v t="ekr.20220525082933.599"><vh>collapse_walrus</vh></v>
</v>
<v t="ekr.20220525082933.600"><vh>@clean checkexpr.py</vh>
<v t="ekr.20220525082933.601"><vh>class TooManyUnions</vh></v>
<v t="ekr.20220525082933.602"><vh>allow_fast_container_literal</vh></v>
<v t="ekr.20220525082933.603"><vh>extract_refexpr_names</vh></v>
<v t="ekr.20220525082933.604"><vh>class Finished</vh></v>
<v t="ekr.20220525082933.605"><vh>class ExpressionChecker</vh>
<v t="ekr.20220525082933.606"><vh>ExpressionChecker.__init__</vh></v>
<v t="ekr.20220525082933.607"><vh>ExpressionChecker.reset</vh></v>
<v t="ekr.20220525082933.608"><vh>ExpressionChecker.visit_name_expr</vh></v>
<v t="ekr.20220525082933.609"><vh>ExpressionChecker.analyze_ref_expr</vh></v>
<v t="ekr.20220525082933.610"><vh>ExpressionChecker.analyze_var_ref</vh></v>
<v t="ekr.20220525082933.611"><vh>ExpressionChecker.visit_call_expr</vh></v>
<v t="ekr.20220525082933.612"><vh>ExpressionChecker.visit_call_expr_inner</vh></v>
<v t="ekr.20220525082933.613"><vh>ExpressionChecker.check_str_format_call</vh></v>
<v t="ekr.20220525082933.614"><vh>ExpressionChecker.method_fullname</vh></v>
<v t="ekr.20220525082933.615"><vh>ExpressionChecker.always_returns_none</vh></v>
<v t="ekr.20220525082933.616"><vh>ExpressionChecker.defn_returns_none</vh></v>
<v t="ekr.20220525082933.617"><vh>ExpressionChecker.check_runtime_protocol_test</vh></v>
<v t="ekr.20220525082933.618"><vh>ExpressionChecker.check_protocol_issubclass</vh></v>
<v t="ekr.20220525082933.619"><vh>ExpressionChecker.check_typeddict_call</vh></v>
<v t="ekr.20220525082933.620"><vh>ExpressionChecker.validate_typeddict_kwargs</vh></v>
<v t="ekr.20220525082933.621"><vh>ExpressionChecker.match_typeddict_call_with_dict</vh></v>
<v t="ekr.20220525082933.622"><vh>ExpressionChecker.check_typeddict_call_with_dict</vh></v>
<v t="ekr.20220525082933.623"><vh>ExpressionChecker.check_typeddict_call_with_kwargs</vh></v>
<v t="ekr.20220525082933.624"><vh>ExpressionChecker.get_partial_self_var</vh></v>
<v t="ekr.20220525082933.625"><vh>ExpressionChecker.Types and methods that can be used to infer partial types.</vh></v>
<v t="ekr.20220525082933.626"><vh>ExpressionChecker.try_infer_partial_type</vh></v>
<v t="ekr.20220525082933.627"><vh>ExpressionChecker.get_partial_var</vh></v>
<v t="ekr.20220525082933.628"><vh>ExpressionChecker.try_infer_partial_value_type_from_call</vh></v>
<v t="ekr.20220525082933.629"><vh>ExpressionChecker.apply_function_plugin</vh></v>
<v t="ekr.20220525082933.630"><vh>ExpressionChecker.apply_signature_hook</vh></v>
<v t="ekr.20220525082933.631"><vh>ExpressionChecker.apply_function_signature_hook</vh></v>
<v t="ekr.20220525082933.632"><vh>ExpressionChecker.apply_method_signature_hook</vh></v>
<v t="ekr.20220525082933.633"><vh>ExpressionChecker.transform_callee_type</vh></v>
<v t="ekr.20220525082933.634"><vh>ExpressionChecker.check_call_expr_with_callee_type</vh></v>
<v t="ekr.20220525082933.635"><vh>ExpressionChecker.check_union_call_expr</vh></v>
<v t="ekr.20220525082933.636"><vh>ExpressionChecker.check_call</vh></v>
<v t="ekr.20220525082933.637"><vh>ExpressionChecker.check_callable_call</vh></v>
<v t="ekr.20220525082933.638"><vh>ExpressionChecker.analyze_type_type_callee</vh></v>
<v t="ekr.20220525082933.639"><vh>ExpressionChecker.infer_arg_types_in_empty_context</vh></v>
<v t="ekr.20220525082933.640"><vh>ExpressionChecker.infer_arg_types_in_context</vh></v>
<v t="ekr.20220525082933.641"><vh>ExpressionChecker.infer_function_type_arguments_using_context</vh></v>
<v t="ekr.20220525082933.642"><vh>ExpressionChecker.infer_function_type_arguments</vh></v>
<v t="ekr.20220525082933.643"><vh>ExpressionChecker.infer_function_type_arguments_pass2</vh></v>
<v t="ekr.20220525082933.644"><vh>ExpressionChecker.argument_infer_context</vh></v>
<v t="ekr.20220525082933.645"><vh>ExpressionChecker.get_arg_infer_passes</vh></v>
<v t="ekr.20220525082933.646"><vh>ExpressionChecker.apply_inferred_arguments</vh></v>
<v t="ekr.20220525082933.647"><vh>ExpressionChecker.check_argument_count</vh></v>
<v t="ekr.20220525082933.648"><vh>ExpressionChecker.check_for_extra_actual_arguments</vh></v>
<v t="ekr.20220525082933.649"><vh>ExpressionChecker.check_argument_types</vh></v>
<v t="ekr.20220525082933.650"><vh>ExpressionChecker.check_arg</vh></v>
<v t="ekr.20220525082933.651"><vh>ExpressionChecker.check_overload_call</vh></v>
<v t="ekr.20220525082933.652"><vh>ExpressionChecker.plausible_overload_call_targets</vh></v>
<v t="ekr.20220525082933.653"><vh>ExpressionChecker.infer_overload_return_type</vh></v>
<v t="ekr.20220525082933.654"><vh>ExpressionChecker.overload_erased_call_targets</vh></v>
<v t="ekr.20220525082933.655"><vh>ExpressionChecker.union_overload_result</vh></v>
<v t="ekr.20220525082933.656"><vh>ExpressionChecker.real_union</vh></v>
<v t="ekr.20220525082933.657"><vh>ExpressionChecker.type_overrides_set</vh></v>
<v t="ekr.20220525082933.658"><vh>ExpressionChecker.combine_function_signatures</vh></v>
<v t="ekr.20220525082933.659"><vh>ExpressionChecker.erased_signature_similarity</vh></v>
<v t="ekr.20220525082933.660"><vh>ExpressionChecker.apply_generic_arguments</vh></v>
<v t="ekr.20220525082933.661"><vh>ExpressionChecker.check_any_type_call</vh></v>
<v t="ekr.20220525082933.662"><vh>ExpressionChecker.check_union_call</vh></v>
<v t="ekr.20220525082933.663"><vh>ExpressionChecker.visit_member_expr</vh></v>
<v t="ekr.20220525082933.664"><vh>ExpressionChecker.analyze_ordinary_member_access</vh></v>
<v t="ekr.20220525082933.665"><vh>ExpressionChecker.analyze_external_member_access</vh></v>
<v t="ekr.20220525082933.666"><vh>ExpressionChecker.is_literal_context</vh></v>
<v t="ekr.20220525082933.667"><vh>ExpressionChecker.infer_literal_expr_type</vh></v>
<v t="ekr.20220525082933.668"><vh>ExpressionChecker.concat_tuples</vh></v>
<v t="ekr.20220525082933.669"><vh>ExpressionChecker.visit_int_expr</vh></v>
<v t="ekr.20220525082933.670"><vh>ExpressionChecker.visit_str_expr</vh></v>
<v t="ekr.20220525082933.671"><vh>ExpressionChecker.visit_bytes_expr</vh></v>
<v t="ekr.20220525082933.672"><vh>ExpressionChecker.visit_unicode_expr</vh></v>
<v t="ekr.20220525082933.673"><vh>ExpressionChecker.visit_float_expr</vh></v>
<v t="ekr.20220525082933.674"><vh>ExpressionChecker.visit_complex_expr</vh></v>
<v t="ekr.20220525082933.675"><vh>ExpressionChecker.visit_ellipsis</vh></v>
<v t="ekr.20220525082933.676"><vh>ExpressionChecker.visit_op_expr</vh></v>
<v t="ekr.20220525082933.677"><vh>ExpressionChecker.visit_comparison_expr</vh></v>
<v t="ekr.20220525082933.678"><vh>ExpressionChecker.find_partial_type_ref_fast_path</vh></v>
<v t="ekr.20220525082933.679"><vh>ExpressionChecker.dangerous_comparison</vh></v>
<v t="ekr.20220525082933.680"><vh>ExpressionChecker.get_operator_method</vh></v>
<v t="ekr.20220525082933.681"><vh>ExpressionChecker.check_method_call_by_name</vh></v>
<v t="ekr.20220525082933.682"><vh>ExpressionChecker.check_union_method_call_by_name</vh></v>
<v t="ekr.20220525082933.683"><vh>ExpressionChecker.check_method_call</vh></v>
<v t="ekr.20220525082933.684"><vh>ExpressionChecker.check_op_reversible</vh></v>
<v t="ekr.20220525082933.685"><vh>ExpressionChecker.check_op</vh></v>
<v t="ekr.20220525082933.686"><vh>ExpressionChecker.get_reverse_op_method</vh></v>
<v t="ekr.20220525082933.687"><vh>ExpressionChecker.check_boolean_op</vh></v>
<v t="ekr.20220525082933.688"><vh>ExpressionChecker.check_list_multiply</vh></v>
<v t="ekr.20220525082933.689"><vh>ExpressionChecker.visit_assignment_expr</vh></v>
<v t="ekr.20220525082933.690"><vh>ExpressionChecker.visit_unary_expr</vh></v>
<v t="ekr.20220525082933.691"><vh>ExpressionChecker.visit_index_expr</vh></v>
<v t="ekr.20220525082933.692"><vh>ExpressionChecker.visit_index_expr_helper</vh></v>
<v t="ekr.20220525082933.693"><vh>ExpressionChecker.visit_index_with_type</vh></v>
<v t="ekr.20220525082933.694"><vh>ExpressionChecker.visit_tuple_slice_helper</vh></v>
<v t="ekr.20220525082933.695"><vh>ExpressionChecker.try_getting_int_literals</vh></v>
<v t="ekr.20220525082933.696"><vh>ExpressionChecker.nonliteral_tuple_index_helper</vh></v>
<v t="ekr.20220525082933.697"><vh>ExpressionChecker.visit_typeddict_index_expr</vh></v>
<v t="ekr.20220525082933.698"><vh>ExpressionChecker.visit_enum_index_expr</vh></v>
<v t="ekr.20220525082933.699"><vh>ExpressionChecker.visit_cast_expr</vh></v>
<v t="ekr.20220525082933.700"><vh>ExpressionChecker.visit_assert_type_expr</vh></v>
<v t="ekr.20220525082933.701"><vh>ExpressionChecker.visit_reveal_expr</vh></v>
<v t="ekr.20220525082933.702"><vh>ExpressionChecker.visit_type_application</vh></v>
<v t="ekr.20220525082933.703"><vh>ExpressionChecker.visit_type_alias_expr</vh></v>
<v t="ekr.20220525082933.704"><vh>ExpressionChecker.alias_type_in_runtime_context</vh></v>
<v t="ekr.20220525082933.705"><vh>ExpressionChecker.apply_type_arguments_to_callable</vh></v>
<v t="ekr.20220525082933.706"><vh>ExpressionChecker.visit_list_expr</vh></v>
<v t="ekr.20220525082933.707"><vh>ExpressionChecker.visit_set_expr</vh></v>
<v t="ekr.20220525082933.708"><vh>ExpressionChecker.fast_container_type</vh></v>
<v t="ekr.20220525082933.709"><vh>ExpressionChecker.check_lst_expr</vh></v>
<v t="ekr.20220525082933.710"><vh>ExpressionChecker.visit_tuple_expr</vh></v>
<v t="ekr.20220525082933.711"><vh>ExpressionChecker.fast_dict_type</vh></v>
<v t="ekr.20220525082933.712"><vh>ExpressionChecker.visit_dict_expr</vh></v>
<v t="ekr.20220525082933.713"><vh>ExpressionChecker.find_typeddict_context</vh></v>
<v t="ekr.20220525082933.714"><vh>ExpressionChecker.visit_lambda_expr</vh></v>
<v t="ekr.20220525082933.715"><vh>ExpressionChecker.infer_lambda_type_using_context</vh></v>
<v t="ekr.20220525082933.716"><vh>ExpressionChecker.visit_super_expr</vh></v>
<v t="ekr.20220525082933.717"><vh>ExpressionChecker._super_arg_types</vh></v>
<v t="ekr.20220525082933.718"><vh>ExpressionChecker.visit_slice_expr</vh></v>
<v t="ekr.20220525082933.719"><vh>ExpressionChecker.visit_list_comprehension</vh></v>
<v t="ekr.20220525082933.720"><vh>ExpressionChecker.visit_set_comprehension</vh></v>
<v t="ekr.20220525082933.721"><vh>ExpressionChecker.visit_generator_expr</vh></v>
<v t="ekr.20220525082933.722"><vh>ExpressionChecker.check_generator_or_comprehension</vh></v>
<v t="ekr.20220525082933.723"><vh>ExpressionChecker.visit_dictionary_comprehension</vh></v>
<v t="ekr.20220525082933.724"><vh>ExpressionChecker.check_for_comp</vh></v>
<v t="ekr.20220525082933.725"><vh>ExpressionChecker.visit_conditional_expr</vh></v>
<v t="ekr.20220525082933.726"><vh>ExpressionChecker.analyze_cond_branch</vh></v>
<v t="ekr.20220525082933.727"><vh>ExpressionChecker.visit_backquote_expr</vh></v>
<v t="ekr.20220525082933.728"><vh>ExpressionChecker.Helpers</vh></v>
<v t="ekr.20220525082933.729"><vh>ExpressionChecker.accept</vh></v>
<v t="ekr.20220525082933.730"><vh>ExpressionChecker.named_type</vh></v>
<v t="ekr.20220525082933.731"><vh>ExpressionChecker.is_valid_var_arg</vh></v>
<v t="ekr.20220525082933.732"><vh>ExpressionChecker.is_valid_keyword_var_arg</vh></v>
<v t="ekr.20220525082933.733"><vh>ExpressionChecker.has_member</vh></v>
<v t="ekr.20220525082933.734"><vh>ExpressionChecker.not_ready_callback</vh></v>
<v t="ekr.20220525082933.735"><vh>ExpressionChecker.visit_yield_expr</vh></v>
<v t="ekr.20220525082933.736"><vh>ExpressionChecker.visit_await_expr</vh></v>
<v t="ekr.20220525082933.737"><vh>ExpressionChecker.check_awaitable_expr</vh></v>
<v t="ekr.20220525082933.738"><vh>ExpressionChecker.visit_yield_from_expr</vh></v>
<v t="ekr.20220525082933.739"><vh>ExpressionChecker.visit_temp_node</vh></v>
<v t="ekr.20220525082933.740"><vh>ExpressionChecker.visit_type_var_expr</vh></v>
<v t="ekr.20220525082933.741"><vh>ExpressionChecker.visit_paramspec_expr</vh></v>
<v t="ekr.20220525082933.742"><vh>ExpressionChecker.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20220525082933.743"><vh>ExpressionChecker.visit_newtype_expr</vh></v>
<v t="ekr.20220525082933.744"><vh>ExpressionChecker.visit_namedtuple_expr</vh></v>
<v t="ekr.20220525082933.745"><vh>ExpressionChecker.visit_enum_call_expr</vh></v>
<v t="ekr.20220525082933.746"><vh>ExpressionChecker.visit_typeddict_expr</vh></v>
<v t="ekr.20220525082933.747"><vh>ExpressionChecker.visit__promote_expr</vh></v>
<v t="ekr.20220525082933.748"><vh>ExpressionChecker.visit_star_expr</vh></v>
<v t="ekr.20220525082933.749"><vh>ExpressionChecker.object_type</vh></v>
<v t="ekr.20220525082933.750"><vh>ExpressionChecker.bool_type</vh></v>
<v t="ekr.20220525082933.751"><vh>ExpressionChecker.narrow_type_from_binder</vh></v>
<v t="ekr.20220525082933.752"><vh>ExpressionChecker.narrow_type_from_binder</vh></v>
<v t="ekr.20220525082933.753"><vh>ExpressionChecker.narrow_type_from_binder</vh></v>
</v>
<v t="ekr.20220525082933.754"><vh>has_any_type</vh></v>
<v t="ekr.20220525082933.755"><vh>class HasAnyType</vh>
<v t="ekr.20220525082933.756"><vh>HasAnyType.__init__</vh></v>
<v t="ekr.20220525082933.757"><vh>HasAnyType.visit_any</vh></v>
<v t="ekr.20220525082933.758"><vh>HasAnyType.visit_callable_type</vh></v>
</v>
<v t="ekr.20220525082933.759"><vh>has_coroutine_decorator</vh></v>
<v t="ekr.20220525082933.760"><vh>is_async_def</vh></v>
<v t="ekr.20220525082933.761"><vh>is_non_empty_tuple</vh></v>
<v t="ekr.20220525082933.762"><vh>is_duplicate_mapping</vh></v>
<v t="ekr.20220525082933.763"><vh>replace_callable_return_type</vh></v>
<v t="ekr.20220525082933.764"><vh>class ArgInferSecondPassQuery</vh>
<v t="ekr.20220525082933.765"><vh>ArgInferSecondPassQuery.__init__</vh></v>
<v t="ekr.20220525082933.766"><vh>ArgInferSecondPassQuery.visit_callable_type</vh></v>
</v>
<v t="ekr.20220525082933.767"><vh>class HasTypeVarQuery</vh></v>
<v t="ekr.20220525082933.768"><vh>has_erased_component</vh></v>
<v t="ekr.20220525082933.769"><vh>class HasErasedComponentsQuery</vh></v>
<v t="ekr.20220525082933.770"><vh>has_uninhabited_component</vh></v>
<v t="ekr.20220525082933.771"><vh>class HasUninhabitedComponentsQuery</vh></v>
<v t="ekr.20220525082933.772"><vh>arg_approximate_similarity</vh>
<v t="ekr.20220525082933.773"><vh>is_typetype_like</vh></v>
</v>
<v t="ekr.20220525082933.774"><vh>any_causes_overload_ambiguity</vh></v>
<v t="ekr.20220525082933.775"><vh>all_same_types</vh></v>
<v t="ekr.20220525082933.776"><vh>merge_typevars_in_callables_by_name</vh></v>
<v t="ekr.20220525082933.777"><vh>try_getting_literal</vh></v>
<v t="ekr.20220525082933.778"><vh>is_expr_literal_type</vh></v>
<v t="ekr.20220525082933.779"><vh>has_bytes_component</vh></v>
<v t="ekr.20220525082933.780"><vh>type_info_from_type</vh></v>
<v t="ekr.20220525082933.781"><vh>is_operator_method</vh></v>
<v t="ekr.20220525082933.782"><vh>get_partial_instance_type</vh></v>
</v>
<v t="ekr.20220525082933.783"><vh>@clean checkmember.py</vh>
<v t="ekr.20220525082933.784"><vh>class MemberContext</vh>
<v t="ekr.20220525082933.785"><vh>MemberContext.__init__</vh></v>
<v t="ekr.20220525082933.786"><vh>MemberContext.named_type</vh></v>
<v t="ekr.20220525082933.787"><vh>MemberContext.not_ready_callback</vh></v>
<v t="ekr.20220525082933.788"><vh>MemberContext.copy_modified</vh></v>
</v>
<v t="ekr.20220525082933.789"><vh>analyze_member_access</vh></v>
<v t="ekr.20220525082933.790"><vh>_analyze_member_access</vh></v>
<v t="ekr.20220525082933.791"><vh>The several functions that follow implement analyze_member_access for various</vh></v>
<v t="ekr.20220525082933.792"><vh>analyze_instance_member_access</vh></v>
<v t="ekr.20220525082933.793"><vh>analyze_type_callable_member_access</vh></v>
<v t="ekr.20220525082933.794"><vh>analyze_type_type_member_access</vh></v>
<v t="ekr.20220525082933.795"><vh>analyze_union_member_access</vh></v>
<v t="ekr.20220525082933.796"><vh>analyze_none_member_access</vh></v>
<v t="ekr.20220525082933.797"><vh>analyze_member_var_access</vh></v>
<v t="ekr.20220525082933.798"><vh>check_final_member</vh></v>
<v t="ekr.20220525082933.799"><vh>analyze_descriptor_access</vh></v>
<v t="ekr.20220525082933.800"><vh>instance_alias_type</vh></v>
<v t="ekr.20220525082933.801"><vh>analyze_var</vh></v>
<v t="ekr.20220525082933.802"><vh>freeze_type_vars</vh></v>
<v t="ekr.20220525082933.803"><vh>lookup_member_var_or_accessor</vh></v>
<v t="ekr.20220525082933.804"><vh>check_self_arg</vh></v>
<v t="ekr.20220525082933.805"><vh>analyze_class_attribute_access</vh></v>
<v t="ekr.20220525082933.806"><vh>apply_class_attr_hook</vh></v>
<v t="ekr.20220525082933.807"><vh>analyze_enum_class_attribute_access</vh></v>
<v t="ekr.20220525082933.808"><vh>analyze_typeddict_access</vh></v>
<v t="ekr.20220525082933.809"><vh>add_class_tvars</vh></v>
<v t="ekr.20220525082933.810"><vh>type_object_type</vh></v>
<v t="ekr.20220525082933.811"><vh>analyze_decorator_or_funcbase_access</vh></v>
<v t="ekr.20220525082933.812"><vh>is_valid_constructor</vh></v>
</v>
<v t="ekr.20220525082933.813"><vh>@clean checkpattern.py</vh>
<v t="ekr.20220525082933.814"><vh>class PatternType</vh></v>
<v t="ekr.20220525082933.815"><vh>class PatternChecker</vh>
<v t="ekr.20220525082933.816"><vh>PatternChecker.__init__</vh></v>
<v t="ekr.20220525082933.817"><vh>PatternChecker.accept</vh></v>
<v t="ekr.20220525082933.818"><vh>PatternChecker.visit_as_pattern</vh></v>
<v t="ekr.20220525082933.819"><vh>PatternChecker.visit_or_pattern</vh></v>
<v t="ekr.20220525082933.820"><vh>PatternChecker.visit_value_pattern</vh></v>
<v t="ekr.20220525082933.821"><vh>PatternChecker.visit_singleton_pattern</vh></v>
<v t="ekr.20220525082933.822"><vh>PatternChecker.visit_sequence_pattern</vh></v>
<v t="ekr.20220525082933.823"><vh>PatternChecker.get_sequence_type</vh></v>
<v t="ekr.20220525082933.824"><vh>PatternChecker.contract_starred_pattern_types</vh></v>
<v t="ekr.20220525082933.825"><vh>PatternChecker.expand_starred_pattern_types</vh></v>
<v t="ekr.20220525082933.826"><vh>PatternChecker.visit_starred_pattern</vh></v>
<v t="ekr.20220525082933.827"><vh>PatternChecker.visit_mapping_pattern</vh></v>
<v t="ekr.20220525082933.828"><vh>PatternChecker.get_mapping_item_type</vh></v>
<v t="ekr.20220525082933.829"><vh>PatternChecker.get_simple_mapping_item_type</vh></v>
<v t="ekr.20220525082933.830"><vh>PatternChecker.visit_class_pattern</vh></v>
<v t="ekr.20220525082933.831"><vh>PatternChecker.should_self_match</vh></v>
<v t="ekr.20220525082933.832"><vh>PatternChecker.can_match_sequence</vh></v>
<v t="ekr.20220525082933.833"><vh>PatternChecker.generate_types_from_names</vh></v>
<v t="ekr.20220525082933.834"><vh>PatternChecker.update_type_map</vh></v>
<v t="ekr.20220525082933.835"><vh>PatternChecker.construct_sequence_child</vh></v>
<v t="ekr.20220525082933.836"><vh>PatternChecker.early_non_match</vh></v>
</v>
<v t="ekr.20220525082933.837"><vh>get_match_arg_names</vh></v>
<v t="ekr.20220525082933.838"><vh>get_var</vh></v>
<v t="ekr.20220525082933.839"><vh>get_type_range</vh></v>
<v t="ekr.20220525082933.840"><vh>is_uninhabited</vh></v>
</v>
<v t="ekr.20220525082933.841"><vh>@clean checkstrformat.py</vh>
<v t="ekr.20220525082933.842"><vh>compile_format_re</vh></v>
<v t="ekr.20220525082933.843"><vh>compile_new_format_re</vh></v>
<v t="ekr.20220525082933.844"><vh>FORMAT_RE: Final = compile_format_re()</vh></v>
<v t="ekr.20220525082933.845"><vh>class ConversionSpecifier</vh>
<v t="ekr.20220525082933.846"><vh>ConversionSpecifier.__init__</vh></v>
<v t="ekr.20220525082933.847"><vh>ConversionSpecifier.has_key</vh></v>
<v t="ekr.20220525082933.848"><vh>ConversionSpecifier.has_star</vh></v>
</v>
<v t="ekr.20220525082933.849"><vh>parse_conversion_specifiers</vh></v>
<v t="ekr.20220525082933.850"><vh>parse_format_value</vh></v>
<v t="ekr.20220525082933.851"><vh>find_non_escaped_targets</vh></v>
<v t="ekr.20220525082933.852"><vh>class StringFormatterChecker</vh>
<v t="ekr.20220525082933.853"><vh>StringFormatterChecker.__init__</vh></v>
<v t="ekr.20220525082933.854"><vh>StringFormatterChecker.check_str_format_call</vh></v>
<v t="ekr.20220525082933.855"><vh>StringFormatterChecker.check_specs_in_format_call</vh></v>
<v t="ekr.20220525082933.856"><vh>StringFormatterChecker.perform_special_format_checks</vh></v>
<v t="ekr.20220525082933.857"><vh>StringFormatterChecker.find_replacements_in_call</vh></v>
<v t="ekr.20220525082933.858"><vh>StringFormatterChecker.get_expr_by_position</vh></v>
<v t="ekr.20220525082933.859"><vh>StringFormatterChecker.get_expr_by_name</vh></v>
<v t="ekr.20220525082933.860"><vh>StringFormatterChecker.auto_generate_keys</vh></v>
<v t="ekr.20220525082933.861"><vh>StringFormatterChecker.apply_field_accessors</vh></v>
<v t="ekr.20220525082933.862"><vh>StringFormatterChecker.validate_and_transform_accessors</vh></v>
<v t="ekr.20220525082933.863"><vh>StringFormatterChecker.check_str_interpolation</vh></v>
<v t="ekr.20220525082933.864"><vh>StringFormatterChecker.analyze_conversion_specifiers</vh></v>
<v t="ekr.20220525082933.865"><vh>StringFormatterChecker.check_simple_str_interpolation</vh></v>
<v t="ekr.20220525082933.866"><vh>StringFormatterChecker.check_mapping_str_interpolation</vh></v>
<v t="ekr.20220525082933.867"><vh>StringFormatterChecker.build_dict_type</vh></v>
<v t="ekr.20220525082933.868"><vh>StringFormatterChecker.build_replacement_checkers</vh></v>
<v t="ekr.20220525082933.869"><vh>StringFormatterChecker.replacement_checkers</vh></v>
<v t="ekr.20220525082933.870"><vh>StringFormatterChecker.checkers_for_star</vh></v>
<v t="ekr.20220525082933.871"><vh>StringFormatterChecker.check_placeholder_type</vh></v>
<v t="ekr.20220525082933.872"><vh>StringFormatterChecker.checkers_for_regular_type</vh></v>
<v t="ekr.20220525082933.873"><vh>StringFormatterChecker.check_s_special_cases</vh></v>
<v t="ekr.20220525082933.874"><vh>StringFormatterChecker.checkers_for_c_type</vh></v>
<v t="ekr.20220525082933.875"><vh>StringFormatterChecker.conversion_type</vh></v>
<v t="ekr.20220525082933.876"><vh>StringFormatterChecker.Helpers</vh></v>
<v t="ekr.20220525082933.877"><vh>StringFormatterChecker.named_type</vh></v>
<v t="ekr.20220525082933.878"><vh>StringFormatterChecker.accept</vh></v>
</v>
<v t="ekr.20220525082933.879"><vh>has_type_component</vh></v>
</v>
<v t="ekr.20220525082933.880"><vh>@clean config_parser.py</vh>
<v t="ekr.20220525082933.881"><vh>parse_version</vh></v>
<v t="ekr.20220525082933.882"><vh>try_split</vh></v>
<v t="ekr.20220525082933.883"><vh>expand_path</vh></v>
<v t="ekr.20220525082933.884"><vh>str_or_array_as_list</vh></v>
<v t="ekr.20220525082933.885"><vh>split_and_match_files_list</vh></v>
<v t="ekr.20220525082933.886"><vh>split_and_match_files</vh></v>
<v t="ekr.20220525082933.887"><vh>check_follow_imports</vh></v>
<v t="ekr.20220525082933.888"><vh>For most options, the type of the default value set in options.py is</vh></v>
<v t="ekr.20220525082933.889"><vh>parse_config_file</vh></v>
<v t="ekr.20220525082933.890"><vh>get_prefix</vh></v>
<v t="ekr.20220525082933.891"><vh>is_toml</vh></v>
<v t="ekr.20220525082933.892"><vh>destructure_overrides</vh></v>
<v t="ekr.20220525082933.893"><vh>parse_section</vh></v>
<v t="ekr.20220525082933.894"><vh>convert_to_boolean</vh></v>
<v t="ekr.20220525082933.895"><vh>split_directive</vh></v>
<v t="ekr.20220525082933.896"><vh>mypy_comments_to_config_map</vh></v>
<v t="ekr.20220525082933.897"><vh>parse_mypy_comments</vh></v>
<v t="ekr.20220525082933.898"><vh>get_config_module_names</vh></v>
<v t="ekr.20220525082933.899"><vh>class ConfigTOMLValueError</vh></v>
</v>
<v t="ekr.20220525082933.900"><vh>@clean constraints.py</vh>
<v t="ekr.20220525082933.901"><vh>class Constraint</vh>
<v t="ekr.20220525082933.902"><vh>Constraint.__init__</vh></v>
<v t="ekr.20220525082933.903"><vh>Constraint.__repr__</vh></v>
</v>
<v t="ekr.20220525082933.904"><vh>infer_constraints_for_callable</vh></v>
<v t="ekr.20220525082933.905"><vh>infer_constraints</vh></v>
<v t="ekr.20220525082933.906"><vh>_infer_constraints</vh></v>
<v t="ekr.20220525082933.907"><vh>infer_constraints_if_possible</vh></v>
<v t="ekr.20220525082933.908"><vh>select_trivial</vh></v>
<v t="ekr.20220525082933.909"><vh>merge_with_any</vh></v>
<v t="ekr.20220525082933.910"><vh>any_constraints</vh></v>
<v t="ekr.20220525082933.911"><vh>is_same_constraints</vh></v>
<v t="ekr.20220525082933.912"><vh>is_same_constraint</vh></v>
<v t="ekr.20220525082933.913"><vh>is_similar_constraints</vh></v>
<v t="ekr.20220525082933.914"><vh>_is_similar_constraints</vh></v>
<v t="ekr.20220525082933.915"><vh>simplify_away_incomplete_types</vh></v>
<v t="ekr.20220525082933.916"><vh>is_complete_type</vh></v>
<v t="ekr.20220525082933.917"><vh>class CompleteTypeVisitor</vh></v>
<v t="ekr.20220525082933.918"><vh>class ConstraintBuilderVisitor</vh>
<v t="ekr.20220525082933.919"><vh>ConstraintBuilderVisitor.__init__</vh></v>
<v t="ekr.20220525082933.920"><vh>ConstraintBuilderVisitor.Trivial leaf types</vh></v>
<v t="ekr.20220525082933.921"><vh>ConstraintBuilderVisitor.visit_unbound_type</vh></v>
<v t="ekr.20220525082933.922"><vh>ConstraintBuilderVisitor.visit_any</vh></v>
<v t="ekr.20220525082933.923"><vh>ConstraintBuilderVisitor.visit_none_type</vh></v>
<v t="ekr.20220525082933.924"><vh>ConstraintBuilderVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082933.925"><vh>ConstraintBuilderVisitor.visit_erased_type</vh></v>
<v t="ekr.20220525082933.926"><vh>ConstraintBuilderVisitor.visit_deleted_type</vh></v>
<v t="ekr.20220525082933.927"><vh>ConstraintBuilderVisitor.visit_literal_type</vh></v>
<v t="ekr.20220525082933.928"><vh>ConstraintBuilderVisitor.Errors</vh></v>
<v t="ekr.20220525082933.929"><vh>ConstraintBuilderVisitor.visit_partial_type</vh></v>
<v t="ekr.20220525082933.930"><vh>ConstraintBuilderVisitor.Non-trivial leaf type</vh></v>
<v t="ekr.20220525082933.931"><vh>ConstraintBuilderVisitor.visit_type_var</vh></v>
<v t="ekr.20220525082933.932"><vh>ConstraintBuilderVisitor.visit_param_spec</vh></v>
<v t="ekr.20220525082933.933"><vh>ConstraintBuilderVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082933.934"><vh>ConstraintBuilderVisitor.visit_unpack_type</vh></v>
<v t="ekr.20220525082933.935"><vh>ConstraintBuilderVisitor.visit_parameters</vh></v>
<v t="ekr.20220525082933.936"><vh>ConstraintBuilderVisitor.Non-leaf types</vh></v>
<v t="ekr.20220525082933.937"><vh>ConstraintBuilderVisitor.visit_instance</vh></v>
<v t="ekr.20220525082933.938"><vh>ConstraintBuilderVisitor.infer_constraints_from_protocol_members</vh></v>
<v t="ekr.20220525082933.939"><vh>ConstraintBuilderVisitor.visit_callable_type</vh></v>
<v t="ekr.20220525082933.940"><vh>ConstraintBuilderVisitor.infer_against_overloaded</vh></v>
<v t="ekr.20220525082933.941"><vh>ConstraintBuilderVisitor.visit_tuple_type</vh></v>
<v t="ekr.20220525082933.942"><vh>ConstraintBuilderVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20220525082933.943"><vh>ConstraintBuilderVisitor.visit_union_type</vh></v>
<v t="ekr.20220525082933.944"><vh>ConstraintBuilderVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20220525082933.945"><vh>ConstraintBuilderVisitor.infer_against_any</vh></v>
<v t="ekr.20220525082933.946"><vh>ConstraintBuilderVisitor.visit_overloaded</vh></v>
<v t="ekr.20220525082933.947"><vh>ConstraintBuilderVisitor.visit_type_type</vh></v>
</v>
<v t="ekr.20220525082933.948"><vh>neg_op</vh></v>
<v t="ekr.20220525082933.949"><vh>find_matching_overload_item</vh></v>
<v t="ekr.20220525082933.950"><vh>find_matching_overload_items</vh></v>
</v>
<v t="ekr.20220525082933.951"><vh>@clean copytype.py</vh>
<v t="ekr.20220525082933.952"><vh>copy_type</vh></v>
<v t="ekr.20220525082933.953"><vh>class TypeShallowCopier</vh>
<v t="ekr.20220525082933.954"><vh>TypeShallowCopier.visit_unbound_type</vh></v>
<v t="ekr.20220525082933.955"><vh>TypeShallowCopier.visit_any</vh></v>
<v t="ekr.20220525082933.956"><vh>TypeShallowCopier.visit_none_type</vh></v>
<v t="ekr.20220525082933.957"><vh>TypeShallowCopier.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082933.958"><vh>TypeShallowCopier.visit_erased_type</vh></v>
<v t="ekr.20220525082933.959"><vh>TypeShallowCopier.visit_deleted_type</vh></v>
<v t="ekr.20220525082933.960"><vh>TypeShallowCopier.visit_instance</vh></v>
<v t="ekr.20220525082933.961"><vh>TypeShallowCopier.visit_type_var</vh></v>
<v t="ekr.20220525082933.962"><vh>TypeShallowCopier.visit_param_spec</vh></v>
<v t="ekr.20220525082933.963"><vh>TypeShallowCopier.visit_parameters</vh></v>
<v t="ekr.20220525082933.964"><vh>TypeShallowCopier.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082933.965"><vh>TypeShallowCopier.visit_unpack_type</vh></v>
<v t="ekr.20220525082933.966"><vh>TypeShallowCopier.visit_partial_type</vh></v>
<v t="ekr.20220525082933.967"><vh>TypeShallowCopier.visit_callable_type</vh></v>
<v t="ekr.20220525082933.968"><vh>TypeShallowCopier.visit_tuple_type</vh></v>
<v t="ekr.20220525082933.969"><vh>TypeShallowCopier.visit_typeddict_type</vh></v>
<v t="ekr.20220525082933.970"><vh>TypeShallowCopier.visit_literal_type</vh></v>
<v t="ekr.20220525082933.971"><vh>TypeShallowCopier.visit_union_type</vh></v>
<v t="ekr.20220525082933.972"><vh>TypeShallowCopier.visit_overloaded</vh></v>
<v t="ekr.20220525082933.973"><vh>TypeShallowCopier.visit_type_type</vh></v>
<v t="ekr.20220525082933.974"><vh>TypeShallowCopier.visit_type_alias_type</vh></v>
<v t="ekr.20220525082933.975"><vh>TypeShallowCopier.copy_common</vh></v>
</v>
</v>
<v t="ekr.20220525082933.976"><vh>@clean defaults.py</vh></v>
<v t="ekr.20220525082933.977"><vh>@clean dmypy_os.py</vh>
<v t="ekr.20220525082933.978"><vh>alive</vh></v>
<v t="ekr.20220525082933.979"><vh>kill</vh></v>
</v>
<v t="ekr.20220525082933.980"><vh>@clean dmypy_server.py</vh>
<v t="ekr.20220525082933.981"><vh>process_start_options</vh></v>
<v t="ekr.20220525082933.982"><vh>ignore_suppressed_imports</vh></v>
<v t="ekr.20220525082933.983"><vh>ModulePathPair = Tuple[str, str]</vh></v>
<v t="ekr.20220525082933.984"><vh>class Server</vh>
<v t="ekr.20220525082933.985"><vh>Server.__init__</vh></v>
<v t="ekr.20220525082933.986"><vh>Server._response_metadata</vh></v>
<v t="ekr.20220525082933.987"><vh>Server.serve</vh></v>
<v t="ekr.20220525082933.988"><vh>Server.run_command</vh></v>
<v t="ekr.20220525082933.989"><vh>Server.Command functions (run in the server via RPC).</vh></v>
<v t="ekr.20220525082933.990"><vh>Server.cmd_status</vh></v>
<v t="ekr.20220525082933.991"><vh>Server.cmd_stop</vh></v>
<v t="ekr.20220525082933.992"><vh>Server.cmd_run</vh></v>
<v t="ekr.20220525082933.993"><vh>Server.cmd_check</vh></v>
<v t="ekr.20220525082933.994"><vh>Server.cmd_recheck</vh></v>
<v t="ekr.20220525082933.995"><vh>Server.check</vh></v>
<v t="ekr.20220525082933.996"><vh>Server.flush_caches</vh></v>
<v t="ekr.20220525082933.997"><vh>Server.update_stats</vh></v>
<v t="ekr.20220525082933.998"><vh>Server.following_imports</vh></v>
<v t="ekr.20220525082933.999"><vh>Server.initialize_fine_grained</vh></v>
<v t="ekr.20220525082933.1000"><vh>Server.fine_grained_increment</vh></v>
<v t="ekr.20220525082933.1001"><vh>Server.fine_grained_increment_follow_imports</vh></v>
<v t="ekr.20220525082933.1002"><vh>Server.find_reachable_changed_modules</vh></v>
<v t="ekr.20220525082933.1003"><vh>Server.direct_imports</vh></v>
<v t="ekr.20220525082933.1004"><vh>Server.find_added_suppressed</vh></v>
<v t="ekr.20220525082933.1005"><vh>Server.increment_output</vh></v>
<v t="ekr.20220525082933.1006"><vh>Server.pretty_messages</vh></v>
<v t="ekr.20220525082933.1007"><vh>Server.update_sources</vh></v>
<v t="ekr.20220525082933.1008"><vh>Server.update_changed</vh></v>
<v t="ekr.20220525082933.1009"><vh>Server.find_changed</vh></v>
<v t="ekr.20220525082933.1010"><vh>Server._find_changed</vh></v>
<v t="ekr.20220525082933.1011"><vh>Server.cmd_suggest</vh></v>
<v t="ekr.20220525082933.1012"><vh>Server.cmd_hang</vh></v>
</v>
<v t="ekr.20220525082933.1013"><vh>Misc utilities.</vh></v>
<v t="ekr.20220525082933.1014"><vh>get_meminfo</vh></v>
<v t="ekr.20220525082933.1015"><vh>find_all_sources_in_build</vh></v>
<v t="ekr.20220525082933.1016"><vh>fix_module_deps</vh></v>
<v t="ekr.20220525082933.1017"><vh>filter_out_missing_top_level_packages</vh></v>
</v>
<v t="ekr.20220525082933.1018"><vh>@clean dmypy_util.py</vh>
<v t="ekr.20220525082933.1019"><vh>receive</vh></v>
</v>
<v t="ekr.20220525082933.1020"><vh>@clean erasetype.py</vh>
<v t="ekr.20220525082933.1021"><vh>erase_type</vh></v>
<v t="ekr.20220525082933.1022"><vh>class EraseTypeVisitor</vh>
<v t="ekr.20220525082933.1023"><vh>EraseTypeVisitor.visit_unbound_type</vh></v>
<v t="ekr.20220525082933.1024"><vh>EraseTypeVisitor.visit_any</vh></v>
<v t="ekr.20220525082933.1025"><vh>EraseTypeVisitor.visit_none_type</vh></v>
<v t="ekr.20220525082933.1026"><vh>EraseTypeVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082933.1027"><vh>EraseTypeVisitor.visit_erased_type</vh></v>
<v t="ekr.20220525082933.1028"><vh>EraseTypeVisitor.visit_partial_type</vh></v>
<v t="ekr.20220525082933.1029"><vh>EraseTypeVisitor.visit_deleted_type</vh></v>
<v t="ekr.20220525082933.1030"><vh>EraseTypeVisitor.visit_instance</vh></v>
<v t="ekr.20220525082933.1031"><vh>EraseTypeVisitor.visit_type_var</vh></v>
<v t="ekr.20220525082933.1032"><vh>EraseTypeVisitor.visit_param_spec</vh></v>
<v t="ekr.20220525082933.1033"><vh>EraseTypeVisitor.visit_parameters</vh></v>
<v t="ekr.20220525082933.1034"><vh>EraseTypeVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082933.1035"><vh>EraseTypeVisitor.visit_unpack_type</vh></v>
<v t="ekr.20220525082933.1036"><vh>EraseTypeVisitor.visit_callable_type</vh></v>
<v t="ekr.20220525082933.1037"><vh>EraseTypeVisitor.visit_overloaded</vh></v>
<v t="ekr.20220525082933.1038"><vh>EraseTypeVisitor.visit_tuple_type</vh></v>
<v t="ekr.20220525082933.1039"><vh>EraseTypeVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20220525082933.1040"><vh>EraseTypeVisitor.visit_literal_type</vh></v>
<v t="ekr.20220525082933.1041"><vh>EraseTypeVisitor.visit_union_type</vh></v>
<v t="ekr.20220525082933.1042"><vh>EraseTypeVisitor.visit_type_type</vh></v>
<v t="ekr.20220525082933.1043"><vh>EraseTypeVisitor.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20220525082933.1044"><vh>erase_typevars</vh>
<v t="ekr.20220525082933.1045"><vh>erase_id</vh></v>
</v>
<v t="ekr.20220525082933.1046"><vh>replace_meta_vars</vh></v>
<v t="ekr.20220525082933.1047"><vh>class TypeVarEraser</vh>
<v t="ekr.20220525082933.1048"><vh>TypeVarEraser.__init__</vh></v>
<v t="ekr.20220525082933.1049"><vh>TypeVarEraser.visit_type_var</vh></v>
<v t="ekr.20220525082933.1050"><vh>TypeVarEraser.visit_param_spec</vh></v>
<v t="ekr.20220525082933.1051"><vh>TypeVarEraser.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20220525082933.1052"><vh>remove_instance_last_known_values</vh></v>
<v t="ekr.20220525082933.1053"><vh>class LastKnownValueEraser</vh>
<v t="ekr.20220525082933.1054"><vh>LastKnownValueEraser.visit_instance</vh></v>
<v t="ekr.20220525082933.1055"><vh>LastKnownValueEraser.visit_type_alias_type</vh></v>
<v t="ekr.20220525082933.1056"><vh>LastKnownValueEraser.visit_union_type</vh></v>
</v>
</v>
<v t="ekr.20220525082933.1057"><vh>@clean errorcodes.py</vh>
<v t="ekr.20220525082933.1058"><vh>class ErrorCode</vh>
<v t="ekr.20220525082933.1059"><vh>ErrorCode.__init__</vh></v>
<v t="ekr.20220525082933.1060"><vh>ErrorCode.__str__</vh></v>
</v>
</v>
<v t="ekr.20220525082933.1061"><vh>@clean errors.py</vh>
<v t="ekr.20220525082933.1062"><vh>class ErrorInfo</vh>
<v t="ekr.20220525082933.1063"><vh>ErrorInfo.__init__</vh></v>
</v>
<v t="ekr.20220525082933.1064"><vh>Type used internally to represent errors:</vh></v>
<v t="ekr.20220525082933.1065"><vh>class ErrorWatcher</vh>
<v t="ekr.20220525082933.1066"><vh>ErrorWatcher.__init__</vh></v>
<v t="ekr.20220525082933.1067"><vh>ErrorWatcher.__enter__</vh></v>
<v t="ekr.20220525082933.1068"><vh>ErrorWatcher.__exit__</vh></v>
<v t="ekr.20220525082933.1069"><vh>ErrorWatcher.on_error</vh></v>
<v t="ekr.20220525082933.1070"><vh>ErrorWatcher.has_new_errors</vh></v>
<v t="ekr.20220525082933.1071"><vh>ErrorWatcher.filtered_errors</vh></v>
</v>
<v t="ekr.20220525082933.1072"><vh>class Errors</vh>
<v t="ekr.20220525082933.1073"><vh>Errors.__init__</vh></v>
<v t="ekr.20220525082933.1074"><vh>Errors.initialize</vh></v>
<v t="ekr.20220525082933.1075"><vh>Errors.reset</vh></v>
<v t="ekr.20220525082933.1076"><vh>Errors.set_ignore_prefix</vh></v>
<v t="ekr.20220525082933.1077"><vh>Errors.simplify_path</vh></v>
<v t="ekr.20220525082933.1078"><vh>Errors.set_file</vh></v>
<v t="ekr.20220525082933.1079"><vh>Errors.set_file_ignored_lines</vh></v>
<v t="ekr.20220525082933.1080"><vh>Errors.current_target</vh></v>
<v t="ekr.20220525082933.1081"><vh>Errors.current_module</vh></v>
<v t="ekr.20220525082933.1082"><vh>Errors.import_context</vh></v>
<v t="ekr.20220525082933.1083"><vh>Errors.set_import_context</vh></v>
<v t="ekr.20220525082933.1084"><vh>Errors.report</vh></v>
<v t="ekr.20220525082933.1085"><vh>Errors._add_error_info</vh></v>
<v t="ekr.20220525082933.1086"><vh>Errors._filter_error</vh></v>
<v t="ekr.20220525082933.1087"><vh>Errors.add_error_info</vh></v>
<v t="ekr.20220525082933.1088"><vh>Errors.has_many_errors</vh></v>
<v t="ekr.20220525082933.1089"><vh>Errors.report_hidden_errors</vh></v>
<v t="ekr.20220525082933.1090"><vh>Errors.is_ignored_error</vh></v>
<v t="ekr.20220525082933.1091"><vh>Errors.is_error_code_enabled</vh></v>
<v t="ekr.20220525082933.1092"><vh>Errors.clear_errors_in_targets</vh></v>
<v t="ekr.20220525082933.1093"><vh>Errors.generate_unused_ignore_errors</vh></v>
<v t="ekr.20220525082933.1094"><vh>Errors.generate_ignore_without_code_errors</vh></v>
<v t="ekr.20220525082933.1095"><vh>Errors.num_messages</vh></v>
<v t="ekr.20220525082933.1096"><vh>Errors.is_errors</vh></v>
<v t="ekr.20220525082933.1097"><vh>Errors.is_blockers</vh></v>
<v t="ekr.20220525082933.1098"><vh>Errors.blocker_module</vh></v>
<v t="ekr.20220525082933.1099"><vh>Errors.is_errors_for_file</vh></v>
<v t="ekr.20220525082933.1100"><vh>Errors.raise_error</vh></v>
<v t="ekr.20220525082933.1101"><vh>Errors.format_messages</vh></v>
<v t="ekr.20220525082933.1102"><vh>Errors.file_messages</vh></v>
<v t="ekr.20220525082933.1103"><vh>Errors.new_messages</vh></v>
<v t="ekr.20220525082933.1104"><vh>Errors.targets</vh></v>
<v t="ekr.20220525082933.1105"><vh>Errors.render_messages</vh></v>
<v t="ekr.20220525082933.1106"><vh>Errors.sort_messages</vh></v>
<v t="ekr.20220525082933.1107"><vh>Errors.remove_duplicates</vh></v>
</v>
<v t="ekr.20220525082933.1108"><vh>class CompileError</vh>
<v t="ekr.20220525082933.1109"><vh>CompileError.__init__</vh></v>
</v>
<v t="ekr.20220525082933.1110"><vh>remove_path_prefix</vh></v>
<v t="ekr.20220525082933.1111"><vh>report_internal_error</vh></v>
</v>
<v t="ekr.20220525082933.1112"><vh>@clean expandtype.py</vh>
<v t="ekr.20220525082934.1"><vh>expand_type</vh></v>
<v t="ekr.20220525082934.2"><vh>expand_type_by_instance</vh></v>
<v t="ekr.20220525082934.3"><vh>F = TypeVar('F', bound=FunctionLike)</vh></v>
<v t="ekr.20220525082934.4"><vh>freshen_function_type_vars</vh></v>
<v t="ekr.20220525082934.5"><vh>class ExpandTypeVisitor</vh>
<v t="ekr.20220525082934.6"><vh>ExpandTypeVisitor.__init__</vh></v>
<v t="ekr.20220525082934.7"><vh>ExpandTypeVisitor.visit_unbound_type</vh></v>
<v t="ekr.20220525082934.8"><vh>ExpandTypeVisitor.visit_any</vh></v>
<v t="ekr.20220525082934.9"><vh>ExpandTypeVisitor.visit_none_type</vh></v>
<v t="ekr.20220525082934.10"><vh>ExpandTypeVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082934.11"><vh>ExpandTypeVisitor.visit_deleted_type</vh></v>
<v t="ekr.20220525082934.12"><vh>ExpandTypeVisitor.visit_erased_type</vh></v>
<v t="ekr.20220525082934.13"><vh>ExpandTypeVisitor.visit_instance</vh></v>
<v t="ekr.20220525082934.14"><vh>ExpandTypeVisitor.visit_type_var</vh></v>
<v t="ekr.20220525082934.15"><vh>ExpandTypeVisitor.visit_param_spec</vh></v>
<v t="ekr.20220525082934.16"><vh>ExpandTypeVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082934.17"><vh>ExpandTypeVisitor.visit_unpack_type</vh></v>
<v t="ekr.20220525082934.18"><vh>ExpandTypeVisitor.expand_unpack</vh></v>
<v t="ekr.20220525082934.19"><vh>ExpandTypeVisitor.visit_parameters</vh></v>
<v t="ekr.20220525082934.20"><vh>ExpandTypeVisitor.visit_callable_type</vh></v>
<v t="ekr.20220525082934.21"><vh>ExpandTypeVisitor.visit_overloaded</vh></v>
<v t="ekr.20220525082934.22"><vh>ExpandTypeVisitor.visit_tuple_type</vh></v>
<v t="ekr.20220525082934.23"><vh>ExpandTypeVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20220525082934.24"><vh>ExpandTypeVisitor.visit_literal_type</vh></v>
<v t="ekr.20220525082934.25"><vh>ExpandTypeVisitor.visit_union_type</vh></v>
<v t="ekr.20220525082934.26"><vh>ExpandTypeVisitor.visit_partial_type</vh></v>
<v t="ekr.20220525082934.27"><vh>ExpandTypeVisitor.visit_type_type</vh></v>
<v t="ekr.20220525082934.28"><vh>ExpandTypeVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20220525082934.29"><vh>ExpandTypeVisitor.expand_types</vh></v>
</v>
</v>
<v t="ekr.20220525082934.30"><vh>@clean exprtotype.py</vh>
<v t="ekr.20220525082934.31"><vh>class TypeTranslationError</vh></v>
<v t="ekr.20220525082934.32"><vh>_extract_argument_name</vh></v>
<v t="ekr.20220525082934.33"><vh>expr_to_unanalyzed_type</vh></v>
</v>
<v t="ekr.20220525082934.34"><vh>@clean fastparse.py</vh>
<v t="ekr.20220525082934.35"><vh>parse</vh></v>
<v t="ekr.20220525082934.36"><vh>parse_type_ignore_tag</vh></v>
<v t="ekr.20220525082934.37"><vh>parse_type_comment</vh></v>
<v t="ekr.20220525082934.38"><vh>parse_type_string</vh></v>
<v t="ekr.20220525082934.39"><vh>is_no_type_check_decorator</vh></v>
<v t="ekr.20220525082934.40"><vh>class ASTConverter</vh>
<v t="ekr.20220525082934.41"><vh>ASTConverter.__init__</vh></v>
<v t="ekr.20220525082934.42"><vh>ASTConverter.note</vh></v>
<v t="ekr.20220525082934.43"><vh>ASTConverter.fail</vh></v>
<v t="ekr.20220525082934.44"><vh>ASTConverter.fail_merge_overload</vh></v>
<v t="ekr.20220525082934.45"><vh>ASTConverter.visit</vh></v>
<v t="ekr.20220525082934.46"><vh>ASTConverter.set_line</vh></v>
<v t="ekr.20220525082934.47"><vh>ASTConverter.translate_opt_expr_list</vh></v>
<v t="ekr.20220525082934.48"><vh>ASTConverter.translate_expr_list</vh></v>
<v t="ekr.20220525082934.49"><vh>ASTConverter.get_lineno</vh></v>
<v t="ekr.20220525082934.50"><vh>ASTConverter.translate_stmt_list</vh></v>
<v t="ekr.20220525082934.51"><vh>ASTConverter.translate_type_comment</vh></v>
<v t="ekr.20220525082934.52"><vh>ASTConverter.op_map: Final[Dict[typing.Type[AST], str]] = {</vh></v>
<v t="ekr.20220525082934.53"><vh>ASTConverter.from_operator</vh></v>
<v t="ekr.20220525082934.54"><vh>ASTConverter.comp_op_map: Final[Dict[typing.Type[AST], str]] = {</vh></v>
<v t="ekr.20220525082934.55"><vh>ASTConverter.from_comp_operator</vh></v>
<v t="ekr.20220525082934.56"><vh>ASTConverter.as_block</vh></v>
<v t="ekr.20220525082934.57"><vh>ASTConverter.as_required_block</vh></v>
<v t="ekr.20220525082934.58"><vh>ASTConverter.fix_function_overloads</vh></v>
<v t="ekr.20220525082934.59"><vh>ASTConverter._check_ifstmt_for_overloads</vh></v>
<v t="ekr.20220525082934.60"><vh>ASTConverter._get_executable_if_block_with_overloads</vh></v>
<v t="ekr.20220525082934.61"><vh>ASTConverter._strip_contents_from_if_stmt</vh></v>
<v t="ekr.20220525082934.62"><vh>ASTConverter._is_stripped_if_stmt</vh></v>
<v t="ekr.20220525082934.63"><vh>ASTConverter.in_method_scope</vh></v>
<v t="ekr.20220525082934.64"><vh>ASTConverter.translate_module_id</vh></v>
<v t="ekr.20220525082934.65"><vh>ASTConverter.visit_Module</vh></v>
<v t="ekr.20220525082934.66"><vh>ASTConverter.visit_FunctionDef</vh></v>
<v t="ekr.20220525082934.67"><vh>ASTConverter.visit_AsyncFunctionDef</vh></v>
<v t="ekr.20220525082934.68"><vh>ASTConverter.do_func_def</vh></v>
<v t="ekr.20220525082934.69"><vh>ASTConverter.set_type_optional</vh></v>
<v t="ekr.20220525082934.70"><vh>ASTConverter.transform_args</vh></v>
<v t="ekr.20220525082934.71"><vh>ASTConverter.make_argument</vh></v>
<v t="ekr.20220525082934.72"><vh>ASTConverter.fail_arg</vh></v>
<v t="ekr.20220525082934.73"><vh>ASTConverter.visit_ClassDef</vh></v>
<v t="ekr.20220525082934.74"><vh>ASTConverter.visit_Return</vh></v>
<v t="ekr.20220525082934.75"><vh>ASTConverter.visit_Delete</vh></v>
<v t="ekr.20220525082934.76"><vh>ASTConverter.visit_Assign</vh></v>
<v t="ekr.20220525082934.77"><vh>ASTConverter.visit_AnnAssign</vh></v>
<v t="ekr.20220525082934.78"><vh>ASTConverter.visit_AugAssign</vh></v>
<v t="ekr.20220525082934.79"><vh>ASTConverter.visit_For</vh></v>
<v t="ekr.20220525082934.80"><vh>ASTConverter.visit_AsyncFor</vh></v>
<v t="ekr.20220525082934.81"><vh>ASTConverter.visit_While</vh></v>
<v t="ekr.20220525082934.82"><vh>ASTConverter.visit_If</vh></v>
<v t="ekr.20220525082934.83"><vh>ASTConverter.visit_With</vh></v>
<v t="ekr.20220525082934.84"><vh>ASTConverter.visit_AsyncWith</vh></v>
<v t="ekr.20220525082934.85"><vh>ASTConverter.visit_Raise</vh></v>
<v t="ekr.20220525082934.86"><vh>ASTConverter.visit_Try</vh></v>
<v t="ekr.20220525082934.87"><vh>ASTConverter.visit_Assert</vh></v>
<v t="ekr.20220525082934.88"><vh>ASTConverter.visit_Import</vh></v>
<v t="ekr.20220525082934.89"><vh>ASTConverter.visit_ImportFrom</vh></v>
<v t="ekr.20220525082934.90"><vh>ASTConverter.visit_Global</vh></v>
<v t="ekr.20220525082934.91"><vh>ASTConverter.visit_Nonlocal</vh></v>
<v t="ekr.20220525082934.92"><vh>ASTConverter.visit_Expr</vh></v>
<v t="ekr.20220525082934.93"><vh>ASTConverter.visit_Pass</vh></v>
<v t="ekr.20220525082934.94"><vh>ASTConverter.visit_Break</vh></v>
<v t="ekr.20220525082934.95"><vh>ASTConverter.visit_Continue</vh></v>
<v t="ekr.20220525082934.96"><vh>ASTConverter.--- expr ---</vh></v>
<v t="ekr.20220525082934.97"><vh>ASTConverter.visit_NamedExpr</vh></v>
<v t="ekr.20220525082934.98"><vh>ASTConverter.visit_BoolOp</vh></v>
<v t="ekr.20220525082934.99"><vh>ASTConverter.group</vh></v>
<v t="ekr.20220525082934.100"><vh>ASTConverter.visit_BinOp</vh></v>
<v t="ekr.20220525082934.101"><vh>ASTConverter.visit_UnaryOp</vh></v>
<v t="ekr.20220525082934.102"><vh>ASTConverter.visit_Lambda</vh></v>
<v t="ekr.20220525082934.103"><vh>ASTConverter.visit_IfExp</vh></v>
<v t="ekr.20220525082934.104"><vh>ASTConverter.visit_Dict</vh></v>
<v t="ekr.20220525082934.105"><vh>ASTConverter.visit_Set</vh></v>
<v t="ekr.20220525082934.106"><vh>ASTConverter.visit_ListComp</vh></v>
<v t="ekr.20220525082934.107"><vh>ASTConverter.visit_SetComp</vh></v>
<v t="ekr.20220525082934.108"><vh>ASTConverter.visit_DictComp</vh></v>
<v t="ekr.20220525082934.109"><vh>ASTConverter.visit_GeneratorExp</vh></v>
<v t="ekr.20220525082934.110"><vh>ASTConverter.visit_Await</vh></v>
<v t="ekr.20220525082934.111"><vh>ASTConverter.visit_Yield</vh></v>
<v t="ekr.20220525082934.112"><vh>ASTConverter.visit_YieldFrom</vh></v>
<v t="ekr.20220525082934.113"><vh>ASTConverter.visit_Compare</vh></v>
<v t="ekr.20220525082934.114"><vh>ASTConverter.visit_Call</vh></v>
<v t="ekr.20220525082934.115"><vh>ASTConverter.visit_Constant</vh></v>
<v t="ekr.20220525082934.116"><vh>ASTConverter.visit_Num</vh></v>
<v t="ekr.20220525082934.117"><vh>ASTConverter.visit_Str</vh></v>
<v t="ekr.20220525082934.118"><vh>ASTConverter.visit_JoinedStr</vh></v>
<v t="ekr.20220525082934.119"><vh>ASTConverter.visit_FormattedValue</vh></v>
<v t="ekr.20220525082934.120"><vh>ASTConverter.visit_Bytes</vh></v>
<v t="ekr.20220525082934.121"><vh>ASTConverter.visit_NameConstant</vh></v>
<v t="ekr.20220525082934.122"><vh>ASTConverter.visit_Ellipsis</vh></v>
<v t="ekr.20220525082934.123"><vh>ASTConverter.visit_Attribute</vh></v>
<v t="ekr.20220525082934.124"><vh>ASTConverter.visit_Subscript</vh></v>
<v t="ekr.20220525082934.125"><vh>ASTConverter.visit_Starred</vh></v>
<v t="ekr.20220525082934.126"><vh>ASTConverter.visit_Name</vh></v>
<v t="ekr.20220525082934.127"><vh>ASTConverter.visit_List</vh></v>
<v t="ekr.20220525082934.128"><vh>ASTConverter.visit_Tuple</vh></v>
<v t="ekr.20220525082934.129"><vh>ASTConverter.--- slice ---</vh></v>
<v t="ekr.20220525082934.130"><vh>ASTConverter.visit_Slice</vh></v>
<v t="ekr.20220525082934.131"><vh>ASTConverter.visit_ExtSlice</vh></v>
<v t="ekr.20220525082934.132"><vh>ASTConverter.visit_Index</vh></v>
<v t="ekr.20220525082934.133"><vh>ASTConverter.visit_Match</vh></v>
<v t="ekr.20220525082934.134"><vh>ASTConverter.visit_MatchValue</vh></v>
<v t="ekr.20220525082934.135"><vh>ASTConverter.visit_MatchSingleton</vh></v>
<v t="ekr.20220525082934.136"><vh>ASTConverter.visit_MatchSequence</vh></v>
<v t="ekr.20220525082934.137"><vh>ASTConverter.visit_MatchStar</vh></v>
<v t="ekr.20220525082934.138"><vh>ASTConverter.visit_MatchMapping</vh></v>
<v t="ekr.20220525082934.139"><vh>ASTConverter.visit_MatchClass</vh></v>
<v t="ekr.20220525082934.140"><vh>ASTConverter.visit_MatchAs</vh></v>
<v t="ekr.20220525082934.141"><vh>ASTConverter.visit_MatchOr</vh></v>
</v>
<v t="ekr.20220525082934.142"><vh>class TypeConverter</vh>
<v t="ekr.20220525082934.143"><vh>TypeConverter.__init__</vh></v>
<v t="ekr.20220525082934.144"><vh>TypeConverter.convert_column</vh></v>
<v t="ekr.20220525082934.145"><vh>TypeConverter.invalid_type</vh></v>
<v t="ekr.20220525082934.146"><vh>TypeConverter.visit</vh></v>
<v t="ekr.20220525082934.147"><vh>TypeConverter.visit</vh></v>
<v t="ekr.20220525082934.148"><vh>TypeConverter.visit</vh></v>
<v t="ekr.20220525082934.149"><vh>TypeConverter.parent</vh></v>
<v t="ekr.20220525082934.150"><vh>TypeConverter.fail</vh></v>
<v t="ekr.20220525082934.151"><vh>TypeConverter.note</vh></v>
<v t="ekr.20220525082934.152"><vh>TypeConverter.translate_expr_list</vh></v>
<v t="ekr.20220525082934.153"><vh>TypeConverter.visit_raw_str</vh></v>
<v t="ekr.20220525082934.154"><vh>TypeConverter.visit_Call</vh></v>
<v t="ekr.20220525082934.155"><vh>TypeConverter.translate_argument_list</vh></v>
<v t="ekr.20220525082934.156"><vh>TypeConverter._extract_argument_name</vh></v>
<v t="ekr.20220525082934.157"><vh>TypeConverter.visit_Name</vh></v>
<v t="ekr.20220525082934.158"><vh>TypeConverter.visit_BinOp</vh></v>
<v t="ekr.20220525082934.159"><vh>TypeConverter.visit_NameConstant</vh></v>
<v t="ekr.20220525082934.160"><vh>TypeConverter.visit_Constant</vh></v>
<v t="ekr.20220525082934.161"><vh>TypeConverter.visit_UnaryOp</vh></v>
<v t="ekr.20220525082934.162"><vh>TypeConverter.numeric_type</vh></v>
<v t="ekr.20220525082934.163"><vh>TypeConverter.These next three methods are only used if we are on python &lt;</vh></v>
<v t="ekr.20220525082934.164"><vh>TypeConverter.visit_Num</vh></v>
<v t="ekr.20220525082934.165"><vh>TypeConverter.visit_Str</vh></v>
<v t="ekr.20220525082934.166"><vh>TypeConverter.visit_Bytes</vh></v>
<v t="ekr.20220525082934.167"><vh>TypeConverter.visit_Index</vh></v>
<v t="ekr.20220525082934.168"><vh>TypeConverter.visit_Slice</vh></v>
<v t="ekr.20220525082934.169"><vh>TypeConverter.visit_Subscript</vh></v>
<v t="ekr.20220525082934.170"><vh>TypeConverter.visit_Tuple</vh></v>
<v t="ekr.20220525082934.171"><vh>TypeConverter.visit_Attribute</vh></v>
<v t="ekr.20220525082934.172"><vh>TypeConverter.visit_Ellipsis</vh></v>
<v t="ekr.20220525082934.173"><vh>TypeConverter.visit_List</vh></v>
</v>
<v t="ekr.20220525082934.174"><vh>stringify_name</vh></v>
</v>
<v t="ekr.20220525082934.175"><vh>@clean fastparse2.py</vh>
<v t="ekr.20220525082934.176"><vh>parse</vh></v>
<v t="ekr.20220525082934.177"><vh>is_no_type_check_decorator</vh></v>
<v t="ekr.20220525082934.178"><vh>class ASTConverter</vh>
<v t="ekr.20220525082934.179"><vh>ASTConverter.__init__</vh></v>
<v t="ekr.20220525082934.180"><vh>ASTConverter.fail</vh></v>
<v t="ekr.20220525082934.181"><vh>ASTConverter.visit</vh></v>
<v t="ekr.20220525082934.182"><vh>ASTConverter.set_line</vh></v>
<v t="ekr.20220525082934.183"><vh>ASTConverter.translate_expr_list</vh></v>
<v t="ekr.20220525082934.184"><vh>ASTConverter.get_lineno</vh></v>
<v t="ekr.20220525082934.185"><vh>ASTConverter.translate_stmt_list</vh></v>
<v t="ekr.20220525082934.186"><vh>ASTConverter.translate_type_comment</vh></v>
<v t="ekr.20220525082934.187"><vh>ASTConverter.op_map: Final[Dict[typing.Type[AST], str]] = {</vh></v>
<v t="ekr.20220525082934.188"><vh>ASTConverter.from_operator</vh></v>
<v t="ekr.20220525082934.189"><vh>ASTConverter.comp_op_map: Final[Dict[typing.Type[AST], str]] = {</vh></v>
<v t="ekr.20220525082934.190"><vh>ASTConverter.from_comp_operator</vh></v>
<v t="ekr.20220525082934.191"><vh>ASTConverter.as_block</vh></v>
<v t="ekr.20220525082934.192"><vh>ASTConverter.as_required_block</vh></v>
<v t="ekr.20220525082934.193"><vh>ASTConverter.fix_function_overloads</vh></v>
<v t="ekr.20220525082934.194"><vh>ASTConverter.in_method_scope</vh></v>
<v t="ekr.20220525082934.195"><vh>ASTConverter.translate_module_id</vh></v>
<v t="ekr.20220525082934.196"><vh>ASTConverter.visit_Module</vh></v>
<v t="ekr.20220525082934.197"><vh>ASTConverter.visit_FunctionDef</vh></v>
<v t="ekr.20220525082934.198"><vh>ASTConverter.set_type_optional</vh></v>
<v t="ekr.20220525082934.199"><vh>ASTConverter.transform_args</vh></v>
<v t="ekr.20220525082934.200"><vh>ASTConverter.extract_names</vh></v>
<v t="ekr.20220525082934.201"><vh>ASTConverter.convert_arg</vh></v>
<v t="ekr.20220525082934.202"><vh>ASTConverter.get_type</vh></v>
<v t="ekr.20220525082934.203"><vh>ASTConverter.stringify_name</vh></v>
<v t="ekr.20220525082934.204"><vh>ASTConverter.visit_ClassDef</vh></v>
<v t="ekr.20220525082934.205"><vh>ASTConverter.visit_Return</vh></v>
<v t="ekr.20220525082934.206"><vh>ASTConverter.visit_Delete</vh></v>
<v t="ekr.20220525082934.207"><vh>ASTConverter.visit_Assign</vh></v>
<v t="ekr.20220525082934.208"><vh>ASTConverter.visit_AugAssign</vh></v>
<v t="ekr.20220525082934.209"><vh>ASTConverter.visit_For</vh></v>
<v t="ekr.20220525082934.210"><vh>ASTConverter.visit_While</vh></v>
<v t="ekr.20220525082934.211"><vh>ASTConverter.visit_If</vh></v>
<v t="ekr.20220525082934.212"><vh>ASTConverter.visit_With</vh></v>
<v t="ekr.20220525082934.213"><vh>ASTConverter.visit_Raise</vh></v>
<v t="ekr.20220525082934.214"><vh>ASTConverter.visit_TryExcept</vh></v>
<v t="ekr.20220525082934.215"><vh>ASTConverter.visit_TryFinally</vh></v>
<v t="ekr.20220525082934.216"><vh>ASTConverter.try_handler</vh></v>
<v t="ekr.20220525082934.217"><vh>ASTConverter.visit_Print</vh></v>
<v t="ekr.20220525082934.218"><vh>ASTConverter.visit_Exec</vh></v>
<v t="ekr.20220525082934.219"><vh>ASTConverter.visit_Repr</vh></v>
<v t="ekr.20220525082934.220"><vh>ASTConverter.visit_Assert</vh></v>
<v t="ekr.20220525082934.221"><vh>ASTConverter.visit_Import</vh></v>
<v t="ekr.20220525082934.222"><vh>ASTConverter.visit_ImportFrom</vh></v>
<v t="ekr.20220525082934.223"><vh>ASTConverter.visit_Global</vh></v>
<v t="ekr.20220525082934.224"><vh>ASTConverter.visit_Expr</vh></v>
<v t="ekr.20220525082934.225"><vh>ASTConverter.visit_Pass</vh></v>
<v t="ekr.20220525082934.226"><vh>ASTConverter.visit_Break</vh></v>
<v t="ekr.20220525082934.227"><vh>ASTConverter.visit_Continue</vh></v>
<v t="ekr.20220525082934.228"><vh>ASTConverter.--- expr ---</vh></v>
<v t="ekr.20220525082934.229"><vh>ASTConverter.visit_BoolOp</vh></v>
<v t="ekr.20220525082934.230"><vh>ASTConverter.group</vh></v>
<v t="ekr.20220525082934.231"><vh>ASTConverter.visit_BinOp</vh></v>
<v t="ekr.20220525082934.232"><vh>ASTConverter.visit_UnaryOp</vh></v>
<v t="ekr.20220525082934.233"><vh>ASTConverter.visit_Lambda</vh></v>
<v t="ekr.20220525082934.234"><vh>ASTConverter.visit_IfExp</vh></v>
<v t="ekr.20220525082934.235"><vh>ASTConverter.visit_Dict</vh></v>
<v t="ekr.20220525082934.236"><vh>ASTConverter.visit_Set</vh></v>
<v t="ekr.20220525082934.237"><vh>ASTConverter.visit_ListComp</vh></v>
<v t="ekr.20220525082934.238"><vh>ASTConverter.visit_SetComp</vh></v>
<v t="ekr.20220525082934.239"><vh>ASTConverter.visit_DictComp</vh></v>
<v t="ekr.20220525082934.240"><vh>ASTConverter.visit_GeneratorExp</vh></v>
<v t="ekr.20220525082934.241"><vh>ASTConverter.visit_Yield</vh></v>
<v t="ekr.20220525082934.242"><vh>ASTConverter.visit_Compare</vh></v>
<v t="ekr.20220525082934.243"><vh>ASTConverter.visit_Call</vh></v>
<v t="ekr.20220525082934.244"><vh>ASTConverter.visit_Num</vh></v>
<v t="ekr.20220525082934.245"><vh>ASTConverter.visit_Str</vh></v>
<v t="ekr.20220525082934.246"><vh>ASTConverter.visit_Ellipsis</vh></v>
<v t="ekr.20220525082934.247"><vh>ASTConverter.visit_Attribute</vh></v>
<v t="ekr.20220525082934.248"><vh>ASTConverter.visit_Subscript</vh></v>
<v t="ekr.20220525082934.249"><vh>ASTConverter.visit_Name</vh></v>
<v t="ekr.20220525082934.250"><vh>ASTConverter.visit_List</vh></v>
<v t="ekr.20220525082934.251"><vh>ASTConverter.visit_Tuple</vh></v>
<v t="ekr.20220525082934.252"><vh>ASTConverter.--- slice ---</vh></v>
<v t="ekr.20220525082934.253"><vh>ASTConverter.visit_Slice</vh></v>
<v t="ekr.20220525082934.254"><vh>ASTConverter.visit_ExtSlice</vh></v>
<v t="ekr.20220525082934.255"><vh>ASTConverter.visit_Index</vh></v>
</v>
</v>
<v t="ekr.20220525082934.256"><vh>@clean find_sources.py</vh>
<v t="ekr.20220525082934.257"><vh>class InvalidSourceList</vh></v>
<v t="ekr.20220525082934.258"><vh>create_source_list</vh></v>
<v t="ekr.20220525082934.259"><vh>keyfunc</vh></v>
<v t="ekr.20220525082934.260"><vh>normalise_package_base</vh></v>
<v t="ekr.20220525082934.261"><vh>get_explicit_package_bases</vh></v>
<v t="ekr.20220525082934.262"><vh>class SourceFinder</vh>
<v t="ekr.20220525082934.263"><vh>SourceFinder.__init__</vh></v>
<v t="ekr.20220525082934.264"><vh>SourceFinder.is_explicit_package_base</vh></v>
<v t="ekr.20220525082934.265"><vh>SourceFinder.find_sources_in_dir</vh></v>
<v t="ekr.20220525082934.266"><vh>SourceFinder.crawl_up</vh></v>
<v t="ekr.20220525082934.267"><vh>SourceFinder.crawl_up_dir</vh></v>
<v t="ekr.20220525082934.268"><vh>SourceFinder._crawl_up_helper</vh></v>
<v t="ekr.20220525082934.269"><vh>SourceFinder.get_init_file</vh></v>
</v>
<v t="ekr.20220525082934.270"><vh>module_join</vh></v>
<v t="ekr.20220525082934.271"><vh>strip_py</vh></v>
</v>
<v t="ekr.20220525082934.272"><vh>@clean fixup.py</vh>
<v t="ekr.20220525082934.273"><vh>fixup_module</vh></v>
<v t="ekr.20220525082934.274"><vh>class NodeFixer</vh>
<v t="ekr.20220525082934.275"><vh>NodeFixer.__init__</vh></v>
<v t="ekr.20220525082934.276"><vh>NodeFixer.visit_type_info</vh></v>
<v t="ekr.20220525082934.277"><vh>NodeFixer.visit_symbol_table</vh></v>
<v t="ekr.20220525082934.278"><vh>NodeFixer.visit_func_def</vh></v>
<v t="ekr.20220525082934.279"><vh>NodeFixer.visit_overloaded_func_def</vh></v>
<v t="ekr.20220525082934.280"><vh>NodeFixer.visit_decorator</vh></v>
<v t="ekr.20220525082934.281"><vh>NodeFixer.visit_class_def</vh></v>
<v t="ekr.20220525082934.282"><vh>NodeFixer.visit_type_var_expr</vh></v>
<v t="ekr.20220525082934.283"><vh>NodeFixer.visit_var</vh></v>
<v t="ekr.20220525082934.284"><vh>NodeFixer.visit_type_alias</vh></v>
</v>
<v t="ekr.20220525082934.285"><vh>class TypeFixer</vh>
<v t="ekr.20220525082934.286"><vh>TypeFixer.__init__</vh></v>
<v t="ekr.20220525082934.287"><vh>TypeFixer.visit_instance</vh></v>
<v t="ekr.20220525082934.288"><vh>TypeFixer.visit_type_alias_type</vh></v>
<v t="ekr.20220525082934.289"><vh>TypeFixer.visit_any</vh></v>
<v t="ekr.20220525082934.290"><vh>TypeFixer.visit_callable_type</vh></v>
<v t="ekr.20220525082934.291"><vh>TypeFixer.visit_overloaded</vh></v>
<v t="ekr.20220525082934.292"><vh>TypeFixer.visit_erased_type</vh></v>
<v t="ekr.20220525082934.293"><vh>TypeFixer.visit_deleted_type</vh></v>
<v t="ekr.20220525082934.294"><vh>TypeFixer.visit_none_type</vh></v>
<v t="ekr.20220525082934.295"><vh>TypeFixer.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082934.296"><vh>TypeFixer.visit_partial_type</vh></v>
<v t="ekr.20220525082934.297"><vh>TypeFixer.visit_tuple_type</vh></v>
<v t="ekr.20220525082934.298"><vh>TypeFixer.visit_typeddict_type</vh></v>
<v t="ekr.20220525082934.299"><vh>TypeFixer.visit_literal_type</vh></v>
<v t="ekr.20220525082934.300"><vh>TypeFixer.visit_type_var</vh></v>
<v t="ekr.20220525082934.301"><vh>TypeFixer.visit_param_spec</vh></v>
<v t="ekr.20220525082934.302"><vh>TypeFixer.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082934.303"><vh>TypeFixer.visit_unpack_type</vh></v>
<v t="ekr.20220525082934.304"><vh>TypeFixer.visit_parameters</vh></v>
<v t="ekr.20220525082934.305"><vh>TypeFixer.visit_unbound_type</vh></v>
<v t="ekr.20220525082934.306"><vh>TypeFixer.visit_union_type</vh></v>
<v t="ekr.20220525082934.307"><vh>TypeFixer.visit_void</vh></v>
<v t="ekr.20220525082934.308"><vh>TypeFixer.visit_type_type</vh></v>
</v>
<v t="ekr.20220525082934.309"><vh>lookup_fully_qualified_typeinfo</vh></v>
<v t="ekr.20220525082934.310"><vh>lookup_fully_qualified_alias</vh></v>
<v t="ekr.20220525082934.311"><vh>_SUGGESTION: Final = "&lt;missing {}: *should* have gone away during fine-grained update&gt;"</vh></v>
<v t="ekr.20220525082934.312"><vh>missing_info</vh></v>
<v t="ekr.20220525082934.313"><vh>missing_alias</vh></v>
</v>
<v t="ekr.20220525082934.314"><vh>@clean freetree.py</vh>
<v t="ekr.20220525082934.315"><vh>class TreeFreer</vh></v>
<v t="ekr.20220525082934.316"><vh>free_tree</vh></v>
</v>
<v t="ekr.20220525082934.317"><vh>@clean fscache.py</vh>
<v t="ekr.20220525082934.318"><vh>class FileSystemCache</vh>
<v t="ekr.20220525082934.319"><vh>FileSystemCache.__init__</vh></v>
<v t="ekr.20220525082934.320"><vh>FileSystemCache.set_package_root</vh></v>
<v t="ekr.20220525082934.321"><vh>FileSystemCache.flush</vh></v>
<v t="ekr.20220525082934.322"><vh>FileSystemCache.stat</vh></v>
<v t="ekr.20220525082934.323"><vh>FileSystemCache.init_under_package_root</vh></v>
<v t="ekr.20220525082934.324"><vh>FileSystemCache._fake_init</vh></v>
<v t="ekr.20220525082934.325"><vh>FileSystemCache.listdir</vh></v>
<v t="ekr.20220525082934.326"><vh>FileSystemCache.isfile</vh></v>
<v t="ekr.20220525082934.327"><vh>FileSystemCache.isfile_case</vh></v>
<v t="ekr.20220525082934.328"><vh>FileSystemCache.exists_case</vh></v>
<v t="ekr.20220525082934.329"><vh>FileSystemCache.isdir</vh></v>
<v t="ekr.20220525082934.330"><vh>FileSystemCache.exists</vh></v>
<v t="ekr.20220525082934.331"><vh>FileSystemCache.read</vh></v>
<v t="ekr.20220525082934.332"><vh>FileSystemCache.hash_digest</vh></v>
<v t="ekr.20220525082934.333"><vh>FileSystemCache.samefile</vh></v>
</v>
<v t="ekr.20220525082934.334"><vh>copy_os_error</vh></v>
</v>
<v t="ekr.20220525082934.335"><vh>@clean fswatcher.py</vh>
<v t="ekr.20220525082934.336"><vh>class FileSystemWatcher</vh>
<v t="ekr.20220525082934.337"><vh>FileSystemWatcher.__init__</vh></v>
<v t="ekr.20220525082934.338"><vh>FileSystemWatcher.dump_file_data</vh></v>
<v t="ekr.20220525082934.339"><vh>FileSystemWatcher.set_file_data</vh></v>
<v t="ekr.20220525082934.340"><vh>FileSystemWatcher.add_watched_paths</vh></v>
<v t="ekr.20220525082934.341"><vh>FileSystemWatcher.remove_watched_paths</vh></v>
<v t="ekr.20220525082934.342"><vh>FileSystemWatcher._update</vh></v>
<v t="ekr.20220525082934.343"><vh>FileSystemWatcher._find_changed</vh></v>
<v t="ekr.20220525082934.344"><vh>FileSystemWatcher.find_changed</vh></v>
<v t="ekr.20220525082934.345"><vh>FileSystemWatcher.update_changed</vh></v>
</v>
</v>
<v t="ekr.20220525082934.346"><vh>@clean gclogger.py</vh>
<v t="ekr.20220525082934.347"><vh>class GcLogger</vh>
<v t="ekr.20220525082934.348"><vh>GcLogger.__enter__</vh></v>
<v t="ekr.20220525082934.349"><vh>GcLogger.gc_callback</vh></v>
<v t="ekr.20220525082934.350"><vh>GcLogger.__exit__</vh></v>
<v t="ekr.20220525082934.351"><vh>GcLogger.get_stats</vh></v>
</v>
</v>
<v t="ekr.20220525082934.352"><vh>@clean git.py</vh>
<v t="ekr.20220525082934.353"><vh>is_git_repo</vh></v>
<v t="ekr.20220525082934.354"><vh>have_git</vh></v>
<v t="ekr.20220525082934.355"><vh>git_revision</vh></v>
<v t="ekr.20220525082934.356"><vh>is_dirty</vh></v>
</v>
<v t="ekr.20220525082934.357"><vh>@clean indirection.py</vh>
<v t="ekr.20220525082934.358"><vh>extract_module_names</vh></v>
<v t="ekr.20220525082934.359"><vh>class TypeIndirectionVisitor</vh>
<v t="ekr.20220525082934.360"><vh>TypeIndirectionVisitor.__init__</vh></v>
<v t="ekr.20220525082934.361"><vh>TypeIndirectionVisitor.find_modules</vh></v>
<v t="ekr.20220525082934.362"><vh>TypeIndirectionVisitor._visit</vh></v>
<v t="ekr.20220525082934.363"><vh>TypeIndirectionVisitor.visit_unbound_type</vh></v>
<v t="ekr.20220525082934.364"><vh>TypeIndirectionVisitor.visit_any</vh></v>
<v t="ekr.20220525082934.365"><vh>TypeIndirectionVisitor.visit_none_type</vh></v>
<v t="ekr.20220525082934.366"><vh>TypeIndirectionVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082934.367"><vh>TypeIndirectionVisitor.visit_erased_type</vh></v>
<v t="ekr.20220525082934.368"><vh>TypeIndirectionVisitor.visit_deleted_type</vh></v>
<v t="ekr.20220525082934.369"><vh>TypeIndirectionVisitor.visit_type_var</vh></v>
<v t="ekr.20220525082934.370"><vh>TypeIndirectionVisitor.visit_param_spec</vh></v>
<v t="ekr.20220525082934.371"><vh>TypeIndirectionVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082934.372"><vh>TypeIndirectionVisitor.visit_unpack_type</vh></v>
<v t="ekr.20220525082934.373"><vh>TypeIndirectionVisitor.visit_parameters</vh></v>
<v t="ekr.20220525082934.374"><vh>TypeIndirectionVisitor.visit_instance</vh></v>
<v t="ekr.20220525082934.375"><vh>TypeIndirectionVisitor.visit_callable_type</vh></v>
<v t="ekr.20220525082934.376"><vh>TypeIndirectionVisitor.visit_overloaded</vh></v>
<v t="ekr.20220525082934.377"><vh>TypeIndirectionVisitor.visit_tuple_type</vh></v>
<v t="ekr.20220525082934.378"><vh>TypeIndirectionVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20220525082934.379"><vh>TypeIndirectionVisitor.visit_literal_type</vh></v>
<v t="ekr.20220525082934.380"><vh>TypeIndirectionVisitor.visit_union_type</vh></v>
<v t="ekr.20220525082934.381"><vh>TypeIndirectionVisitor.visit_partial_type</vh></v>
<v t="ekr.20220525082934.382"><vh>TypeIndirectionVisitor.visit_type_type</vh></v>
<v t="ekr.20220525082934.383"><vh>TypeIndirectionVisitor.visit_type_alias_type</vh></v>
</v>
</v>
<v t="ekr.20220525082934.384"><vh>@clean infer.py</vh>
<v t="ekr.20220525082934.385"><vh>class ArgumentInferContext</vh></v>
<v t="ekr.20220525082934.386"><vh>infer_function_type_arguments</vh></v>
<v t="ekr.20220525082934.387"><vh>infer_type_arguments</vh></v>
</v>
<v t="ekr.20220525082934.388"><vh>@clean ipc.py</vh>
<v t="ekr.20220525082934.389"><vh>class IPCException</vh></v>
<v t="ekr.20220525082934.390"><vh>class IPCBase</vh>
<v t="ekr.20220525082934.391"><vh>IPCBase.__init__</vh></v>
<v t="ekr.20220525082934.392"><vh>IPCBase.read</vh></v>
<v t="ekr.20220525082934.393"><vh>IPCBase.write</vh></v>
<v t="ekr.20220525082934.394"><vh>IPCBase.close</vh></v>
</v>
<v t="ekr.20220525082934.395"><vh>class IPCClient</vh>
<v t="ekr.20220525082934.396"><vh>IPCClient.__init__</vh></v>
<v t="ekr.20220525082934.397"><vh>IPCClient.__enter__</vh></v>
<v t="ekr.20220525082934.398"><vh>IPCClient.__exit__</vh></v>
</v>
<v t="ekr.20220525082934.399"><vh>class IPCServer</vh>
<v t="ekr.20220525082934.400"><vh>IPCServer.__init__</vh></v>
<v t="ekr.20220525082934.401"><vh>IPCServer.__enter__</vh></v>
<v t="ekr.20220525082934.402"><vh>IPCServer.__exit__</vh></v>
<v t="ekr.20220525082934.403"><vh>IPCServer.cleanup</vh></v>
<v t="ekr.20220525082934.404"><vh>IPCServer.connection_name</vh></v>
</v>
</v>
<v t="ekr.20220525082934.405"><vh>@clean join.py</vh>
<v t="ekr.20220525082934.406"><vh>class InstanceJoiner</vh>
<v t="ekr.20220525082934.407"><vh>InstanceJoiner.__init__</vh></v>
<v t="ekr.20220525082934.408"><vh>InstanceJoiner.join_instances</vh></v>
<v t="ekr.20220525082934.409"><vh>InstanceJoiner.join_instances_via_supertype</vh></v>
</v>
<v t="ekr.20220525082934.410"><vh>join_simple</vh></v>
<v t="ekr.20220525082934.411"><vh>trivial_join</vh></v>
<v t="ekr.20220525082934.412"><vh>join_types</vh></v>
<v t="ekr.20220525082934.413"><vh>class TypeJoinVisitor</vh>
<v t="ekr.20220525082934.414"><vh>TypeJoinVisitor.__init__</vh></v>
<v t="ekr.20220525082934.415"><vh>TypeJoinVisitor.visit_unbound_type</vh></v>
<v t="ekr.20220525082934.416"><vh>TypeJoinVisitor.visit_union_type</vh></v>
<v t="ekr.20220525082934.417"><vh>TypeJoinVisitor.visit_any</vh></v>
<v t="ekr.20220525082934.418"><vh>TypeJoinVisitor.visit_none_type</vh></v>
<v t="ekr.20220525082934.419"><vh>TypeJoinVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082934.420"><vh>TypeJoinVisitor.visit_deleted_type</vh></v>
<v t="ekr.20220525082934.421"><vh>TypeJoinVisitor.visit_erased_type</vh></v>
<v t="ekr.20220525082934.422"><vh>TypeJoinVisitor.visit_type_var</vh></v>
<v t="ekr.20220525082934.423"><vh>TypeJoinVisitor.visit_param_spec</vh></v>
<v t="ekr.20220525082934.424"><vh>TypeJoinVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082934.425"><vh>TypeJoinVisitor.visit_unpack_type</vh></v>
<v t="ekr.20220525082934.426"><vh>TypeJoinVisitor.visit_parameters</vh></v>
<v t="ekr.20220525082934.427"><vh>TypeJoinVisitor.visit_instance</vh></v>
<v t="ekr.20220525082934.428"><vh>TypeJoinVisitor.visit_callable_type</vh></v>
<v t="ekr.20220525082934.429"><vh>TypeJoinVisitor.visit_overloaded</vh></v>
<v t="ekr.20220525082934.430"><vh>TypeJoinVisitor.visit_tuple_type</vh></v>
<v t="ekr.20220525082934.431"><vh>TypeJoinVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20220525082934.432"><vh>TypeJoinVisitor.visit_literal_type</vh></v>
<v t="ekr.20220525082934.433"><vh>TypeJoinVisitor.visit_partial_type</vh></v>
<v t="ekr.20220525082934.434"><vh>TypeJoinVisitor.visit_type_type</vh></v>
<v t="ekr.20220525082934.435"><vh>TypeJoinVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20220525082934.436"><vh>TypeJoinVisitor.join</vh></v>
<v t="ekr.20220525082934.437"><vh>TypeJoinVisitor.default</vh></v>
</v>
<v t="ekr.20220525082934.438"><vh>is_better</vh></v>
<v t="ekr.20220525082934.439"><vh>is_similar_callables</vh></v>
<v t="ekr.20220525082934.440"><vh>join_similar_callables</vh></v>
<v t="ekr.20220525082934.441"><vh>combine_similar_callables</vh></v>
<v t="ekr.20220525082934.442"><vh>combine_arg_names</vh></v>
<v t="ekr.20220525082934.443"><vh>object_from_instance</vh></v>
<v t="ekr.20220525082934.444"><vh>object_or_any_from_type</vh></v>
<v t="ekr.20220525082934.445"><vh>join_type_list</vh></v>
<v t="ekr.20220525082934.446"><vh>unpack_callback_protocol</vh></v>
</v>
<v t="ekr.20220525082934.447"><vh>@clean literals.py</vh>
<v t="ekr.20220525082934.448"><vh>literal</vh></v>
<v t="ekr.20220525082934.449"><vh>Key = Tuple[Any, ...]</vh></v>
<v t="ekr.20220525082934.450"><vh>subkeys</vh></v>
<v t="ekr.20220525082934.451"><vh>literal_hash</vh></v>
<v t="ekr.20220525082934.452"><vh>class _Hasher</vh>
<v t="ekr.20220525082934.453"><vh>_Hasher.visit_int_expr</vh></v>
<v t="ekr.20220525082934.454"><vh>_Hasher.visit_str_expr</vh></v>
<v t="ekr.20220525082934.455"><vh>_Hasher.visit_bytes_expr</vh></v>
<v t="ekr.20220525082934.456"><vh>_Hasher.visit_unicode_expr</vh></v>
<v t="ekr.20220525082934.457"><vh>_Hasher.visit_float_expr</vh></v>
<v t="ekr.20220525082934.458"><vh>_Hasher.visit_complex_expr</vh></v>
<v t="ekr.20220525082934.459"><vh>_Hasher.visit_star_expr</vh></v>
<v t="ekr.20220525082934.460"><vh>_Hasher.visit_name_expr</vh></v>
<v t="ekr.20220525082934.461"><vh>_Hasher.visit_member_expr</vh></v>
<v t="ekr.20220525082934.462"><vh>_Hasher.visit_op_expr</vh></v>
<v t="ekr.20220525082934.463"><vh>_Hasher.visit_comparison_expr</vh></v>
<v t="ekr.20220525082934.464"><vh>_Hasher.visit_unary_expr</vh></v>
<v t="ekr.20220525082934.465"><vh>_Hasher.seq_expr</vh></v>
<v t="ekr.20220525082934.466"><vh>_Hasher.visit_list_expr</vh></v>
<v t="ekr.20220525082934.467"><vh>_Hasher.visit_dict_expr</vh></v>
<v t="ekr.20220525082934.468"><vh>_Hasher.visit_tuple_expr</vh></v>
<v t="ekr.20220525082934.469"><vh>_Hasher.visit_set_expr</vh></v>
<v t="ekr.20220525082934.470"><vh>_Hasher.visit_index_expr</vh></v>
<v t="ekr.20220525082934.471"><vh>_Hasher.visit_assignment_expr</vh></v>
<v t="ekr.20220525082934.472"><vh>_Hasher.visit_call_expr</vh></v>
<v t="ekr.20220525082934.473"><vh>_Hasher.visit_slice_expr</vh></v>
<v t="ekr.20220525082934.474"><vh>_Hasher.visit_cast_expr</vh></v>
<v t="ekr.20220525082934.475"><vh>_Hasher.visit_assert_type_expr</vh></v>
<v t="ekr.20220525082934.476"><vh>_Hasher.visit_conditional_expr</vh></v>
<v t="ekr.20220525082934.477"><vh>_Hasher.visit_ellipsis</vh></v>
<v t="ekr.20220525082934.478"><vh>_Hasher.visit_yield_from_expr</vh></v>
<v t="ekr.20220525082934.479"><vh>_Hasher.visit_yield_expr</vh></v>
<v t="ekr.20220525082934.480"><vh>_Hasher.visit_reveal_expr</vh></v>
<v t="ekr.20220525082934.481"><vh>_Hasher.visit_super_expr</vh></v>
<v t="ekr.20220525082934.482"><vh>_Hasher.visit_type_application</vh></v>
<v t="ekr.20220525082934.483"><vh>_Hasher.visit_lambda_expr</vh></v>
<v t="ekr.20220525082934.484"><vh>_Hasher.visit_list_comprehension</vh></v>
<v t="ekr.20220525082934.485"><vh>_Hasher.visit_set_comprehension</vh></v>
<v t="ekr.20220525082934.486"><vh>_Hasher.visit_dictionary_comprehension</vh></v>
<v t="ekr.20220525082934.487"><vh>_Hasher.visit_generator_expr</vh></v>
<v t="ekr.20220525082934.488"><vh>_Hasher.visit_backquote_expr</vh></v>
<v t="ekr.20220525082934.489"><vh>_Hasher.visit_type_var_expr</vh></v>
<v t="ekr.20220525082934.490"><vh>_Hasher.visit_paramspec_expr</vh></v>
<v t="ekr.20220525082934.491"><vh>_Hasher.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20220525082934.492"><vh>_Hasher.visit_type_alias_expr</vh></v>
<v t="ekr.20220525082934.493"><vh>_Hasher.visit_namedtuple_expr</vh></v>
<v t="ekr.20220525082934.494"><vh>_Hasher.visit_enum_call_expr</vh></v>
<v t="ekr.20220525082934.495"><vh>_Hasher.visit_typeddict_expr</vh></v>
<v t="ekr.20220525082934.496"><vh>_Hasher.visit_newtype_expr</vh></v>
<v t="ekr.20220525082934.497"><vh>_Hasher.visit__promote_expr</vh></v>
<v t="ekr.20220525082934.498"><vh>_Hasher.visit_await_expr</vh></v>
<v t="ekr.20220525082934.499"><vh>_Hasher.visit_temp_node</vh></v>
</v>
</v>
<v t="ekr.20220525082934.500"><vh>@clean lookup.py</vh>
<v t="ekr.20220525082934.501"><vh>lookup_fully_qualified</vh></v>
</v>
<v t="ekr.20220525082934.502"><vh>@clean main.py</vh>
<v t="ekr.20220525082934.503"><vh>stat_proxy</vh></v>
<v t="ekr.20220525082934.504"><vh>main</vh></v>
<v t="ekr.20220525082934.505"><vh>run_build</vh>
<v t="ekr.20220525082934.506"><vh>flush_errors</vh></v>
</v>
<v t="ekr.20220525082934.507"><vh>show_messages</vh></v>
<v t="ekr.20220525082934.508"><vh>class AugmentedHelpFormatter</vh>
<v t="ekr.20220525082934.509"><vh>AugmentedHelpFormatter.__init__</vh></v>
<v t="ekr.20220525082934.510"><vh>AugmentedHelpFormatter._fill_text</vh></v>
</v>
<v t="ekr.20220525082934.511"><vh>Define pairs of flag prefixes with inverse meaning.</vh></v>
<v t="ekr.20220525082934.512"><vh>invert_flag_name</vh></v>
<v t="ekr.20220525082934.513"><vh>class PythonExecutableInferenceError</vh></v>
<v t="ekr.20220525082934.514"><vh>python_executable_prefix</vh></v>
<v t="ekr.20220525082934.515"><vh>_python_executable_from_version</vh></v>
<v t="ekr.20220525082934.516"><vh>infer_python_executable</vh></v>
<v t="ekr.20220525082934.517"><vh>HEADER: Final = """%(prog)s [-h] [-v] [-V] [more options; see below]</vh></v>
<v t="ekr.20220525082934.518"><vh>class CapturableArgumentParser</vh>
<v t="ekr.20220525082934.519"><vh>CapturableArgumentParser.__init__</vh></v>
<v t="ekr.20220525082934.520"><vh>CapturableArgumentParser.print_usage</vh></v>
<v t="ekr.20220525082934.521"><vh>CapturableArgumentParser.print_help</vh></v>
<v t="ekr.20220525082934.522"><vh>CapturableArgumentParser._print_message</vh></v>
<v t="ekr.20220525082934.523"><vh>CapturableArgumentParser.exit</vh></v>
<v t="ekr.20220525082934.524"><vh>CapturableArgumentParser.error</vh></v>
</v>
<v t="ekr.20220525082934.525"><vh>class CapturableVersionAction</vh>
<v t="ekr.20220525082934.526"><vh>CapturableVersionAction.__init__</vh></v>
<v t="ekr.20220525082934.527"><vh>CapturableVersionAction.__call__</vh></v>
</v>
<v t="ekr.20220525082934.528"><vh>process_options</vh>
<v t="ekr.20220525082934.529"><vh>add_invertible_flag</vh></v>
<v t="ekr.20220525082934.530"><vh>Unless otherwise specified, arguments will be parsed directly onto an</vh></v>
<v t="ekr.20220525082934.531"><vh>set_strict_flags</vh></v>
</v>
<v t="ekr.20220525082934.532"><vh>process_package_roots</vh></v>
<v t="ekr.20220525082934.533"><vh>process_cache_map</vh></v>
<v t="ekr.20220525082934.534"><vh>maybe_write_junit_xml</vh></v>
<v t="ekr.20220525082934.535"><vh>fail</vh></v>
<v t="ekr.20220525082934.536"><vh>read_types_packages_to_install</vh></v>
<v t="ekr.20220525082934.537"><vh>install_types</vh></v>
</v>
<v t="ekr.20220525082934.538"><vh>@clean maptype.py</vh>
<v t="ekr.20220525082934.539"><vh>map_instance_to_supertype</vh></v>
<v t="ekr.20220525082934.540"><vh>map_instance_to_supertypes</vh></v>
<v t="ekr.20220525082934.541"><vh>class_derivation_paths</vh></v>
<v t="ekr.20220525082934.542"><vh>map_instance_to_direct_supertypes</vh></v>
<v t="ekr.20220525082934.543"><vh>instance_to_type_environment</vh></v>
</v>
<v t="ekr.20220525082934.544"><vh>@clean meet.py</vh>
<v t="ekr.20220525082934.545"><vh>trivial_meet</vh></v>
<v t="ekr.20220525082934.546"><vh>meet_types</vh></v>
<v t="ekr.20220525082934.547"><vh>narrow_declared_type</vh></v>
<v t="ekr.20220525082934.548"><vh>get_possible_variants</vh></v>
<v t="ekr.20220525082934.549"><vh>is_enum_overlapping_union</vh></v>
<v t="ekr.20220525082934.550"><vh>is_literal_in_union</vh></v>
<v t="ekr.20220525082934.551"><vh>is_overlapping_types</vh>
<v t="ekr.20220525082934.552"><vh>_is_overlapping_types</vh></v>
<v t="ekr.20220525082934.553"><vh>We should never encounter this type.</vh></v>
<v t="ekr.20220525082934.554"><vh>is_none_typevar_overlap</vh></v>
<v t="ekr.20220525082934.555"><vh>if prohibit_none_typevar_overlap:</vh></v>
<v t="ekr.20220525082934.556"><vh>_type_object_overlap</vh></v>
</v>
<v t="ekr.20220525082934.557"><vh>is_overlapping_erased_types</vh></v>
<v t="ekr.20220525082934.558"><vh>are_typed_dicts_overlapping</vh></v>
<v t="ekr.20220525082934.559"><vh>are_tuples_overlapping</vh></v>
<v t="ekr.20220525082934.560"><vh>adjust_tuple</vh></v>
<v t="ekr.20220525082934.561"><vh>is_tuple</vh></v>
<v t="ekr.20220525082934.562"><vh>class TypeMeetVisitor</vh>
<v t="ekr.20220525082934.563"><vh>TypeMeetVisitor.__init__</vh></v>
<v t="ekr.20220525082934.564"><vh>TypeMeetVisitor.visit_unbound_type</vh></v>
<v t="ekr.20220525082934.565"><vh>TypeMeetVisitor.visit_any</vh></v>
<v t="ekr.20220525082934.566"><vh>TypeMeetVisitor.visit_union_type</vh></v>
<v t="ekr.20220525082934.567"><vh>TypeMeetVisitor.visit_none_type</vh></v>
<v t="ekr.20220525082934.568"><vh>TypeMeetVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082934.569"><vh>TypeMeetVisitor.visit_deleted_type</vh></v>
<v t="ekr.20220525082934.570"><vh>TypeMeetVisitor.visit_erased_type</vh></v>
<v t="ekr.20220525082934.571"><vh>TypeMeetVisitor.visit_type_var</vh></v>
<v t="ekr.20220525082934.572"><vh>TypeMeetVisitor.visit_param_spec</vh></v>
<v t="ekr.20220525082934.573"><vh>TypeMeetVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082934.574"><vh>TypeMeetVisitor.visit_unpack_type</vh></v>
<v t="ekr.20220525082934.575"><vh>TypeMeetVisitor.visit_parameters</vh></v>
<v t="ekr.20220525082934.576"><vh>TypeMeetVisitor.visit_instance</vh></v>
<v t="ekr.20220525082934.577"><vh>TypeMeetVisitor.visit_callable_type</vh></v>
<v t="ekr.20220525082934.578"><vh>TypeMeetVisitor.visit_overloaded</vh></v>
<v t="ekr.20220525082934.579"><vh>TypeMeetVisitor.visit_tuple_type</vh></v>
<v t="ekr.20220525082934.580"><vh>TypeMeetVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20220525082934.581"><vh>TypeMeetVisitor.visit_literal_type</vh></v>
<v t="ekr.20220525082934.582"><vh>TypeMeetVisitor.visit_partial_type</vh></v>
<v t="ekr.20220525082934.583"><vh>TypeMeetVisitor.visit_type_type</vh></v>
<v t="ekr.20220525082934.584"><vh>TypeMeetVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20220525082934.585"><vh>TypeMeetVisitor.meet</vh></v>
<v t="ekr.20220525082934.586"><vh>TypeMeetVisitor.default</vh></v>
</v>
<v t="ekr.20220525082934.587"><vh>meet_similar_callables</vh></v>
<v t="ekr.20220525082934.588"><vh>meet_type_list</vh></v>
<v t="ekr.20220525082934.589"><vh>typed_dict_mapping_pair</vh></v>
<v t="ekr.20220525082934.590"><vh>typed_dict_mapping_overlap</vh></v>
</v>
<v t="ekr.20220525082934.591"><vh>@clean memprofile.py</vh>
<v t="ekr.20220525082934.592"><vh>collect_memory_stats</vh></v>
<v t="ekr.20220525082934.593"><vh>print_memory_profile</vh></v>
<v t="ekr.20220525082934.594"><vh>find_recursive_objects</vh>
<v t="ekr.20220525082934.595"><vh>visit</vh></v>
</v>
</v>
<v t="ekr.20220525082934.596"><vh>@clean messages.py</vh>
<v t="ekr.20220525082934.597"><vh>class MessageBuilder</vh>
<v t="ekr.20220525082934.598"><vh>MessageBuilder.__init__</vh></v>
<v t="ekr.20220525082934.599"><vh>MessageBuilder.Helpers</vh></v>
<v t="ekr.20220525082934.600"><vh>MessageBuilder.filter_errors</vh></v>
<v t="ekr.20220525082934.601"><vh>MessageBuilder.add_errors</vh></v>
<v t="ekr.20220525082934.602"><vh>MessageBuilder.disable_type_names</vh></v>
<v t="ekr.20220525082934.603"><vh>MessageBuilder.are_type_names_disabled</vh></v>
<v t="ekr.20220525082934.604"><vh>MessageBuilder.report</vh></v>
<v t="ekr.20220525082934.605"><vh>MessageBuilder.fail</vh></v>
<v t="ekr.20220525082934.606"><vh>MessageBuilder.note</vh></v>
<v t="ekr.20220525082934.607"><vh>MessageBuilder.note_multiline</vh></v>
<v t="ekr.20220525082934.608"><vh>MessageBuilder.Specific operations</vh></v>
<v t="ekr.20220525082934.609"><vh>MessageBuilder.has_no_attr</vh></v>
<v t="ekr.20220525082934.610"><vh>MessageBuilder.unsupported_operand_types</vh></v>
<v t="ekr.20220525082934.611"><vh>MessageBuilder.unsupported_left_operand</vh></v>
<v t="ekr.20220525082934.612"><vh>MessageBuilder.not_callable</vh></v>
<v t="ekr.20220525082934.613"><vh>MessageBuilder.untyped_function_call</vh></v>
<v t="ekr.20220525082934.614"><vh>MessageBuilder.incompatible_argument</vh></v>
<v t="ekr.20220525082934.615"><vh>MessageBuilder.incompatible_argument_note</vh></v>
<v t="ekr.20220525082934.616"><vh>MessageBuilder.maybe_note_concatenate_pos_args</vh></v>
<v t="ekr.20220525082934.617"><vh>MessageBuilder.invalid_index_type</vh></v>
<v t="ekr.20220525082934.618"><vh>MessageBuilder.too_few_arguments</vh></v>
<v t="ekr.20220525082934.619"><vh>MessageBuilder.missing_named_argument</vh></v>
<v t="ekr.20220525082934.620"><vh>MessageBuilder.too_many_arguments</vh></v>
<v t="ekr.20220525082934.621"><vh>MessageBuilder.too_many_arguments_from_typed_dict</vh></v>
<v t="ekr.20220525082934.622"><vh>MessageBuilder.too_many_positional_arguments</vh></v>
<v t="ekr.20220525082934.623"><vh>MessageBuilder.maybe_note_about_special_args</vh></v>
<v t="ekr.20220525082934.624"><vh>MessageBuilder.unexpected_keyword_argument</vh></v>
<v t="ekr.20220525082934.625"><vh>MessageBuilder.duplicate_argument_value</vh></v>
<v t="ekr.20220525082934.626"><vh>MessageBuilder.does_not_return_value</vh></v>
<v t="ekr.20220525082934.627"><vh>MessageBuilder.underscore_function_call</vh></v>
<v t="ekr.20220525082934.628"><vh>MessageBuilder.deleted_as_rvalue</vh></v>
<v t="ekr.20220525082934.629"><vh>MessageBuilder.deleted_as_lvalue</vh></v>
<v t="ekr.20220525082934.630"><vh>MessageBuilder.no_variant_matches_arguments</vh></v>
<v t="ekr.20220525082934.631"><vh>MessageBuilder.wrong_number_values_to_unpack</vh></v>
<v t="ekr.20220525082934.632"><vh>MessageBuilder.unpacking_strings_disallowed</vh></v>
<v t="ekr.20220525082934.633"><vh>MessageBuilder.type_not_iterable</vh></v>
<v t="ekr.20220525082934.634"><vh>MessageBuilder.incompatible_operator_assignment</vh></v>
<v t="ekr.20220525082934.635"><vh>MessageBuilder.overload_signature_incompatible_with_supertype</vh></v>
<v t="ekr.20220525082934.636"><vh>MessageBuilder.signature_incompatible_with_supertype</vh></v>
<v t="ekr.20220525082934.637"><vh>MessageBuilder.pretty_callable_or_overload</vh></v>
<v t="ekr.20220525082934.638"><vh>MessageBuilder.argument_incompatible_with_supertype</vh></v>
<v t="ekr.20220525082934.639"><vh>MessageBuilder.comparison_method_example_msg</vh></v>
<v t="ekr.20220525082934.640"><vh>MessageBuilder.return_type_incompatible_with_supertype</vh></v>
<v t="ekr.20220525082934.641"><vh>MessageBuilder.override_target</vh></v>
<v t="ekr.20220525082934.642"><vh>MessageBuilder.incompatible_type_application</vh></v>
<v t="ekr.20220525082934.643"><vh>MessageBuilder.could_not_infer_type_arguments</vh></v>
<v t="ekr.20220525082934.644"><vh>MessageBuilder.invalid_var_arg</vh></v>
<v t="ekr.20220525082934.645"><vh>MessageBuilder.invalid_keyword_var_arg</vh></v>
<v t="ekr.20220525082934.646"><vh>MessageBuilder.undefined_in_superclass</vh></v>
<v t="ekr.20220525082934.647"><vh>MessageBuilder.first_argument_for_super_must_be_type</vh></v>
<v t="ekr.20220525082934.648"><vh>MessageBuilder.too_few_string_formatting_arguments</vh></v>
<v t="ekr.20220525082934.649"><vh>MessageBuilder.too_many_string_formatting_arguments</vh></v>
<v t="ekr.20220525082934.650"><vh>MessageBuilder.unsupported_placeholder</vh></v>
<v t="ekr.20220525082934.651"><vh>MessageBuilder.string_interpolation_with_star_and_key</vh></v>
<v t="ekr.20220525082934.652"><vh>MessageBuilder.requires_int_or_single_byte</vh></v>
<v t="ekr.20220525082934.653"><vh>MessageBuilder.requires_int_or_char</vh></v>
<v t="ekr.20220525082934.654"><vh>MessageBuilder.key_not_in_mapping</vh></v>
<v t="ekr.20220525082934.655"><vh>MessageBuilder.string_interpolation_mixing_key_and_non_keys</vh></v>
<v t="ekr.20220525082934.656"><vh>MessageBuilder.cannot_determine_type</vh></v>
<v t="ekr.20220525082934.657"><vh>MessageBuilder.cannot_determine_type_in_base</vh></v>
<v t="ekr.20220525082934.658"><vh>MessageBuilder.no_formal_self</vh></v>
<v t="ekr.20220525082934.659"><vh>MessageBuilder.incompatible_self_argument</vh></v>
<v t="ekr.20220525082934.660"><vh>MessageBuilder.incompatible_conditional_function_def</vh></v>
<v t="ekr.20220525082934.661"><vh>MessageBuilder.cannot_instantiate_abstract_class</vh></v>
<v t="ekr.20220525082934.662"><vh>MessageBuilder.base_class_definitions_incompatible</vh></v>
<v t="ekr.20220525082934.663"><vh>MessageBuilder.cant_assign_to_method</vh></v>
<v t="ekr.20220525082934.664"><vh>MessageBuilder.cant_assign_to_classvar</vh></v>
<v t="ekr.20220525082934.665"><vh>MessageBuilder.final_cant_override_writable</vh></v>
<v t="ekr.20220525082934.666"><vh>MessageBuilder.cant_override_final</vh></v>
<v t="ekr.20220525082934.667"><vh>MessageBuilder.cant_assign_to_final</vh></v>
<v t="ekr.20220525082934.668"><vh>MessageBuilder.protocol_members_cant_be_final</vh></v>
<v t="ekr.20220525082934.669"><vh>MessageBuilder.final_without_value</vh></v>
<v t="ekr.20220525082934.670"><vh>MessageBuilder.read_only_property</vh></v>
<v t="ekr.20220525082934.671"><vh>MessageBuilder.incompatible_typevar_value</vh></v>
<v t="ekr.20220525082934.672"><vh>MessageBuilder.dangerous_comparison</vh></v>
<v t="ekr.20220525082934.673"><vh>MessageBuilder.overload_inconsistently_applies_decorator</vh></v>
<v t="ekr.20220525082934.674"><vh>MessageBuilder.overloaded_signatures_overlap</vh></v>
<v t="ekr.20220525082934.675"><vh>MessageBuilder.overloaded_signature_will_never_match</vh></v>
<v t="ekr.20220525082934.676"><vh>MessageBuilder.overloaded_signatures_typevar_specific</vh></v>
<v t="ekr.20220525082934.677"><vh>MessageBuilder.overloaded_signatures_arg_specific</vh></v>
<v t="ekr.20220525082934.678"><vh>MessageBuilder.overloaded_signatures_ret_specific</vh></v>
<v t="ekr.20220525082934.679"><vh>MessageBuilder.warn_both_operands_are_from_unions</vh></v>
<v t="ekr.20220525082934.680"><vh>MessageBuilder.warn_operand_was_from_union</vh></v>
<v t="ekr.20220525082934.681"><vh>MessageBuilder.operator_method_signatures_overlap</vh></v>
<v t="ekr.20220525082934.682"><vh>MessageBuilder.forward_operator_not_callable</vh></v>
<v t="ekr.20220525082934.683"><vh>MessageBuilder.signatures_incompatible</vh></v>
<v t="ekr.20220525082934.684"><vh>MessageBuilder.yield_from_invalid_operand_type</vh></v>
<v t="ekr.20220525082934.685"><vh>MessageBuilder.invalid_signature</vh></v>
<v t="ekr.20220525082934.686"><vh>MessageBuilder.invalid_signature_for_special_method</vh></v>
<v t="ekr.20220525082934.687"><vh>MessageBuilder.reveal_type</vh></v>
<v t="ekr.20220525082934.688"><vh>MessageBuilder.reveal_locals</vh></v>
<v t="ekr.20220525082934.689"><vh>MessageBuilder.unsupported_type_type</vh></v>
<v t="ekr.20220525082934.690"><vh>MessageBuilder.redundant_cast</vh></v>
<v t="ekr.20220525082934.691"><vh>MessageBuilder.assert_type_fail</vh></v>
<v t="ekr.20220525082934.692"><vh>MessageBuilder.unimported_type_becomes_any</vh></v>
<v t="ekr.20220525082934.693"><vh>MessageBuilder.need_annotation_for_var</vh></v>
<v t="ekr.20220525082934.694"><vh>MessageBuilder.explicit_any</vh></v>
<v t="ekr.20220525082934.695"><vh>MessageBuilder.unexpected_typeddict_keys</vh></v>
<v t="ekr.20220525082934.696"><vh>MessageBuilder.typeddict_key_must_be_string_literal</vh></v>
<v t="ekr.20220525082934.697"><vh>MessageBuilder.typeddict_key_not_found</vh></v>
<v t="ekr.20220525082934.698"><vh>MessageBuilder.typeddict_context_ambiguous</vh></v>
<v t="ekr.20220525082934.699"><vh>MessageBuilder.typeddict_key_cannot_be_deleted</vh></v>
<v t="ekr.20220525082934.700"><vh>MessageBuilder.typeddict_setdefault_arguments_inconsistent</vh></v>
<v t="ekr.20220525082934.701"><vh>MessageBuilder.type_arguments_not_allowed</vh></v>
<v t="ekr.20220525082934.702"><vh>MessageBuilder.disallowed_any_type</vh></v>
<v t="ekr.20220525082934.703"><vh>MessageBuilder.incorrectly_returning_any</vh></v>
<v t="ekr.20220525082934.704"><vh>MessageBuilder.incorrect__exit__return</vh></v>
<v t="ekr.20220525082934.705"><vh>MessageBuilder.untyped_decorated_function</vh></v>
<v t="ekr.20220525082934.706"><vh>MessageBuilder.typed_function_untyped_decorator</vh></v>
<v t="ekr.20220525082934.707"><vh>MessageBuilder.bad_proto_variance</vh></v>
<v t="ekr.20220525082934.708"><vh>MessageBuilder.concrete_only_assign</vh></v>
<v t="ekr.20220525082934.709"><vh>MessageBuilder.concrete_only_call</vh></v>
<v t="ekr.20220525082934.710"><vh>MessageBuilder.cannot_use_function_with_type</vh></v>
<v t="ekr.20220525082934.711"><vh>MessageBuilder.report_non_method_protocol</vh></v>
<v t="ekr.20220525082934.712"><vh>MessageBuilder.note_call</vh></v>
<v t="ekr.20220525082934.713"><vh>MessageBuilder.unreachable_statement</vh></v>
<v t="ekr.20220525082934.714"><vh>MessageBuilder.redundant_left_operand</vh></v>
<v t="ekr.20220525082934.715"><vh>MessageBuilder.unreachable_right_operand</vh></v>
<v t="ekr.20220525082934.716"><vh>MessageBuilder.redundant_condition_in_comprehension</vh></v>
<v t="ekr.20220525082934.717"><vh>MessageBuilder.redundant_condition_in_if</vh></v>
<v t="ekr.20220525082934.718"><vh>MessageBuilder.redundant_expr</vh></v>
<v t="ekr.20220525082934.719"><vh>MessageBuilder.impossible_intersection</vh></v>
<v t="ekr.20220525082934.720"><vh>MessageBuilder.report_protocol_problems</vh></v>
<v t="ekr.20220525082934.721"><vh>MessageBuilder.pretty_overload</vh></v>
<v t="ekr.20220525082934.722"><vh>MessageBuilder.print_more</vh></v>
<v t="ekr.20220525082934.723"><vh>MessageBuilder.try_report_long_tuple_assignment_error</vh></v>
<v t="ekr.20220525082934.724"><vh>MessageBuilder.format_long_tuple_type</vh></v>
<v t="ekr.20220525082934.725"><vh>MessageBuilder.generate_incompatible_tuple_error</vh></v>
<v t="ekr.20220525082934.726"><vh>MessageBuilder.add_fixture_note</vh></v>
</v>
<v t="ekr.20220525082934.727"><vh>quote_type_string</vh></v>
<v t="ekr.20220525082934.728"><vh>format_callable_args</vh></v>
<v t="ekr.20220525082934.729"><vh>format_type_inner</vh>
<v t="ekr.20220525082934.730"><vh>format</vh></v>
<v t="ekr.20220525082934.731"><vh>format_list</vh></v>
<v t="ekr.20220525082934.732"><vh>format_literal_value</vh></v>
</v>
<v t="ekr.20220525082934.733"><vh>collect_all_instances</vh></v>
<v t="ekr.20220525082934.734"><vh>class CollectAllInstancesQuery</vh></v>
<v t="ekr.20220525082934.735"><vh>find_type_overlaps</vh></v>
<v t="ekr.20220525082934.736"><vh>format_type</vh></v>
<v t="ekr.20220525082934.737"><vh>format_type_bare</vh></v>
<v t="ekr.20220525082934.738"><vh>format_type_distinctly</vh></v>
<v t="ekr.20220525082934.739"><vh>pretty_class_or_static_decorator</vh></v>
<v t="ekr.20220525082934.740"><vh>pretty_callable</vh></v>
<v t="ekr.20220525082934.741"><vh>variance_string</vh></v>
<v t="ekr.20220525082934.742"><vh>get_missing_protocol_members</vh></v>
<v t="ekr.20220525082934.743"><vh>get_conflict_protocol_types</vh></v>
<v t="ekr.20220525082934.744"><vh>get_bad_protocol_flags</vh></v>
<v t="ekr.20220525082934.745"><vh>capitalize</vh></v>
<v t="ekr.20220525082934.746"><vh>extract_type</vh></v>
<v t="ekr.20220525082934.747"><vh>strip_quotes</vh></v>
<v t="ekr.20220525082934.748"><vh>plural_s</vh></v>
<v t="ekr.20220525082934.749"><vh>format_string_list</vh></v>
<v t="ekr.20220525082934.750"><vh>format_item_name_list</vh></v>
<v t="ekr.20220525082934.751"><vh>callable_name</vh></v>
<v t="ekr.20220525082934.752"><vh>for_function</vh></v>
<v t="ekr.20220525082934.753"><vh>find_defining_module</vh></v>
<v t="ekr.20220525082934.754"><vh>For hard-coding suggested missing member alternatives.</vh></v>
<v t="ekr.20220525082934.755"><vh>best_matches</vh></v>
<v t="ekr.20220525082934.756"><vh>pretty_seq</vh></v>
<v t="ekr.20220525082934.757"><vh>append_invariance_notes</vh></v>
<v t="ekr.20220525082934.758"><vh>make_inferred_type_note</vh></v>
<v t="ekr.20220525082934.759"><vh>format_key_list</vh></v>
</v>
<v t="ekr.20220525082934.760"><vh>@clean message_registry.py</vh>
<v t="ekr.20220525082934.761"><vh>class ErrorMessage</vh></v>
</v>
<v t="ekr.20220525082934.762"><vh>@clean metastore.py</vh>
<v t="ekr.20220525082934.763"><vh>class MetadataStore</vh>
<v t="ekr.20220525082934.764"><vh>MetadataStore.getmtime</vh></v>
<v t="ekr.20220525082934.765"><vh>MetadataStore.read</vh></v>
<v t="ekr.20220525082934.766"><vh>MetadataStore.write</vh></v>
<v t="ekr.20220525082934.767"><vh>MetadataStore.remove</vh></v>
<v t="ekr.20220525082934.768"><vh>MetadataStore.commit</vh></v>
<v t="ekr.20220525082934.769"><vh>MetadataStore.list_all</vh></v>
</v>
<v t="ekr.20220525082934.770"><vh>random_string</vh></v>
<v t="ekr.20220525082934.771"><vh>class FilesystemMetadataStore</vh>
<v t="ekr.20220525082934.772"><vh>FilesystemMetadataStore.__init__</vh></v>
<v t="ekr.20220525082934.773"><vh>FilesystemMetadataStore.getmtime</vh></v>
<v t="ekr.20220525082934.774"><vh>FilesystemMetadataStore.read</vh></v>
<v t="ekr.20220525082934.775"><vh>FilesystemMetadataStore.write</vh></v>
<v t="ekr.20220525082934.776"><vh>FilesystemMetadataStore.remove</vh></v>
<v t="ekr.20220525082934.777"><vh>FilesystemMetadataStore.commit</vh></v>
<v t="ekr.20220525082934.778"><vh>FilesystemMetadataStore.list_all</vh></v>
</v>
<v t="ekr.20220525082934.779"><vh>SCHEMA = '''</vh></v>
<v t="ekr.20220525082934.780"><vh>connect_db</vh></v>
<v t="ekr.20220525082934.781"><vh>class SqliteMetadataStore</vh>
<v t="ekr.20220525082934.782"><vh>SqliteMetadataStore.__init__</vh></v>
<v t="ekr.20220525082934.783"><vh>SqliteMetadataStore._query</vh></v>
<v t="ekr.20220525082934.784"><vh>SqliteMetadataStore.getmtime</vh></v>
<v t="ekr.20220525082934.785"><vh>SqliteMetadataStore.read</vh></v>
<v t="ekr.20220525082934.786"><vh>SqliteMetadataStore.write</vh></v>
<v t="ekr.20220525082934.787"><vh>SqliteMetadataStore.remove</vh></v>
<v t="ekr.20220525082934.788"><vh>SqliteMetadataStore.commit</vh></v>
<v t="ekr.20220525082934.789"><vh>SqliteMetadataStore.list_all</vh></v>
</v>
</v>
<v t="ekr.20220525082934.790"><vh>@clean mixedtraverser.py</vh>
<v t="ekr.20220525082934.791"><vh>class MixedTraverserVisitor</vh>
<v t="ekr.20220525082934.792"><vh>MixedTraverserVisitor.visit_var</vh></v>
<v t="ekr.20220525082934.793"><vh>MixedTraverserVisitor.visit_func</vh></v>
<v t="ekr.20220525082934.794"><vh>MixedTraverserVisitor.visit_class_def</vh></v>
<v t="ekr.20220525082934.795"><vh>MixedTraverserVisitor.visit_type_alias_expr</vh></v>
<v t="ekr.20220525082934.796"><vh>MixedTraverserVisitor.visit_type_var_expr</vh></v>
<v t="ekr.20220525082934.797"><vh>MixedTraverserVisitor.visit_typeddict_expr</vh></v>
<v t="ekr.20220525082934.798"><vh>MixedTraverserVisitor.visit_namedtuple_expr</vh></v>
<v t="ekr.20220525082934.799"><vh>MixedTraverserVisitor.visit__promote_expr</vh></v>
<v t="ekr.20220525082934.800"><vh>MixedTraverserVisitor.visit_newtype_expr</vh></v>
<v t="ekr.20220525082934.801"><vh>MixedTraverserVisitor.Statements</vh></v>
<v t="ekr.20220525082934.802"><vh>MixedTraverserVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20220525082934.803"><vh>MixedTraverserVisitor.visit_for_stmt</vh></v>
<v t="ekr.20220525082934.804"><vh>MixedTraverserVisitor.visit_with_stmt</vh></v>
<v t="ekr.20220525082934.805"><vh>MixedTraverserVisitor.Expressions</vh></v>
<v t="ekr.20220525082934.806"><vh>MixedTraverserVisitor.visit_cast_expr</vh></v>
<v t="ekr.20220525082934.807"><vh>MixedTraverserVisitor.visit_assert_type_expr</vh></v>
<v t="ekr.20220525082934.808"><vh>MixedTraverserVisitor.visit_type_application</vh></v>
<v t="ekr.20220525082934.809"><vh>MixedTraverserVisitor.Helpers</vh></v>
<v t="ekr.20220525082934.810"><vh>MixedTraverserVisitor.visit_optional_type</vh></v>
</v>
</v>
<v t="ekr.20220525082934.811"><vh>@clean modulefinder.py</vh>
<v t="ekr.20220525082934.812"><vh>class ModuleNotFoundReason</vh>
<v t="ekr.20220525082934.813"><vh>ModuleNotFoundReason.error_message_templates</vh></v>
</v>
<v t="ekr.20220525082934.814"><vh>If we found the module, returns the path to the module as a str.</vh></v>
<v t="ekr.20220525082934.815"><vh>class BuildSource</vh>
<v t="ekr.20220525082934.816"><vh>BuildSource.__init__</vh></v>
<v t="ekr.20220525082934.817"><vh>BuildSource.__repr__</vh></v>
</v>
<v t="ekr.20220525082934.818"><vh>class BuildSourceSet</vh>
<v t="ekr.20220525082934.819"><vh>BuildSourceSet.__init__</vh></v>
<v t="ekr.20220525082934.820"><vh>BuildSourceSet.is_source</vh></v>
</v>
<v t="ekr.20220525082934.821"><vh>class FindModuleCache</vh>
<v t="ekr.20220525082934.822"><vh>FindModuleCache.__init__</vh></v>
<v t="ekr.20220525082934.823"><vh>FindModuleCache.clear</vh></v>
<v t="ekr.20220525082934.824"><vh>FindModuleCache.find_module_via_source_set</vh></v>
<v t="ekr.20220525082934.825"><vh>FindModuleCache.find_lib_path_dirs</vh></v>
<v t="ekr.20220525082934.826"><vh>FindModuleCache.get_toplevel_possibilities</vh></v>
<v t="ekr.20220525082934.827"><vh>FindModuleCache.find_module</vh></v>
<v t="ekr.20220525082934.828"><vh>FindModuleCache._typeshed_has_version</vh></v>
<v t="ekr.20220525082934.829"><vh>FindModuleCache._find_module_non_stub_helper</vh></v>
<v t="ekr.20220525082934.830"><vh>FindModuleCache._update_ns_ancestors</vh></v>
<v t="ekr.20220525082934.831"><vh>FindModuleCache._can_find_module_in_parent_dir</vh></v>
<v t="ekr.20220525082934.832"><vh>FindModuleCache._find_module</vh></v>
<v t="ekr.20220525082934.833"><vh>FindModuleCache._is_compatible_stub_package</vh></v>
<v t="ekr.20220525082934.834"><vh>FindModuleCache.find_modules_recursive</vh></v>
</v>
<v t="ekr.20220525082934.835"><vh>matches_exclude</vh></v>
<v t="ekr.20220525082934.836"><vh>verify_module</vh></v>
<v t="ekr.20220525082934.837"><vh>highest_init_level</vh></v>
<v t="ekr.20220525082934.838"><vh>mypy_path</vh></v>
<v t="ekr.20220525082934.839"><vh>default_lib_path</vh></v>
<v t="ekr.20220525082934.840"><vh>get_prefixes</vh></v>
<v t="ekr.20220525082934.841"><vh>get_site_packages_dirs</vh></v>
<v t="ekr.20220525082934.842"><vh>expand_site_packages</vh></v>
<v t="ekr.20220525082934.843"><vh>_parse_pth_file</vh></v>
<v t="ekr.20220525082934.844"><vh>_make_abspath</vh></v>
<v t="ekr.20220525082934.845"><vh>add_py2_mypypath_entries</vh></v>
<v t="ekr.20220525082934.846"><vh>compute_search_paths</vh></v>
<v t="ekr.20220525082934.847"><vh>load_stdlib_py_versions</vh></v>
<v t="ekr.20220525082934.848"><vh>parse_version</vh></v>
<v t="ekr.20220525082934.849"><vh>typeshed_py_version</vh></v>
<v t="ekr.20220525082934.850"><vh>filter_redundant_py2_dirs</vh></v>
</v>
<v t="ekr.20220525082934.851"><vh>@clean moduleinspect.py</vh>
<v t="ekr.20220525082934.852"><vh>class ModuleProperties</vh>
<v t="ekr.20220525082934.853"><vh>ModuleProperties.__init__</vh></v>
</v>
<v t="ekr.20220525082934.854"><vh>is_c_module</vh></v>
<v t="ekr.20220525082934.855"><vh>class InspectError</vh></v>
<v t="ekr.20220525082934.856"><vh>get_package_properties</vh></v>
<v t="ekr.20220525082934.857"><vh>worker</vh></v>
<v t="ekr.20220525082934.858"><vh>class ModuleInspect</vh>
<v t="ekr.20220525082934.859"><vh>ModuleInspect.__init__</vh></v>
<v t="ekr.20220525082934.860"><vh>ModuleInspect._start</vh></v>
<v t="ekr.20220525082934.861"><vh>ModuleInspect.close</vh></v>
<v t="ekr.20220525082934.862"><vh>ModuleInspect.get_package_properties</vh></v>
<v t="ekr.20220525082934.863"><vh>ModuleInspect._get_from_queue</vh></v>
<v t="ekr.20220525082934.864"><vh>ModuleInspect.__enter__</vh></v>
<v t="ekr.20220525082934.865"><vh>ModuleInspect.__exit__</vh></v>
</v>
</v>
<v t="ekr.20220525082934.866"><vh>@clean mro.py</vh>
<v t="ekr.20220525082934.867"><vh>calculate_mro</vh></v>
<v t="ekr.20220525082934.868"><vh>class MroError</vh></v>
<v t="ekr.20220525082934.869"><vh>linearize_hierarchy</vh></v>
<v t="ekr.20220525082934.870"><vh>merge</vh></v>
</v>
<v t="ekr.20220525082934.871"><vh>@clean nodes.py</vh>
<v t="ekr.20220525082934.872"><vh>class Context</vh>
<v t="ekr.20220525082934.873"><vh>Context.__init__</vh></v>
<v t="ekr.20220525082934.874"><vh>Context.set_line</vh></v>
<v t="ekr.20220525082934.875"><vh>Context.get_line</vh></v>
<v t="ekr.20220525082934.876"><vh>Context.get_column</vh></v>
</v>
<v t="ekr.20220525082934.877"><vh>if TYPE_CHECKING:</vh></v>
<v t="ekr.20220525082934.878"><vh>get_nongen_builtins</vh></v>
<v t="ekr.20220525082934.879"><vh>RUNTIME_PROTOCOL_DECOS: Final = (</vh></v>
<v t="ekr.20220525082934.880"><vh>class Node</vh>
<v t="ekr.20220525082934.881"><vh>Node.__str__</vh></v>
<v t="ekr.20220525082934.882"><vh>Node.accept</vh></v>
</v>
<v t="ekr.20220525082934.883"><vh>class Statement</vh>
<v t="ekr.20220525082934.884"><vh>Statement.accept</vh></v>
</v>
<v t="ekr.20220525082934.885"><vh>class Expression</vh>
<v t="ekr.20220525082934.886"><vh>Expression.accept</vh></v>
</v>
<v t="ekr.20220525082934.887"><vh>class FakeExpression</vh></v>
<v t="ekr.20220525082934.888"><vh>TODO:</vh></v>
<v t="ekr.20220525082934.889"><vh>class SymbolNode</vh>
<v t="ekr.20220525082934.890"><vh>SymbolNode.name</vh></v>
<v t="ekr.20220525082934.891"><vh>SymbolNode.fullname</vh></v>
<v t="ekr.20220525082934.892"><vh>SymbolNode.serialize</vh></v>
<v t="ekr.20220525082934.893"><vh>SymbolNode.deserialize</vh></v>
</v>
<v t="ekr.20220525082934.894"><vh>Items: fullname, related symbol table node, surrounding type (if any)</vh></v>
<v t="ekr.20220525082934.895"><vh>class MypyFile</vh>
<v t="ekr.20220525082934.896"><vh>MypyFile.__init__</vh></v>
<v t="ekr.20220525082934.897"><vh>MypyFile.local_definitions</vh></v>
<v t="ekr.20220525082934.898"><vh>MypyFile.name</vh></v>
<v t="ekr.20220525082934.899"><vh>MypyFile.fullname</vh></v>
<v t="ekr.20220525082934.900"><vh>MypyFile.accept</vh></v>
<v t="ekr.20220525082934.901"><vh>MypyFile.is_package_init_file</vh></v>
<v t="ekr.20220525082934.902"><vh>MypyFile.is_future_flag_set</vh></v>
<v t="ekr.20220525082934.903"><vh>MypyFile.serialize</vh></v>
<v t="ekr.20220525082934.904"><vh>MypyFile.deserialize</vh></v>
</v>
<v t="ekr.20220525082934.905"><vh>class ImportBase</vh>
<v t="ekr.20220525082934.906"><vh>ImportBase.__init__</vh></v>
</v>
<v t="ekr.20220525082934.907"><vh>class Import</vh>
<v t="ekr.20220525082934.908"><vh>Import.__init__</vh></v>
<v t="ekr.20220525082934.909"><vh>Import.accept</vh></v>
</v>
<v t="ekr.20220525082934.910"><vh>class ImportFrom</vh>
<v t="ekr.20220525082934.911"><vh>ImportFrom.__init__</vh></v>
<v t="ekr.20220525082934.912"><vh>ImportFrom.accept</vh></v>
</v>
<v t="ekr.20220525082934.913"><vh>class ImportAll</vh>
<v t="ekr.20220525082934.914"><vh>ImportAll.__init__</vh></v>
<v t="ekr.20220525082934.915"><vh>ImportAll.accept</vh></v>
</v>
<v t="ekr.20220525082934.916"><vh>class ImportedName</vh>
<v t="ekr.20220525082934.917"><vh>ImportedName.__init__</vh></v>
<v t="ekr.20220525082934.918"><vh>ImportedName.name</vh></v>
<v t="ekr.20220525082934.919"><vh>ImportedName.fullname</vh></v>
<v t="ekr.20220525082934.920"><vh>ImportedName.serialize</vh></v>
<v t="ekr.20220525082934.921"><vh>ImportedName.deserialize</vh></v>
<v t="ekr.20220525082934.922"><vh>ImportedName.__str__</vh></v>
</v>
<v t="ekr.20220525082934.923"><vh>FUNCBASE_FLAGS: Final = ["is_property", "is_class", "is_static", "is_final"]</vh></v>
<v t="ekr.20220525082934.924"><vh>class FuncBase</vh>
<v t="ekr.20220525082934.925"><vh>FuncBase.__init__</vh></v>
<v t="ekr.20220525082934.926"><vh>FuncBase.name</vh></v>
<v t="ekr.20220525082934.927"><vh>FuncBase.fullname</vh></v>
</v>
<v t="ekr.20220525082934.928"><vh>OverloadPart: _TypeAlias = Union['FuncDef', 'Decorator']</vh></v>
<v t="ekr.20220525082934.929"><vh>class OverloadedFuncDef</vh>
<v t="ekr.20220525082934.930"><vh>OverloadedFuncDef.__init__</vh></v>
<v t="ekr.20220525082934.931"><vh>OverloadedFuncDef.name</vh></v>
<v t="ekr.20220525082934.932"><vh>OverloadedFuncDef.accept</vh></v>
<v t="ekr.20220525082934.933"><vh>OverloadedFuncDef.serialize</vh></v>
<v t="ekr.20220525082934.934"><vh>OverloadedFuncDef.deserialize</vh></v>
</v>
<v t="ekr.20220525082934.935"><vh>class Argument</vh>
<v t="ekr.20220525082934.936"><vh>Argument.__init__</vh></v>
<v t="ekr.20220525082934.937"><vh>Argument.set_line</vh></v>
</v>
<v t="ekr.20220525082934.938"><vh>FUNCITEM_FLAGS: Final = FUNCBASE_FLAGS + [</vh></v>
<v t="ekr.20220525082934.939"><vh>class FuncItem</vh>
<v t="ekr.20220525082934.940"><vh>FuncItem.__init__</vh></v>
<v t="ekr.20220525082934.941"><vh>FuncItem.max_fixed_argc</vh></v>
<v t="ekr.20220525082934.942"><vh>FuncItem.set_line</vh></v>
<v t="ekr.20220525082934.943"><vh>FuncItem.is_dynamic</vh></v>
</v>
<v t="ekr.20220525082934.944"><vh>FUNCDEF_FLAGS: Final = FUNCITEM_FLAGS + [</vh></v>
<v t="ekr.20220525082934.945"><vh>class FuncDef</vh>
<v t="ekr.20220525082934.946"><vh>FuncDef.__init__</vh></v>
<v t="ekr.20220525082934.947"><vh>FuncDef.name</vh></v>
<v t="ekr.20220525082934.948"><vh>FuncDef.accept</vh></v>
<v t="ekr.20220525082934.949"><vh>FuncDef.serialize</vh></v>
<v t="ekr.20220525082934.950"><vh>FuncDef.deserialize</vh></v>
</v>
<v t="ekr.20220525082934.951"><vh>All types that are both SymbolNodes and FuncBases. See the FuncBase</vh></v>
<v t="ekr.20220525082934.952"><vh>class Decorator</vh>
<v t="ekr.20220525082934.953"><vh>Decorator.__init__</vh></v>
<v t="ekr.20220525082934.954"><vh>Decorator.name</vh></v>
<v t="ekr.20220525082934.955"><vh>Decorator.fullname</vh></v>
<v t="ekr.20220525082934.956"><vh>Decorator.is_final</vh></v>
<v t="ekr.20220525082934.957"><vh>Decorator.info</vh></v>
<v t="ekr.20220525082934.958"><vh>Decorator.type</vh></v>
<v t="ekr.20220525082934.959"><vh>Decorator.accept</vh></v>
<v t="ekr.20220525082934.960"><vh>Decorator.serialize</vh></v>
<v t="ekr.20220525082934.961"><vh>Decorator.deserialize</vh></v>
</v>
<v t="ekr.20220525082934.962"><vh>VAR_FLAGS: Final = [</vh></v>
<v t="ekr.20220525082934.963"><vh>class Var</vh>
<v t="ekr.20220525082934.964"><vh>Var.__init__</vh></v>
<v t="ekr.20220525082934.965"><vh>Var.name</vh></v>
<v t="ekr.20220525082934.966"><vh>Var.fullname</vh></v>
<v t="ekr.20220525082934.967"><vh>Var.accept</vh></v>
<v t="ekr.20220525082934.968"><vh>Var.serialize</vh></v>
<v t="ekr.20220525082934.969"><vh>Var.deserialize</vh></v>
</v>
<v t="ekr.20220525082934.970"><vh>class ClassDef</vh>
<v t="ekr.20220525082934.971"><vh>ClassDef.__init__</vh></v>
<v t="ekr.20220525082934.972"><vh>ClassDef.accept</vh></v>
<v t="ekr.20220525082934.973"><vh>ClassDef.is_generic</vh></v>
<v t="ekr.20220525082934.974"><vh>ClassDef.serialize</vh></v>
<v t="ekr.20220525082934.975"><vh>ClassDef.deserialize</vh></v>
</v>
<v t="ekr.20220525082934.976"><vh>class GlobalDecl</vh>
<v t="ekr.20220525082934.977"><vh>GlobalDecl.__init__</vh></v>
<v t="ekr.20220525082934.978"><vh>GlobalDecl.accept</vh></v>
</v>
<v t="ekr.20220525082934.979"><vh>class NonlocalDecl</vh>
<v t="ekr.20220525082934.980"><vh>NonlocalDecl.__init__</vh></v>
<v t="ekr.20220525082934.981"><vh>NonlocalDecl.accept</vh></v>
</v>
<v t="ekr.20220525082934.982"><vh>class Block</vh>
<v t="ekr.20220525082934.983"><vh>Block.__init__</vh></v>
<v t="ekr.20220525082934.984"><vh>Block.accept</vh></v>
</v>
<v t="ekr.20220525082934.985"><vh>Statements</vh></v>
<v t="ekr.20220525082934.986"><vh>class ExpressionStmt</vh>
<v t="ekr.20220525082934.987"><vh>ExpressionStmt.__init__</vh></v>
<v t="ekr.20220525082934.988"><vh>ExpressionStmt.accept</vh></v>
</v>
<v t="ekr.20220525082934.989"><vh>class AssignmentStmt</vh>
<v t="ekr.20220525082934.990"><vh>AssignmentStmt.__init__</vh></v>
<v t="ekr.20220525082934.991"><vh>AssignmentStmt.accept</vh></v>
</v>
<v t="ekr.20220525082934.992"><vh>class OperatorAssignmentStmt</vh>
<v t="ekr.20220525082934.993"><vh>OperatorAssignmentStmt.__init__</vh></v>
<v t="ekr.20220525082934.994"><vh>OperatorAssignmentStmt.accept</vh></v>
</v>
<v t="ekr.20220525082934.995"><vh>class WhileStmt</vh>
<v t="ekr.20220525082934.996"><vh>WhileStmt.__init__</vh></v>
<v t="ekr.20220525082934.997"><vh>WhileStmt.accept</vh></v>
</v>
<v t="ekr.20220525082934.998"><vh>class ForStmt</vh>
<v t="ekr.20220525082934.999"><vh>ForStmt.__init__</vh></v>
<v t="ekr.20220525082934.1000"><vh>ForStmt.accept</vh></v>
</v>
<v t="ekr.20220525082934.1001"><vh>class ReturnStmt</vh>
<v t="ekr.20220525082934.1002"><vh>ReturnStmt.__init__</vh></v>
<v t="ekr.20220525082934.1003"><vh>ReturnStmt.accept</vh></v>
</v>
<v t="ekr.20220525082934.1004"><vh>class AssertStmt</vh>
<v t="ekr.20220525082934.1005"><vh>AssertStmt.__init__</vh></v>
<v t="ekr.20220525082934.1006"><vh>AssertStmt.accept</vh></v>
</v>
<v t="ekr.20220525082934.1007"><vh>class DelStmt</vh>
<v t="ekr.20220525082934.1008"><vh>DelStmt.__init__</vh></v>
<v t="ekr.20220525082934.1009"><vh>DelStmt.accept</vh></v>
</v>
<v t="ekr.20220525082934.1010"><vh>class BreakStmt</vh></v>
<v t="ekr.20220525082934.1011"><vh>class ContinueStmt</vh></v>
<v t="ekr.20220525082934.1012"><vh>class PassStmt</vh></v>
<v t="ekr.20220525082934.1013"><vh>class IfStmt</vh>
<v t="ekr.20220525082934.1014"><vh>IfStmt.__init__</vh></v>
<v t="ekr.20220525082934.1015"><vh>IfStmt.accept</vh></v>
</v>
<v t="ekr.20220525082934.1016"><vh>class RaiseStmt</vh>
<v t="ekr.20220525082934.1017"><vh>RaiseStmt.__init__</vh></v>
<v t="ekr.20220525082934.1018"><vh>RaiseStmt.accept</vh></v>
</v>
<v t="ekr.20220525082934.1019"><vh>class TryStmt</vh>
<v t="ekr.20220525082934.1020"><vh>TryStmt.__init__</vh></v>
<v t="ekr.20220525082934.1021"><vh>TryStmt.accept</vh></v>
</v>
<v t="ekr.20220525082934.1022"><vh>class WithStmt</vh>
<v t="ekr.20220525082934.1023"><vh>WithStmt.__init__</vh></v>
<v t="ekr.20220525082934.1024"><vh>WithStmt.accept</vh></v>
</v>
<v t="ekr.20220525082934.1025"><vh>class MatchStmt</vh>
<v t="ekr.20220525082934.1026"><vh>MatchStmt.__init__</vh></v>
<v t="ekr.20220525082934.1027"><vh>MatchStmt.accept</vh></v>
</v>
<v t="ekr.20220525082934.1028"><vh>class PrintStmt</vh>
<v t="ekr.20220525082934.1029"><vh>PrintStmt.__init__</vh></v>
<v t="ekr.20220525082934.1030"><vh>PrintStmt.accept</vh></v>
</v>
<v t="ekr.20220525082934.1031"><vh>class ExecStmt</vh>
<v t="ekr.20220525082934.1032"><vh>ExecStmt.__init__</vh></v>
<v t="ekr.20220525082934.1033"><vh>ExecStmt.accept</vh></v>
</v>
<v t="ekr.20220525082934.1034"><vh>Expressions</vh></v>
<v t="ekr.20220525082934.1035"><vh>class IntExpr</vh>
<v t="ekr.20220525082934.1036"><vh>IntExpr.__init__</vh></v>
<v t="ekr.20220525082934.1037"><vh>IntExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1038"><vh>How mypy uses StrExpr, BytesExpr, and UnicodeExpr:</vh></v>
<v t="ekr.20220525082934.1039"><vh>class StrExpr</vh>
<v t="ekr.20220525082934.1040"><vh>StrExpr.__init__</vh></v>
<v t="ekr.20220525082934.1041"><vh>StrExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1042"><vh>class BytesExpr</vh>
<v t="ekr.20220525082934.1043"><vh>BytesExpr.__init__</vh></v>
<v t="ekr.20220525082934.1044"><vh>BytesExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1045"><vh>class UnicodeExpr</vh>
<v t="ekr.20220525082934.1046"><vh>UnicodeExpr.__init__</vh></v>
<v t="ekr.20220525082934.1047"><vh>UnicodeExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1048"><vh>class FloatExpr</vh>
<v t="ekr.20220525082934.1049"><vh>FloatExpr.__init__</vh></v>
<v t="ekr.20220525082934.1050"><vh>FloatExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1051"><vh>class ComplexExpr</vh>
<v t="ekr.20220525082934.1052"><vh>ComplexExpr.__init__</vh></v>
<v t="ekr.20220525082934.1053"><vh>ComplexExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1054"><vh>class EllipsisExpr</vh></v>
<v t="ekr.20220525082934.1055"><vh>class StarExpr</vh>
<v t="ekr.20220525082934.1056"><vh>StarExpr.__init__</vh></v>
<v t="ekr.20220525082934.1057"><vh>StarExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1058"><vh>class RefExpr</vh>
<v t="ekr.20220525082934.1059"><vh>RefExpr.__init__</vh></v>
</v>
<v t="ekr.20220525082934.1060"><vh>class NameExpr</vh>
<v t="ekr.20220525082934.1061"><vh>NameExpr.__init__</vh></v>
<v t="ekr.20220525082934.1062"><vh>NameExpr.accept</vh></v>
<v t="ekr.20220525082934.1063"><vh>NameExpr.serialize</vh></v>
</v>
<v t="ekr.20220525082934.1064"><vh>class MemberExpr</vh>
<v t="ekr.20220525082934.1065"><vh>MemberExpr.__init__</vh></v>
<v t="ekr.20220525082934.1066"><vh>MemberExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1067"><vh>class ArgKind</vh>
<v t="ekr.20220525082934.1068"><vh>ArgKind.is_positional</vh></v>
<v t="ekr.20220525082934.1069"><vh>ArgKind.is_named</vh></v>
<v t="ekr.20220525082934.1070"><vh>ArgKind.is_required</vh></v>
<v t="ekr.20220525082934.1071"><vh>ArgKind.is_optional</vh></v>
<v t="ekr.20220525082934.1072"><vh>ArgKind.is_star</vh></v>
</v>
<v t="ekr.20220525082934.1073"><vh>ARG_POS: Final = ArgKind.ARG_POS</vh></v>
<v t="ekr.20220525082934.1074"><vh>class CallExpr</vh>
<v t="ekr.20220525082934.1075"><vh>CallExpr.__init__</vh></v>
<v t="ekr.20220525082934.1076"><vh>CallExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1077"><vh>class YieldFromExpr</vh>
<v t="ekr.20220525082934.1078"><vh>YieldFromExpr.__init__</vh></v>
<v t="ekr.20220525082934.1079"><vh>YieldFromExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1080"><vh>class YieldExpr</vh>
<v t="ekr.20220525082934.1081"><vh>YieldExpr.__init__</vh></v>
<v t="ekr.20220525082934.1082"><vh>YieldExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1083"><vh>class IndexExpr</vh>
<v t="ekr.20220525082934.1084"><vh>IndexExpr.__init__</vh></v>
<v t="ekr.20220525082934.1085"><vh>IndexExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1086"><vh>class UnaryExpr</vh>
<v t="ekr.20220525082934.1087"><vh>UnaryExpr.__init__</vh></v>
<v t="ekr.20220525082934.1088"><vh>UnaryExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1089"><vh>class AssignmentExpr</vh>
<v t="ekr.20220525082934.1090"><vh>AssignmentExpr.__init__</vh></v>
<v t="ekr.20220525082934.1091"><vh>AssignmentExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1092"><vh>class OpExpr</vh>
<v t="ekr.20220525082934.1093"><vh>OpExpr.__init__</vh></v>
<v t="ekr.20220525082934.1094"><vh>OpExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1095"><vh>class ComparisonExpr</vh>
<v t="ekr.20220525082934.1096"><vh>ComparisonExpr.__init__</vh></v>
<v t="ekr.20220525082934.1097"><vh>ComparisonExpr.pairwise</vh></v>
<v t="ekr.20220525082934.1098"><vh>ComparisonExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1099"><vh>class SliceExpr</vh>
<v t="ekr.20220525082934.1100"><vh>SliceExpr.__init__</vh></v>
<v t="ekr.20220525082934.1101"><vh>SliceExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1102"><vh>class CastExpr</vh>
<v t="ekr.20220525082934.1103"><vh>CastExpr.__init__</vh></v>
<v t="ekr.20220525082934.1104"><vh>CastExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1105"><vh>class AssertTypeExpr</vh>
<v t="ekr.20220525082934.1106"><vh>AssertTypeExpr.__init__</vh></v>
<v t="ekr.20220525082934.1107"><vh>AssertTypeExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1108"><vh>class RevealExpr</vh>
<v t="ekr.20220525082934.1109"><vh>RevealExpr.__init__</vh></v>
<v t="ekr.20220525082934.1110"><vh>RevealExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1111"><vh>class SuperExpr</vh>
<v t="ekr.20220525082934.1112"><vh>SuperExpr.__init__</vh></v>
<v t="ekr.20220525082934.1113"><vh>SuperExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1114"><vh>class LambdaExpr</vh>
<v t="ekr.20220525082934.1115"><vh>LambdaExpr.name</vh></v>
<v t="ekr.20220525082934.1116"><vh>LambdaExpr.expr</vh></v>
<v t="ekr.20220525082934.1117"><vh>LambdaExpr.accept</vh></v>
<v t="ekr.20220525082934.1118"><vh>LambdaExpr.is_dynamic</vh></v>
</v>
<v t="ekr.20220525082934.1119"><vh>class ListExpr</vh>
<v t="ekr.20220525082934.1120"><vh>ListExpr.__init__</vh></v>
<v t="ekr.20220525082934.1121"><vh>ListExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1122"><vh>class DictExpr</vh>
<v t="ekr.20220525082934.1123"><vh>DictExpr.__init__</vh></v>
<v t="ekr.20220525082934.1124"><vh>DictExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1125"><vh>class TupleExpr</vh>
<v t="ekr.20220525082934.1126"><vh>TupleExpr.__init__</vh></v>
<v t="ekr.20220525082934.1127"><vh>TupleExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1128"><vh>class SetExpr</vh>
<v t="ekr.20220525082934.1129"><vh>SetExpr.__init__</vh></v>
<v t="ekr.20220525082934.1130"><vh>SetExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1131"><vh>class GeneratorExpr</vh>
<v t="ekr.20220525082934.1132"><vh>GeneratorExpr.__init__</vh></v>
<v t="ekr.20220525082934.1133"><vh>GeneratorExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1134"><vh>class ListComprehension</vh>
<v t="ekr.20220525082934.1135"><vh>ListComprehension.__init__</vh></v>
<v t="ekr.20220525082934.1136"><vh>ListComprehension.accept</vh></v>
</v>
<v t="ekr.20220525082934.1137"><vh>class SetComprehension</vh>
<v t="ekr.20220525082934.1138"><vh>SetComprehension.__init__</vh></v>
<v t="ekr.20220525082934.1139"><vh>SetComprehension.accept</vh></v>
</v>
<v t="ekr.20220525082934.1140"><vh>class DictionaryComprehension</vh>
<v t="ekr.20220525082934.1141"><vh>DictionaryComprehension.__init__</vh></v>
<v t="ekr.20220525082934.1142"><vh>DictionaryComprehension.accept</vh></v>
</v>
<v t="ekr.20220525082934.1143"><vh>class ConditionalExpr</vh>
<v t="ekr.20220525082934.1144"><vh>ConditionalExpr.__init__</vh></v>
<v t="ekr.20220525082934.1145"><vh>ConditionalExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1146"><vh>class BackquoteExpr</vh>
<v t="ekr.20220525082934.1147"><vh>BackquoteExpr.__init__</vh></v>
<v t="ekr.20220525082934.1148"><vh>BackquoteExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1149"><vh>class TypeApplication</vh>
<v t="ekr.20220525082934.1150"><vh>TypeApplication.__init__</vh></v>
<v t="ekr.20220525082934.1151"><vh>TypeApplication.accept</vh></v>
</v>
<v t="ekr.20220525082934.1152"><vh>Variance of a type variable. For example, T in the definition of</vh></v>
<v t="ekr.20220525082934.1153"><vh>class TypeVarLikeExpr</vh>
<v t="ekr.20220525082934.1154"><vh>TypeVarLikeExpr.__init__</vh></v>
<v t="ekr.20220525082934.1155"><vh>TypeVarLikeExpr.name</vh></v>
<v t="ekr.20220525082934.1156"><vh>TypeVarLikeExpr.fullname</vh></v>
</v>
<v t="ekr.20220525082934.1157"><vh>class TypeVarExpr</vh>
<v t="ekr.20220525082934.1158"><vh>TypeVarExpr.__init__</vh></v>
<v t="ekr.20220525082934.1159"><vh>TypeVarExpr.accept</vh></v>
<v t="ekr.20220525082934.1160"><vh>TypeVarExpr.serialize</vh></v>
<v t="ekr.20220525082934.1161"><vh>TypeVarExpr.deserialize</vh></v>
</v>
<v t="ekr.20220525082934.1162"><vh>class ParamSpecExpr</vh>
<v t="ekr.20220525082934.1163"><vh>ParamSpecExpr.accept</vh></v>
<v t="ekr.20220525082934.1164"><vh>ParamSpecExpr.serialize</vh></v>
<v t="ekr.20220525082934.1165"><vh>ParamSpecExpr.deserialize</vh></v>
</v>
<v t="ekr.20220525082934.1166"><vh>class TypeVarTupleExpr</vh>
<v t="ekr.20220525082934.1167"><vh>TypeVarTupleExpr.accept</vh></v>
<v t="ekr.20220525082934.1168"><vh>TypeVarTupleExpr.serialize</vh></v>
<v t="ekr.20220525082934.1169"><vh>TypeVarTupleExpr.deserialize</vh></v>
</v>
<v t="ekr.20220525082934.1170"><vh>class TypeAliasExpr</vh>
<v t="ekr.20220525082934.1171"><vh>TypeAliasExpr.__init__</vh></v>
<v t="ekr.20220525082934.1172"><vh>TypeAliasExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1173"><vh>class NamedTupleExpr</vh>
<v t="ekr.20220525082934.1174"><vh>NamedTupleExpr.__init__</vh></v>
<v t="ekr.20220525082934.1175"><vh>NamedTupleExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1176"><vh>class TypedDictExpr</vh>
<v t="ekr.20220525082934.1177"><vh>TypedDictExpr.__init__</vh></v>
<v t="ekr.20220525082934.1178"><vh>TypedDictExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1179"><vh>class EnumCallExpr</vh>
<v t="ekr.20220525082934.1180"><vh>EnumCallExpr.__init__</vh></v>
<v t="ekr.20220525082934.1181"><vh>EnumCallExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1182"><vh>class PromoteExpr</vh>
<v t="ekr.20220525082934.1183"><vh>PromoteExpr.__init__</vh></v>
<v t="ekr.20220525082934.1184"><vh>PromoteExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1185"><vh>class NewTypeExpr</vh>
<v t="ekr.20220525082934.1186"><vh>NewTypeExpr.__init__</vh></v>
<v t="ekr.20220525082934.1187"><vh>NewTypeExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1188"><vh>class AwaitExpr</vh>
<v t="ekr.20220525082934.1189"><vh>AwaitExpr.__init__</vh></v>
<v t="ekr.20220525082934.1190"><vh>AwaitExpr.accept</vh></v>
</v>
<v t="ekr.20220525082934.1191"><vh>Constants</vh></v>
<v t="ekr.20220525082934.1192"><vh>class TempNode</vh>
<v t="ekr.20220525082934.1193"><vh>TempNode.__init__</vh></v>
<v t="ekr.20220525082934.1194"><vh>TempNode.__repr__</vh></v>
<v t="ekr.20220525082934.1195"><vh>TempNode.accept</vh></v>
</v>
<v t="ekr.20220525082934.1196"><vh>class TypeInfo</vh>
<v t="ekr.20220525082934.1197"><vh>TypeInfo.__init__</vh></v>
<v t="ekr.20220525082934.1198"><vh>TypeInfo.add_type_vars</vh></v>
<v t="ekr.20220525082934.1199"><vh>TypeInfo.name</vh></v>
<v t="ekr.20220525082934.1200"><vh>TypeInfo.fullname</vh></v>
<v t="ekr.20220525082934.1201"><vh>TypeInfo.is_generic</vh></v>
<v t="ekr.20220525082934.1202"><vh>TypeInfo.get</vh></v>
<v t="ekr.20220525082934.1203"><vh>TypeInfo.get_containing_type_info</vh></v>
<v t="ekr.20220525082934.1204"><vh>TypeInfo.protocol_members</vh></v>
<v t="ekr.20220525082934.1205"><vh>TypeInfo.__getitem__</vh></v>
<v t="ekr.20220525082934.1206"><vh>TypeInfo.__repr__</vh></v>
<v t="ekr.20220525082934.1207"><vh>TypeInfo.__bool__</vh></v>
<v t="ekr.20220525082934.1208"><vh>TypeInfo.has_readable_member</vh></v>
<v t="ekr.20220525082934.1209"><vh>TypeInfo.get_method</vh></v>
<v t="ekr.20220525082934.1210"><vh>TypeInfo.calculate_metaclass_type</vh></v>
<v t="ekr.20220525082934.1211"><vh>TypeInfo.is_metaclass</vh></v>
<v t="ekr.20220525082934.1212"><vh>TypeInfo.has_base</vh></v>
<v t="ekr.20220525082934.1213"><vh>TypeInfo.direct_base_classes</vh></v>
<v t="ekr.20220525082934.1214"><vh>TypeInfo.__str__</vh></v>
<v t="ekr.20220525082934.1215"><vh>TypeInfo.dump</vh></v>
<v t="ekr.20220525082934.1216"><vh>TypeInfo.serialize</vh></v>
<v t="ekr.20220525082934.1217"><vh>TypeInfo.deserialize</vh></v>
</v>
<v t="ekr.20220525082934.1218"><vh>class FakeInfo</vh>
<v t="ekr.20220525082934.1219"><vh>FakeInfo.__init__</vh></v>
<v t="ekr.20220525082934.1220"><vh>FakeInfo.__getattribute__</vh></v>
</v>
<v t="ekr.20220525082934.1221"><vh>VAR_NO_INFO: Final[TypeInfo] = FakeInfo("Var is lacking info")</vh></v>
<v t="ekr.20220525082934.1222"><vh>class TypeAlias</vh>
<v t="ekr.20220525082934.1223"><vh>TypeAlias.__init__</vh></v>
<v t="ekr.20220525082934.1224"><vh>TypeAlias.name</vh></v>
<v t="ekr.20220525082934.1225"><vh>TypeAlias.fullname</vh></v>
<v t="ekr.20220525082934.1226"><vh>TypeAlias.serialize</vh></v>
<v t="ekr.20220525082934.1227"><vh>TypeAlias.accept</vh></v>
<v t="ekr.20220525082934.1228"><vh>TypeAlias.deserialize</vh></v>
</v>
<v t="ekr.20220525082934.1229"><vh>class PlaceholderNode</vh>
<v t="ekr.20220525082934.1230"><vh>PlaceholderNode.__init__</vh></v>
<v t="ekr.20220525082934.1231"><vh>PlaceholderNode.name</vh></v>
<v t="ekr.20220525082934.1232"><vh>PlaceholderNode.fullname</vh></v>
<v t="ekr.20220525082934.1233"><vh>PlaceholderNode.serialize</vh></v>
<v t="ekr.20220525082934.1234"><vh>PlaceholderNode.accept</vh></v>
</v>
<v t="ekr.20220525082934.1235"><vh>class SymbolTableNode</vh>
<v t="ekr.20220525082934.1236"><vh>SymbolTableNode.__init__</vh></v>
<v t="ekr.20220525082934.1237"><vh>SymbolTableNode.fullname</vh></v>
<v t="ekr.20220525082934.1238"><vh>SymbolTableNode.type</vh></v>
<v t="ekr.20220525082934.1239"><vh>SymbolTableNode.copy</vh></v>
<v t="ekr.20220525082934.1240"><vh>SymbolTableNode.__str__</vh></v>
<v t="ekr.20220525082934.1241"><vh>SymbolTableNode.serialize</vh></v>
<v t="ekr.20220525082934.1242"><vh>SymbolTableNode.deserialize</vh></v>
</v>
<v t="ekr.20220525082934.1243"><vh>class SymbolTable</vh>
<v t="ekr.20220525082934.1244"><vh>SymbolTable.__str__</vh></v>
<v t="ekr.20220525082934.1245"><vh>SymbolTable.copy</vh></v>
<v t="ekr.20220525082934.1246"><vh>SymbolTable.serialize</vh></v>
<v t="ekr.20220525082934.1247"><vh>SymbolTable.deserialize</vh></v>
</v>
<v t="ekr.20220525082934.1248"><vh>get_flags</vh></v>
<v t="ekr.20220525082934.1249"><vh>set_flags</vh></v>
<v t="ekr.20220525082934.1250"><vh>get_member_expr_fullname</vh></v>
<v t="ekr.20220525082934.1251"><vh>deserialize_map: Final = {</vh></v>
<v t="ekr.20220525082934.1252"><vh>check_arg_kinds</vh></v>
<v t="ekr.20220525082934.1253"><vh>check_arg_names</vh></v>
<v t="ekr.20220525082934.1254"><vh>is_class_var</vh></v>
<v t="ekr.20220525082934.1255"><vh>is_final_node</vh></v>
<v t="ekr.20220525082934.1256"><vh>local_definitions</vh></v>
</v>
<v t="ekr.20220525082934.1257"><vh>@clean operators.py</vh></v>
<v t="ekr.20220525082934.1258"><vh>@clean options.py</vh>
<v t="ekr.20220525082934.1259"><vh>class BuildType</vh></v>
<v t="ekr.20220525082934.1260"><vh>PER_MODULE_OPTIONS: Final = {</vh></v>
<v t="ekr.20220525082934.1261"><vh>class Options</vh>
<v t="ekr.20220525082934.1262"><vh>Options.__init__</vh></v>
<v t="ekr.20220525082934.1263"><vh>Options.new_semantic_analyzer</vh></v>
<v t="ekr.20220525082934.1264"><vh>Options.snapshot</vh></v>
<v t="ekr.20220525082934.1265"><vh>Options.__repr__</vh></v>
<v t="ekr.20220525082934.1266"><vh>Options.apply_changes</vh></v>
<v t="ekr.20220525082934.1267"><vh>Options.build_per_module_cache</vh></v>
<v t="ekr.20220525082934.1268"><vh>Options.clone_for_module</vh></v>
<v t="ekr.20220525082934.1269"><vh>Options.compile_glob</vh></v>
<v t="ekr.20220525082934.1270"><vh>Options.select_options_affecting_cache</vh></v>
</v>
</v>
<v t="ekr.20220525082934.1271"><vh>@clean parse.py</vh>
<v t="ekr.20220525082934.1272"><vh>parse</vh></v>
</v>
<v t="ekr.20220525082934.1273"><vh>@clean patterns.py</vh>
<v t="ekr.20220525082934.1274"><vh>class Pattern</vh>
<v t="ekr.20220525082934.1275"><vh>Pattern.accept</vh></v>
</v>
<v t="ekr.20220525082934.1276"><vh>class AsPattern</vh>
<v t="ekr.20220525082934.1277"><vh>AsPattern.__init__</vh></v>
<v t="ekr.20220525082934.1278"><vh>AsPattern.accept</vh></v>
</v>
<v t="ekr.20220525082934.1279"><vh>class OrPattern</vh>
<v t="ekr.20220525082934.1280"><vh>OrPattern.__init__</vh></v>
<v t="ekr.20220525082934.1281"><vh>OrPattern.accept</vh></v>
</v>
<v t="ekr.20220525082934.1282"><vh>class ValuePattern</vh>
<v t="ekr.20220525082934.1283"><vh>ValuePattern.__init__</vh></v>
<v t="ekr.20220525082934.1284"><vh>ValuePattern.accept</vh></v>
</v>
<v t="ekr.20220525082934.1285"><vh>class SingletonPattern</vh>
<v t="ekr.20220525082934.1286"><vh>SingletonPattern.__init__</vh></v>
<v t="ekr.20220525082934.1287"><vh>SingletonPattern.accept</vh></v>
</v>
<v t="ekr.20220525082934.1288"><vh>class SequencePattern</vh>
<v t="ekr.20220525082934.1289"><vh>SequencePattern.__init__</vh></v>
<v t="ekr.20220525082934.1290"><vh>SequencePattern.accept</vh></v>
</v>
<v t="ekr.20220525082934.1291"><vh>class StarredPattern</vh>
<v t="ekr.20220525082934.1292"><vh>StarredPattern.__init__</vh></v>
<v t="ekr.20220525082934.1293"><vh>StarredPattern.accept</vh></v>
</v>
<v t="ekr.20220525082934.1294"><vh>class MappingPattern</vh>
<v t="ekr.20220525082934.1295"><vh>MappingPattern.__init__</vh></v>
<v t="ekr.20220525082934.1296"><vh>MappingPattern.accept</vh></v>
</v>
<v t="ekr.20220525082934.1297"><vh>class ClassPattern</vh>
<v t="ekr.20220525082934.1298"><vh>ClassPattern.__init__</vh></v>
<v t="ekr.20220525082934.1299"><vh>ClassPattern.accept</vh></v>
</v>
</v>
<v t="ekr.20220525082934.1300"><vh>@clean plugin.py</vh>
<v t="ekr.20220525082934.1301"><vh>class TypeAnalyzerPluginInterface</vh>
<v t="ekr.20220525082934.1302"><vh>TypeAnalyzerPluginInterface.fail</vh></v>
<v t="ekr.20220525082934.1303"><vh>TypeAnalyzerPluginInterface.named_type</vh></v>
<v t="ekr.20220525082934.1304"><vh>TypeAnalyzerPluginInterface.analyze_type</vh></v>
<v t="ekr.20220525082934.1305"><vh>TypeAnalyzerPluginInterface.analyze_callable_args</vh></v>
</v>
<v t="ekr.20220525082934.1306"><vh>class AnalyzeTypeContext</vh></v>
<v t="ekr.20220525082934.1307"><vh>class CommonPluginApi</vh>
<v t="ekr.20220525082934.1308"><vh>CommonPluginApi.lookup_fully_qualified</vh></v>
</v>
<v t="ekr.20220525082934.1309"><vh>class CheckerPluginInterface</vh>
<v t="ekr.20220525082934.1310"><vh>CheckerPluginInterface.type_context</vh></v>
<v t="ekr.20220525082934.1311"><vh>CheckerPluginInterface.fail</vh></v>
<v t="ekr.20220525082934.1312"><vh>CheckerPluginInterface.named_generic_type</vh></v>
</v>
<v t="ekr.20220525082934.1313"><vh>class SemanticAnalyzerPluginInterface</vh>
<v t="ekr.20220525082934.1314"><vh>SemanticAnalyzerPluginInterface.named_type</vh></v>
<v t="ekr.20220525082934.1315"><vh>SemanticAnalyzerPluginInterface.builtin_type</vh></v>
<v t="ekr.20220525082934.1316"><vh>SemanticAnalyzerPluginInterface.named_type_or_none</vh></v>
<v t="ekr.20220525082934.1317"><vh>SemanticAnalyzerPluginInterface.basic_new_typeinfo</vh></v>
<v t="ekr.20220525082934.1318"><vh>SemanticAnalyzerPluginInterface.parse_bool</vh></v>
<v t="ekr.20220525082934.1319"><vh>SemanticAnalyzerPluginInterface.fail</vh></v>
<v t="ekr.20220525082934.1320"><vh>SemanticAnalyzerPluginInterface.anal_type</vh></v>
<v t="ekr.20220525082934.1321"><vh>SemanticAnalyzerPluginInterface.class_type</vh></v>
<v t="ekr.20220525082934.1322"><vh>SemanticAnalyzerPluginInterface.lookup_fully_qualified</vh></v>
<v t="ekr.20220525082934.1323"><vh>SemanticAnalyzerPluginInterface.lookup_fully_qualified_or_none</vh></v>
<v t="ekr.20220525082934.1324"><vh>SemanticAnalyzerPluginInterface.lookup_qualified</vh></v>
<v t="ekr.20220525082934.1325"><vh>SemanticAnalyzerPluginInterface.add_plugin_dependency</vh></v>
<v t="ekr.20220525082934.1326"><vh>SemanticAnalyzerPluginInterface.add_symbol_table_node</vh></v>
<v t="ekr.20220525082934.1327"><vh>SemanticAnalyzerPluginInterface.qualified_name</vh></v>
<v t="ekr.20220525082934.1328"><vh>SemanticAnalyzerPluginInterface.defer</vh></v>
<v t="ekr.20220525082934.1329"><vh>SemanticAnalyzerPluginInterface.final_iteration</vh></v>
<v t="ekr.20220525082934.1330"><vh>SemanticAnalyzerPluginInterface.is_stub_file</vh></v>
</v>
<v t="ekr.20220525082934.1331"><vh>class ReportConfigContext</vh></v>
<v t="ekr.20220525082934.1332"><vh>class FunctionSigContext</vh></v>
<v t="ekr.20220525082934.1333"><vh>class FunctionContext</vh></v>
<v t="ekr.20220525082934.1334"><vh>class MethodSigContext</vh></v>
<v t="ekr.20220525082934.1335"><vh>class MethodContext</vh></v>
<v t="ekr.20220525082934.1336"><vh>class AttributeContext</vh></v>
<v t="ekr.20220525082934.1337"><vh>class ClassDefContext</vh></v>
<v t="ekr.20220525082934.1338"><vh>class DynamicClassDefContext</vh></v>
<v t="ekr.20220525082934.1339"><vh>class Plugin</vh>
<v t="ekr.20220525082934.1340"><vh>Plugin.__init__</vh></v>
<v t="ekr.20220525082934.1341"><vh>Plugin.set_modules</vh></v>
<v t="ekr.20220525082934.1342"><vh>Plugin.lookup_fully_qualified</vh></v>
<v t="ekr.20220525082934.1343"><vh>Plugin.report_config_data</vh></v>
<v t="ekr.20220525082934.1344"><vh>Plugin.get_additional_deps</vh></v>
<v t="ekr.20220525082934.1345"><vh>Plugin.get_type_analyze_hook</vh></v>
<v t="ekr.20220525082934.1346"><vh>Plugin.get_function_signature_hook</vh></v>
<v t="ekr.20220525082934.1347"><vh>Plugin.get_function_hook</vh></v>
<v t="ekr.20220525082934.1348"><vh>Plugin.get_method_signature_hook</vh></v>
<v t="ekr.20220525082934.1349"><vh>Plugin.get_method_hook</vh></v>
<v t="ekr.20220525082934.1350"><vh>Plugin.get_attribute_hook</vh></v>
<v t="ekr.20220525082934.1351"><vh>Plugin.get_class_attribute_hook</vh></v>
<v t="ekr.20220525082934.1352"><vh>Plugin.get_class_decorator_hook</vh></v>
<v t="ekr.20220525082934.1353"><vh>Plugin.get_class_decorator_hook_2</vh></v>
<v t="ekr.20220525082934.1354"><vh>Plugin.get_metaclass_hook</vh></v>
<v t="ekr.20220525082934.1355"><vh>Plugin.get_base_class_hook</vh></v>
<v t="ekr.20220525082934.1356"><vh>Plugin.get_customize_class_mro_hook</vh></v>
<v t="ekr.20220525082934.1357"><vh>Plugin.get_dynamic_class_hook</vh></v>
</v>
<v t="ekr.20220525082934.1358"><vh>T = TypeVar('T')</vh></v>
<v t="ekr.20220525082934.1359"><vh>class ChainedPlugin</vh>
<v t="ekr.20220525082934.1360"><vh>ChainedPlugin.__init__</vh></v>
<v t="ekr.20220525082934.1361"><vh>ChainedPlugin.set_modules</vh></v>
<v t="ekr.20220525082934.1362"><vh>ChainedPlugin.report_config_data</vh></v>
<v t="ekr.20220525082934.1363"><vh>ChainedPlugin.get_additional_deps</vh></v>
<v t="ekr.20220525082934.1364"><vh>ChainedPlugin.get_type_analyze_hook</vh></v>
<v t="ekr.20220525082934.1365"><vh>ChainedPlugin.get_function_signature_hook</vh></v>
<v t="ekr.20220525082934.1366"><vh>ChainedPlugin.get_function_hook</vh></v>
<v t="ekr.20220525082934.1367"><vh>ChainedPlugin.get_method_signature_hook</vh></v>
<v t="ekr.20220525082934.1368"><vh>ChainedPlugin.get_method_hook</vh></v>
<v t="ekr.20220525082934.1369"><vh>ChainedPlugin.get_attribute_hook</vh></v>
<v t="ekr.20220525082934.1370"><vh>ChainedPlugin.get_class_attribute_hook</vh></v>
<v t="ekr.20220525082934.1371"><vh>ChainedPlugin.get_class_decorator_hook</vh></v>
<v t="ekr.20220525082934.1372"><vh>ChainedPlugin.get_class_decorator_hook_2</vh></v>
<v t="ekr.20220525082934.1373"><vh>ChainedPlugin.get_metaclass_hook</vh></v>
<v t="ekr.20220525082934.1374"><vh>ChainedPlugin.get_base_class_hook</vh></v>
<v t="ekr.20220525082934.1375"><vh>ChainedPlugin.get_customize_class_mro_hook</vh></v>
<v t="ekr.20220525082934.1376"><vh>ChainedPlugin.get_dynamic_class_hook</vh></v>
<v t="ekr.20220525082934.1377"><vh>ChainedPlugin._find_hook</vh></v>
</v>
</v>
<v t="ekr.20220525082934.1378"><vh>@clean pyinfo.py</vh>
<v t="ekr.20220525082934.1379"><vh>getprefixes</vh></v>
<v t="ekr.20220525082934.1380"><vh>getsitepackages</vh></v>
</v>
<v t="ekr.20220525082934.1381"><vh>@clean reachability.py</vh>
<v t="ekr.20220525082934.1382"><vh>infer_reachability_of_if_statement</vh></v>
<v t="ekr.20220525082934.1383"><vh>infer_reachability_of_match_statement</vh></v>
<v t="ekr.20220525082934.1384"><vh>assert_will_always_fail</vh></v>
<v t="ekr.20220525082934.1385"><vh>infer_condition_value</vh></v>
<v t="ekr.20220525082934.1386"><vh>infer_pattern_value</vh></v>
<v t="ekr.20220525082934.1387"><vh>consider_sys_version_info</vh></v>
<v t="ekr.20220525082934.1388"><vh>consider_sys_platform</vh></v>
<v t="ekr.20220525082934.1389"><vh>Targ = TypeVar('Targ', int, str, Tuple[int, ...])</vh></v>
<v t="ekr.20220525082934.1390"><vh>fixed_comparison</vh></v>
<v t="ekr.20220525082934.1391"><vh>contains_int_or_tuple_of_ints</vh></v>
<v t="ekr.20220525082934.1392"><vh>contains_sys_version_info</vh></v>
<v t="ekr.20220525082934.1393"><vh>is_sys_attr</vh></v>
<v t="ekr.20220525082934.1394"><vh>mark_block_unreachable</vh></v>
<v t="ekr.20220525082934.1395"><vh>class MarkImportsUnreachableVisitor</vh>
<v t="ekr.20220525082934.1396"><vh>MarkImportsUnreachableVisitor.visit_import</vh></v>
<v t="ekr.20220525082934.1397"><vh>MarkImportsUnreachableVisitor.visit_import_from</vh></v>
<v t="ekr.20220525082934.1398"><vh>MarkImportsUnreachableVisitor.visit_import_all</vh></v>
</v>
<v t="ekr.20220525082934.1399"><vh>mark_block_mypy_only</vh></v>
<v t="ekr.20220525082934.1400"><vh>class MarkImportsMypyOnlyVisitor</vh>
<v t="ekr.20220525082934.1401"><vh>MarkImportsMypyOnlyVisitor.visit_import</vh></v>
<v t="ekr.20220525082934.1402"><vh>MarkImportsMypyOnlyVisitor.visit_import_from</vh></v>
<v t="ekr.20220525082934.1403"><vh>MarkImportsMypyOnlyVisitor.visit_import_all</vh></v>
</v>
</v>
<v t="ekr.20220525082934.1404"><vh>@clean renaming.py</vh>
<v t="ekr.20220525082934.1405"><vh>class VariableRenameVisitor</vh>
<v t="ekr.20220525082934.1406"><vh>VariableRenameVisitor.__init__</vh></v>
<v t="ekr.20220525082934.1407"><vh>VariableRenameVisitor.visit_mypy_file</vh></v>
<v t="ekr.20220525082934.1408"><vh>VariableRenameVisitor.visit_func_def</vh></v>
<v t="ekr.20220525082934.1409"><vh>VariableRenameVisitor.visit_class_def</vh></v>
<v t="ekr.20220525082934.1410"><vh>VariableRenameVisitor.visit_block</vh></v>
<v t="ekr.20220525082934.1411"><vh>VariableRenameVisitor.visit_while_stmt</vh></v>
<v t="ekr.20220525082934.1412"><vh>VariableRenameVisitor.visit_for_stmt</vh></v>
<v t="ekr.20220525082934.1413"><vh>VariableRenameVisitor.visit_break_stmt</vh></v>
<v t="ekr.20220525082934.1414"><vh>VariableRenameVisitor.visit_continue_stmt</vh></v>
<v t="ekr.20220525082934.1415"><vh>VariableRenameVisitor.visit_try_stmt</vh></v>
<v t="ekr.20220525082934.1416"><vh>VariableRenameVisitor.visit_with_stmt</vh></v>
<v t="ekr.20220525082934.1417"><vh>VariableRenameVisitor.visit_import</vh></v>
<v t="ekr.20220525082934.1418"><vh>VariableRenameVisitor.visit_import_from</vh></v>
<v t="ekr.20220525082934.1419"><vh>VariableRenameVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20220525082934.1420"><vh>VariableRenameVisitor.visit_match_stmt</vh></v>
<v t="ekr.20220525082934.1421"><vh>VariableRenameVisitor.visit_capture_pattern</vh></v>
<v t="ekr.20220525082934.1422"><vh>VariableRenameVisitor.analyze_lvalue</vh></v>
<v t="ekr.20220525082934.1423"><vh>VariableRenameVisitor.visit_name_expr</vh></v>
<v t="ekr.20220525082934.1424"><vh>VariableRenameVisitor.Helpers for renaming references</vh></v>
<v t="ekr.20220525082934.1425"><vh>VariableRenameVisitor.handle_arg</vh></v>
<v t="ekr.20220525082934.1426"><vh>VariableRenameVisitor.handle_def</vh></v>
<v t="ekr.20220525082934.1427"><vh>VariableRenameVisitor.handle_refine</vh></v>
<v t="ekr.20220525082934.1428"><vh>VariableRenameVisitor.handle_ref</vh></v>
<v t="ekr.20220525082934.1429"><vh>VariableRenameVisitor.flush_refs</vh></v>
<v t="ekr.20220525082934.1430"><vh>VariableRenameVisitor.Helpers for determining which assignments define new variables</vh></v>
<v t="ekr.20220525082934.1431"><vh>VariableRenameVisitor.clear</vh></v>
<v t="ekr.20220525082934.1432"><vh>VariableRenameVisitor.enter_block</vh></v>
<v t="ekr.20220525082934.1433"><vh>VariableRenameVisitor.enter_try</vh></v>
<v t="ekr.20220525082934.1434"><vh>VariableRenameVisitor.enter_loop</vh></v>
<v t="ekr.20220525082934.1435"><vh>VariableRenameVisitor.current_block</vh></v>
<v t="ekr.20220525082934.1436"><vh>VariableRenameVisitor.enter_scope</vh></v>
<v t="ekr.20220525082934.1437"><vh>VariableRenameVisitor.is_nested</vh></v>
<v t="ekr.20220525082934.1438"><vh>VariableRenameVisitor.reject_redefinition_of_vars_in_scope</vh></v>
<v t="ekr.20220525082934.1439"><vh>VariableRenameVisitor.reject_redefinition_of_vars_in_loop</vh></v>
<v t="ekr.20220525082934.1440"><vh>VariableRenameVisitor.record_assignment</vh></v>
</v>
<v t="ekr.20220525082934.1441"><vh>class LimitedVariableRenameVisitor</vh>
<v t="ekr.20220525082934.1442"><vh>LimitedVariableRenameVisitor.__init__</vh></v>
<v t="ekr.20220525082934.1443"><vh>LimitedVariableRenameVisitor.visit_mypy_file</vh></v>
<v t="ekr.20220525082934.1444"><vh>LimitedVariableRenameVisitor.visit_func_def</vh></v>
<v t="ekr.20220525082934.1445"><vh>LimitedVariableRenameVisitor.visit_class_def</vh></v>
<v t="ekr.20220525082934.1446"><vh>LimitedVariableRenameVisitor.visit_with_stmt</vh></v>
<v t="ekr.20220525082934.1447"><vh>LimitedVariableRenameVisitor.analyze_lvalue</vh></v>
<v t="ekr.20220525082934.1448"><vh>LimitedVariableRenameVisitor.visit_import</vh></v>
<v t="ekr.20220525082934.1449"><vh>LimitedVariableRenameVisitor.visit_import_from</vh></v>
<v t="ekr.20220525082934.1450"><vh>LimitedVariableRenameVisitor.visit_import_all</vh></v>
<v t="ekr.20220525082934.1451"><vh>LimitedVariableRenameVisitor.visit_name_expr</vh></v>
<v t="ekr.20220525082934.1452"><vh>LimitedVariableRenameVisitor.enter_scope</vh></v>
<v t="ekr.20220525082934.1453"><vh>LimitedVariableRenameVisitor.reject_redefinition_of_vars_in_scope</vh></v>
<v t="ekr.20220525082934.1454"><vh>LimitedVariableRenameVisitor.record_skipped</vh></v>
<v t="ekr.20220525082934.1455"><vh>LimitedVariableRenameVisitor.flush_refs</vh></v>
</v>
<v t="ekr.20220525082934.1456"><vh>rename_refs</vh></v>
</v>
<v t="ekr.20220525082934.1457"><vh>@clean report.py</vh>
<v t="ekr.20220525082934.1458"><vh>class Reports</vh>
<v t="ekr.20220525082934.1459"><vh>Reports.__init__</vh></v>
<v t="ekr.20220525082934.1460"><vh>Reports.add_report</vh></v>
<v t="ekr.20220525082934.1461"><vh>Reports.file</vh></v>
<v t="ekr.20220525082934.1462"><vh>Reports.finish</vh></v>
</v>
<v t="ekr.20220525082934.1463"><vh>class AbstractReporter</vh>
<v t="ekr.20220525082934.1464"><vh>AbstractReporter.__init__</vh></v>
<v t="ekr.20220525082934.1465"><vh>AbstractReporter.on_file</vh></v>
<v t="ekr.20220525082934.1466"><vh>AbstractReporter.on_finish</vh></v>
</v>
<v t="ekr.20220525082934.1467"><vh>register_reporter</vh></v>
<v t="ekr.20220525082934.1468"><vh>alias_reporter</vh></v>
<v t="ekr.20220525082934.1469"><vh>should_skip_path</vh></v>
<v t="ekr.20220525082934.1470"><vh>iterate_python_lines</vh></v>
<v t="ekr.20220525082934.1471"><vh>class FuncCounterVisitor</vh></v>
<v t="ekr.20220525082934.1472"><vh>class LineCountReporter</vh>
<v t="ekr.20220525082934.1473"><vh>LineCountReporter.__init__</vh></v>
<v t="ekr.20220525082934.1474"><vh>LineCountReporter.on_file</vh></v>
<v t="ekr.20220525082934.1475"><vh>LineCountReporter.on_finish</vh></v>
</v>
<v t="ekr.20220525082934.1476"><vh>register_reporter('linecount', LineCountReporter)</vh></v>
<v t="ekr.20220525082934.1477"><vh>class AnyExpressionsReporter</vh>
<v t="ekr.20220525082934.1478"><vh>AnyExpressionsReporter.__init__</vh></v>
<v t="ekr.20220525082934.1479"><vh>AnyExpressionsReporter.on_file</vh></v>
<v t="ekr.20220525082934.1480"><vh>AnyExpressionsReporter.on_finish</vh></v>
<v t="ekr.20220525082934.1481"><vh>AnyExpressionsReporter._write_out_report</vh></v>
<v t="ekr.20220525082934.1482"><vh>AnyExpressionsReporter._report_any_exprs</vh></v>
<v t="ekr.20220525082934.1483"><vh>AnyExpressionsReporter._report_types_of_anys</vh></v>
</v>
<v t="ekr.20220525082934.1484"><vh>register_reporter('any-exprs', AnyExpressionsReporter)</vh></v>
<v t="ekr.20220525082934.1485"><vh>class LineCoverageVisitor</vh>
<v t="ekr.20220525082934.1486"><vh>LineCoverageVisitor.__init__</vh></v>
<v t="ekr.20220525082934.1487"><vh>LineCoverageVisitor.The Python AST has position information for the starts of</vh></v>
<v t="ekr.20220525082934.1488"><vh>LineCoverageVisitor.indentation_level</vh></v>
<v t="ekr.20220525082934.1489"><vh>LineCoverageVisitor.visit_func_def</vh></v>
</v>
<v t="ekr.20220525082934.1490"><vh>class LineCoverageReporter</vh>
<v t="ekr.20220525082934.1491"><vh>LineCoverageReporter.__init__</vh></v>
<v t="ekr.20220525082934.1492"><vh>LineCoverageReporter.on_file</vh></v>
<v t="ekr.20220525082934.1493"><vh>LineCoverageReporter.on_finish</vh></v>
</v>
<v t="ekr.20220525082934.1494"><vh>register_reporter('linecoverage', LineCoverageReporter)</vh></v>
<v t="ekr.20220525082934.1495"><vh>class FileInfo</vh>
<v t="ekr.20220525082934.1496"><vh>FileInfo.__init__</vh></v>
<v t="ekr.20220525082934.1497"><vh>FileInfo.total</vh></v>
<v t="ekr.20220525082934.1498"><vh>FileInfo.attrib</vh></v>
</v>
<v t="ekr.20220525082934.1499"><vh>class MemoryXmlReporter</vh>
<v t="ekr.20220525082934.1500"><vh>MemoryXmlReporter.__init__</vh></v>
<v t="ekr.20220525082934.1501"><vh>MemoryXmlReporter.XML doesn't like control characters, but they are sometimes</vh></v>
<v t="ekr.20220525082934.1502"><vh>MemoryXmlReporter.on_file</vh></v>
<v t="ekr.20220525082934.1503"><vh>MemoryXmlReporter._get_any_info_for_line</vh></v>
<v t="ekr.20220525082934.1504"><vh>MemoryXmlReporter.on_finish</vh></v>
</v>
<v t="ekr.20220525082934.1505"><vh>register_reporter('memory-xml', MemoryXmlReporter, needs_lxml=True)</vh></v>
<v t="ekr.20220525082934.1506"><vh>get_line_rate</vh></v>
<v t="ekr.20220525082934.1507"><vh>class CoberturaPackage</vh>
<v t="ekr.20220525082934.1508"><vh>CoberturaPackage.__init__</vh></v>
<v t="ekr.20220525082934.1509"><vh>CoberturaPackage.as_xml</vh></v>
<v t="ekr.20220525082934.1510"><vh>CoberturaPackage.add_packages</vh></v>
</v>
<v t="ekr.20220525082934.1511"><vh>class CoberturaXmlReporter</vh>
<v t="ekr.20220525082934.1512"><vh>CoberturaXmlReporter.__init__</vh></v>
<v t="ekr.20220525082934.1513"><vh>CoberturaXmlReporter.on_file</vh></v>
<v t="ekr.20220525082934.1514"><vh>CoberturaXmlReporter.on_finish</vh></v>
</v>
<v t="ekr.20220525082934.1515"><vh>register_reporter('cobertura-xml', CoberturaXmlReporter, needs_lxml=True)</vh></v>
<v t="ekr.20220525082934.1516"><vh>class AbstractXmlReporter</vh>
<v t="ekr.20220525082934.1517"><vh>AbstractXmlReporter.__init__</vh></v>
</v>
<v t="ekr.20220525082934.1518"><vh>class XmlReporter</vh>
<v t="ekr.20220525082934.1519"><vh>XmlReporter.on_file</vh></v>
<v t="ekr.20220525082934.1520"><vh>XmlReporter.on_finish</vh></v>
</v>
<v t="ekr.20220525082934.1521"><vh>register_reporter('xml', XmlReporter, needs_lxml=True)</vh></v>
<v t="ekr.20220525082934.1522"><vh>class XsltHtmlReporter</vh>
<v t="ekr.20220525082934.1523"><vh>XsltHtmlReporter.__init__</vh></v>
<v t="ekr.20220525082934.1524"><vh>XsltHtmlReporter.on_file</vh></v>
<v t="ekr.20220525082934.1525"><vh>XsltHtmlReporter.on_finish</vh></v>
</v>
<v t="ekr.20220525082934.1526"><vh>register_reporter('xslt-html', XsltHtmlReporter, needs_lxml=True)</vh></v>
<v t="ekr.20220525082934.1527"><vh>class XsltTxtReporter</vh>
<v t="ekr.20220525082934.1528"><vh>XsltTxtReporter.__init__</vh></v>
<v t="ekr.20220525082934.1529"><vh>XsltTxtReporter.on_file</vh></v>
<v t="ekr.20220525082934.1530"><vh>XsltTxtReporter.on_finish</vh></v>
</v>
<v t="ekr.20220525082934.1531"><vh>register_reporter('xslt-txt', XsltTxtReporter, needs_lxml=True)</vh></v>
<v t="ekr.20220525082934.1532"><vh>class LinePrecisionReporter</vh>
<v t="ekr.20220525082934.1533"><vh>LinePrecisionReporter.__init__</vh></v>
<v t="ekr.20220525082934.1534"><vh>LinePrecisionReporter.on_file</vh></v>
<v t="ekr.20220525082934.1535"><vh>LinePrecisionReporter.on_finish</vh></v>
</v>
</v>
<v t="ekr.20220525082934.1536"><vh>@clean sametypes.py</vh>
<v t="ekr.20220525082934.1537"><vh>is_same_type</vh></v>
<v t="ekr.20220525082934.1538"><vh>simplify_union</vh></v>
<v t="ekr.20220525082934.1539"><vh>is_same_types</vh></v>
<v t="ekr.20220525082934.1540"><vh>_extract_literals</vh></v>
<v t="ekr.20220525082934.1541"><vh>class SameTypeVisitor</vh>
<v t="ekr.20220525082934.1542"><vh>SameTypeVisitor.__init__</vh></v>
<v t="ekr.20220525082934.1543"><vh>SameTypeVisitor.visit_x(left) means: is left (which is an instance of X) the same type as</vh></v>
<v t="ekr.20220525082934.1544"><vh>SameTypeVisitor.visit_unbound_type</vh></v>
<v t="ekr.20220525082934.1545"><vh>SameTypeVisitor.visit_any</vh></v>
<v t="ekr.20220525082934.1546"><vh>SameTypeVisitor.visit_none_type</vh></v>
<v t="ekr.20220525082934.1547"><vh>SameTypeVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082934.1548"><vh>SameTypeVisitor.visit_erased_type</vh></v>
<v t="ekr.20220525082934.1549"><vh>SameTypeVisitor.visit_deleted_type</vh></v>
<v t="ekr.20220525082934.1550"><vh>SameTypeVisitor.visit_instance</vh></v>
<v t="ekr.20220525082934.1551"><vh>SameTypeVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20220525082934.1552"><vh>SameTypeVisitor.visit_type_var</vh></v>
<v t="ekr.20220525082934.1553"><vh>SameTypeVisitor.visit_param_spec</vh></v>
<v t="ekr.20220525082934.1554"><vh>SameTypeVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082934.1555"><vh>SameTypeVisitor.visit_unpack_type</vh></v>
<v t="ekr.20220525082934.1556"><vh>SameTypeVisitor.visit_parameters</vh></v>
<v t="ekr.20220525082934.1557"><vh>SameTypeVisitor.visit_callable_type</vh></v>
<v t="ekr.20220525082934.1558"><vh>SameTypeVisitor.visit_tuple_type</vh></v>
<v t="ekr.20220525082934.1559"><vh>SameTypeVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20220525082934.1560"><vh>SameTypeVisitor.visit_literal_type</vh></v>
<v t="ekr.20220525082934.1561"><vh>SameTypeVisitor.visit_union_type</vh></v>
<v t="ekr.20220525082934.1562"><vh>SameTypeVisitor.visit_overloaded</vh></v>
<v t="ekr.20220525082934.1563"><vh>SameTypeVisitor.visit_partial_type</vh></v>
<v t="ekr.20220525082934.1564"><vh>SameTypeVisitor.visit_type_type</vh></v>
</v>
</v>
<v t="ekr.20220525082934.1565"><vh>@clean scope.py</vh>
<v t="ekr.20220525082934.1566"><vh>class Scope</vh>
<v t="ekr.20220525082934.1567"><vh>Scope.__init__</vh></v>
<v t="ekr.20220525082934.1568"><vh>Scope.current_module_id</vh></v>
<v t="ekr.20220525082934.1569"><vh>Scope.current_target</vh></v>
<v t="ekr.20220525082934.1570"><vh>Scope.current_full_target</vh></v>
<v t="ekr.20220525082934.1571"><vh>Scope.current_type_name</vh></v>
<v t="ekr.20220525082934.1572"><vh>Scope.current_function_name</vh></v>
<v t="ekr.20220525082934.1573"><vh>Scope.module_scope</vh></v>
<v t="ekr.20220525082934.1574"><vh>Scope.function_scope</vh></v>
<v t="ekr.20220525082934.1575"><vh>Scope.enter_class</vh></v>
<v t="ekr.20220525082934.1576"><vh>Scope.leave_class</vh></v>
<v t="ekr.20220525082934.1577"><vh>Scope.class_scope</vh></v>
<v t="ekr.20220525082934.1578"><vh>Scope.save</vh></v>
<v t="ekr.20220525082934.1579"><vh>Scope.saved_scope</vh></v>
</v>
</v>
<v t="ekr.20220525082934.1580"><vh>@clean semanal.py</vh>
<v t="ekr.20220525082935.1"><vh>class SemanticAnalyzer</vh>
<v t="ekr.20220525082935.2"><vh>SemanticAnalyzer.__init__</vh></v>
<v t="ekr.20220525082935.3"><vh>SemanticAnalyzer.is_stub_file</vh></v>
<v t="ekr.20220525082935.4"><vh>SemanticAnalyzer.is_typeshed_stub_file</vh></v>
<v t="ekr.20220525082935.5"><vh>SemanticAnalyzer.final_iteration</vh></v>
<v t="ekr.20220525082935.6"><vh>SemanticAnalyzer.Preparing module (performed before semantic analysis)</vh></v>
<v t="ekr.20220525082935.7"><vh>SemanticAnalyzer.prepare_file</vh></v>
<v t="ekr.20220525082935.8"><vh>SemanticAnalyzer.prepare_typing_namespace</vh></v>
<v t="ekr.20220525082935.9"><vh>SemanticAnalyzer.prepare_builtins_namespace</vh></v>
<v t="ekr.20220525082935.10"><vh>SemanticAnalyzer.Analyzing a target</vh></v>
<v t="ekr.20220525082935.11"><vh>SemanticAnalyzer.refresh_partial</vh></v>
<v t="ekr.20220525082935.12"><vh>SemanticAnalyzer.refresh_top_level</vh></v>
<v t="ekr.20220525082935.13"><vh>SemanticAnalyzer.add_implicit_module_attrs</vh></v>
<v t="ekr.20220525082935.14"><vh>SemanticAnalyzer.add_builtin_aliases</vh></v>
<v t="ekr.20220525082935.15"><vh>SemanticAnalyzer.add_typing_extension_aliases</vh></v>
<v t="ekr.20220525082935.16"><vh>SemanticAnalyzer.create_alias</vh></v>
<v t="ekr.20220525082935.17"><vh>SemanticAnalyzer.adjust_public_exports</vh></v>
<v t="ekr.20220525082935.18"><vh>SemanticAnalyzer.file_context</vh></v>
<v t="ekr.20220525082935.19"><vh>SemanticAnalyzer.Functions</vh></v>
<v t="ekr.20220525082935.20"><vh>SemanticAnalyzer.visit_func_def</vh></v>
<v t="ekr.20220525082935.21"><vh>SemanticAnalyzer.analyze_func_def</vh></v>
<v t="ekr.20220525082935.22"><vh>SemanticAnalyzer.prepare_method_signature</vh></v>
<v t="ekr.20220525082935.23"><vh>SemanticAnalyzer.set_original_def</vh></v>
<v t="ekr.20220525082935.24"><vh>SemanticAnalyzer.update_function_type_variables</vh></v>
<v t="ekr.20220525082935.25"><vh>SemanticAnalyzer.visit_overloaded_func_def</vh></v>
<v t="ekr.20220525082935.26"><vh>SemanticAnalyzer.analyze_overloaded_func_def</vh></v>
<v t="ekr.20220525082935.27"><vh>SemanticAnalyzer.analyze_overload_sigs_and_impl</vh></v>
<v t="ekr.20220525082935.28"><vh>SemanticAnalyzer.handle_missing_overload_decorators</vh></v>
<v t="ekr.20220525082935.29"><vh>SemanticAnalyzer.handle_missing_overload_implementation</vh></v>
<v t="ekr.20220525082935.30"><vh>SemanticAnalyzer.process_final_in_overload</vh></v>
<v t="ekr.20220525082935.31"><vh>SemanticAnalyzer.process_static_or_class_method_in_overload</vh></v>
<v t="ekr.20220525082935.32"><vh>SemanticAnalyzer.analyze_property_with_multi_part_definition</vh></v>
<v t="ekr.20220525082935.33"><vh>SemanticAnalyzer.add_function_to_symbol_table</vh></v>
<v t="ekr.20220525082935.34"><vh>SemanticAnalyzer.analyze_arg_initializers</vh></v>
<v t="ekr.20220525082935.35"><vh>SemanticAnalyzer.analyze_function_body</vh></v>
<v t="ekr.20220525082935.36"><vh>SemanticAnalyzer.check_classvar_in_signature</vh></v>
<v t="ekr.20220525082935.37"><vh>SemanticAnalyzer.check_function_signature</vh></v>
<v t="ekr.20220525082935.38"><vh>SemanticAnalyzer.visit_decorator</vh></v>
<v t="ekr.20220525082935.39"><vh>SemanticAnalyzer.check_decorated_function_is_method</vh></v>
<v t="ekr.20220525082935.40"><vh>SemanticAnalyzer.Classes</vh></v>
<v t="ekr.20220525082935.41"><vh>SemanticAnalyzer.visit_class_def</vh></v>
<v t="ekr.20220525082935.42"><vh>SemanticAnalyzer.analyze_class</vh></v>
<v t="ekr.20220525082935.43"><vh>SemanticAnalyzer.is_core_builtin_class</vh></v>
<v t="ekr.20220525082935.44"><vh>SemanticAnalyzer.analyze_class_body_common</vh></v>
<v t="ekr.20220525082935.45"><vh>SemanticAnalyzer.analyze_namedtuple_classdef</vh></v>
<v t="ekr.20220525082935.46"><vh>SemanticAnalyzer.apply_class_plugin_hooks</vh></v>
<v t="ekr.20220525082935.47"><vh>SemanticAnalyzer.get_fullname_for_hook</vh></v>
<v t="ekr.20220525082935.48"><vh>SemanticAnalyzer.analyze_class_keywords</vh></v>
<v t="ekr.20220525082935.49"><vh>SemanticAnalyzer.enter_class</vh></v>
<v t="ekr.20220525082935.50"><vh>SemanticAnalyzer.leave_class</vh></v>
<v t="ekr.20220525082935.51"><vh>SemanticAnalyzer.analyze_class_decorator</vh></v>
<v t="ekr.20220525082935.52"><vh>SemanticAnalyzer.clean_up_bases_and_infer_type_variables</vh></v>
<v t="ekr.20220525082935.53"><vh>SemanticAnalyzer.analyze_class_typevar_declaration</vh></v>
<v t="ekr.20220525082935.54"><vh>SemanticAnalyzer.analyze_unbound_tvar</vh></v>
<v t="ekr.20220525082935.55"><vh>SemanticAnalyzer.get_all_bases_tvars</vh></v>
<v t="ekr.20220525082935.56"><vh>SemanticAnalyzer.prepare_class_def</vh></v>
<v t="ekr.20220525082935.57"><vh>SemanticAnalyzer.make_empty_type_info</vh></v>
<v t="ekr.20220525082935.58"><vh>SemanticAnalyzer.get_name_repr_of_expr</vh></v>
<v t="ekr.20220525082935.59"><vh>SemanticAnalyzer.analyze_base_classes</vh></v>
<v t="ekr.20220525082935.60"><vh>SemanticAnalyzer.configure_base_classes</vh></v>
<v t="ekr.20220525082935.61"><vh>SemanticAnalyzer.configure_tuple_base_class</vh></v>
<v t="ekr.20220525082935.62"><vh>SemanticAnalyzer.set_dummy_mro</vh></v>
<v t="ekr.20220525082935.63"><vh>SemanticAnalyzer.calculate_class_mro</vh></v>
<v t="ekr.20220525082935.64"><vh>SemanticAnalyzer.update_metaclass</vh></v>
<v t="ekr.20220525082935.65"><vh>SemanticAnalyzer.verify_base_classes</vh></v>
<v t="ekr.20220525082935.66"><vh>SemanticAnalyzer.is_base_class</vh></v>
<v t="ekr.20220525082935.67"><vh>SemanticAnalyzer.analyze_metaclass</vh></v>
<v t="ekr.20220525082935.68"><vh>SemanticAnalyzer.Imports</vh></v>
<v t="ekr.20220525082935.69"><vh>SemanticAnalyzer.visit_import</vh></v>
<v t="ekr.20220525082935.70"><vh>SemanticAnalyzer.visit_import_from</vh></v>
<v t="ekr.20220525082935.71"><vh>SemanticAnalyzer.process_imported_symbol</vh></v>
<v t="ekr.20220525082935.72"><vh>SemanticAnalyzer.report_missing_module_attribute</vh></v>
<v t="ekr.20220525082935.73"><vh>SemanticAnalyzer.process_import_over_existing_name</vh></v>
<v t="ekr.20220525082935.74"><vh>SemanticAnalyzer.correct_relative_import</vh></v>
<v t="ekr.20220525082935.75"><vh>SemanticAnalyzer.visit_import_all</vh></v>
<v t="ekr.20220525082935.76"><vh>SemanticAnalyzer.Assignment</vh></v>
<v t="ekr.20220525082935.77"><vh>SemanticAnalyzer.visit_assignment_expr</vh></v>
<v t="ekr.20220525082935.78"><vh>SemanticAnalyzer.visit_assignment_stmt</vh></v>
<v t="ekr.20220525082935.79"><vh>SemanticAnalyzer.analyze_identity_global_assignment</vh></v>
<v t="ekr.20220525082935.80"><vh>SemanticAnalyzer.should_wait_rhs</vh></v>
<v t="ekr.20220525082935.81"><vh>SemanticAnalyzer.can_be_type_alias</vh></v>
<v t="ekr.20220525082935.82"><vh>SemanticAnalyzer.is_type_ref</vh></v>
<v t="ekr.20220525082935.83"><vh>SemanticAnalyzer.is_none_alias</vh></v>
<v t="ekr.20220525082935.84"><vh>SemanticAnalyzer.record_special_form_lvalue</vh></v>
<v t="ekr.20220525082935.85"><vh>SemanticAnalyzer.analyze_enum_assign</vh></v>
<v t="ekr.20220525082935.86"><vh>SemanticAnalyzer.analyze_namedtuple_assign</vh></v>
<v t="ekr.20220525082935.87"><vh>SemanticAnalyzer.analyze_typeddict_assign</vh></v>
<v t="ekr.20220525082935.88"><vh>SemanticAnalyzer.analyze_lvalues</vh></v>
<v t="ekr.20220525082935.89"><vh>SemanticAnalyzer.apply_dynamic_class_hook</vh></v>
<v t="ekr.20220525082935.90"><vh>SemanticAnalyzer.unwrap_final</vh></v>
<v t="ekr.20220525082935.91"><vh>SemanticAnalyzer.check_final_implicit_def</vh></v>
<v t="ekr.20220525082935.92"><vh>SemanticAnalyzer.store_final_status</vh></v>
<v t="ekr.20220525082935.93"><vh>SemanticAnalyzer.flatten_lvalues</vh></v>
<v t="ekr.20220525082935.94"><vh>SemanticAnalyzer.unbox_literal</vh></v>
<v t="ekr.20220525082935.95"><vh>SemanticAnalyzer.process_type_annotation</vh></v>
<v t="ekr.20220525082935.96"><vh>SemanticAnalyzer.is_annotated_protocol_member</vh></v>
<v t="ekr.20220525082935.97"><vh>SemanticAnalyzer.analyze_simple_literal_type</vh></v>
<v t="ekr.20220525082935.98"><vh>SemanticAnalyzer.analyze_alias</vh></v>
<v t="ekr.20220525082935.99"><vh>SemanticAnalyzer.check_and_set_up_type_alias</vh></v>
<v t="ekr.20220525082935.100"><vh>SemanticAnalyzer.analyze_lvalue</vh></v>
<v t="ekr.20220525082935.101"><vh>SemanticAnalyzer.analyze_name_lvalue</vh></v>
<v t="ekr.20220525082935.102"><vh>SemanticAnalyzer.is_final_redefinition</vh></v>
<v t="ekr.20220525082935.103"><vh>SemanticAnalyzer.is_alias_for_final_name</vh></v>
<v t="ekr.20220525082935.104"><vh>SemanticAnalyzer.make_name_lvalue_var</vh></v>
<v t="ekr.20220525082935.105"><vh>SemanticAnalyzer.make_name_lvalue_point_to_existing_def</vh></v>
<v t="ekr.20220525082935.106"><vh>SemanticAnalyzer.analyze_tuple_or_list_lvalue</vh></v>
<v t="ekr.20220525082935.107"><vh>SemanticAnalyzer.analyze_member_lvalue</vh></v>
<v t="ekr.20220525082935.108"><vh>SemanticAnalyzer.is_self_member_ref</vh></v>
<v t="ekr.20220525082935.109"><vh>SemanticAnalyzer.check_lvalue_validity</vh></v>
<v t="ekr.20220525082935.110"><vh>SemanticAnalyzer.store_declared_types</vh></v>
<v t="ekr.20220525082935.111"><vh>SemanticAnalyzer.process_typevar_declaration</vh></v>
<v t="ekr.20220525082935.112"><vh>SemanticAnalyzer.check_typevarlike_name</vh></v>
<v t="ekr.20220525082935.113"><vh>SemanticAnalyzer.get_typevarlike_declaration</vh></v>
<v t="ekr.20220525082935.114"><vh>SemanticAnalyzer.process_typevar_parameters</vh></v>
<v t="ekr.20220525082935.115"><vh>SemanticAnalyzer.extract_typevarlike_name</vh></v>
<v t="ekr.20220525082935.116"><vh>SemanticAnalyzer.process_paramspec_declaration</vh></v>
<v t="ekr.20220525082935.117"><vh>SemanticAnalyzer.process_typevartuple_declaration</vh></v>
<v t="ekr.20220525082935.118"><vh>SemanticAnalyzer.basic_new_typeinfo</vh></v>
<v t="ekr.20220525082935.119"><vh>SemanticAnalyzer.analyze_value_types</vh></v>
<v t="ekr.20220525082935.120"><vh>SemanticAnalyzer.check_classvar</vh></v>
<v t="ekr.20220525082935.121"><vh>SemanticAnalyzer.is_classvar</vh></v>
<v t="ekr.20220525082935.122"><vh>SemanticAnalyzer.is_final_type</vh></v>
<v t="ekr.20220525082935.123"><vh>SemanticAnalyzer.fail_invalid_classvar</vh></v>
<v t="ekr.20220525082935.124"><vh>SemanticAnalyzer.process_module_assignment</vh></v>
<v t="ekr.20220525082935.125"><vh>SemanticAnalyzer.process__all__</vh></v>
<v t="ekr.20220525082935.126"><vh>SemanticAnalyzer.process__deletable__</vh></v>
<v t="ekr.20220525082935.127"><vh>SemanticAnalyzer.process__slots__</vh></v>
<v t="ekr.20220525082935.128"><vh>SemanticAnalyzer.Misc statements</vh></v>
<v t="ekr.20220525082935.129"><vh>SemanticAnalyzer.visit_block</vh></v>
<v t="ekr.20220525082935.130"><vh>SemanticAnalyzer.visit_block_maybe</vh></v>
<v t="ekr.20220525082935.131"><vh>SemanticAnalyzer.visit_expression_stmt</vh></v>
<v t="ekr.20220525082935.132"><vh>SemanticAnalyzer.visit_return_stmt</vh></v>
<v t="ekr.20220525082935.133"><vh>SemanticAnalyzer.visit_raise_stmt</vh></v>
<v t="ekr.20220525082935.134"><vh>SemanticAnalyzer.visit_assert_stmt</vh></v>
<v t="ekr.20220525082935.135"><vh>SemanticAnalyzer.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20220525082935.136"><vh>SemanticAnalyzer.visit_while_stmt</vh></v>
<v t="ekr.20220525082935.137"><vh>SemanticAnalyzer.visit_for_stmt</vh></v>
<v t="ekr.20220525082935.138"><vh>SemanticAnalyzer.visit_break_stmt</vh></v>
<v t="ekr.20220525082935.139"><vh>SemanticAnalyzer.visit_continue_stmt</vh></v>
<v t="ekr.20220525082935.140"><vh>SemanticAnalyzer.visit_if_stmt</vh></v>
<v t="ekr.20220525082935.141"><vh>SemanticAnalyzer.visit_try_stmt</vh></v>
<v t="ekr.20220525082935.142"><vh>SemanticAnalyzer.analyze_try_stmt</vh></v>
<v t="ekr.20220525082935.143"><vh>SemanticAnalyzer.visit_with_stmt</vh></v>
<v t="ekr.20220525082935.144"><vh>SemanticAnalyzer.visit_del_stmt</vh></v>
<v t="ekr.20220525082935.145"><vh>SemanticAnalyzer.is_valid_del_target</vh></v>
<v t="ekr.20220525082935.146"><vh>SemanticAnalyzer.visit_global_decl</vh></v>
<v t="ekr.20220525082935.147"><vh>SemanticAnalyzer.visit_nonlocal_decl</vh></v>
<v t="ekr.20220525082935.148"><vh>SemanticAnalyzer.visit_print_stmt</vh></v>
<v t="ekr.20220525082935.149"><vh>SemanticAnalyzer.visit_exec_stmt</vh></v>
<v t="ekr.20220525082935.150"><vh>SemanticAnalyzer.visit_match_stmt</vh></v>
<v t="ekr.20220525082935.151"><vh>SemanticAnalyzer.Expressions</vh></v>
<v t="ekr.20220525082935.152"><vh>SemanticAnalyzer.visit_name_expr</vh></v>
<v t="ekr.20220525082935.153"><vh>SemanticAnalyzer.bind_name_expr</vh></v>
<v t="ekr.20220525082935.154"><vh>SemanticAnalyzer.visit_super_expr</vh></v>
<v t="ekr.20220525082935.155"><vh>SemanticAnalyzer.visit_tuple_expr</vh></v>
<v t="ekr.20220525082935.156"><vh>SemanticAnalyzer.visit_list_expr</vh></v>
<v t="ekr.20220525082935.157"><vh>SemanticAnalyzer.visit_set_expr</vh></v>
<v t="ekr.20220525082935.158"><vh>SemanticAnalyzer.visit_dict_expr</vh></v>
<v t="ekr.20220525082935.159"><vh>SemanticAnalyzer.visit_star_expr</vh></v>
<v t="ekr.20220525082935.160"><vh>SemanticAnalyzer.visit_yield_from_expr</vh></v>
<v t="ekr.20220525082935.161"><vh>SemanticAnalyzer.visit_call_expr</vh></v>
<v t="ekr.20220525082935.162"><vh>SemanticAnalyzer.translate_dict_call</vh></v>
<v t="ekr.20220525082935.163"><vh>SemanticAnalyzer.check_fixed_args</vh></v>
<v t="ekr.20220525082935.164"><vh>SemanticAnalyzer.visit_member_expr</vh></v>
<v t="ekr.20220525082935.165"><vh>SemanticAnalyzer.visit_op_expr</vh></v>
<v t="ekr.20220525082935.166"><vh>SemanticAnalyzer.visit_comparison_expr</vh></v>
<v t="ekr.20220525082935.167"><vh>SemanticAnalyzer.visit_unary_expr</vh></v>
<v t="ekr.20220525082935.168"><vh>SemanticAnalyzer.visit_index_expr</vh></v>
<v t="ekr.20220525082935.169"><vh>SemanticAnalyzer.analyze_type_application</vh></v>
<v t="ekr.20220525082935.170"><vh>SemanticAnalyzer.analyze_type_application_args</vh></v>
<v t="ekr.20220525082935.171"><vh>SemanticAnalyzer.visit_slice_expr</vh></v>
<v t="ekr.20220525082935.172"><vh>SemanticAnalyzer.visit_cast_expr</vh></v>
<v t="ekr.20220525082935.173"><vh>SemanticAnalyzer.visit_assert_type_expr</vh></v>
<v t="ekr.20220525082935.174"><vh>SemanticAnalyzer.visit_reveal_expr</vh></v>
<v t="ekr.20220525082935.175"><vh>SemanticAnalyzer.visit_type_application</vh></v>
<v t="ekr.20220525082935.176"><vh>SemanticAnalyzer.visit_list_comprehension</vh></v>
<v t="ekr.20220525082935.177"><vh>SemanticAnalyzer.visit_set_comprehension</vh></v>
<v t="ekr.20220525082935.178"><vh>SemanticAnalyzer.visit_dictionary_comprehension</vh></v>
<v t="ekr.20220525082935.179"><vh>SemanticAnalyzer.visit_generator_expr</vh></v>
<v t="ekr.20220525082935.180"><vh>SemanticAnalyzer.analyze_comp_for</vh></v>
<v t="ekr.20220525082935.181"><vh>SemanticAnalyzer.analyze_comp_for_2</vh></v>
<v t="ekr.20220525082935.182"><vh>SemanticAnalyzer.visit_lambda_expr</vh></v>
<v t="ekr.20220525082935.183"><vh>SemanticAnalyzer.visit_conditional_expr</vh></v>
<v t="ekr.20220525082935.184"><vh>SemanticAnalyzer.visit_backquote_expr</vh></v>
<v t="ekr.20220525082935.185"><vh>SemanticAnalyzer.visit__promote_expr</vh></v>
<v t="ekr.20220525082935.186"><vh>SemanticAnalyzer.visit_yield_expr</vh></v>
<v t="ekr.20220525082935.187"><vh>SemanticAnalyzer.visit_await_expr</vh></v>
<v t="ekr.20220525082935.188"><vh>SemanticAnalyzer.Patterns</vh></v>
<v t="ekr.20220525082935.189"><vh>SemanticAnalyzer.visit_as_pattern</vh></v>
<v t="ekr.20220525082935.190"><vh>SemanticAnalyzer.visit_or_pattern</vh></v>
<v t="ekr.20220525082935.191"><vh>SemanticAnalyzer.visit_value_pattern</vh></v>
<v t="ekr.20220525082935.192"><vh>SemanticAnalyzer.visit_sequence_pattern</vh></v>
<v t="ekr.20220525082935.193"><vh>SemanticAnalyzer.visit_starred_pattern</vh></v>
<v t="ekr.20220525082935.194"><vh>SemanticAnalyzer.visit_mapping_pattern</vh></v>
<v t="ekr.20220525082935.195"><vh>SemanticAnalyzer.visit_class_pattern</vh></v>
<v t="ekr.20220525082935.196"><vh>SemanticAnalyzer.Lookup functions</vh></v>
<v t="ekr.20220525082935.197"><vh>SemanticAnalyzer.lookup</vh></v>
<v t="ekr.20220525082935.198"><vh>SemanticAnalyzer.is_active_symbol_in_class_body</vh></v>
<v t="ekr.20220525082935.199"><vh>SemanticAnalyzer.is_textually_before_statement</vh></v>
<v t="ekr.20220525082935.200"><vh>SemanticAnalyzer.is_overloaded_item</vh></v>
<v t="ekr.20220525082935.201"><vh>SemanticAnalyzer.is_defined_in_current_module</vh></v>
<v t="ekr.20220525082935.202"><vh>SemanticAnalyzer.lookup_qualified</vh></v>
<v t="ekr.20220525082935.203"><vh>SemanticAnalyzer.lookup_type_node</vh></v>
<v t="ekr.20220525082935.204"><vh>SemanticAnalyzer.get_module_symbol</vh></v>
<v t="ekr.20220525082935.205"><vh>SemanticAnalyzer.is_missing_module</vh></v>
<v t="ekr.20220525082935.206"><vh>SemanticAnalyzer.implicit_symbol</vh></v>
<v t="ekr.20220525082935.207"><vh>SemanticAnalyzer.create_getattr_var</vh></v>
<v t="ekr.20220525082935.208"><vh>SemanticAnalyzer.lookup_fully_qualified</vh></v>
<v t="ekr.20220525082935.209"><vh>SemanticAnalyzer.lookup_fully_qualified_or_none</vh></v>
<v t="ekr.20220525082935.210"><vh>SemanticAnalyzer.object_type</vh></v>
<v t="ekr.20220525082935.211"><vh>SemanticAnalyzer.str_type</vh></v>
<v t="ekr.20220525082935.212"><vh>SemanticAnalyzer.named_type</vh></v>
<v t="ekr.20220525082935.213"><vh>SemanticAnalyzer.named_type_or_none</vh></v>
<v t="ekr.20220525082935.214"><vh>SemanticAnalyzer.builtin_type</vh></v>
<v t="ekr.20220525082935.215"><vh>SemanticAnalyzer.lookup_current_scope</vh></v>
<v t="ekr.20220525082935.216"><vh>SemanticAnalyzer.Adding symbols</vh></v>
<v t="ekr.20220525082935.217"><vh>SemanticAnalyzer.add_symbol</vh></v>
<v t="ekr.20220525082935.218"><vh>SemanticAnalyzer.add_symbol_skip_local</vh></v>
<v t="ekr.20220525082935.219"><vh>SemanticAnalyzer.add_symbol_table_node</vh></v>
<v t="ekr.20220525082935.220"><vh>SemanticAnalyzer.add_redefinition</vh></v>
<v t="ekr.20220525082935.221"><vh>SemanticAnalyzer.add_local</vh></v>
<v t="ekr.20220525082935.222"><vh>SemanticAnalyzer.add_module_symbol</vh></v>
<v t="ekr.20220525082935.223"><vh>SemanticAnalyzer._get_node_for_class_scoped_import</vh></v>
<v t="ekr.20220525082935.224"><vh>SemanticAnalyzer.add_imported_symbol</vh></v>
<v t="ekr.20220525082935.225"><vh>SemanticAnalyzer.add_unknown_imported_symbol</vh></v>
<v t="ekr.20220525082935.226"><vh>SemanticAnalyzer.Other helpers</vh></v>
<v t="ekr.20220525082935.227"><vh>SemanticAnalyzer.tvar_scope_frame</vh></v>
<v t="ekr.20220525082935.228"><vh>SemanticAnalyzer.defer</vh></v>
<v t="ekr.20220525082935.229"><vh>SemanticAnalyzer.track_incomplete_refs</vh></v>
<v t="ekr.20220525082935.230"><vh>SemanticAnalyzer.found_incomplete_ref</vh></v>
<v t="ekr.20220525082935.231"><vh>SemanticAnalyzer.record_incomplete_ref</vh></v>
<v t="ekr.20220525082935.232"><vh>SemanticAnalyzer.mark_incomplete</vh></v>
<v t="ekr.20220525082935.233"><vh>SemanticAnalyzer.is_incomplete_namespace</vh></v>
<v t="ekr.20220525082935.234"><vh>SemanticAnalyzer.process_placeholder</vh></v>
<v t="ekr.20220525082935.235"><vh>SemanticAnalyzer.cannot_resolve_name</vh></v>
<v t="ekr.20220525082935.236"><vh>SemanticAnalyzer.qualified_name</vh></v>
<v t="ekr.20220525082935.237"><vh>SemanticAnalyzer.enter</vh></v>
<v t="ekr.20220525082935.238"><vh>SemanticAnalyzer.is_func_scope</vh></v>
<v t="ekr.20220525082935.239"><vh>SemanticAnalyzer.is_nested_within_func_scope</vh></v>
<v t="ekr.20220525082935.240"><vh>SemanticAnalyzer.is_class_scope</vh></v>
<v t="ekr.20220525082935.241"><vh>SemanticAnalyzer.is_module_scope</vh></v>
<v t="ekr.20220525082935.242"><vh>SemanticAnalyzer.current_symbol_kind</vh></v>
<v t="ekr.20220525082935.243"><vh>SemanticAnalyzer.current_symbol_table</vh></v>
<v t="ekr.20220525082935.244"><vh>SemanticAnalyzer.is_global_or_nonlocal</vh></v>
<v t="ekr.20220525082935.245"><vh>SemanticAnalyzer.add_exports</vh></v>
<v t="ekr.20220525082935.246"><vh>SemanticAnalyzer.name_not_defined</vh></v>
<v t="ekr.20220525082935.247"><vh>SemanticAnalyzer.already_defined</vh></v>
<v t="ekr.20220525082935.248"><vh>SemanticAnalyzer.name_already_defined</vh></v>
<v t="ekr.20220525082935.249"><vh>SemanticAnalyzer.attribute_already_defined</vh></v>
<v t="ekr.20220525082935.250"><vh>SemanticAnalyzer.is_local_name</vh></v>
<v t="ekr.20220525082935.251"><vh>SemanticAnalyzer.in_checked_function</vh></v>
<v t="ekr.20220525082935.252"><vh>SemanticAnalyzer.fail</vh></v>
<v t="ekr.20220525082935.253"><vh>SemanticAnalyzer.note</vh></v>
<v t="ekr.20220525082935.254"><vh>SemanticAnalyzer.accept</vh></v>
<v t="ekr.20220525082935.255"><vh>SemanticAnalyzer.expr_to_analyzed_type</vh></v>
<v t="ekr.20220525082935.256"><vh>SemanticAnalyzer.analyze_type_expr</vh></v>
<v t="ekr.20220525082935.257"><vh>SemanticAnalyzer.type_analyzer</vh></v>
<v t="ekr.20220525082935.258"><vh>SemanticAnalyzer.expr_to_unanalyzed_type</vh></v>
<v t="ekr.20220525082935.259"><vh>SemanticAnalyzer.anal_type</vh></v>
<v t="ekr.20220525082935.260"><vh>SemanticAnalyzer.class_type</vh></v>
<v t="ekr.20220525082935.261"><vh>SemanticAnalyzer.schedule_patch</vh></v>
<v t="ekr.20220525082935.262"><vh>SemanticAnalyzer.report_hang</vh></v>
<v t="ekr.20220525082935.263"><vh>SemanticAnalyzer.add_plugin_dependency</vh></v>
<v t="ekr.20220525082935.264"><vh>SemanticAnalyzer.add_type_alias_deps</vh></v>
<v t="ekr.20220525082935.265"><vh>SemanticAnalyzer.is_mangled_global</vh></v>
<v t="ekr.20220525082935.266"><vh>SemanticAnalyzer.is_initial_mangled_global</vh></v>
<v t="ekr.20220525082935.267"><vh>SemanticAnalyzer.parse_bool</vh></v>
<v t="ekr.20220525082935.268"><vh>SemanticAnalyzer.set_future_import_flags</vh></v>
<v t="ekr.20220525082935.269"><vh>SemanticAnalyzer.is_future_flag_set</vh></v>
</v>
<v t="ekr.20220525082935.270"><vh>class HasPlaceholders</vh></v>
<v t="ekr.20220525082935.271"><vh>has_placeholder</vh></v>
<v t="ekr.20220525082935.272"><vh>replace_implicit_first_type</vh></v>
<v t="ekr.20220525082935.273"><vh>refers_to_fullname</vh></v>
<v t="ekr.20220525082935.274"><vh>refers_to_class_or_function</vh></v>
<v t="ekr.20220525082935.275"><vh>find_duplicate</vh></v>
<v t="ekr.20220525082935.276"><vh>remove_imported_names_from_symtable</vh></v>
<v t="ekr.20220525082935.277"><vh>make_any_non_explicit</vh></v>
<v t="ekr.20220525082935.278"><vh>class MakeAnyNonExplicit</vh>
<v t="ekr.20220525082935.279"><vh>MakeAnyNonExplicit.visit_any</vh></v>
<v t="ekr.20220525082935.280"><vh>MakeAnyNonExplicit.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20220525082935.281"><vh>apply_semantic_analyzer_patches</vh></v>
<v t="ekr.20220525082935.282"><vh>names_modified_by_assignment</vh></v>
<v t="ekr.20220525082935.283"><vh>names_modified_in_lvalue</vh></v>
<v t="ekr.20220525082935.284"><vh>is_same_var_from_getattr</vh></v>
<v t="ekr.20220525082935.285"><vh>dummy_context</vh></v>
<v t="ekr.20220525082935.286"><vh>is_valid_replacement</vh></v>
<v t="ekr.20220525082935.287"><vh>is_same_symbol</vh></v>
</v>
<v t="ekr.20220525082935.288"><vh>@clean semanal_classprop.py</vh>
<v t="ekr.20220525082935.289"><vh>calculate_class_abstract_status</vh></v>
<v t="ekr.20220525082935.290"><vh>check_protocol_status</vh></v>
<v t="ekr.20220525082935.291"><vh>calculate_class_vars</vh></v>
<v t="ekr.20220525082935.292"><vh>add_type_promotion</vh></v>
</v>
<v t="ekr.20220525082935.293"><vh>@clean semanal_enum.py</vh>
<v t="ekr.20220525082935.294"><vh>class EnumCallAnalyzer</vh>
<v t="ekr.20220525082935.295"><vh>EnumCallAnalyzer.__init__</vh></v>
<v t="ekr.20220525082935.296"><vh>EnumCallAnalyzer.process_enum_call</vh></v>
<v t="ekr.20220525082935.297"><vh>EnumCallAnalyzer.check_enum_call</vh></v>
<v t="ekr.20220525082935.298"><vh>EnumCallAnalyzer.build_enum_call_typeinfo</vh></v>
<v t="ekr.20220525082935.299"><vh>EnumCallAnalyzer.parse_enum_call_args</vh></v>
<v t="ekr.20220525082935.300"><vh>EnumCallAnalyzer.fail_enum_call_arg</vh></v>
<v t="ekr.20220525082935.301"><vh>EnumCallAnalyzer.Helpers</vh></v>
<v t="ekr.20220525082935.302"><vh>EnumCallAnalyzer.fail</vh></v>
</v>
</v>
<v t="ekr.20220525082935.303"><vh>@clean semanal_infer.py</vh>
<v t="ekr.20220525082935.304"><vh>infer_decorator_signature_if_simple</vh></v>
<v t="ekr.20220525082935.305"><vh>is_identity_signature</vh></v>
<v t="ekr.20220525082935.306"><vh>calculate_return_type</vh></v>
<v t="ekr.20220525082935.307"><vh>find_fixed_callable_return</vh></v>
</v>
<v t="ekr.20220525082935.308"><vh>@clean semanal_main.py</vh>
<v t="ekr.20220525082935.309"><vh>semantic_analysis_for_scc</vh></v>
<v t="ekr.20220525082935.310"><vh>cleanup_builtin_scc</vh></v>
<v t="ekr.20220525082935.311"><vh>semantic_analysis_for_targets</vh></v>
<v t="ekr.20220525082935.312"><vh>restore_saved_attrs</vh></v>
<v t="ekr.20220525082935.313"><vh>process_top_levels</vh></v>
<v t="ekr.20220525082935.314"><vh>process_functions</vh></v>
<v t="ekr.20220525082935.315"><vh>process_top_level_function</vh></v>
<v t="ekr.20220525082935.316"><vh>TargetInfo = Tuple[str, Union[MypyFile, FuncDef, OverloadedFuncDef, Decorator], Optional[TypeInfo]]</vh></v>
<v t="ekr.20220525082935.317"><vh>get_all_leaf_targets</vh></v>
<v t="ekr.20220525082935.318"><vh>semantic_analyze_target</vh></v>
<v t="ekr.20220525082935.319"><vh>check_type_arguments</vh></v>
<v t="ekr.20220525082935.320"><vh>check_type_arguments_in_targets</vh></v>
<v t="ekr.20220525082935.321"><vh>apply_class_plugin_hooks</vh></v>
<v t="ekr.20220525082935.322"><vh>apply_hooks_to_class</vh></v>
<v t="ekr.20220525082935.323"><vh>calculate_class_properties</vh></v>
<v t="ekr.20220525082935.324"><vh>check_blockers</vh></v>
</v>
<v t="ekr.20220525082935.325"><vh>@clean semanal_namedtuple.py</vh>
<v t="ekr.20220525082935.326"><vh>class NamedTupleAnalyzer</vh>
<v t="ekr.20220525082935.327"><vh>NamedTupleAnalyzer.__init__</vh></v>
<v t="ekr.20220525082935.328"><vh>NamedTupleAnalyzer.analyze_namedtuple_classdef</vh></v>
<v t="ekr.20220525082935.329"><vh>NamedTupleAnalyzer.check_namedtuple_classdef</vh></v>
<v t="ekr.20220525082935.330"><vh>NamedTupleAnalyzer.check_namedtuple</vh></v>
<v t="ekr.20220525082935.331"><vh>NamedTupleAnalyzer.store_namedtuple_info</vh></v>
<v t="ekr.20220525082935.332"><vh>NamedTupleAnalyzer.parse_namedtuple_args</vh></v>
<v t="ekr.20220525082935.333"><vh>NamedTupleAnalyzer.parse_namedtuple_fields_with_types</vh></v>
<v t="ekr.20220525082935.334"><vh>NamedTupleAnalyzer.build_namedtuple_typeinfo</vh></v>
<v t="ekr.20220525082935.335"><vh>NamedTupleAnalyzer.save_namedtuple_body</vh></v>
<v t="ekr.20220525082935.336"><vh>NamedTupleAnalyzer.Helpers</vh></v>
<v t="ekr.20220525082935.337"><vh>NamedTupleAnalyzer.fail</vh></v>
</v>
</v>
<v t="ekr.20220525082935.338"><vh>@clean semanal_newtype.py</vh>
<v t="ekr.20220525082935.339"><vh>class NewTypeAnalyzer</vh>
<v t="ekr.20220525082935.340"><vh>NewTypeAnalyzer.__init__</vh></v>
<v t="ekr.20220525082935.341"><vh>NewTypeAnalyzer.process_newtype_declaration</vh></v>
<v t="ekr.20220525082935.342"><vh>NewTypeAnalyzer.analyze_newtype_declaration</vh></v>
<v t="ekr.20220525082935.343"><vh>NewTypeAnalyzer.check_newtype_args</vh></v>
<v t="ekr.20220525082935.344"><vh>NewTypeAnalyzer.build_newtype_typeinfo</vh></v>
<v t="ekr.20220525082935.345"><vh>NewTypeAnalyzer.Helpers</vh></v>
<v t="ekr.20220525082935.346"><vh>NewTypeAnalyzer.make_argument</vh></v>
<v t="ekr.20220525082935.347"><vh>NewTypeAnalyzer.fail</vh></v>
</v>
</v>
<v t="ekr.20220525082935.348"><vh>@clean semanal_pass1.py</vh>
<v t="ekr.20220525082935.349"><vh>class SemanticAnalyzerPreAnalysis</vh>
<v t="ekr.20220525082935.350"><vh>SemanticAnalyzerPreAnalysis.visit_file</vh></v>
<v t="ekr.20220525082935.351"><vh>SemanticAnalyzerPreAnalysis.visit_func_def</vh></v>
<v t="ekr.20220525082935.352"><vh>SemanticAnalyzerPreAnalysis.visit_class_def</vh></v>
<v t="ekr.20220525082935.353"><vh>SemanticAnalyzerPreAnalysis.visit_import_from</vh></v>
<v t="ekr.20220525082935.354"><vh>SemanticAnalyzerPreAnalysis.visit_import_all</vh></v>
<v t="ekr.20220525082935.355"><vh>SemanticAnalyzerPreAnalysis.visit_import</vh></v>
<v t="ekr.20220525082935.356"><vh>SemanticAnalyzerPreAnalysis.visit_if_stmt</vh></v>
<v t="ekr.20220525082935.357"><vh>SemanticAnalyzerPreAnalysis.visit_block</vh></v>
<v t="ekr.20220525082935.358"><vh>SemanticAnalyzerPreAnalysis.visit_match_stmt</vh></v>
<v t="ekr.20220525082935.359"><vh>SemanticAnalyzerPreAnalysis.The remaining methods are an optimization: don't visit nested expressions</vh></v>
<v t="ekr.20220525082935.360"><vh>SemanticAnalyzerPreAnalysis.visit_assignment_stmt</vh></v>
<v t="ekr.20220525082935.361"><vh>SemanticAnalyzerPreAnalysis.visit_expression_stmt</vh></v>
<v t="ekr.20220525082935.362"><vh>SemanticAnalyzerPreAnalysis.visit_return_stmt</vh></v>
<v t="ekr.20220525082935.363"><vh>SemanticAnalyzerPreAnalysis.visit_for_stmt</vh></v>
</v>
</v>
<v t="ekr.20220525082935.364"><vh>@clean semanal_shared.py</vh>
<v t="ekr.20220525082935.365"><vh>class SemanticAnalyzerCoreInterface</vh>
<v t="ekr.20220525082935.366"><vh>SemanticAnalyzerCoreInterface.lookup_qualified</vh></v>
<v t="ekr.20220525082935.367"><vh>SemanticAnalyzerCoreInterface.lookup_fully_qualified</vh></v>
<v t="ekr.20220525082935.368"><vh>SemanticAnalyzerCoreInterface.lookup_fully_qualified_or_none</vh></v>
<v t="ekr.20220525082935.369"><vh>SemanticAnalyzerCoreInterface.fail</vh></v>
<v t="ekr.20220525082935.370"><vh>SemanticAnalyzerCoreInterface.note</vh></v>
<v t="ekr.20220525082935.371"><vh>SemanticAnalyzerCoreInterface.record_incomplete_ref</vh></v>
<v t="ekr.20220525082935.372"><vh>SemanticAnalyzerCoreInterface.defer</vh></v>
<v t="ekr.20220525082935.373"><vh>SemanticAnalyzerCoreInterface.is_incomplete_namespace</vh></v>
<v t="ekr.20220525082935.374"><vh>SemanticAnalyzerCoreInterface.final_iteration</vh></v>
<v t="ekr.20220525082935.375"><vh>SemanticAnalyzerCoreInterface.is_future_flag_set</vh></v>
<v t="ekr.20220525082935.376"><vh>SemanticAnalyzerCoreInterface.is_stub_file</vh></v>
</v>
<v t="ekr.20220525082935.377"><vh>class SemanticAnalyzerInterface</vh>
<v t="ekr.20220525082935.378"><vh>SemanticAnalyzerInterface.lookup</vh></v>
<v t="ekr.20220525082935.379"><vh>SemanticAnalyzerInterface.named_type</vh></v>
<v t="ekr.20220525082935.380"><vh>SemanticAnalyzerInterface.named_type_or_none</vh></v>
<v t="ekr.20220525082935.381"><vh>SemanticAnalyzerInterface.accept</vh></v>
<v t="ekr.20220525082935.382"><vh>SemanticAnalyzerInterface.anal_type</vh></v>
<v t="ekr.20220525082935.383"><vh>SemanticAnalyzerInterface.basic_new_typeinfo</vh></v>
<v t="ekr.20220525082935.384"><vh>SemanticAnalyzerInterface.schedule_patch</vh></v>
<v t="ekr.20220525082935.385"><vh>SemanticAnalyzerInterface.add_symbol_table_node</vh></v>
<v t="ekr.20220525082935.386"><vh>SemanticAnalyzerInterface.current_symbol_table</vh></v>
<v t="ekr.20220525082935.387"><vh>SemanticAnalyzerInterface.add_symbol</vh></v>
<v t="ekr.20220525082935.388"><vh>SemanticAnalyzerInterface.add_symbol_skip_local</vh></v>
<v t="ekr.20220525082935.389"><vh>SemanticAnalyzerInterface.parse_bool</vh></v>
<v t="ekr.20220525082935.390"><vh>SemanticAnalyzerInterface.qualified_name</vh></v>
<v t="ekr.20220525082935.391"><vh>SemanticAnalyzerInterface.is_typeshed_stub_file</vh></v>
<v t="ekr.20220525082935.392"><vh>SemanticAnalyzerInterface.is_func_scope</vh></v>
</v>
<v t="ekr.20220525082935.393"><vh>set_callable_name</vh></v>
<v t="ekr.20220525082935.394"><vh>calculate_tuple_fallback</vh></v>
<v t="ekr.20220525082935.395"><vh>class _NamedTypeCallback</vh></v>
<v t="ekr.20220525082935.396"><vh>paramspec_args</vh></v>
<v t="ekr.20220525082935.397"><vh>paramspec_kwargs</vh></v>
</v>
<v t="ekr.20220525082935.398"><vh>@clean semanal_typeargs.py</vh>
<v t="ekr.20220525082935.399"><vh>class TypeArgumentAnalyzer</vh>
<v t="ekr.20220525082935.400"><vh>TypeArgumentAnalyzer.__init__</vh></v>
<v t="ekr.20220525082935.401"><vh>TypeArgumentAnalyzer.visit_mypy_file</vh></v>
<v t="ekr.20220525082935.402"><vh>TypeArgumentAnalyzer.visit_func</vh></v>
<v t="ekr.20220525082935.403"><vh>TypeArgumentAnalyzer.visit_class_def</vh></v>
<v t="ekr.20220525082935.404"><vh>TypeArgumentAnalyzer.visit_block</vh></v>
<v t="ekr.20220525082935.405"><vh>TypeArgumentAnalyzer.visit_type_alias_type</vh></v>
<v t="ekr.20220525082935.406"><vh>TypeArgumentAnalyzer.visit_instance</vh></v>
<v t="ekr.20220525082935.407"><vh>TypeArgumentAnalyzer.visit_unpack_type</vh></v>
<v t="ekr.20220525082935.408"><vh>TypeArgumentAnalyzer.check_type_var_values</vh></v>
<v t="ekr.20220525082935.409"><vh>TypeArgumentAnalyzer.fail</vh></v>
</v>
</v>
<v t="ekr.20220525082935.410"><vh>@clean semanal_typeddict.py</vh>
<v t="ekr.20220525082935.411"><vh>class TypedDictAnalyzer</vh>
<v t="ekr.20220525082935.412"><vh>TypedDictAnalyzer.__init__</vh></v>
<v t="ekr.20220525082935.413"><vh>TypedDictAnalyzer.analyze_typeddict_classdef</vh></v>
<v t="ekr.20220525082935.414"><vh>TypedDictAnalyzer.analyze_typeddict_classdef_fields</vh></v>
<v t="ekr.20220525082935.415"><vh>TypedDictAnalyzer.check_typeddict</vh></v>
<v t="ekr.20220525082935.416"><vh>TypedDictAnalyzer.parse_typeddict_args</vh></v>
<v t="ekr.20220525082935.417"><vh>TypedDictAnalyzer.parse_typeddict_fields_with_types</vh></v>
<v t="ekr.20220525082935.418"><vh>TypedDictAnalyzer.fail_typeddict_arg</vh></v>
<v t="ekr.20220525082935.419"><vh>TypedDictAnalyzer.build_typeddict_typeinfo</vh></v>
<v t="ekr.20220525082935.420"><vh>TypedDictAnalyzer.Helpers</vh></v>
<v t="ekr.20220525082935.421"><vh>TypedDictAnalyzer.is_typeddict</vh></v>
<v t="ekr.20220525082935.422"><vh>TypedDictAnalyzer.fail</vh></v>
<v t="ekr.20220525082935.423"><vh>TypedDictAnalyzer.note</vh></v>
</v>
</v>
<v t="ekr.20220525082935.424"><vh>@clean sharedparse.py</vh>
<v t="ekr.20220525082935.425"><vh>special_function_elide_names</vh></v>
<v t="ekr.20220525082935.426"><vh>argument_elide_name</vh></v>
</v>
<v t="ekr.20220525082935.427"><vh>@clean solve.py</vh>
<v t="ekr.20220525082935.428"><vh>solve_constraints</vh></v>
</v>
<v t="ekr.20220525082935.429"><vh>@clean split_namespace.py</vh>
<v t="ekr.20220525082935.430"><vh>class SplitNamespace</vh>
<v t="ekr.20220525082935.431"><vh>SplitNamespace.__init__</vh></v>
<v t="ekr.20220525082935.432"><vh>SplitNamespace._get</vh></v>
<v t="ekr.20220525082935.433"><vh>SplitNamespace.__setattr__</vh></v>
<v t="ekr.20220525082935.434"><vh>SplitNamespace.__getattr__</vh></v>
</v>
</v>
<v t="ekr.20220525082935.435"><vh>@clean state.py</vh>
<v t="ekr.20220525082935.436"><vh>class StrictOptionalState</vh>
<v t="ekr.20220525082935.437"><vh>StrictOptionalState.__init__</vh></v>
<v t="ekr.20220525082935.438"><vh>StrictOptionalState.strict_optional_set</vh></v>
</v>
</v>
<v t="ekr.20220525082935.439"><vh>@clean stats.py</vh>
<v t="ekr.20220525082935.440"><vh>class StatisticsVisitor</vh>
<v t="ekr.20220525082935.441"><vh>StatisticsVisitor.__init__</vh></v>
<v t="ekr.20220525082935.442"><vh>StatisticsVisitor.visit_mypy_file</vh></v>
<v t="ekr.20220525082935.443"><vh>StatisticsVisitor.visit_import_from</vh></v>
<v t="ekr.20220525082935.444"><vh>StatisticsVisitor.visit_import_all</vh></v>
<v t="ekr.20220525082935.445"><vh>StatisticsVisitor.process_import</vh></v>
<v t="ekr.20220525082935.446"><vh>StatisticsVisitor.visit_import</vh></v>
<v t="ekr.20220525082935.447"><vh>StatisticsVisitor.visit_func_def</vh></v>
<v t="ekr.20220525082935.448"><vh>StatisticsVisitor.enter_scope</vh></v>
<v t="ekr.20220525082935.449"><vh>StatisticsVisitor.is_checked_scope</vh></v>
<v t="ekr.20220525082935.450"><vh>StatisticsVisitor.visit_class_def</vh></v>
<v t="ekr.20220525082935.451"><vh>StatisticsVisitor.visit_type_application</vh></v>
<v t="ekr.20220525082935.452"><vh>StatisticsVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20220525082935.453"><vh>StatisticsVisitor.visit_expression_stmt</vh></v>
<v t="ekr.20220525082935.454"><vh>StatisticsVisitor.visit_pass_stmt</vh></v>
<v t="ekr.20220525082935.455"><vh>StatisticsVisitor.visit_break_stmt</vh></v>
<v t="ekr.20220525082935.456"><vh>StatisticsVisitor.visit_continue_stmt</vh></v>
<v t="ekr.20220525082935.457"><vh>StatisticsVisitor.visit_name_expr</vh></v>
<v t="ekr.20220525082935.458"><vh>StatisticsVisitor.visit_yield_from_expr</vh></v>
<v t="ekr.20220525082935.459"><vh>StatisticsVisitor.visit_call_expr</vh></v>
<v t="ekr.20220525082935.460"><vh>StatisticsVisitor.record_call_target_precision</vh></v>
<v t="ekr.20220525082935.461"><vh>StatisticsVisitor.record_callable_target_precision</vh></v>
<v t="ekr.20220525082935.462"><vh>StatisticsVisitor.visit_member_expr</vh></v>
<v t="ekr.20220525082935.463"><vh>StatisticsVisitor.visit_op_expr</vh></v>
<v t="ekr.20220525082935.464"><vh>StatisticsVisitor.visit_comparison_expr</vh></v>
<v t="ekr.20220525082935.465"><vh>StatisticsVisitor.visit_index_expr</vh></v>
<v t="ekr.20220525082935.466"><vh>StatisticsVisitor.visit_assignment_expr</vh></v>
<v t="ekr.20220525082935.467"><vh>StatisticsVisitor.visit_unary_expr</vh></v>
<v t="ekr.20220525082935.468"><vh>StatisticsVisitor.visit_str_expr</vh></v>
<v t="ekr.20220525082935.469"><vh>StatisticsVisitor.visit_unicode_expr</vh></v>
<v t="ekr.20220525082935.470"><vh>StatisticsVisitor.visit_bytes_expr</vh></v>
<v t="ekr.20220525082935.471"><vh>StatisticsVisitor.visit_int_expr</vh></v>
<v t="ekr.20220525082935.472"><vh>StatisticsVisitor.visit_float_expr</vh></v>
<v t="ekr.20220525082935.473"><vh>StatisticsVisitor.visit_complex_expr</vh></v>
<v t="ekr.20220525082935.474"><vh>StatisticsVisitor.visit_ellipsis</vh></v>
<v t="ekr.20220525082935.475"><vh>StatisticsVisitor.Helpers</vh></v>
<v t="ekr.20220525082935.476"><vh>StatisticsVisitor.process_node</vh></v>
<v t="ekr.20220525082935.477"><vh>StatisticsVisitor.record_precise_if_checked_scope</vh></v>
<v t="ekr.20220525082935.478"><vh>StatisticsVisitor.type</vh></v>
<v t="ekr.20220525082935.479"><vh>StatisticsVisitor.log</vh></v>
<v t="ekr.20220525082935.480"><vh>StatisticsVisitor.record_line</vh></v>
</v>
<v t="ekr.20220525082935.481"><vh>dump_type_stats</vh></v>
<v t="ekr.20220525082935.482"><vh>is_special_module</vh></v>
<v t="ekr.20220525082935.483"><vh>is_imprecise</vh></v>
<v t="ekr.20220525082935.484"><vh>class HasAnyQuery</vh></v>
<v t="ekr.20220525082935.485"><vh>is_imprecise2</vh></v>
<v t="ekr.20220525082935.486"><vh>class HasAnyQuery2</vh></v>
<v t="ekr.20220525082935.487"><vh>is_generic</vh></v>
<v t="ekr.20220525082935.488"><vh>is_complex</vh></v>
<v t="ekr.20220525082935.489"><vh>ensure_dir_exists</vh></v>
<v t="ekr.20220525082935.490"><vh>is_special_form_any</vh></v>
<v t="ekr.20220525082935.491"><vh>get_original_any</vh></v>
</v>
<v t="ekr.20220525082935.492"><vh>@clean strconv.py</vh>
<v t="ekr.20220525082935.493"><vh>class StrConv</vh>
<v t="ekr.20220525082935.494"><vh>StrConv.__init__</vh></v>
<v t="ekr.20220525082935.495"><vh>StrConv.get_id</vh></v>
<v t="ekr.20220525082935.496"><vh>StrConv.format_id</vh></v>
<v t="ekr.20220525082935.497"><vh>StrConv.dump</vh></v>
<v t="ekr.20220525082935.498"><vh>StrConv.func_helper</vh></v>
<v t="ekr.20220525082935.499"><vh>StrConv.Top-level structures</vh></v>
<v t="ekr.20220525082935.500"><vh>StrConv.visit_mypy_file</vh></v>
<v t="ekr.20220525082935.501"><vh>StrConv.visit_import</vh></v>
<v t="ekr.20220525082935.502"><vh>StrConv.visit_import_from</vh></v>
<v t="ekr.20220525082935.503"><vh>StrConv.visit_import_all</vh></v>
<v t="ekr.20220525082935.504"><vh>StrConv.Definitions</vh></v>
<v t="ekr.20220525082935.505"><vh>StrConv.visit_func_def</vh></v>
<v t="ekr.20220525082935.506"><vh>StrConv.visit_overloaded_func_def</vh></v>
<v t="ekr.20220525082935.507"><vh>StrConv.visit_class_def</vh></v>
<v t="ekr.20220525082935.508"><vh>StrConv.visit_var</vh></v>
<v t="ekr.20220525082935.509"><vh>StrConv.visit_global_decl</vh></v>
<v t="ekr.20220525082935.510"><vh>StrConv.visit_nonlocal_decl</vh></v>
<v t="ekr.20220525082935.511"><vh>StrConv.visit_decorator</vh></v>
<v t="ekr.20220525082935.512"><vh>StrConv.Statements</vh></v>
<v t="ekr.20220525082935.513"><vh>StrConv.visit_block</vh></v>
<v t="ekr.20220525082935.514"><vh>StrConv.visit_expression_stmt</vh></v>
<v t="ekr.20220525082935.515"><vh>StrConv.visit_assignment_stmt</vh></v>
<v t="ekr.20220525082935.516"><vh>StrConv.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20220525082935.517"><vh>StrConv.visit_while_stmt</vh></v>
<v t="ekr.20220525082935.518"><vh>StrConv.visit_for_stmt</vh></v>
<v t="ekr.20220525082935.519"><vh>StrConv.visit_return_stmt</vh></v>
<v t="ekr.20220525082935.520"><vh>StrConv.visit_if_stmt</vh></v>
<v t="ekr.20220525082935.521"><vh>StrConv.visit_break_stmt</vh></v>
<v t="ekr.20220525082935.522"><vh>StrConv.visit_continue_stmt</vh></v>
<v t="ekr.20220525082935.523"><vh>StrConv.visit_pass_stmt</vh></v>
<v t="ekr.20220525082935.524"><vh>StrConv.visit_raise_stmt</vh></v>
<v t="ekr.20220525082935.525"><vh>StrConv.visit_assert_stmt</vh></v>
<v t="ekr.20220525082935.526"><vh>StrConv.visit_await_expr</vh></v>
<v t="ekr.20220525082935.527"><vh>StrConv.visit_del_stmt</vh></v>
<v t="ekr.20220525082935.528"><vh>StrConv.visit_try_stmt</vh></v>
<v t="ekr.20220525082935.529"><vh>StrConv.visit_with_stmt</vh></v>
<v t="ekr.20220525082935.530"><vh>StrConv.visit_print_stmt</vh></v>
<v t="ekr.20220525082935.531"><vh>StrConv.visit_exec_stmt</vh></v>
<v t="ekr.20220525082935.532"><vh>StrConv.visit_match_stmt</vh></v>
<v t="ekr.20220525082935.533"><vh>StrConv.Expressions</vh></v>
<v t="ekr.20220525082935.534"><vh>StrConv.visit_int_expr</vh></v>
<v t="ekr.20220525082935.535"><vh>StrConv.visit_str_expr</vh></v>
<v t="ekr.20220525082935.536"><vh>StrConv.visit_bytes_expr</vh></v>
<v t="ekr.20220525082935.537"><vh>StrConv.visit_unicode_expr</vh></v>
<v t="ekr.20220525082935.538"><vh>StrConv.str_repr</vh></v>
<v t="ekr.20220525082935.539"><vh>StrConv.visit_float_expr</vh></v>
<v t="ekr.20220525082935.540"><vh>StrConv.visit_complex_expr</vh></v>
<v t="ekr.20220525082935.541"><vh>StrConv.visit_ellipsis</vh></v>
<v t="ekr.20220525082935.542"><vh>StrConv.visit_star_expr</vh></v>
<v t="ekr.20220525082935.543"><vh>StrConv.visit_name_expr</vh></v>
<v t="ekr.20220525082935.544"><vh>StrConv.pretty_name</vh></v>
<v t="ekr.20220525082935.545"><vh>StrConv.visit_member_expr</vh></v>
<v t="ekr.20220525082935.546"><vh>StrConv.visit_yield_expr</vh></v>
<v t="ekr.20220525082935.547"><vh>StrConv.visit_yield_from_expr</vh></v>
<v t="ekr.20220525082935.548"><vh>StrConv.visit_call_expr</vh></v>
<v t="ekr.20220525082935.549"><vh>StrConv.visit_op_expr</vh></v>
<v t="ekr.20220525082935.550"><vh>StrConv.visit_comparison_expr</vh></v>
<v t="ekr.20220525082935.551"><vh>StrConv.visit_cast_expr</vh></v>
<v t="ekr.20220525082935.552"><vh>StrConv.visit_assert_type_expr</vh></v>
<v t="ekr.20220525082935.553"><vh>StrConv.visit_reveal_expr</vh></v>
<v t="ekr.20220525082935.554"><vh>StrConv.visit_assignment_expr</vh></v>
<v t="ekr.20220525082935.555"><vh>StrConv.visit_unary_expr</vh></v>
<v t="ekr.20220525082935.556"><vh>StrConv.visit_list_expr</vh></v>
<v t="ekr.20220525082935.557"><vh>StrConv.visit_dict_expr</vh></v>
<v t="ekr.20220525082935.558"><vh>StrConv.visit_set_expr</vh></v>
<v t="ekr.20220525082935.559"><vh>StrConv.visit_tuple_expr</vh></v>
<v t="ekr.20220525082935.560"><vh>StrConv.visit_index_expr</vh></v>
<v t="ekr.20220525082935.561"><vh>StrConv.visit_super_expr</vh></v>
<v t="ekr.20220525082935.562"><vh>StrConv.visit_type_application</vh></v>
<v t="ekr.20220525082935.563"><vh>StrConv.visit_type_var_expr</vh></v>
<v t="ekr.20220525082935.564"><vh>StrConv.visit_paramspec_expr</vh></v>
<v t="ekr.20220525082935.565"><vh>StrConv.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20220525082935.566"><vh>StrConv.visit_type_alias_expr</vh></v>
<v t="ekr.20220525082935.567"><vh>StrConv.visit_namedtuple_expr</vh></v>
<v t="ekr.20220525082935.568"><vh>StrConv.visit_enum_call_expr</vh></v>
<v t="ekr.20220525082935.569"><vh>StrConv.visit_typeddict_expr</vh></v>
<v t="ekr.20220525082935.570"><vh>StrConv.visit__promote_expr</vh></v>
<v t="ekr.20220525082935.571"><vh>StrConv.visit_newtype_expr</vh></v>
<v t="ekr.20220525082935.572"><vh>StrConv.visit_lambda_expr</vh></v>
<v t="ekr.20220525082935.573"><vh>StrConv.visit_generator_expr</vh></v>
<v t="ekr.20220525082935.574"><vh>StrConv.visit_list_comprehension</vh></v>
<v t="ekr.20220525082935.575"><vh>StrConv.visit_set_comprehension</vh></v>
<v t="ekr.20220525082935.576"><vh>StrConv.visit_dictionary_comprehension</vh></v>
<v t="ekr.20220525082935.577"><vh>StrConv.visit_conditional_expr</vh></v>
<v t="ekr.20220525082935.578"><vh>StrConv.visit_slice_expr</vh></v>
<v t="ekr.20220525082935.579"><vh>StrConv.visit_backquote_expr</vh></v>
<v t="ekr.20220525082935.580"><vh>StrConv.visit_temp_node</vh></v>
<v t="ekr.20220525082935.581"><vh>StrConv.visit_as_pattern</vh></v>
<v t="ekr.20220525082935.582"><vh>StrConv.visit_or_pattern</vh></v>
<v t="ekr.20220525082935.583"><vh>StrConv.visit_value_pattern</vh></v>
<v t="ekr.20220525082935.584"><vh>StrConv.visit_singleton_pattern</vh></v>
<v t="ekr.20220525082935.585"><vh>StrConv.visit_sequence_pattern</vh></v>
<v t="ekr.20220525082935.586"><vh>StrConv.visit_starred_pattern</vh></v>
<v t="ekr.20220525082935.587"><vh>StrConv.visit_mapping_pattern</vh></v>
<v t="ekr.20220525082935.588"><vh>StrConv.visit_class_pattern</vh></v>
</v>
<v t="ekr.20220525082935.589"><vh>dump_tagged</vh></v>
<v t="ekr.20220525082935.590"><vh>indent</vh></v>
</v>
<v t="ekr.20220525082935.591"><vh>@clean stubdoc.py</vh>
<v t="ekr.20220525082935.592"><vh>is_valid_type</vh></v>
<v t="ekr.20220525082935.593"><vh>class ArgSig</vh>
<v t="ekr.20220525082935.594"><vh>ArgSig.__init__</vh></v>
<v t="ekr.20220525082935.595"><vh>ArgSig.__repr__</vh></v>
<v t="ekr.20220525082935.596"><vh>ArgSig.__eq__</vh></v>
</v>
<v t="ekr.20220525082935.597"><vh>class FunctionSig</vh></v>
<v t="ekr.20220525082935.598"><vh>States of the docstring parser.</vh></v>
<v t="ekr.20220525082935.599"><vh>class DocStringParser</vh>
<v t="ekr.20220525082935.600"><vh>DocStringParser.__init__</vh></v>
<v t="ekr.20220525082935.601"><vh>DocStringParser.add_token</vh></v>
<v t="ekr.20220525082935.602"><vh>DocStringParser.reset</vh></v>
<v t="ekr.20220525082935.603"><vh>DocStringParser.get_signatures</vh></v>
</v>
<v t="ekr.20220525082935.604"><vh>infer_sig_from_docstring</vh>
<v t="ekr.20220525082935.605"><vh>is_unique_args</vh></v>
</v>
<v t="ekr.20220525082935.606"><vh>infer_arg_sig_from_anon_docstring</vh></v>
<v t="ekr.20220525082935.607"><vh>infer_ret_type_sig_from_docstring</vh></v>
<v t="ekr.20220525082935.608"><vh>infer_ret_type_sig_from_anon_docstring</vh></v>
<v t="ekr.20220525082935.609"><vh>parse_signature</vh></v>
<v t="ekr.20220525082935.610"><vh>build_signature</vh></v>
<v t="ekr.20220525082935.611"><vh>parse_all_signatures</vh></v>
<v t="ekr.20220525082935.612"><vh>find_unique_signatures</vh></v>
<v t="ekr.20220525082935.613"><vh>infer_prop_type_from_docstring</vh></v>
</v>
<v t="ekr.20220525082935.614"><vh>@clean stubgen.py</vh>
<v t="ekr.20220525082935.615"><vh>class Options</vh>
<v t="ekr.20220525082935.616"><vh>Options.__init__</vh></v>
</v>
<v t="ekr.20220525082935.617"><vh>class StubSource</vh>
<v t="ekr.20220525082935.618"><vh>StubSource.__init__</vh></v>
<v t="ekr.20220525082935.619"><vh>StubSource.module</vh></v>
<v t="ekr.20220525082935.620"><vh>StubSource.path</vh></v>
</v>
<v t="ekr.20220525082935.621"><vh>What was generated previously in the stub file. We keep track of these to generate</vh></v>
<v t="ekr.20220525082935.622"><vh>class AnnotationPrinter</vh>
<v t="ekr.20220525082935.623"><vh>AnnotationPrinter.__init__</vh></v>
<v t="ekr.20220525082935.624"><vh>AnnotationPrinter.visit_any</vh></v>
<v t="ekr.20220525082935.625"><vh>AnnotationPrinter.visit_unbound_type</vh></v>
<v t="ekr.20220525082935.626"><vh>AnnotationPrinter.visit_none_type</vh></v>
<v t="ekr.20220525082935.627"><vh>AnnotationPrinter.visit_type_list</vh></v>
<v t="ekr.20220525082935.628"><vh>AnnotationPrinter.args_str</vh></v>
</v>
<v t="ekr.20220525082935.629"><vh>class AliasPrinter</vh>
<v t="ekr.20220525082935.630"><vh>AliasPrinter.__init__</vh></v>
<v t="ekr.20220525082935.631"><vh>AliasPrinter.visit_call_expr</vh></v>
<v t="ekr.20220525082935.632"><vh>AliasPrinter.visit_name_expr</vh></v>
<v t="ekr.20220525082935.633"><vh>AliasPrinter.visit_member_expr</vh></v>
<v t="ekr.20220525082935.634"><vh>AliasPrinter.visit_str_expr</vh></v>
<v t="ekr.20220525082935.635"><vh>AliasPrinter.visit_index_expr</vh></v>
<v t="ekr.20220525082935.636"><vh>AliasPrinter.visit_tuple_expr</vh></v>
<v t="ekr.20220525082935.637"><vh>AliasPrinter.visit_list_expr</vh></v>
<v t="ekr.20220525082935.638"><vh>AliasPrinter.visit_ellipsis</vh></v>
</v>
<v t="ekr.20220525082935.639"><vh>class ImportTracker</vh>
<v t="ekr.20220525082935.640"><vh>ImportTracker.__init__</vh></v>
<v t="ekr.20220525082935.641"><vh>ImportTracker.add_import_from</vh></v>
<v t="ekr.20220525082935.642"><vh>ImportTracker.add_import</vh></v>
<v t="ekr.20220525082935.643"><vh>ImportTracker.require_name</vh></v>
<v t="ekr.20220525082935.644"><vh>ImportTracker.reexport</vh></v>
<v t="ekr.20220525082935.645"><vh>ImportTracker.import_lines</vh></v>
</v>
<v t="ekr.20220525082935.646"><vh>find_defined_names</vh></v>
<v t="ekr.20220525082935.647"><vh>class DefinitionFinder</vh>
<v t="ekr.20220525082935.648"><vh>DefinitionFinder.__init__</vh></v>
<v t="ekr.20220525082935.649"><vh>DefinitionFinder.visit_class_def</vh></v>
<v t="ekr.20220525082935.650"><vh>DefinitionFinder.visit_func_def</vh></v>
</v>
<v t="ekr.20220525082935.651"><vh>find_referenced_names</vh></v>
<v t="ekr.20220525082935.652"><vh>class ReferenceFinder</vh>
<v t="ekr.20220525082935.653"><vh>ReferenceFinder.__init__</vh></v>
<v t="ekr.20220525082935.654"><vh>ReferenceFinder.visit_block</vh></v>
<v t="ekr.20220525082935.655"><vh>ReferenceFinder.visit_name_expr</vh></v>
<v t="ekr.20220525082935.656"><vh>ReferenceFinder.visit_instance</vh></v>
<v t="ekr.20220525082935.657"><vh>ReferenceFinder.visit_unbound_type</vh></v>
<v t="ekr.20220525082935.658"><vh>ReferenceFinder.visit_tuple_type</vh></v>
<v t="ekr.20220525082935.659"><vh>ReferenceFinder.visit_callable_type</vh></v>
<v t="ekr.20220525082935.660"><vh>ReferenceFinder.add_ref</vh></v>
</v>
<v t="ekr.20220525082935.661"><vh>class StubGenerator</vh>
<v t="ekr.20220525082935.662"><vh>StubGenerator.__init__</vh></v>
<v t="ekr.20220525082935.663"><vh>StubGenerator.visit_mypy_file</vh></v>
<v t="ekr.20220525082935.664"><vh>StubGenerator.visit_overloaded_func_def</vh></v>
<v t="ekr.20220525082935.665"><vh>StubGenerator.visit_func_def</vh></v>
<v t="ekr.20220525082935.666"><vh>StubGenerator.is_none_expr</vh></v>
<v t="ekr.20220525082935.667"><vh>StubGenerator.visit_decorator</vh></v>
<v t="ekr.20220525082935.668"><vh>StubGenerator.process_decorator</vh></v>
<v t="ekr.20220525082935.669"><vh>StubGenerator.process_name_expr_decorator</vh></v>
<v t="ekr.20220525082935.670"><vh>StubGenerator.refers_to_fullname</vh></v>
<v t="ekr.20220525082935.671"><vh>StubGenerator.process_member_expr_decorator</vh></v>
<v t="ekr.20220525082935.672"><vh>StubGenerator.visit_class_def</vh></v>
<v t="ekr.20220525082935.673"><vh>StubGenerator.get_base_types</vh></v>
<v t="ekr.20220525082935.674"><vh>StubGenerator.visit_block</vh></v>
<v t="ekr.20220525082935.675"><vh>StubGenerator.visit_assignment_stmt</vh></v>
<v t="ekr.20220525082935.676"><vh>StubGenerator.is_namedtuple</vh></v>
<v t="ekr.20220525082935.677"><vh>StubGenerator.process_namedtuple</vh></v>
<v t="ekr.20220525082935.678"><vh>StubGenerator.is_alias_expression</vh></v>
<v t="ekr.20220525082935.679"><vh>StubGenerator.process_typealias</vh></v>
<v t="ekr.20220525082935.680"><vh>StubGenerator.visit_if_stmt</vh></v>
<v t="ekr.20220525082935.681"><vh>StubGenerator.visit_import_all</vh></v>
<v t="ekr.20220525082935.682"><vh>StubGenerator.visit_import_from</vh></v>
<v t="ekr.20220525082935.683"><vh>StubGenerator.visit_import</vh></v>
<v t="ekr.20220525082935.684"><vh>StubGenerator.get_init</vh></v>
<v t="ekr.20220525082935.685"><vh>StubGenerator.add</vh></v>
<v t="ekr.20220525082935.686"><vh>StubGenerator.add_decorator</vh></v>
<v t="ekr.20220525082935.687"><vh>StubGenerator.clear_decorators</vh></v>
<v t="ekr.20220525082935.688"><vh>StubGenerator.typing_name</vh></v>
<v t="ekr.20220525082935.689"><vh>StubGenerator.add_typing_import</vh></v>
<v t="ekr.20220525082935.690"><vh>StubGenerator.add_abc_import</vh></v>
<v t="ekr.20220525082935.691"><vh>StubGenerator.add_import_line</vh></v>
<v t="ekr.20220525082935.692"><vh>StubGenerator.add_coroutine_decorator</vh></v>
<v t="ekr.20220525082935.693"><vh>StubGenerator.output</vh></v>
<v t="ekr.20220525082935.694"><vh>StubGenerator.is_not_in_all</vh></v>
<v t="ekr.20220525082935.695"><vh>StubGenerator.is_private_name</vh></v>
<v t="ekr.20220525082935.696"><vh>StubGenerator.is_private_member</vh></v>
<v t="ekr.20220525082935.697"><vh>StubGenerator.get_str_type_of_node</vh></v>
<v t="ekr.20220525082935.698"><vh>StubGenerator.print_annotation</vh></v>
<v t="ekr.20220525082935.699"><vh>StubGenerator.is_top_level</vh></v>
<v t="ekr.20220525082935.700"><vh>StubGenerator.record_name</vh></v>
<v t="ekr.20220525082935.701"><vh>StubGenerator.is_recorded_name</vh></v>
</v>
<v t="ekr.20220525082935.702"><vh>find_method_names</vh></v>
<v t="ekr.20220525082935.703"><vh>class SelfTraverser</vh>
<v t="ekr.20220525082935.704"><vh>SelfTraverser.__init__</vh></v>
<v t="ekr.20220525082935.705"><vh>SelfTraverser.visit_assignment_stmt</vh></v>
</v>
<v t="ekr.20220525082935.706"><vh>find_self_initializers</vh></v>
<v t="ekr.20220525082935.707"><vh>get_qualified_name</vh></v>
<v t="ekr.20220525082935.708"><vh>remove_blacklisted_modules</vh></v>
<v t="ekr.20220525082935.709"><vh>is_blacklisted_path</vh></v>
<v t="ekr.20220525082935.710"><vh>normalize_path_separators</vh></v>
<v t="ekr.20220525082935.711"><vh>collect_build_targets</vh></v>
<v t="ekr.20220525082935.712"><vh>find_module_paths_using_imports</vh></v>
<v t="ekr.20220525082935.713"><vh>is_non_library_module</vh></v>
<v t="ekr.20220525082935.714"><vh>translate_module_name</vh></v>
<v t="ekr.20220525082935.715"><vh>find_module_paths_using_search</vh></v>
<v t="ekr.20220525082935.716"><vh>mypy_options</vh></v>
<v t="ekr.20220525082935.717"><vh>parse_source_file</vh></v>
<v t="ekr.20220525082935.718"><vh>generate_asts_for_modules</vh></v>
<v t="ekr.20220525082935.719"><vh>generate_stub_from_ast</vh></v>
<v t="ekr.20220525082935.720"><vh>collect_docs_signatures</vh></v>
<v t="ekr.20220525082935.721"><vh>generate_stubs</vh></v>
<v t="ekr.20220525082935.722"><vh>HEADER = """%(prog)s [-h] [--py2] [more options, see -h]</vh></v>
<v t="ekr.20220525082935.723"><vh>parse_options</vh></v>
<v t="ekr.20220525082935.724"><vh>main</vh></v>
</v>
<v t="ekr.20220525082935.725"><vh>@clean stubgenc.py</vh>
<v t="ekr.20220525082935.726"><vh>generate_stub_for_c_module</vh></v>
<v t="ekr.20220525082935.727"><vh>add_typing_import</vh></v>
<v t="ekr.20220525082935.728"><vh>is_c_function</vh></v>
<v t="ekr.20220525082935.729"><vh>is_c_method</vh></v>
<v t="ekr.20220525082935.730"><vh>is_c_classmethod</vh></v>
<v t="ekr.20220525082935.731"><vh>is_c_property</vh></v>
<v t="ekr.20220525082935.732"><vh>is_c_property_readonly</vh></v>
<v t="ekr.20220525082935.733"><vh>is_c_type</vh></v>
<v t="ekr.20220525082935.734"><vh>is_pybind11_overloaded_function_docstring</vh></v>
<v t="ekr.20220525082935.735"><vh>generate_c_function_stub</vh></v>
<v t="ekr.20220525082935.736"><vh>strip_or_import</vh></v>
<v t="ekr.20220525082935.737"><vh>is_static_property</vh></v>
<v t="ekr.20220525082935.738"><vh>generate_c_property_stub</vh>
<v t="ekr.20220525082935.739"><vh>infer_prop_type</vh></v>
</v>
<v t="ekr.20220525082935.740"><vh>generate_c_type_stub</vh></v>
<v t="ekr.20220525082935.741"><vh>get_type_fullname</vh></v>
<v t="ekr.20220525082935.742"><vh>method_name_sort_key</vh></v>
<v t="ekr.20220525082935.743"><vh>is_pybind_skipped_attribute</vh></v>
<v t="ekr.20220525082935.744"><vh>is_skipped_attribute</vh></v>
<v t="ekr.20220525082935.745"><vh>infer_method_sig</vh></v>
</v>
<v t="ekr.20220525082935.746"><vh>@clean stubinfo.py</vh>
<v t="ekr.20220525082935.747"><vh>class StubInfo</vh></v>
<v t="ekr.20220525082935.748"><vh>is_legacy_bundled_package</vh></v>
</v>
<v t="ekr.20220525082935.749"><vh>@clean stubtest.py</vh>
<v t="ekr.20220525082935.750"><vh>class Missing</vh></v>
<v t="ekr.20220525082935.751"><vh>MISSING = Missing()</vh></v>
<v t="ekr.20220525082935.752"><vh>_style</vh></v>
<v t="ekr.20220525082935.753"><vh>class Error</vh>
<v t="ekr.20220525082935.754"><vh>Error.__init__</vh></v>
<v t="ekr.20220525082935.755"><vh>Error.is_missing_stub</vh></v>
<v t="ekr.20220525082935.756"><vh>Error.is_positional_only_related</vh></v>
<v t="ekr.20220525082935.757"><vh>Error.get_description</vh></v>
</v>
<v t="ekr.20220525082935.758"><vh>====================</vh></v>
<v t="ekr.20220525082935.759"><vh>test_module</vh></v>
<v t="ekr.20220525082935.760"><vh>verify</vh></v>
<v t="ekr.20220525082935.761"><vh>verify_mypyfile</vh>
<v t="ekr.20220525082935.762"><vh>_belongs_to_runtime</vh></v>
</v>
<v t="ekr.20220525082935.763"><vh>if sys.version_info &gt;= (3, 7):</vh></v>
<v t="ekr.20220525082935.764"><vh>verify_typeinfo</vh></v>
<v t="ekr.20220525082935.765"><vh>_verify_static_class_methods</vh></v>
<v t="ekr.20220525082935.766"><vh>_verify_arg_name</vh>
<v t="ekr.20220525082935.767"><vh>strip_prefix</vh></v>
<v t="ekr.20220525082935.768"><vh>if strip_prefix(stub_arg.variable.name, "__") == runtime_arg.name:</vh></v>
<v t="ekr.20220525082935.769"><vh>names_approx_match</vh></v>
</v>
<v t="ekr.20220525082935.770"><vh>_verify_arg_default_value</vh></v>
<v t="ekr.20220525082935.771"><vh>maybe_strip_cls</vh></v>
<v t="ekr.20220525082935.772"><vh>class Signature</vh>
<v t="ekr.20220525082935.773"><vh>Signature.__init__</vh></v>
<v t="ekr.20220525082935.774"><vh>Signature.__str__</vh></v>
<v t="ekr.20220525082935.775"><vh>Signature.from_funcitem</vh></v>
<v t="ekr.20220525082935.776"><vh>Signature.from_inspect_signature</vh></v>
<v t="ekr.20220525082935.777"><vh>Signature.from_overloadedfuncdef</vh></v>
</v>
<v t="ekr.20220525082935.778"><vh>_verify_signature</vh></v>
<v t="ekr.20220525082935.779"><vh>verify_funcitem</vh></v>
<v t="ekr.20220525082935.780"><vh>verify_none</vh></v>
<v t="ekr.20220525082935.781"><vh>verify_var</vh></v>
<v t="ekr.20220525082935.782"><vh>verify_overloadedfuncdef</vh></v>
<v t="ekr.20220525082935.783"><vh>verify_typevarexpr</vh></v>
<v t="ekr.20220525082935.784"><vh>_verify_readonly_property</vh></v>
<v t="ekr.20220525082935.785"><vh>_resolve_funcitem_from_decorator</vh>
<v t="ekr.20220525082935.786"><vh>apply_decorator_to_funcitem</vh></v>
</v>
<v t="ekr.20220525082935.787"><vh>verify_decorator</vh></v>
<v t="ekr.20220525082935.788"><vh>verify_typealias</vh></v>
<v t="ekr.20220525082935.789"><vh>====================</vh></v>
<v t="ekr.20220525082935.790"><vh>is_probably_private</vh></v>
<v t="ekr.20220525082935.791"><vh>is_probably_a_function</vh></v>
<v t="ekr.20220525082935.792"><vh>is_read_only_property</vh></v>
<v t="ekr.20220525082935.793"><vh>safe_inspect_signature</vh></v>
<v t="ekr.20220525082935.794"><vh>is_subtype_helper</vh></v>
<v t="ekr.20220525082935.795"><vh>get_mypy_type_of_runtime_value</vh>
<v t="ekr.20220525082935.796"><vh>anytype</vh></v>
</v>
<v t="ekr.20220525082935.797"><vh>====================</vh></v>
<v t="ekr.20220525082935.798"><vh>build_stubs</vh></v>
<v t="ekr.20220525082935.799"><vh>get_stub</vh></v>
<v t="ekr.20220525082935.800"><vh>get_typeshed_stdlib_modules</vh>
<v t="ekr.20220525082935.801"><vh>exists_in_version</vh></v>
</v>
<v t="ekr.20220525082935.802"><vh>get_allowlist_entries</vh>
<v t="ekr.20220525082935.803"><vh>strip_comments</vh></v>
</v>
<v t="ekr.20220525082935.804"><vh>test_stubs</vh></v>
<v t="ekr.20220525082935.805"><vh>parse_options</vh></v>
<v t="ekr.20220525082935.806"><vh>main</vh></v>
</v>
<v t="ekr.20220525082935.807"><vh>@clean stubutil.py</vh>
<v t="ekr.20220525082935.808"><vh>class CantImport</vh></v>
<v t="ekr.20220525082935.809"><vh>default_py2_interpreter</vh></v>
<v t="ekr.20220525082935.810"><vh>walk_packages</vh></v>
<v t="ekr.20220525082935.811"><vh>find_module_path_and_all_py2</vh></v>
<v t="ekr.20220525082935.812"><vh>find_module_path_using_py2_sys_path</vh></v>
<v t="ekr.20220525082935.813"><vh>find_module_path_using_sys_path</vh></v>
<v t="ekr.20220525082935.814"><vh>find_module_path_and_all_py3</vh></v>
<v t="ekr.20220525082935.815"><vh>generate_guarded</vh></v>
<v t="ekr.20220525082935.816"><vh>PY2_MODULES = {'cStringIO', 'urlparse', 'collections.UserDict'}</vh></v>
<v t="ekr.20220525082935.817"><vh>report_missing</vh></v>
<v t="ekr.20220525082935.818"><vh>fail_missing</vh></v>
<v t="ekr.20220525082935.819"><vh>remove_misplaced_type_comments</vh></v>
<v t="ekr.20220525082935.820"><vh>remove_misplaced_type_comments</vh></v>
<v t="ekr.20220525082935.821"><vh>remove_misplaced_type_comments</vh></v>
<v t="ekr.20220525082935.822"><vh>common_dir_prefix</vh></v>
</v>
<v t="ekr.20220525082935.823"><vh>@clean subtypes.py</vh>
<v t="ekr.20220525082935.824"><vh>check_type_parameter</vh></v>
<v t="ekr.20220525082935.825"><vh>ignore_type_parameter</vh></v>
<v t="ekr.20220525082935.826"><vh>is_subtype</vh></v>
<v t="ekr.20220525082935.827"><vh>_is_subtype</vh></v>
<v t="ekr.20220525082935.828"><vh>is_equivalent</vh></v>
<v t="ekr.20220525082935.829"><vh>class SubtypeVisitor</vh>
<v t="ekr.20220525082935.830"><vh>SubtypeVisitor.__init__</vh></v>
<v t="ekr.20220525082935.831"><vh>SubtypeVisitor.build_subtype_kind</vh></v>
<v t="ekr.20220525082935.832"><vh>SubtypeVisitor._is_subtype</vh></v>
<v t="ekr.20220525082935.833"><vh>SubtypeVisitor.visit_x(left) means: is left (which is an instance of X) a subtype of</vh></v>
<v t="ekr.20220525082935.834"><vh>SubtypeVisitor.visit_unbound_type</vh></v>
<v t="ekr.20220525082935.835"><vh>SubtypeVisitor.visit_any</vh></v>
<v t="ekr.20220525082935.836"><vh>SubtypeVisitor.visit_none_type</vh></v>
<v t="ekr.20220525082935.837"><vh>SubtypeVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082935.838"><vh>SubtypeVisitor.visit_erased_type</vh></v>
<v t="ekr.20220525082935.839"><vh>SubtypeVisitor.visit_deleted_type</vh></v>
<v t="ekr.20220525082935.840"><vh>SubtypeVisitor.visit_instance</vh></v>
<v t="ekr.20220525082935.841"><vh>SubtypeVisitor.visit_type_var</vh></v>
<v t="ekr.20220525082935.842"><vh>SubtypeVisitor.visit_param_spec</vh></v>
<v t="ekr.20220525082935.843"><vh>SubtypeVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082935.844"><vh>SubtypeVisitor.visit_unpack_type</vh></v>
<v t="ekr.20220525082935.845"><vh>SubtypeVisitor.visit_parameters</vh></v>
<v t="ekr.20220525082935.846"><vh>SubtypeVisitor.visit_callable_type</vh></v>
<v t="ekr.20220525082935.847"><vh>SubtypeVisitor.visit_tuple_type</vh></v>
<v t="ekr.20220525082935.848"><vh>SubtypeVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20220525082935.849"><vh>SubtypeVisitor.visit_literal_type</vh></v>
<v t="ekr.20220525082935.850"><vh>SubtypeVisitor.visit_overloaded</vh></v>
<v t="ekr.20220525082935.851"><vh>SubtypeVisitor.visit_union_type</vh></v>
<v t="ekr.20220525082935.852"><vh>SubtypeVisitor.visit_partial_type</vh></v>
<v t="ekr.20220525082935.853"><vh>SubtypeVisitor.visit_type_type</vh></v>
<v t="ekr.20220525082935.854"><vh>SubtypeVisitor.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20220525082935.855"><vh>T = TypeVar('T', Instance, TypeAliasType)</vh></v>
<v t="ekr.20220525082935.856"><vh>pop_on_exit</vh></v>
<v t="ekr.20220525082935.857"><vh>is_protocol_implementation</vh></v>
<v t="ekr.20220525082935.858"><vh>find_member</vh></v>
<v t="ekr.20220525082935.859"><vh>get_member_flags</vh></v>
<v t="ekr.20220525082935.860"><vh>find_node_type</vh></v>
<v t="ekr.20220525082935.861"><vh>non_method_protocol_members</vh></v>
<v t="ekr.20220525082935.862"><vh>is_callable_compatible</vh></v>
<v t="ekr.20220525082935.863"><vh>are_parameters_compatible</vh>
<v t="ekr.20220525082935.864"><vh>_incompatible</vh></v>
</v>
<v t="ekr.20220525082935.865"><vh>are_args_compatible</vh>
<v t="ekr.20220525082935.866"><vh>is_different</vh></v>
</v>
<v t="ekr.20220525082935.867"><vh>flip_compat_check</vh></v>
<v t="ekr.20220525082935.868"><vh>unify_generic_callable</vh>
<v t="ekr.20220525082935.869"><vh>report</vh></v>
</v>
<v t="ekr.20220525082935.870"><vh>try_restrict_literal_union</vh></v>
<v t="ekr.20220525082935.871"><vh>restrict_subtype_away</vh></v>
<v t="ekr.20220525082935.872"><vh>covers_at_runtime</vh></v>
<v t="ekr.20220525082935.873"><vh>is_proper_subtype</vh></v>
<v t="ekr.20220525082935.874"><vh>_is_proper_subtype</vh></v>
<v t="ekr.20220525082935.875"><vh>class ProperSubtypeVisitor</vh>
<v t="ekr.20220525082935.876"><vh>ProperSubtypeVisitor.__init__</vh></v>
<v t="ekr.20220525082935.877"><vh>ProperSubtypeVisitor.build_subtype_kind</vh></v>
<v t="ekr.20220525082935.878"><vh>ProperSubtypeVisitor._is_proper_subtype</vh></v>
<v t="ekr.20220525082935.879"><vh>ProperSubtypeVisitor.visit_unbound_type</vh></v>
<v t="ekr.20220525082935.880"><vh>ProperSubtypeVisitor.visit_any</vh></v>
<v t="ekr.20220525082935.881"><vh>ProperSubtypeVisitor.visit_none_type</vh></v>
<v t="ekr.20220525082935.882"><vh>ProperSubtypeVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082935.883"><vh>ProperSubtypeVisitor.visit_erased_type</vh></v>
<v t="ekr.20220525082935.884"><vh>ProperSubtypeVisitor.visit_deleted_type</vh></v>
<v t="ekr.20220525082935.885"><vh>ProperSubtypeVisitor.visit_instance</vh></v>
<v t="ekr.20220525082935.886"><vh>ProperSubtypeVisitor.visit_type_var</vh></v>
<v t="ekr.20220525082935.887"><vh>ProperSubtypeVisitor.visit_param_spec</vh></v>
<v t="ekr.20220525082935.888"><vh>ProperSubtypeVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082935.889"><vh>ProperSubtypeVisitor.visit_unpack_type</vh></v>
<v t="ekr.20220525082935.890"><vh>ProperSubtypeVisitor.visit_parameters</vh></v>
<v t="ekr.20220525082935.891"><vh>ProperSubtypeVisitor.visit_callable_type</vh></v>
<v t="ekr.20220525082935.892"><vh>ProperSubtypeVisitor.visit_tuple_type</vh></v>
<v t="ekr.20220525082935.893"><vh>ProperSubtypeVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20220525082935.894"><vh>ProperSubtypeVisitor.visit_literal_type</vh></v>
<v t="ekr.20220525082935.895"><vh>ProperSubtypeVisitor.visit_overloaded</vh></v>
<v t="ekr.20220525082935.896"><vh>ProperSubtypeVisitor.visit_union_type</vh></v>
<v t="ekr.20220525082935.897"><vh>ProperSubtypeVisitor.visit_partial_type</vh></v>
<v t="ekr.20220525082935.898"><vh>ProperSubtypeVisitor.visit_type_type</vh></v>
<v t="ekr.20220525082935.899"><vh>ProperSubtypeVisitor.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20220525082935.900"><vh>is_more_precise</vh></v>
</v>
<v t="ekr.20220525082935.901"><vh>@clean suggestions.py</vh>
<v t="ekr.20220525082935.902"><vh>class PyAnnotateSignature</vh></v>
<v t="ekr.20220525082935.903"><vh>class Callsite</vh></v>
<v t="ekr.20220525082935.904"><vh>class SuggestionPlugin</vh>
<v t="ekr.20220525082935.905"><vh>SuggestionPlugin.__init__</vh></v>
<v t="ekr.20220525082935.906"><vh>SuggestionPlugin.get_function_hook</vh></v>
<v t="ekr.20220525082935.907"><vh>SuggestionPlugin.get_method_hook</vh></v>
<v t="ekr.20220525082935.908"><vh>SuggestionPlugin.log</vh></v>
</v>
<v t="ekr.20220525082935.909"><vh>class ReturnFinder</vh>
<v t="ekr.20220525082935.910"><vh>ReturnFinder.__init__</vh></v>
<v t="ekr.20220525082935.911"><vh>ReturnFinder.visit_return_stmt</vh></v>
<v t="ekr.20220525082935.912"><vh>ReturnFinder.visit_func_def</vh></v>
</v>
<v t="ekr.20220525082935.913"><vh>get_return_types</vh></v>
<v t="ekr.20220525082935.914"><vh>class ArgUseFinder</vh>
<v t="ekr.20220525082935.915"><vh>ArgUseFinder.__init__</vh></v>
<v t="ekr.20220525082935.916"><vh>ArgUseFinder.visit_call_expr</vh></v>
</v>
<v t="ekr.20220525082935.917"><vh>get_arg_uses</vh></v>
<v t="ekr.20220525082935.918"><vh>class SuggestionFailure</vh></v>
<v t="ekr.20220525082935.919"><vh>is_explicit_any</vh></v>
<v t="ekr.20220525082935.920"><vh>is_implicit_any</vh></v>
<v t="ekr.20220525082935.921"><vh>class SuggestionEngine</vh>
<v t="ekr.20220525082935.922"><vh>SuggestionEngine.__init__</vh></v>
<v t="ekr.20220525082935.923"><vh>SuggestionEngine.suggest</vh></v>
<v t="ekr.20220525082935.924"><vh>SuggestionEngine.suggest_callsites</vh></v>
<v t="ekr.20220525082935.925"><vh>SuggestionEngine.restore_after</vh></v>
<v t="ekr.20220525082935.926"><vh>SuggestionEngine.with_export_types</vh></v>
<v t="ekr.20220525082935.927"><vh>SuggestionEngine.get_trivial_type</vh></v>
<v t="ekr.20220525082935.928"><vh>SuggestionEngine.get_starting_type</vh></v>
<v t="ekr.20220525082935.929"><vh>SuggestionEngine.get_args</vh></v>
<v t="ekr.20220525082935.930"><vh>SuggestionEngine.get_default_arg_types</vh></v>
<v t="ekr.20220525082935.931"><vh>SuggestionEngine.add_adjustments</vh></v>
<v t="ekr.20220525082935.932"><vh>SuggestionEngine.get_guesses</vh></v>
<v t="ekr.20220525082935.933"><vh>SuggestionEngine.get_callsites</vh></v>
<v t="ekr.20220525082935.934"><vh>SuggestionEngine.filter_options</vh></v>
<v t="ekr.20220525082935.935"><vh>SuggestionEngine.find_best</vh></v>
<v t="ekr.20220525082935.936"><vh>SuggestionEngine.get_guesses_from_parent</vh></v>
<v t="ekr.20220525082935.937"><vh>SuggestionEngine.get_suggestion</vh></v>
<v t="ekr.20220525082935.938"><vh>SuggestionEngine.format_args</vh></v>
<v t="ekr.20220525082935.939"><vh>SuggestionEngine.find_node</vh></v>
<v t="ekr.20220525082935.940"><vh>SuggestionEngine.find_node_by_module_and_name</vh></v>
<v t="ekr.20220525082935.941"><vh>SuggestionEngine.find_node_by_file_and_line</vh></v>
<v t="ekr.20220525082935.942"><vh>SuggestionEngine.extract_from_decorator</vh></v>
<v t="ekr.20220525082935.943"><vh>SuggestionEngine.try_type</vh></v>
<v t="ekr.20220525082935.944"><vh>SuggestionEngine.reload</vh></v>
<v t="ekr.20220525082935.945"><vh>SuggestionEngine.ensure_loaded</vh></v>
<v t="ekr.20220525082935.946"><vh>SuggestionEngine.named_type</vh></v>
<v t="ekr.20220525082935.947"><vh>SuggestionEngine.json_suggestion</vh></v>
<v t="ekr.20220525082935.948"><vh>SuggestionEngine.pyannotate_signature</vh></v>
<v t="ekr.20220525082935.949"><vh>SuggestionEngine.format_signature</vh></v>
<v t="ekr.20220525082935.950"><vh>SuggestionEngine.format_type</vh></v>
<v t="ekr.20220525082935.951"><vh>SuggestionEngine.score_type</vh></v>
<v t="ekr.20220525082935.952"><vh>SuggestionEngine.score_callable</vh></v>
</v>
<v t="ekr.20220525082935.953"><vh>any_score_type</vh></v>
<v t="ekr.20220525082935.954"><vh>any_score_callable</vh></v>
<v t="ekr.20220525082935.955"><vh>is_tricky_callable</vh></v>
<v t="ekr.20220525082935.956"><vh>class TypeFormatter</vh>
<v t="ekr.20220525082935.957"><vh>TypeFormatter.__init__</vh></v>
<v t="ekr.20220525082935.958"><vh>TypeFormatter.visit_any</vh></v>
<v t="ekr.20220525082935.959"><vh>TypeFormatter.visit_instance</vh></v>
<v t="ekr.20220525082935.960"><vh>TypeFormatter.visit_tuple_type</vh></v>
<v t="ekr.20220525082935.961"><vh>TypeFormatter.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082935.962"><vh>TypeFormatter.visit_typeddict_type</vh></v>
<v t="ekr.20220525082935.963"><vh>TypeFormatter.visit_union_type</vh></v>
<v t="ekr.20220525082935.964"><vh>TypeFormatter.visit_callable_type</vh></v>
</v>
<v t="ekr.20220525082935.965"><vh>class StrToText</vh>
<v t="ekr.20220525082935.966"><vh>StrToText.__init__</vh></v>
<v t="ekr.20220525082935.967"><vh>StrToText.visit_type_alias_type</vh></v>
<v t="ekr.20220525082935.968"><vh>StrToText.visit_instance</vh></v>
</v>
<v t="ekr.20220525082935.969"><vh>TType = TypeVar('TType', bound=Type)</vh></v>
<v t="ekr.20220525082935.970"><vh>make_suggestion_anys</vh></v>
<v t="ekr.20220525082935.971"><vh>class MakeSuggestionAny</vh>
<v t="ekr.20220525082935.972"><vh>MakeSuggestionAny.visit_any</vh></v>
<v t="ekr.20220525082935.973"><vh>MakeSuggestionAny.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20220525082935.974"><vh>generate_type_combinations</vh></v>
<v t="ekr.20220525082935.975"><vh>count_errors</vh></v>
<v t="ekr.20220525082935.976"><vh>refine_type</vh></v>
<v t="ekr.20220525082935.977"><vh>refine_union</vh></v>
<v t="ekr.20220525082935.978"><vh>refine_callable</vh></v>
<v t="ekr.20220525082935.979"><vh>T = TypeVar('T')</vh></v>
<v t="ekr.20220525082935.980"><vh>dedup</vh></v>
</v>
<v t="ekr.20220525082935.981"><vh>@clean traverser.py</vh>
<v t="ekr.20220525082935.982"><vh>class TraverserVisitor</vh>
<v t="ekr.20220525082935.983"><vh>TraverserVisitor.__init__</vh></v>
<v t="ekr.20220525082935.984"><vh>TraverserVisitor.Visit methods</vh></v>
<v t="ekr.20220525082935.985"><vh>TraverserVisitor.visit_mypy_file</vh></v>
<v t="ekr.20220525082935.986"><vh>TraverserVisitor.visit_block</vh></v>
<v t="ekr.20220525082935.987"><vh>TraverserVisitor.visit_func</vh></v>
<v t="ekr.20220525082935.988"><vh>TraverserVisitor.visit_func_def</vh></v>
<v t="ekr.20220525082935.989"><vh>TraverserVisitor.visit_overloaded_func_def</vh></v>
<v t="ekr.20220525082935.990"><vh>TraverserVisitor.visit_class_def</vh></v>
<v t="ekr.20220525082935.991"><vh>TraverserVisitor.visit_decorator</vh></v>
<v t="ekr.20220525082935.992"><vh>TraverserVisitor.visit_expression_stmt</vh></v>
<v t="ekr.20220525082935.993"><vh>TraverserVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20220525082935.994"><vh>TraverserVisitor.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20220525082935.995"><vh>TraverserVisitor.visit_while_stmt</vh></v>
<v t="ekr.20220525082935.996"><vh>TraverserVisitor.visit_for_stmt</vh></v>
<v t="ekr.20220525082935.997"><vh>TraverserVisitor.visit_return_stmt</vh></v>
<v t="ekr.20220525082935.998"><vh>TraverserVisitor.visit_assert_stmt</vh></v>
<v t="ekr.20220525082935.999"><vh>TraverserVisitor.visit_del_stmt</vh></v>
<v t="ekr.20220525082935.1000"><vh>TraverserVisitor.visit_if_stmt</vh></v>
<v t="ekr.20220525082935.1001"><vh>TraverserVisitor.visit_raise_stmt</vh></v>
<v t="ekr.20220525082935.1002"><vh>TraverserVisitor.visit_try_stmt</vh></v>
<v t="ekr.20220525082935.1003"><vh>TraverserVisitor.visit_with_stmt</vh></v>
<v t="ekr.20220525082935.1004"><vh>TraverserVisitor.visit_match_stmt</vh></v>
<v t="ekr.20220525082935.1005"><vh>TraverserVisitor.visit_member_expr</vh></v>
<v t="ekr.20220525082935.1006"><vh>TraverserVisitor.visit_yield_from_expr</vh></v>
<v t="ekr.20220525082935.1007"><vh>TraverserVisitor.visit_yield_expr</vh></v>
<v t="ekr.20220525082935.1008"><vh>TraverserVisitor.visit_call_expr</vh></v>
<v t="ekr.20220525082935.1009"><vh>TraverserVisitor.visit_op_expr</vh></v>
<v t="ekr.20220525082935.1010"><vh>TraverserVisitor.visit_comparison_expr</vh></v>
<v t="ekr.20220525082935.1011"><vh>TraverserVisitor.visit_slice_expr</vh></v>
<v t="ekr.20220525082935.1012"><vh>TraverserVisitor.visit_cast_expr</vh></v>
<v t="ekr.20220525082935.1013"><vh>TraverserVisitor.visit_assert_type_expr</vh></v>
<v t="ekr.20220525082935.1014"><vh>TraverserVisitor.visit_reveal_expr</vh></v>
<v t="ekr.20220525082935.1015"><vh>TraverserVisitor.visit_assignment_expr</vh></v>
<v t="ekr.20220525082935.1016"><vh>TraverserVisitor.visit_unary_expr</vh></v>
<v t="ekr.20220525082935.1017"><vh>TraverserVisitor.visit_list_expr</vh></v>
<v t="ekr.20220525082935.1018"><vh>TraverserVisitor.visit_tuple_expr</vh></v>
<v t="ekr.20220525082935.1019"><vh>TraverserVisitor.visit_dict_expr</vh></v>
<v t="ekr.20220525082935.1020"><vh>TraverserVisitor.visit_set_expr</vh></v>
<v t="ekr.20220525082935.1021"><vh>TraverserVisitor.visit_index_expr</vh></v>
<v t="ekr.20220525082935.1022"><vh>TraverserVisitor.visit_generator_expr</vh></v>
<v t="ekr.20220525082935.1023"><vh>TraverserVisitor.visit_dictionary_comprehension</vh></v>
<v t="ekr.20220525082935.1024"><vh>TraverserVisitor.visit_list_comprehension</vh></v>
<v t="ekr.20220525082935.1025"><vh>TraverserVisitor.visit_set_comprehension</vh></v>
<v t="ekr.20220525082935.1026"><vh>TraverserVisitor.visit_conditional_expr</vh></v>
<v t="ekr.20220525082935.1027"><vh>TraverserVisitor.visit_type_application</vh></v>
<v t="ekr.20220525082935.1028"><vh>TraverserVisitor.visit_lambda_expr</vh></v>
<v t="ekr.20220525082935.1029"><vh>TraverserVisitor.visit_star_expr</vh></v>
<v t="ekr.20220525082935.1030"><vh>TraverserVisitor.visit_backquote_expr</vh></v>
<v t="ekr.20220525082935.1031"><vh>TraverserVisitor.visit_await_expr</vh></v>
<v t="ekr.20220525082935.1032"><vh>TraverserVisitor.visit_super_expr</vh></v>
<v t="ekr.20220525082935.1033"><vh>TraverserVisitor.visit_as_pattern</vh></v>
<v t="ekr.20220525082935.1034"><vh>TraverserVisitor.visit_or_pattern</vh></v>
<v t="ekr.20220525082935.1035"><vh>TraverserVisitor.visit_value_pattern</vh></v>
<v t="ekr.20220525082935.1036"><vh>TraverserVisitor.visit_sequence_pattern</vh></v>
<v t="ekr.20220525082935.1037"><vh>TraverserVisitor.visit_starred_patten</vh></v>
<v t="ekr.20220525082935.1038"><vh>TraverserVisitor.visit_mapping_pattern</vh></v>
<v t="ekr.20220525082935.1039"><vh>TraverserVisitor.visit_class_pattern</vh></v>
<v t="ekr.20220525082935.1040"><vh>TraverserVisitor.visit_import</vh></v>
<v t="ekr.20220525082935.1041"><vh>TraverserVisitor.visit_import_from</vh></v>
<v t="ekr.20220525082935.1042"><vh>TraverserVisitor.visit_print_stmt</vh></v>
<v t="ekr.20220525082935.1043"><vh>TraverserVisitor.visit_exec_stmt</vh></v>
</v>
<v t="ekr.20220525082935.1044"><vh>class ReturnSeeker</vh>
<v t="ekr.20220525082935.1045"><vh>ReturnSeeker.__init__</vh></v>
<v t="ekr.20220525082935.1046"><vh>ReturnSeeker.visit_return_stmt</vh></v>
</v>
<v t="ekr.20220525082935.1047"><vh>has_return_statement</vh></v>
<v t="ekr.20220525082935.1048"><vh>class FuncCollectorBase</vh>
<v t="ekr.20220525082935.1049"><vh>FuncCollectorBase.__init__</vh></v>
<v t="ekr.20220525082935.1050"><vh>FuncCollectorBase.visit_func_def</vh></v>
</v>
<v t="ekr.20220525082935.1051"><vh>class YieldSeeker</vh></v>
<v t="ekr.20220525082935.1052"><vh>has_yield_expression</vh></v>
<v t="ekr.20220525082935.1053"><vh>class ReturnCollector</vh></v>
<v t="ekr.20220525082935.1054"><vh>all_return_statements</vh></v>
<v t="ekr.20220525082935.1055"><vh>class YieldCollector</vh>
<v t="ekr.20220525082935.1056"><vh>YieldCollector.__init__</vh></v>
<v t="ekr.20220525082935.1057"><vh>YieldCollector.visit_assignment_stmt</vh></v>
<v t="ekr.20220525082935.1058"><vh>YieldCollector.visit_yield_expr</vh></v>
</v>
<v t="ekr.20220525082935.1059"><vh>all_yield_expressions</vh></v>
</v>
<v t="ekr.20220525082935.1060"><vh>@clean treetransform.py</vh>
<v t="ekr.20220525082935.1061"><vh>class TransformVisitor</vh>
<v t="ekr.20220525082935.1062"><vh>TransformVisitor.__init__</vh></v>
<v t="ekr.20220525082935.1063"><vh>TransformVisitor.visit_mypy_file</vh></v>
<v t="ekr.20220525082935.1064"><vh>TransformVisitor.visit_import</vh></v>
<v t="ekr.20220525082935.1065"><vh>TransformVisitor.visit_import_from</vh></v>
<v t="ekr.20220525082935.1066"><vh>TransformVisitor.visit_import_all</vh></v>
<v t="ekr.20220525082935.1067"><vh>TransformVisitor.copy_argument</vh></v>
<v t="ekr.20220525082935.1068"><vh>TransformVisitor.visit_func_def</vh></v>
<v t="ekr.20220525082935.1069"><vh>TransformVisitor.visit_lambda_expr</vh></v>
<v t="ekr.20220525082935.1070"><vh>TransformVisitor.copy_function_attributes</vh></v>
<v t="ekr.20220525082935.1071"><vh>TransformVisitor.visit_overloaded_func_def</vh></v>
<v t="ekr.20220525082935.1072"><vh>TransformVisitor.visit_class_def</vh></v>
<v t="ekr.20220525082935.1073"><vh>TransformVisitor.visit_global_decl</vh></v>
<v t="ekr.20220525082935.1074"><vh>TransformVisitor.visit_nonlocal_decl</vh></v>
<v t="ekr.20220525082935.1075"><vh>TransformVisitor.visit_block</vh></v>
<v t="ekr.20220525082935.1076"><vh>TransformVisitor.visit_decorator</vh></v>
<v t="ekr.20220525082935.1077"><vh>TransformVisitor.visit_var</vh></v>
<v t="ekr.20220525082935.1078"><vh>TransformVisitor.visit_expression_stmt</vh></v>
<v t="ekr.20220525082935.1079"><vh>TransformVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20220525082935.1080"><vh>TransformVisitor.duplicate_assignment</vh></v>
<v t="ekr.20220525082935.1081"><vh>TransformVisitor.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20220525082935.1082"><vh>TransformVisitor.visit_while_stmt</vh></v>
<v t="ekr.20220525082935.1083"><vh>TransformVisitor.visit_for_stmt</vh></v>
<v t="ekr.20220525082935.1084"><vh>TransformVisitor.visit_return_stmt</vh></v>
<v t="ekr.20220525082935.1085"><vh>TransformVisitor.visit_assert_stmt</vh></v>
<v t="ekr.20220525082935.1086"><vh>TransformVisitor.visit_del_stmt</vh></v>
<v t="ekr.20220525082935.1087"><vh>TransformVisitor.visit_if_stmt</vh></v>
<v t="ekr.20220525082935.1088"><vh>TransformVisitor.visit_break_stmt</vh></v>
<v t="ekr.20220525082935.1089"><vh>TransformVisitor.visit_continue_stmt</vh></v>
<v t="ekr.20220525082935.1090"><vh>TransformVisitor.visit_pass_stmt</vh></v>
<v t="ekr.20220525082935.1091"><vh>TransformVisitor.visit_raise_stmt</vh></v>
<v t="ekr.20220525082935.1092"><vh>TransformVisitor.visit_try_stmt</vh></v>
<v t="ekr.20220525082935.1093"><vh>TransformVisitor.visit_with_stmt</vh></v>
<v t="ekr.20220525082935.1094"><vh>TransformVisitor.visit_print_stmt</vh></v>
<v t="ekr.20220525082935.1095"><vh>TransformVisitor.visit_exec_stmt</vh></v>
<v t="ekr.20220525082935.1096"><vh>TransformVisitor.visit_star_expr</vh></v>
<v t="ekr.20220525082935.1097"><vh>TransformVisitor.visit_int_expr</vh></v>
<v t="ekr.20220525082935.1098"><vh>TransformVisitor.visit_str_expr</vh></v>
<v t="ekr.20220525082935.1099"><vh>TransformVisitor.visit_bytes_expr</vh></v>
<v t="ekr.20220525082935.1100"><vh>TransformVisitor.visit_unicode_expr</vh></v>
<v t="ekr.20220525082935.1101"><vh>TransformVisitor.visit_float_expr</vh></v>
<v t="ekr.20220525082935.1102"><vh>TransformVisitor.visit_complex_expr</vh></v>
<v t="ekr.20220525082935.1103"><vh>TransformVisitor.visit_ellipsis</vh></v>
<v t="ekr.20220525082935.1104"><vh>TransformVisitor.visit_name_expr</vh></v>
<v t="ekr.20220525082935.1105"><vh>TransformVisitor.duplicate_name</vh></v>
<v t="ekr.20220525082935.1106"><vh>TransformVisitor.visit_member_expr</vh></v>
<v t="ekr.20220525082935.1107"><vh>TransformVisitor.copy_ref</vh></v>
<v t="ekr.20220525082935.1108"><vh>TransformVisitor.visit_yield_from_expr</vh></v>
<v t="ekr.20220525082935.1109"><vh>TransformVisitor.visit_yield_expr</vh></v>
<v t="ekr.20220525082935.1110"><vh>TransformVisitor.visit_await_expr</vh></v>
<v t="ekr.20220525082935.1111"><vh>TransformVisitor.visit_call_expr</vh></v>
<v t="ekr.20220525082935.1112"><vh>TransformVisitor.visit_op_expr</vh></v>
<v t="ekr.20220525082935.1113"><vh>TransformVisitor.visit_comparison_expr</vh></v>
<v t="ekr.20220525082935.1114"><vh>TransformVisitor.visit_cast_expr</vh></v>
<v t="ekr.20220525082935.1115"><vh>TransformVisitor.visit_assert_type_expr</vh></v>
<v t="ekr.20220525082935.1116"><vh>TransformVisitor.visit_reveal_expr</vh></v>
<v t="ekr.20220525082935.1117"><vh>TransformVisitor.visit_super_expr</vh></v>
<v t="ekr.20220525082935.1118"><vh>TransformVisitor.visit_assignment_expr</vh></v>
<v t="ekr.20220525082935.1119"><vh>TransformVisitor.visit_unary_expr</vh></v>
<v t="ekr.20220525082935.1120"><vh>TransformVisitor.visit_list_expr</vh></v>
<v t="ekr.20220525082935.1121"><vh>TransformVisitor.visit_dict_expr</vh></v>
<v t="ekr.20220525082935.1122"><vh>TransformVisitor.visit_tuple_expr</vh></v>
<v t="ekr.20220525082935.1123"><vh>TransformVisitor.visit_set_expr</vh></v>
<v t="ekr.20220525082935.1124"><vh>TransformVisitor.visit_index_expr</vh></v>
<v t="ekr.20220525082935.1125"><vh>TransformVisitor.visit_type_application</vh></v>
<v t="ekr.20220525082935.1126"><vh>TransformVisitor.visit_list_comprehension</vh></v>
<v t="ekr.20220525082935.1127"><vh>TransformVisitor.visit_set_comprehension</vh></v>
<v t="ekr.20220525082935.1128"><vh>TransformVisitor.visit_dictionary_comprehension</vh></v>
<v t="ekr.20220525082935.1129"><vh>TransformVisitor.visit_generator_expr</vh></v>
<v t="ekr.20220525082935.1130"><vh>TransformVisitor.duplicate_generator</vh></v>
<v t="ekr.20220525082935.1131"><vh>TransformVisitor.visit_slice_expr</vh></v>
<v t="ekr.20220525082935.1132"><vh>TransformVisitor.visit_conditional_expr</vh></v>
<v t="ekr.20220525082935.1133"><vh>TransformVisitor.visit_backquote_expr</vh></v>
<v t="ekr.20220525082935.1134"><vh>TransformVisitor.visit_type_var_expr</vh></v>
<v t="ekr.20220525082935.1135"><vh>TransformVisitor.visit_paramspec_expr</vh></v>
<v t="ekr.20220525082935.1136"><vh>TransformVisitor.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20220525082935.1137"><vh>TransformVisitor.visit_type_alias_expr</vh></v>
<v t="ekr.20220525082935.1138"><vh>TransformVisitor.visit_newtype_expr</vh></v>
<v t="ekr.20220525082935.1139"><vh>TransformVisitor.visit_namedtuple_expr</vh></v>
<v t="ekr.20220525082935.1140"><vh>TransformVisitor.visit_enum_call_expr</vh></v>
<v t="ekr.20220525082935.1141"><vh>TransformVisitor.visit_typeddict_expr</vh></v>
<v t="ekr.20220525082935.1142"><vh>TransformVisitor.visit__promote_expr</vh></v>
<v t="ekr.20220525082935.1143"><vh>TransformVisitor.visit_temp_node</vh></v>
<v t="ekr.20220525082935.1144"><vh>TransformVisitor.node</vh></v>
<v t="ekr.20220525082935.1145"><vh>TransformVisitor.mypyfile</vh></v>
<v t="ekr.20220525082935.1146"><vh>TransformVisitor.expr</vh></v>
<v t="ekr.20220525082935.1147"><vh>TransformVisitor.stmt</vh></v>
<v t="ekr.20220525082935.1148"><vh>TransformVisitor.Helpers</vh></v>
<v t="ekr.20220525082935.1149"><vh>TransformVisitor.optional_expr</vh></v>
<v t="ekr.20220525082935.1150"><vh>TransformVisitor.block</vh></v>
<v t="ekr.20220525082935.1151"><vh>TransformVisitor.optional_block</vh></v>
<v t="ekr.20220525082935.1152"><vh>TransformVisitor.statements</vh></v>
<v t="ekr.20220525082935.1153"><vh>TransformVisitor.expressions</vh></v>
<v t="ekr.20220525082935.1154"><vh>TransformVisitor.optional_expressions</vh></v>
<v t="ekr.20220525082935.1155"><vh>TransformVisitor.blocks</vh></v>
<v t="ekr.20220525082935.1156"><vh>TransformVisitor.names</vh></v>
<v t="ekr.20220525082935.1157"><vh>TransformVisitor.optional_names</vh></v>
<v t="ekr.20220525082935.1158"><vh>TransformVisitor.type</vh></v>
<v t="ekr.20220525082935.1159"><vh>TransformVisitor.optional_type</vh></v>
<v t="ekr.20220525082935.1160"><vh>TransformVisitor.types</vh></v>
</v>
<v t="ekr.20220525082935.1161"><vh>class FuncMapInitializer</vh>
<v t="ekr.20220525082935.1162"><vh>FuncMapInitializer.__init__</vh></v>
<v t="ekr.20220525082935.1163"><vh>FuncMapInitializer.visit_func_def</vh></v>
</v>
</v>
<v t="ekr.20220525082935.1164"><vh>@clean tvar_scope.py</vh>
<v t="ekr.20220525082935.1165"><vh>class TypeVarLikeScope</vh>
<v t="ekr.20220525082935.1166"><vh>TypeVarLikeScope.__init__</vh></v>
<v t="ekr.20220525082935.1167"><vh>TypeVarLikeScope.get_function_scope</vh></v>
<v t="ekr.20220525082935.1168"><vh>TypeVarLikeScope.allow_binding</vh></v>
<v t="ekr.20220525082935.1169"><vh>TypeVarLikeScope.method_frame</vh></v>
<v t="ekr.20220525082935.1170"><vh>TypeVarLikeScope.class_frame</vh></v>
<v t="ekr.20220525082935.1171"><vh>TypeVarLikeScope.bind_new</vh></v>
<v t="ekr.20220525082935.1172"><vh>TypeVarLikeScope.bind_existing</vh></v>
<v t="ekr.20220525082935.1173"><vh>TypeVarLikeScope.get_binding</vh></v>
<v t="ekr.20220525082935.1174"><vh>TypeVarLikeScope.__str__</vh></v>
</v>
</v>
<v t="ekr.20220525082935.1175"><vh>@clean typeanal.py</vh>
<v t="ekr.20220525082935.1176"><vh>analyze_type_alias</vh></v>
<v t="ekr.20220525082935.1177"><vh>no_subscript_builtin_alias</vh></v>
<v t="ekr.20220525082935.1178"><vh>class TypeAnalyser</vh>
<v t="ekr.20220525082935.1179"><vh>TypeAnalyser.__init__</vh></v>
<v t="ekr.20220525082935.1180"><vh>TypeAnalyser.visit_unbound_type</vh></v>
<v t="ekr.20220525082935.1181"><vh>TypeAnalyser.visit_unbound_type_nonoptional</vh></v>
<v t="ekr.20220525082935.1182"><vh>TypeAnalyser.cannot_resolve_type</vh></v>
<v t="ekr.20220525082935.1183"><vh>TypeAnalyser.apply_concatenate_operator</vh></v>
<v t="ekr.20220525082935.1184"><vh>TypeAnalyser.try_analyze_special_unbound_type</vh></v>
<v t="ekr.20220525082935.1185"><vh>TypeAnalyser.get_omitted_any</vh></v>
<v t="ekr.20220525082935.1186"><vh>TypeAnalyser.analyze_type_with_type_info</vh></v>
<v t="ekr.20220525082935.1187"><vh>TypeAnalyser.analyze_unbound_type_without_type_info</vh></v>
<v t="ekr.20220525082935.1188"><vh>TypeAnalyser.visit_any</vh></v>
<v t="ekr.20220525082935.1189"><vh>TypeAnalyser.visit_none_type</vh></v>
<v t="ekr.20220525082935.1190"><vh>TypeAnalyser.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082935.1191"><vh>TypeAnalyser.visit_erased_type</vh></v>
<v t="ekr.20220525082935.1192"><vh>TypeAnalyser.visit_deleted_type</vh></v>
<v t="ekr.20220525082935.1193"><vh>TypeAnalyser.visit_type_list</vh></v>
<v t="ekr.20220525082935.1194"><vh>TypeAnalyser.visit_callable_argument</vh></v>
<v t="ekr.20220525082935.1195"><vh>TypeAnalyser.visit_instance</vh></v>
<v t="ekr.20220525082935.1196"><vh>TypeAnalyser.visit_type_alias_type</vh></v>
<v t="ekr.20220525082935.1197"><vh>TypeAnalyser.visit_type_var</vh></v>
<v t="ekr.20220525082935.1198"><vh>TypeAnalyser.visit_param_spec</vh></v>
<v t="ekr.20220525082935.1199"><vh>TypeAnalyser.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082935.1200"><vh>TypeAnalyser.visit_unpack_type</vh></v>
<v t="ekr.20220525082935.1201"><vh>TypeAnalyser.visit_parameters</vh></v>
<v t="ekr.20220525082935.1202"><vh>TypeAnalyser.visit_callable_type</vh></v>
<v t="ekr.20220525082935.1203"><vh>TypeAnalyser.anal_type_guard</vh></v>
<v t="ekr.20220525082935.1204"><vh>TypeAnalyser.anal_type_guard_arg</vh></v>
<v t="ekr.20220525082935.1205"><vh>TypeAnalyser.anal_star_arg_type</vh></v>
<v t="ekr.20220525082935.1206"><vh>TypeAnalyser.visit_overloaded</vh></v>
<v t="ekr.20220525082935.1207"><vh>TypeAnalyser.visit_tuple_type</vh></v>
<v t="ekr.20220525082935.1208"><vh>TypeAnalyser.visit_typeddict_type</vh></v>
<v t="ekr.20220525082935.1209"><vh>TypeAnalyser.visit_raw_expression_type</vh></v>
<v t="ekr.20220525082935.1210"><vh>TypeAnalyser.visit_literal_type</vh></v>
<v t="ekr.20220525082935.1211"><vh>TypeAnalyser.visit_star_type</vh></v>
<v t="ekr.20220525082935.1212"><vh>TypeAnalyser.visit_union_type</vh></v>
<v t="ekr.20220525082935.1213"><vh>TypeAnalyser.visit_partial_type</vh></v>
<v t="ekr.20220525082935.1214"><vh>TypeAnalyser.visit_ellipsis_type</vh></v>
<v t="ekr.20220525082935.1215"><vh>TypeAnalyser.visit_type_type</vh></v>
<v t="ekr.20220525082935.1216"><vh>TypeAnalyser.visit_placeholder_type</vh></v>
<v t="ekr.20220525082935.1217"><vh>TypeAnalyser.analyze_callable_args_for_paramspec</vh></v>
<v t="ekr.20220525082935.1218"><vh>TypeAnalyser.analyze_callable_args_for_concatenate</vh></v>
<v t="ekr.20220525082935.1219"><vh>TypeAnalyser.analyze_callable_type</vh></v>
<v t="ekr.20220525082935.1220"><vh>TypeAnalyser.analyze_callable_args</vh></v>
<v t="ekr.20220525082935.1221"><vh>TypeAnalyser.analyze_literal_type</vh></v>
<v t="ekr.20220525082935.1222"><vh>TypeAnalyser.analyze_literal_param</vh></v>
<v t="ekr.20220525082935.1223"><vh>TypeAnalyser.analyze_type</vh></v>
<v t="ekr.20220525082935.1224"><vh>TypeAnalyser.fail</vh></v>
<v t="ekr.20220525082935.1225"><vh>TypeAnalyser.note</vh></v>
<v t="ekr.20220525082935.1226"><vh>TypeAnalyser.tvar_scope_frame</vh></v>
<v t="ekr.20220525082935.1227"><vh>TypeAnalyser.infer_type_variables</vh></v>
<v t="ekr.20220525082935.1228"><vh>TypeAnalyser.bind_function_type_variables</vh></v>
<v t="ekr.20220525082935.1229"><vh>TypeAnalyser.is_defined_type_var</vh></v>
<v t="ekr.20220525082935.1230"><vh>TypeAnalyser.anal_array</vh></v>
<v t="ekr.20220525082935.1231"><vh>TypeAnalyser.anal_type</vh></v>
<v t="ekr.20220525082935.1232"><vh>TypeAnalyser.anal_var_def</vh></v>
<v t="ekr.20220525082935.1233"><vh>TypeAnalyser.anal_var_defs</vh></v>
<v t="ekr.20220525082935.1234"><vh>TypeAnalyser.named_type_with_normalized_str</vh></v>
<v t="ekr.20220525082935.1235"><vh>TypeAnalyser.named_type</vh></v>
<v t="ekr.20220525082935.1236"><vh>TypeAnalyser.tuple_type</vh></v>
<v t="ekr.20220525082935.1237"><vh>TypeAnalyser.set_allow_param_spec_literals</vh></v>
</v>
<v t="ekr.20220525082935.1238"><vh>TypeVarLikeList = List[Tuple[str, TypeVarLikeExpr]]</vh></v>
<v t="ekr.20220525082935.1239"><vh>class MsgCallback</vh>
<v t="ekr.20220525082935.1240"><vh>MsgCallback.__call__</vh></v>
</v>
<v t="ekr.20220525082935.1241"><vh>get_omitted_any</vh></v>
<v t="ekr.20220525082935.1242"><vh>fix_instance</vh></v>
<v t="ekr.20220525082935.1243"><vh>expand_type_alias</vh></v>
<v t="ekr.20220525082935.1244"><vh>set_any_tvars</vh></v>
<v t="ekr.20220525082935.1245"><vh>remove_dups</vh></v>
<v t="ekr.20220525082935.1246"><vh>flatten_tvars</vh></v>
<v t="ekr.20220525082935.1247"><vh>class TypeVarLikeQuery</vh>
<v t="ekr.20220525082935.1248"><vh>TypeVarLikeQuery.__init__</vh></v>
<v t="ekr.20220525082935.1249"><vh>TypeVarLikeQuery._seems_like_callable</vh></v>
<v t="ekr.20220525082935.1250"><vh>TypeVarLikeQuery.visit_unbound_type</vh></v>
<v t="ekr.20220525082935.1251"><vh>TypeVarLikeQuery.visit_callable_type</vh></v>
</v>
<v t="ekr.20220525082935.1252"><vh>check_for_explicit_any</vh></v>
<v t="ekr.20220525082935.1253"><vh>has_explicit_any</vh></v>
<v t="ekr.20220525082935.1254"><vh>class HasExplicitAny</vh>
<v t="ekr.20220525082935.1255"><vh>HasExplicitAny.__init__</vh></v>
<v t="ekr.20220525082935.1256"><vh>HasExplicitAny.visit_any</vh></v>
<v t="ekr.20220525082935.1257"><vh>HasExplicitAny.visit_typeddict_type</vh></v>
</v>
<v t="ekr.20220525082935.1258"><vh>has_any_from_unimported_type</vh></v>
<v t="ekr.20220525082935.1259"><vh>class HasAnyFromUnimportedType</vh>
<v t="ekr.20220525082935.1260"><vh>HasAnyFromUnimportedType.__init__</vh></v>
<v t="ekr.20220525082935.1261"><vh>HasAnyFromUnimportedType.visit_any</vh></v>
<v t="ekr.20220525082935.1262"><vh>HasAnyFromUnimportedType.visit_typeddict_type</vh></v>
</v>
<v t="ekr.20220525082935.1263"><vh>collect_all_inner_types</vh></v>
<v t="ekr.20220525082935.1264"><vh>class CollectAllInnerTypesQuery</vh>
<v t="ekr.20220525082935.1265"><vh>CollectAllInnerTypesQuery.__init__</vh></v>
<v t="ekr.20220525082935.1266"><vh>CollectAllInnerTypesQuery.query_types</vh></v>
<v t="ekr.20220525082935.1267"><vh>CollectAllInnerTypesQuery.combine_lists_strategy</vh></v>
</v>
<v t="ekr.20220525082935.1268"><vh>make_optional_type</vh></v>
<v t="ekr.20220525082935.1269"><vh>fix_instance_types</vh></v>
<v t="ekr.20220525082935.1270"><vh>class InstanceFixer</vh>
<v t="ekr.20220525082935.1271"><vh>InstanceFixer.__init__</vh></v>
<v t="ekr.20220525082935.1272"><vh>InstanceFixer.visit_instance</vh></v>
</v>
</v>
<v t="ekr.20220525082935.1273"><vh>@clean typeops.py</vh>
<v t="ekr.20220525082935.1274"><vh>is_recursive_pair</vh></v>
<v t="ekr.20220525082935.1275"><vh>tuple_fallback</vh></v>
<v t="ekr.20220525082935.1276"><vh>type_object_type_from_function</vh></v>
<v t="ekr.20220525082935.1277"><vh>class_callable</vh></v>
<v t="ekr.20220525082935.1278"><vh>map_type_from_supertype</vh></v>
<v t="ekr.20220525082935.1279"><vh>supported_self_type</vh></v>
<v t="ekr.20220525082935.1280"><vh>F = TypeVar('F', bound=FunctionLike)</vh></v>
<v t="ekr.20220525082935.1281"><vh>bind_self</vh></v>
<v t="ekr.20220525082935.1282"><vh>erase_to_bound</vh></v>
<v t="ekr.20220525082935.1283"><vh>callable_corresponding_argument</vh></v>
<v t="ekr.20220525082935.1284"><vh>simple_literal_value_key</vh></v>
<v t="ekr.20220525082935.1285"><vh>simple_literal_type</vh></v>
<v t="ekr.20220525082935.1286"><vh>is_simple_literal</vh></v>
<v t="ekr.20220525082935.1287"><vh>make_simplified_union</vh></v>
<v t="ekr.20220525082935.1288"><vh>_remove_redundant_union_items</vh></v>
<v t="ekr.20220525082935.1289"><vh>_get_type_special_method_bool_ret_type</vh></v>
<v t="ekr.20220525082935.1290"><vh>true_only</vh></v>
<v t="ekr.20220525082935.1291"><vh>false_only</vh></v>
<v t="ekr.20220525082935.1292"><vh>true_or_false</vh></v>
<v t="ekr.20220525082935.1293"><vh>erase_def_to_union_or_bound</vh></v>
<v t="ekr.20220525082935.1294"><vh>erase_to_union_or_bound</vh></v>
<v t="ekr.20220525082935.1295"><vh>function_type</vh></v>
<v t="ekr.20220525082935.1296"><vh>callable_type</vh></v>
<v t="ekr.20220525082935.1297"><vh>try_getting_str_literals</vh></v>
<v t="ekr.20220525082935.1298"><vh>try_getting_str_literals_from_type</vh></v>
<v t="ekr.20220525082935.1299"><vh>try_getting_int_literals_from_type</vh></v>
<v t="ekr.20220525082935.1300"><vh>T = TypeVar('T')</vh></v>
<v t="ekr.20220525082935.1301"><vh>try_getting_literals_from_type</vh></v>
<v t="ekr.20220525082935.1302"><vh>is_literal_type_like</vh></v>
<v t="ekr.20220525082935.1303"><vh>get_enum_values</vh></v>
<v t="ekr.20220525082935.1304"><vh>is_singleton_type</vh></v>
<v t="ekr.20220525082935.1305"><vh>try_expanding_sum_type_to_union</vh></v>
<v t="ekr.20220525082935.1306"><vh>try_contracting_literals_in_union</vh></v>
<v t="ekr.20220525082935.1307"><vh>coerce_to_literal</vh></v>
<v t="ekr.20220525082935.1308"><vh>get_type_vars</vh></v>
<v t="ekr.20220525082935.1309"><vh>class TypeVarExtractor</vh>
<v t="ekr.20220525082935.1310"><vh>TypeVarExtractor.__init__</vh></v>
<v t="ekr.20220525082935.1311"><vh>TypeVarExtractor._merge</vh></v>
<v t="ekr.20220525082935.1312"><vh>TypeVarExtractor.visit_type_var</vh></v>
</v>
<v t="ekr.20220525082935.1313"><vh>custom_special_method</vh></v>
<v t="ekr.20220525082935.1314"><vh>is_redundant_literal_instance</vh></v>
<v t="ekr.20220525082935.1315"><vh>separate_union_literals</vh></v>
</v>
<v t="ekr.20220525082935.1316"><vh>@clean types.py</vh>
<v t="ekr.20220525082935.1317"><vh>class TypeOfAny</vh></v>
<v t="ekr.20220525082935.1318"><vh>deserialize_type</vh></v>
<v t="ekr.20220525082935.1319"><vh>class Type</vh>
<v t="ekr.20220525082935.1320"><vh>Type.__init__</vh></v>
<v t="ekr.20220525082935.1321"><vh>Type.can_be_true_default</vh></v>
<v t="ekr.20220525082935.1322"><vh>Type.can_be_false_default</vh></v>
<v t="ekr.20220525082935.1323"><vh>Type.accept</vh></v>
<v t="ekr.20220525082935.1324"><vh>Type.__repr__</vh></v>
<v t="ekr.20220525082935.1325"><vh>Type.serialize</vh></v>
<v t="ekr.20220525082935.1326"><vh>Type.deserialize</vh></v>
</v>
<v t="ekr.20220525082935.1327"><vh>class TypeAliasType</vh>
<v t="ekr.20220525082935.1328"><vh>TypeAliasType.__init__</vh></v>
<v t="ekr.20220525082935.1329"><vh>TypeAliasType._expand_once</vh></v>
<v t="ekr.20220525082935.1330"><vh>TypeAliasType._partial_expansion</vh></v>
<v t="ekr.20220525082935.1331"><vh>TypeAliasType.expand_all_if_possible</vh></v>
<v t="ekr.20220525082935.1332"><vh>TypeAliasType.is_recursive</vh></v>
<v t="ekr.20220525082935.1333"><vh>TypeAliasType.can_be_true_default</vh></v>
<v t="ekr.20220525082935.1334"><vh>TypeAliasType.can_be_false_default</vh></v>
<v t="ekr.20220525082935.1335"><vh>TypeAliasType.accept</vh></v>
<v t="ekr.20220525082935.1336"><vh>TypeAliasType.__hash__</vh></v>
<v t="ekr.20220525082935.1337"><vh>TypeAliasType.__eq__</vh></v>
<v t="ekr.20220525082935.1338"><vh>TypeAliasType.serialize</vh></v>
<v t="ekr.20220525082935.1339"><vh>TypeAliasType.deserialize</vh></v>
<v t="ekr.20220525082935.1340"><vh>TypeAliasType.copy_modified</vh></v>
</v>
<v t="ekr.20220525082935.1341"><vh>class TypeGuardedType</vh>
<v t="ekr.20220525082935.1342"><vh>TypeGuardedType.__init__</vh></v>
<v t="ekr.20220525082935.1343"><vh>TypeGuardedType.__repr__</vh></v>
</v>
<v t="ekr.20220525082935.1344"><vh>class RequiredType</vh>
<v t="ekr.20220525082935.1345"><vh>RequiredType.__init__</vh></v>
<v t="ekr.20220525082935.1346"><vh>RequiredType.__repr__</vh></v>
<v t="ekr.20220525082935.1347"><vh>RequiredType.accept</vh></v>
</v>
<v t="ekr.20220525082935.1348"><vh>class ProperType</vh></v>
<v t="ekr.20220525082935.1349"><vh>class TypeVarId</vh>
<v t="ekr.20220525082935.1350"><vh>TypeVarId.__init__</vh></v>
<v t="ekr.20220525082935.1351"><vh>TypeVarId.new</vh></v>
<v t="ekr.20220525082935.1352"><vh>TypeVarId.__repr__</vh></v>
<v t="ekr.20220525082935.1353"><vh>TypeVarId.__eq__</vh></v>
<v t="ekr.20220525082935.1354"><vh>TypeVarId.__ne__</vh></v>
<v t="ekr.20220525082935.1355"><vh>TypeVarId.__hash__</vh></v>
<v t="ekr.20220525082935.1356"><vh>TypeVarId.is_meta_var</vh></v>
</v>
<v t="ekr.20220525082935.1357"><vh>class TypeVarLikeType</vh>
<v t="ekr.20220525082935.1358"><vh>TypeVarLikeType.__init__</vh></v>
<v t="ekr.20220525082935.1359"><vh>TypeVarLikeType.serialize</vh></v>
<v t="ekr.20220525082935.1360"><vh>TypeVarLikeType.deserialize</vh></v>
</v>
<v t="ekr.20220525082935.1361"><vh>class TypeVarType</vh>
<v t="ekr.20220525082935.1362"><vh>TypeVarType.__init__</vh></v>
<v t="ekr.20220525082935.1363"><vh>TypeVarType.new_unification_variable</vh></v>
<v t="ekr.20220525082935.1364"><vh>TypeVarType.accept</vh></v>
<v t="ekr.20220525082935.1365"><vh>TypeVarType.__hash__</vh></v>
<v t="ekr.20220525082935.1366"><vh>TypeVarType.__eq__</vh></v>
<v t="ekr.20220525082935.1367"><vh>TypeVarType.serialize</vh></v>
<v t="ekr.20220525082935.1368"><vh>TypeVarType.deserialize</vh></v>
</v>
<v t="ekr.20220525082935.1369"><vh>class ParamSpecFlavor</vh></v>
<v t="ekr.20220525082935.1370"><vh>class ParamSpecType</vh>
<v t="ekr.20220525082935.1371"><vh>ParamSpecType.__init__</vh></v>
<v t="ekr.20220525082935.1372"><vh>ParamSpecType.new_unification_variable</vh></v>
<v t="ekr.20220525082935.1373"><vh>ParamSpecType.with_flavor</vh></v>
<v t="ekr.20220525082935.1374"><vh>ParamSpecType.copy_modified</vh></v>
<v t="ekr.20220525082935.1375"><vh>ParamSpecType.accept</vh></v>
<v t="ekr.20220525082935.1376"><vh>ParamSpecType.name_with_suffix</vh></v>
<v t="ekr.20220525082935.1377"><vh>ParamSpecType.__hash__</vh></v>
<v t="ekr.20220525082935.1378"><vh>ParamSpecType.__eq__</vh></v>
<v t="ekr.20220525082935.1379"><vh>ParamSpecType.serialize</vh></v>
<v t="ekr.20220525082935.1380"><vh>ParamSpecType.deserialize</vh></v>
</v>
<v t="ekr.20220525082935.1381"><vh>class TypeVarTupleType</vh>
<v t="ekr.20220525082935.1382"><vh>TypeVarTupleType.serialize</vh></v>
<v t="ekr.20220525082935.1383"><vh>TypeVarTupleType.deserialize</vh></v>
<v t="ekr.20220525082935.1384"><vh>TypeVarTupleType.accept</vh></v>
<v t="ekr.20220525082935.1385"><vh>TypeVarTupleType.__hash__</vh></v>
<v t="ekr.20220525082935.1386"><vh>TypeVarTupleType.__eq__</vh></v>
<v t="ekr.20220525082935.1387"><vh>TypeVarTupleType.new_unification_variable</vh></v>
</v>
<v t="ekr.20220525082935.1388"><vh>class UnboundType</vh>
<v t="ekr.20220525082935.1389"><vh>UnboundType.__init__</vh></v>
<v t="ekr.20220525082935.1390"><vh>UnboundType.copy_modified</vh></v>
<v t="ekr.20220525082935.1391"><vh>UnboundType.accept</vh></v>
<v t="ekr.20220525082935.1392"><vh>UnboundType.__hash__</vh></v>
<v t="ekr.20220525082935.1393"><vh>UnboundType.__eq__</vh></v>
<v t="ekr.20220525082935.1394"><vh>UnboundType.serialize</vh></v>
<v t="ekr.20220525082935.1395"><vh>UnboundType.deserialize</vh></v>
</v>
<v t="ekr.20220525082935.1396"><vh>class CallableArgument</vh>
<v t="ekr.20220525082935.1397"><vh>CallableArgument.__init__</vh></v>
<v t="ekr.20220525082935.1398"><vh>CallableArgument.accept</vh></v>
<v t="ekr.20220525082935.1399"><vh>CallableArgument.serialize</vh></v>
</v>
<v t="ekr.20220525082935.1400"><vh>class TypeList</vh>
<v t="ekr.20220525082935.1401"><vh>TypeList.__init__</vh></v>
<v t="ekr.20220525082935.1402"><vh>TypeList.accept</vh></v>
<v t="ekr.20220525082935.1403"><vh>TypeList.serialize</vh></v>
</v>
<v t="ekr.20220525082935.1404"><vh>class UnpackType</vh>
<v t="ekr.20220525082935.1405"><vh>UnpackType.__init__</vh></v>
<v t="ekr.20220525082935.1406"><vh>UnpackType.accept</vh></v>
<v t="ekr.20220525082935.1407"><vh>UnpackType.serialize</vh></v>
<v t="ekr.20220525082935.1408"><vh>UnpackType.deserialize</vh></v>
</v>
<v t="ekr.20220525082935.1409"><vh>class AnyType</vh>
<v t="ekr.20220525082935.1410"><vh>AnyType.__init__</vh></v>
<v t="ekr.20220525082935.1411"><vh>AnyType.is_from_error</vh></v>
<v t="ekr.20220525082935.1412"><vh>AnyType.accept</vh></v>
<v t="ekr.20220525082935.1413"><vh>AnyType.copy_modified</vh></v>
<v t="ekr.20220525082935.1414"><vh>AnyType.__hash__</vh></v>
<v t="ekr.20220525082935.1415"><vh>AnyType.__eq__</vh></v>
<v t="ekr.20220525082935.1416"><vh>AnyType.serialize</vh></v>
<v t="ekr.20220525082935.1417"><vh>AnyType.deserialize</vh></v>
</v>
<v t="ekr.20220525082935.1418"><vh>class UninhabitedType</vh>
<v t="ekr.20220525082935.1419"><vh>UninhabitedType.__init__</vh></v>
<v t="ekr.20220525082935.1420"><vh>UninhabitedType.can_be_true_default</vh></v>
<v t="ekr.20220525082935.1421"><vh>UninhabitedType.can_be_false_default</vh></v>
<v t="ekr.20220525082935.1422"><vh>UninhabitedType.accept</vh></v>
<v t="ekr.20220525082935.1423"><vh>UninhabitedType.__hash__</vh></v>
<v t="ekr.20220525082935.1424"><vh>UninhabitedType.__eq__</vh></v>
<v t="ekr.20220525082935.1425"><vh>UninhabitedType.serialize</vh></v>
<v t="ekr.20220525082935.1426"><vh>UninhabitedType.deserialize</vh></v>
</v>
<v t="ekr.20220525082935.1427"><vh>class NoneType</vh>
<v t="ekr.20220525082935.1428"><vh>NoneType.__init__</vh></v>
<v t="ekr.20220525082935.1429"><vh>NoneType.can_be_true_default</vh></v>
<v t="ekr.20220525082935.1430"><vh>NoneType.__hash__</vh></v>
<v t="ekr.20220525082935.1431"><vh>NoneType.__eq__</vh></v>
<v t="ekr.20220525082935.1432"><vh>NoneType.accept</vh></v>
<v t="ekr.20220525082935.1433"><vh>NoneType.serialize</vh></v>
<v t="ekr.20220525082935.1434"><vh>NoneType.deserialize</vh></v>
</v>
<v t="ekr.20220525082935.1435"><vh>NoneType used to be called NoneTyp so to avoid needlessly breaking</vh></v>
<v t="ekr.20220525082935.1436"><vh>class ErasedType</vh>
<v t="ekr.20220525082935.1437"><vh>ErasedType.accept</vh></v>
</v>
<v t="ekr.20220525082935.1438"><vh>class DeletedType</vh>
<v t="ekr.20220525082935.1439"><vh>DeletedType.__init__</vh></v>
<v t="ekr.20220525082935.1440"><vh>DeletedType.accept</vh></v>
<v t="ekr.20220525082935.1441"><vh>DeletedType.serialize</vh></v>
<v t="ekr.20220525082935.1442"><vh>DeletedType.deserialize</vh></v>
</v>
<v t="ekr.20220525082935.1443"><vh>Fake TypeInfo to be used as a placeholder during Instance de-serialization.</vh></v>
<v t="ekr.20220525082935.1444"><vh>class Instance</vh>
<v t="ekr.20220525082935.1445"><vh>Instance.__init__</vh></v>
<v t="ekr.20220525082935.1446"><vh>Instance.accept</vh></v>
<v t="ekr.20220525082935.1447"><vh>Instance.__hash__</vh></v>
<v t="ekr.20220525082935.1448"><vh>Instance.__eq__</vh></v>
<v t="ekr.20220525082935.1449"><vh>Instance.serialize</vh></v>
<v t="ekr.20220525082935.1450"><vh>Instance.deserialize</vh></v>
<v t="ekr.20220525082935.1451"><vh>Instance.copy_modified</vh></v>
<v t="ekr.20220525082935.1452"><vh>Instance.has_readable_member</vh></v>
</v>
<v t="ekr.20220525082935.1453"><vh>class FunctionLike</vh>
<v t="ekr.20220525082935.1454"><vh>FunctionLike.__init__</vh></v>
<v t="ekr.20220525082935.1455"><vh>FunctionLike.is_type_obj</vh></v>
<v t="ekr.20220525082935.1456"><vh>FunctionLike.type_object</vh></v>
<v t="ekr.20220525082935.1457"><vh>FunctionLike.items</vh></v>
<v t="ekr.20220525082935.1458"><vh>FunctionLike.with_name</vh></v>
<v t="ekr.20220525082935.1459"><vh>FunctionLike.get_name</vh></v>
</v>
<v t="ekr.20220525082935.1460"><vh>class FormalArgument</vh></v>
<v t="ekr.20220525082935.1461"><vh>class Parameters</vh>
<v t="ekr.20220525082935.1462"><vh>Parameters.__init__</vh></v>
<v t="ekr.20220525082935.1463"><vh>Parameters.copy_modified</vh></v>
<v t="ekr.20220525082935.1464"><vh>Parameters.var_arg</vh></v>
<v t="ekr.20220525082935.1465"><vh>Parameters.kw_arg</vh></v>
<v t="ekr.20220525082935.1466"><vh>Parameters.formal_arguments</vh></v>
<v t="ekr.20220525082935.1467"><vh>Parameters.argument_by_name</vh></v>
<v t="ekr.20220525082935.1468"><vh>Parameters.argument_by_position</vh></v>
<v t="ekr.20220525082935.1469"><vh>Parameters.try_synthesizing_arg_from_kwarg</vh></v>
<v t="ekr.20220525082935.1470"><vh>Parameters.try_synthesizing_arg_from_vararg</vh></v>
<v t="ekr.20220525082935.1471"><vh>Parameters.accept</vh></v>
<v t="ekr.20220525082935.1472"><vh>Parameters.serialize</vh></v>
<v t="ekr.20220525082935.1473"><vh>Parameters.deserialize</vh></v>
<v t="ekr.20220525082935.1474"><vh>Parameters.__hash__</vh></v>
<v t="ekr.20220525082935.1475"><vh>Parameters.__eq__</vh></v>
</v>
<v t="ekr.20220525082935.1476"><vh>class CallableType</vh>
<v t="ekr.20220525082935.1477"><vh>CallableType.__init__</vh></v>
<v t="ekr.20220525082935.1478"><vh>CallableType.copy_modified</vh></v>
<v t="ekr.20220525082935.1479"><vh>CallableType.var_arg</vh></v>
<v t="ekr.20220525082935.1480"><vh>CallableType.kw_arg</vh></v>
<v t="ekr.20220525082935.1481"><vh>CallableType.is_var_arg</vh></v>
<v t="ekr.20220525082935.1482"><vh>CallableType.is_kw_arg</vh></v>
<v t="ekr.20220525082935.1483"><vh>CallableType.is_type_obj</vh></v>
<v t="ekr.20220525082935.1484"><vh>CallableType.type_object</vh></v>
<v t="ekr.20220525082935.1485"><vh>CallableType.accept</vh></v>
<v t="ekr.20220525082935.1486"><vh>CallableType.with_name</vh></v>
<v t="ekr.20220525082935.1487"><vh>CallableType.get_name</vh></v>
<v t="ekr.20220525082935.1488"><vh>CallableType.max_possible_positional_args</vh></v>
<v t="ekr.20220525082935.1489"><vh>CallableType.formal_arguments</vh></v>
<v t="ekr.20220525082935.1490"><vh>CallableType.argument_by_name</vh></v>
<v t="ekr.20220525082935.1491"><vh>CallableType.argument_by_position</vh></v>
<v t="ekr.20220525082935.1492"><vh>CallableType.try_synthesizing_arg_from_kwarg</vh></v>
<v t="ekr.20220525082935.1493"><vh>CallableType.try_synthesizing_arg_from_vararg</vh></v>
<v t="ekr.20220525082935.1494"><vh>CallableType.items</vh></v>
<v t="ekr.20220525082935.1495"><vh>CallableType.is_generic</vh></v>
<v t="ekr.20220525082935.1496"><vh>CallableType.type_var_ids</vh></v>
<v t="ekr.20220525082935.1497"><vh>CallableType.param_spec</vh></v>
<v t="ekr.20220525082935.1498"><vh>CallableType.expand_param_spec</vh></v>
<v t="ekr.20220525082935.1499"><vh>CallableType.__hash__</vh></v>
<v t="ekr.20220525082935.1500"><vh>CallableType.__eq__</vh></v>
<v t="ekr.20220525082935.1501"><vh>CallableType.serialize</vh></v>
<v t="ekr.20220525082935.1502"><vh>CallableType.deserialize</vh></v>
</v>
<v t="ekr.20220525082935.1503"><vh>class Overloaded</vh>
<v t="ekr.20220525082935.1504"><vh>Overloaded.__init__</vh></v>
<v t="ekr.20220525082935.1505"><vh>Overloaded.items</vh></v>
<v t="ekr.20220525082935.1506"><vh>Overloaded.name</vh></v>
<v t="ekr.20220525082935.1507"><vh>Overloaded.is_type_obj</vh></v>
<v t="ekr.20220525082935.1508"><vh>Overloaded.type_object</vh></v>
<v t="ekr.20220525082935.1509"><vh>Overloaded.with_name</vh></v>
<v t="ekr.20220525082935.1510"><vh>Overloaded.get_name</vh></v>
<v t="ekr.20220525082935.1511"><vh>Overloaded.accept</vh></v>
<v t="ekr.20220525082935.1512"><vh>Overloaded.__hash__</vh></v>
<v t="ekr.20220525082935.1513"><vh>Overloaded.__eq__</vh></v>
<v t="ekr.20220525082935.1514"><vh>Overloaded.serialize</vh></v>
<v t="ekr.20220525082935.1515"><vh>Overloaded.deserialize</vh></v>
</v>
<v t="ekr.20220525082935.1516"><vh>class TupleType</vh>
<v t="ekr.20220525082935.1517"><vh>TupleType.__init__</vh></v>
<v t="ekr.20220525082935.1518"><vh>TupleType.can_be_true_default</vh></v>
<v t="ekr.20220525082935.1519"><vh>TupleType.can_be_false_default</vh></v>
<v t="ekr.20220525082935.1520"><vh>TupleType.can_be_any_bool</vh></v>
<v t="ekr.20220525082935.1521"><vh>TupleType.length</vh></v>
<v t="ekr.20220525082935.1522"><vh>TupleType.accept</vh></v>
<v t="ekr.20220525082935.1523"><vh>TupleType.__hash__</vh></v>
<v t="ekr.20220525082935.1524"><vh>TupleType.__eq__</vh></v>
<v t="ekr.20220525082935.1525"><vh>TupleType.serialize</vh></v>
<v t="ekr.20220525082935.1526"><vh>TupleType.deserialize</vh></v>
<v t="ekr.20220525082935.1527"><vh>TupleType.copy_modified</vh></v>
<v t="ekr.20220525082935.1528"><vh>TupleType.slice</vh></v>
</v>
<v t="ekr.20220525082935.1529"><vh>class TypedDictType</vh>
<v t="ekr.20220525082935.1530"><vh>TypedDictType.__init__</vh></v>
<v t="ekr.20220525082935.1531"><vh>TypedDictType.accept</vh></v>
<v t="ekr.20220525082935.1532"><vh>TypedDictType.__hash__</vh></v>
<v t="ekr.20220525082935.1533"><vh>TypedDictType.__eq__</vh></v>
<v t="ekr.20220525082935.1534"><vh>TypedDictType.serialize</vh></v>
<v t="ekr.20220525082935.1535"><vh>TypedDictType.deserialize</vh></v>
<v t="ekr.20220525082935.1536"><vh>TypedDictType.is_anonymous</vh></v>
<v t="ekr.20220525082935.1537"><vh>TypedDictType.as_anonymous</vh></v>
<v t="ekr.20220525082935.1538"><vh>TypedDictType.copy_modified</vh></v>
<v t="ekr.20220525082935.1539"><vh>TypedDictType.create_anonymous_fallback</vh></v>
<v t="ekr.20220525082935.1540"><vh>TypedDictType.names_are_wider_than</vh></v>
<v t="ekr.20220525082935.1541"><vh>TypedDictType.zip</vh></v>
<v t="ekr.20220525082935.1542"><vh>TypedDictType.zipall</vh></v>
</v>
<v t="ekr.20220525082935.1543"><vh>class RawExpressionType</vh>
<v t="ekr.20220525082935.1544"><vh>RawExpressionType.__init__</vh></v>
<v t="ekr.20220525082935.1545"><vh>RawExpressionType.simple_name</vh></v>
<v t="ekr.20220525082935.1546"><vh>RawExpressionType.accept</vh></v>
<v t="ekr.20220525082935.1547"><vh>RawExpressionType.serialize</vh></v>
<v t="ekr.20220525082935.1548"><vh>RawExpressionType.__hash__</vh></v>
<v t="ekr.20220525082935.1549"><vh>RawExpressionType.__eq__</vh></v>
</v>
<v t="ekr.20220525082935.1550"><vh>class LiteralType</vh>
<v t="ekr.20220525082935.1551"><vh>LiteralType.__init__</vh></v>
<v t="ekr.20220525082935.1552"><vh>LiteralType.can_be_false_default</vh></v>
<v t="ekr.20220525082935.1553"><vh>LiteralType.can_be_true_default</vh></v>
<v t="ekr.20220525082935.1554"><vh>LiteralType.accept</vh></v>
<v t="ekr.20220525082935.1555"><vh>LiteralType.__hash__</vh></v>
<v t="ekr.20220525082935.1556"><vh>LiteralType.__eq__</vh></v>
<v t="ekr.20220525082935.1557"><vh>LiteralType.is_enum_literal</vh></v>
<v t="ekr.20220525082935.1558"><vh>LiteralType.value_repr</vh></v>
<v t="ekr.20220525082935.1559"><vh>LiteralType.serialize</vh></v>
<v t="ekr.20220525082935.1560"><vh>LiteralType.deserialize</vh></v>
</v>
<v t="ekr.20220525082935.1561"><vh>class StarType</vh>
<v t="ekr.20220525082935.1562"><vh>StarType.__init__</vh></v>
<v t="ekr.20220525082935.1563"><vh>StarType.accept</vh></v>
<v t="ekr.20220525082935.1564"><vh>StarType.serialize</vh></v>
</v>
<v t="ekr.20220525082935.1565"><vh>class UnionType</vh>
<v t="ekr.20220525082935.1566"><vh>UnionType.__init__</vh></v>
<v t="ekr.20220525082935.1567"><vh>UnionType.__hash__</vh></v>
<v t="ekr.20220525082935.1568"><vh>UnionType.__eq__</vh></v>
<v t="ekr.20220525082935.1569"><vh>UnionType.make_union</vh></v>
<v t="ekr.20220525082935.1570"><vh>UnionType.make_union</vh></v>
<v t="ekr.20220525082935.1571"><vh>UnionType.make_union</vh></v>
<v t="ekr.20220525082935.1572"><vh>UnionType.length</vh></v>
<v t="ekr.20220525082935.1573"><vh>UnionType.accept</vh></v>
<v t="ekr.20220525082935.1574"><vh>UnionType.has_readable_member</vh></v>
<v t="ekr.20220525082935.1575"><vh>UnionType.relevant_items</vh></v>
<v t="ekr.20220525082935.1576"><vh>UnionType.serialize</vh></v>
<v t="ekr.20220525082935.1577"><vh>UnionType.deserialize</vh></v>
</v>
<v t="ekr.20220525082935.1578"><vh>class PartialType</vh>
<v t="ekr.20220525082935.1579"><vh>PartialType.__init__</vh></v>
<v t="ekr.20220525082935.1580"><vh>PartialType.accept</vh></v>
</v>
<v t="ekr.20220525082935.1581"><vh>class EllipsisType</vh>
<v t="ekr.20220525082935.1582"><vh>EllipsisType.accept</vh></v>
<v t="ekr.20220525082935.1583"><vh>EllipsisType.serialize</vh></v>
</v>
<v t="ekr.20220525082935.1584"><vh>class TypeType</vh>
<v t="ekr.20220525082935.1585"><vh>TypeType.__init__</vh></v>
<v t="ekr.20220525082935.1586"><vh>TypeType.make_normalized</vh></v>
<v t="ekr.20220525082935.1587"><vh>TypeType.accept</vh></v>
<v t="ekr.20220525082935.1588"><vh>TypeType.__hash__</vh></v>
<v t="ekr.20220525082935.1589"><vh>TypeType.__eq__</vh></v>
<v t="ekr.20220525082935.1590"><vh>TypeType.serialize</vh></v>
<v t="ekr.20220525082935.1591"><vh>TypeType.deserialize</vh></v>
</v>
<v t="ekr.20220525082935.1592"><vh>class PlaceholderType</vh>
<v t="ekr.20220525082935.1593"><vh>PlaceholderType.__init__</vh></v>
<v t="ekr.20220525082935.1594"><vh>PlaceholderType.accept</vh></v>
<v t="ekr.20220525082935.1595"><vh>PlaceholderType.serialize</vh></v>
</v>
<v t="ekr.20220525082935.1596"><vh>get_proper_type</vh></v>
<v t="ekr.20220525082935.1597"><vh>get_proper_type</vh></v>
<v t="ekr.20220525082935.1598"><vh>get_proper_type</vh></v>
<v t="ekr.20220525082935.1599"><vh>get_proper_types</vh></v>
<v t="ekr.20220525082935.1600"><vh>get_proper_types</vh></v>
<v t="ekr.20220525082935.1601"><vh>get_proper_types</vh></v>
<v t="ekr.20220525082935.1602"><vh>We split off the type visitor base classes to another module</vh></v>
<v t="ekr.20220525082935.1603"><vh>class TypeStrVisitor</vh>
<v t="ekr.20220525082935.1604"><vh>TypeStrVisitor.__init__</vh></v>
<v t="ekr.20220525082935.1605"><vh>TypeStrVisitor.visit_unbound_type</vh></v>
<v t="ekr.20220525082935.1606"><vh>TypeStrVisitor.visit_type_list</vh></v>
<v t="ekr.20220525082935.1607"><vh>TypeStrVisitor.visit_callable_argument</vh></v>
<v t="ekr.20220525082935.1608"><vh>TypeStrVisitor.visit_any</vh></v>
<v t="ekr.20220525082935.1609"><vh>TypeStrVisitor.visit_none_type</vh></v>
<v t="ekr.20220525082935.1610"><vh>TypeStrVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082935.1611"><vh>TypeStrVisitor.visit_erased_type</vh></v>
<v t="ekr.20220525082935.1612"><vh>TypeStrVisitor.visit_deleted_type</vh></v>
<v t="ekr.20220525082935.1613"><vh>TypeStrVisitor.visit_instance</vh></v>
<v t="ekr.20220525082935.1614"><vh>TypeStrVisitor.visit_type_var</vh></v>
<v t="ekr.20220525082935.1615"><vh>TypeStrVisitor.visit_param_spec</vh></v>
<v t="ekr.20220525082935.1616"><vh>TypeStrVisitor.visit_parameters</vh></v>
<v t="ekr.20220525082935.1617"><vh>TypeStrVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082935.1618"><vh>TypeStrVisitor.visit_callable_type</vh></v>
<v t="ekr.20220525082935.1619"><vh>TypeStrVisitor.visit_overloaded</vh></v>
<v t="ekr.20220525082935.1620"><vh>TypeStrVisitor.visit_tuple_type</vh></v>
<v t="ekr.20220525082935.1621"><vh>TypeStrVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20220525082935.1622"><vh>TypeStrVisitor.visit_raw_expression_type</vh></v>
<v t="ekr.20220525082935.1623"><vh>TypeStrVisitor.visit_literal_type</vh></v>
<v t="ekr.20220525082935.1624"><vh>TypeStrVisitor.visit_star_type</vh></v>
<v t="ekr.20220525082935.1625"><vh>TypeStrVisitor.visit_union_type</vh></v>
<v t="ekr.20220525082935.1626"><vh>TypeStrVisitor.visit_partial_type</vh></v>
<v t="ekr.20220525082935.1627"><vh>TypeStrVisitor.visit_ellipsis_type</vh></v>
<v t="ekr.20220525082935.1628"><vh>TypeStrVisitor.visit_type_type</vh></v>
<v t="ekr.20220525082935.1629"><vh>TypeStrVisitor.visit_placeholder_type</vh></v>
<v t="ekr.20220525082935.1630"><vh>TypeStrVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20220525082935.1631"><vh>TypeStrVisitor.visit_unpack_type</vh></v>
<v t="ekr.20220525082935.1632"><vh>TypeStrVisitor.list_str</vh></v>
</v>
<v t="ekr.20220525082935.1633"><vh>class UnrollAliasVisitor</vh>
<v t="ekr.20220525082935.1634"><vh>UnrollAliasVisitor.__init__</vh></v>
<v t="ekr.20220525082935.1635"><vh>UnrollAliasVisitor.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20220525082935.1636"><vh>strip_type</vh></v>
<v t="ekr.20220525082935.1637"><vh>is_named_instance</vh></v>
<v t="ekr.20220525082935.1638"><vh>class InstantiateAliasVisitor</vh>
<v t="ekr.20220525082935.1639"><vh>InstantiateAliasVisitor.__init__</vh></v>
<v t="ekr.20220525082935.1640"><vh>InstantiateAliasVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20220525082935.1641"><vh>InstantiateAliasVisitor.visit_unbound_type</vh></v>
<v t="ekr.20220525082935.1642"><vh>InstantiateAliasVisitor.visit_type_var</vh></v>
</v>
<v t="ekr.20220525082935.1643"><vh>replace_alias_tvars</vh></v>
<v t="ekr.20220525082935.1644"><vh>class HasTypeVars</vh></v>
<v t="ekr.20220525082935.1645"><vh>has_type_vars</vh></v>
<v t="ekr.20220525082935.1646"><vh>flatten_nested_unions</vh></v>
<v t="ekr.20220525082935.1647"><vh>union_items</vh></v>
<v t="ekr.20220525082935.1648"><vh>is_union_with_any</vh></v>
<v t="ekr.20220525082935.1649"><vh>is_generic_instance</vh></v>
<v t="ekr.20220525082935.1650"><vh>is_optional</vh></v>
<v t="ekr.20220525082935.1651"><vh>remove_optional</vh></v>
<v t="ekr.20220525082935.1652"><vh>is_literal_type</vh></v>
<v t="ekr.20220525082935.1653"><vh>names: Final = globals().copy()</vh></v>
<v t="ekr.20220525082935.1654"><vh>callable_with_ellipsis</vh></v>
</v>
<v t="ekr.20220525082935.1655"><vh>@clean typestate.py</vh>
<v t="ekr.20220525082935.1656"><vh>class TypeState</vh>
<v t="ekr.20220525082935.1657"><vh>TypeState.is_assumed_subtype</vh></v>
<v t="ekr.20220525082935.1658"><vh>TypeState.is_assumed_proper_subtype</vh></v>
<v t="ekr.20220525082935.1659"><vh>TypeState.reset_all_subtype_caches</vh></v>
<v t="ekr.20220525082935.1660"><vh>TypeState.reset_subtype_caches_for</vh></v>
<v t="ekr.20220525082935.1661"><vh>TypeState.reset_all_subtype_caches_for</vh></v>
<v t="ekr.20220525082935.1662"><vh>TypeState.is_cached_subtype_check</vh></v>
<v t="ekr.20220525082935.1663"><vh>TypeState.record_subtype_cache_entry</vh></v>
<v t="ekr.20220525082935.1664"><vh>TypeState.reset_protocol_deps</vh></v>
<v t="ekr.20220525082935.1665"><vh>TypeState.record_protocol_subtype_check</vh></v>
<v t="ekr.20220525082935.1666"><vh>TypeState._snapshot_protocol_deps</vh></v>
<v t="ekr.20220525082935.1667"><vh>TypeState.update_protocol_deps</vh></v>
<v t="ekr.20220525082935.1668"><vh>TypeState.add_all_protocol_deps</vh></v>
</v>
<v t="ekr.20220525082935.1669"><vh>reset_global_state</vh></v>
</v>
<v t="ekr.20220525082935.1670"><vh>@clean typetraverser.py</vh>
<v t="ekr.20220525082935.1671"><vh>class TypeTraverserVisitor</vh>
<v t="ekr.20220525082935.1672"><vh>TypeTraverserVisitor.visit_any</vh></v>
<v t="ekr.20220525082935.1673"><vh>TypeTraverserVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082935.1674"><vh>TypeTraverserVisitor.visit_none_type</vh></v>
<v t="ekr.20220525082935.1675"><vh>TypeTraverserVisitor.visit_erased_type</vh></v>
<v t="ekr.20220525082935.1676"><vh>TypeTraverserVisitor.visit_deleted_type</vh></v>
<v t="ekr.20220525082935.1677"><vh>TypeTraverserVisitor.visit_type_var</vh></v>
<v t="ekr.20220525082935.1678"><vh>TypeTraverserVisitor.visit_param_spec</vh></v>
<v t="ekr.20220525082935.1679"><vh>TypeTraverserVisitor.visit_parameters</vh></v>
<v t="ekr.20220525082935.1680"><vh>TypeTraverserVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082935.1681"><vh>TypeTraverserVisitor.visit_literal_type</vh></v>
<v t="ekr.20220525082935.1682"><vh>TypeTraverserVisitor.Composite types</vh></v>
<v t="ekr.20220525082935.1683"><vh>TypeTraverserVisitor.visit_instance</vh></v>
<v t="ekr.20220525082935.1684"><vh>TypeTraverserVisitor.visit_callable_type</vh></v>
<v t="ekr.20220525082935.1685"><vh>TypeTraverserVisitor.visit_tuple_type</vh></v>
<v t="ekr.20220525082935.1686"><vh>TypeTraverserVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20220525082935.1687"><vh>TypeTraverserVisitor.visit_union_type</vh></v>
<v t="ekr.20220525082935.1688"><vh>TypeTraverserVisitor.visit_overloaded</vh></v>
<v t="ekr.20220525082935.1689"><vh>TypeTraverserVisitor.visit_type_type</vh></v>
<v t="ekr.20220525082935.1690"><vh>TypeTraverserVisitor.Special types (not real types)</vh></v>
<v t="ekr.20220525082935.1691"><vh>TypeTraverserVisitor.visit_callable_argument</vh></v>
<v t="ekr.20220525082935.1692"><vh>TypeTraverserVisitor.visit_unbound_type</vh></v>
<v t="ekr.20220525082935.1693"><vh>TypeTraverserVisitor.visit_type_list</vh></v>
<v t="ekr.20220525082935.1694"><vh>TypeTraverserVisitor.visit_star_type</vh></v>
<v t="ekr.20220525082935.1695"><vh>TypeTraverserVisitor.visit_ellipsis_type</vh></v>
<v t="ekr.20220525082935.1696"><vh>TypeTraverserVisitor.visit_placeholder_type</vh></v>
<v t="ekr.20220525082935.1697"><vh>TypeTraverserVisitor.visit_partial_type</vh></v>
<v t="ekr.20220525082935.1698"><vh>TypeTraverserVisitor.visit_raw_expression_type</vh></v>
<v t="ekr.20220525082935.1699"><vh>TypeTraverserVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20220525082935.1700"><vh>TypeTraverserVisitor.visit_unpack_type</vh></v>
<v t="ekr.20220525082935.1701"><vh>TypeTraverserVisitor.Helpers</vh></v>
<v t="ekr.20220525082935.1702"><vh>TypeTraverserVisitor.traverse_types</vh></v>
</v>
</v>
<v t="ekr.20220525082935.1703"><vh>@clean typevars.py</vh>
<v t="ekr.20220525082936.1"><vh>fill_typevars</vh></v>
<v t="ekr.20220525082936.2"><vh>fill_typevars_with_any</vh></v>
<v t="ekr.20220525082936.3"><vh>has_no_typevars</vh></v>
</v>
<v t="ekr.20220525082936.4"><vh>@clean type_visitor.py</vh>
<v t="ekr.20220525082936.5"><vh>class TypeVisitor</vh>
<v t="ekr.20220525082936.6"><vh>TypeVisitor.visit_unbound_type</vh></v>
<v t="ekr.20220525082936.7"><vh>TypeVisitor.visit_any</vh></v>
<v t="ekr.20220525082936.8"><vh>TypeVisitor.visit_none_type</vh></v>
<v t="ekr.20220525082936.9"><vh>TypeVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082936.10"><vh>TypeVisitor.visit_erased_type</vh></v>
<v t="ekr.20220525082936.11"><vh>TypeVisitor.visit_deleted_type</vh></v>
<v t="ekr.20220525082936.12"><vh>TypeVisitor.visit_type_var</vh></v>
<v t="ekr.20220525082936.13"><vh>TypeVisitor.visit_param_spec</vh></v>
<v t="ekr.20220525082936.14"><vh>TypeVisitor.visit_parameters</vh></v>
<v t="ekr.20220525082936.15"><vh>TypeVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082936.16"><vh>TypeVisitor.visit_instance</vh></v>
<v t="ekr.20220525082936.17"><vh>TypeVisitor.visit_callable_type</vh></v>
<v t="ekr.20220525082936.18"><vh>TypeVisitor.visit_overloaded</vh></v>
<v t="ekr.20220525082936.19"><vh>TypeVisitor.visit_tuple_type</vh></v>
<v t="ekr.20220525082936.20"><vh>TypeVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20220525082936.21"><vh>TypeVisitor.visit_literal_type</vh></v>
<v t="ekr.20220525082936.22"><vh>TypeVisitor.visit_union_type</vh></v>
<v t="ekr.20220525082936.23"><vh>TypeVisitor.visit_partial_type</vh></v>
<v t="ekr.20220525082936.24"><vh>TypeVisitor.visit_type_type</vh></v>
<v t="ekr.20220525082936.25"><vh>TypeVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20220525082936.26"><vh>TypeVisitor.visit_unpack_type</vh></v>
</v>
<v t="ekr.20220525082936.27"><vh>class SyntheticTypeVisitor</vh>
<v t="ekr.20220525082936.28"><vh>SyntheticTypeVisitor.visit_star_type</vh></v>
<v t="ekr.20220525082936.29"><vh>SyntheticTypeVisitor.visit_type_list</vh></v>
<v t="ekr.20220525082936.30"><vh>SyntheticTypeVisitor.visit_callable_argument</vh></v>
<v t="ekr.20220525082936.31"><vh>SyntheticTypeVisitor.visit_ellipsis_type</vh></v>
<v t="ekr.20220525082936.32"><vh>SyntheticTypeVisitor.visit_raw_expression_type</vh></v>
<v t="ekr.20220525082936.33"><vh>SyntheticTypeVisitor.visit_placeholder_type</vh></v>
</v>
<v t="ekr.20220525082936.34"><vh>class TypeTranslator</vh>
<v t="ekr.20220525082936.35"><vh>TypeTranslator.visit_unbound_type</vh></v>
<v t="ekr.20220525082936.36"><vh>TypeTranslator.visit_any</vh></v>
<v t="ekr.20220525082936.37"><vh>TypeTranslator.visit_none_type</vh></v>
<v t="ekr.20220525082936.38"><vh>TypeTranslator.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082936.39"><vh>TypeTranslator.visit_erased_type</vh></v>
<v t="ekr.20220525082936.40"><vh>TypeTranslator.visit_deleted_type</vh></v>
<v t="ekr.20220525082936.41"><vh>TypeTranslator.visit_instance</vh></v>
<v t="ekr.20220525082936.42"><vh>TypeTranslator.visit_type_var</vh></v>
<v t="ekr.20220525082936.43"><vh>TypeTranslator.visit_param_spec</vh></v>
<v t="ekr.20220525082936.44"><vh>TypeTranslator.visit_parameters</vh></v>
<v t="ekr.20220525082936.45"><vh>TypeTranslator.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082936.46"><vh>TypeTranslator.visit_partial_type</vh></v>
<v t="ekr.20220525082936.47"><vh>TypeTranslator.visit_unpack_type</vh></v>
<v t="ekr.20220525082936.48"><vh>TypeTranslator.visit_callable_type</vh></v>
<v t="ekr.20220525082936.49"><vh>TypeTranslator.visit_tuple_type</vh></v>
<v t="ekr.20220525082936.50"><vh>TypeTranslator.visit_typeddict_type</vh></v>
<v t="ekr.20220525082936.51"><vh>TypeTranslator.visit_literal_type</vh></v>
<v t="ekr.20220525082936.52"><vh>TypeTranslator.visit_union_type</vh></v>
<v t="ekr.20220525082936.53"><vh>TypeTranslator.translate_types</vh></v>
<v t="ekr.20220525082936.54"><vh>TypeTranslator.translate_variables</vh></v>
<v t="ekr.20220525082936.55"><vh>TypeTranslator.visit_overloaded</vh></v>
<v t="ekr.20220525082936.56"><vh>TypeTranslator.visit_type_type</vh></v>
<v t="ekr.20220525082936.57"><vh>TypeTranslator.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20220525082936.58"><vh>class TypeQuery</vh>
<v t="ekr.20220525082936.59"><vh>TypeQuery.__init__</vh></v>
<v t="ekr.20220525082936.60"><vh>TypeQuery.visit_unbound_type</vh></v>
<v t="ekr.20220525082936.61"><vh>TypeQuery.visit_type_list</vh></v>
<v t="ekr.20220525082936.62"><vh>TypeQuery.visit_callable_argument</vh></v>
<v t="ekr.20220525082936.63"><vh>TypeQuery.visit_any</vh></v>
<v t="ekr.20220525082936.64"><vh>TypeQuery.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082936.65"><vh>TypeQuery.visit_none_type</vh></v>
<v t="ekr.20220525082936.66"><vh>TypeQuery.visit_erased_type</vh></v>
<v t="ekr.20220525082936.67"><vh>TypeQuery.visit_deleted_type</vh></v>
<v t="ekr.20220525082936.68"><vh>TypeQuery.visit_type_var</vh></v>
<v t="ekr.20220525082936.69"><vh>TypeQuery.visit_param_spec</vh></v>
<v t="ekr.20220525082936.70"><vh>TypeQuery.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082936.71"><vh>TypeQuery.visit_unpack_type</vh></v>
<v t="ekr.20220525082936.72"><vh>TypeQuery.visit_parameters</vh></v>
<v t="ekr.20220525082936.73"><vh>TypeQuery.visit_partial_type</vh></v>
<v t="ekr.20220525082936.74"><vh>TypeQuery.visit_instance</vh></v>
<v t="ekr.20220525082936.75"><vh>TypeQuery.visit_callable_type</vh></v>
<v t="ekr.20220525082936.76"><vh>TypeQuery.visit_tuple_type</vh></v>
<v t="ekr.20220525082936.77"><vh>TypeQuery.visit_typeddict_type</vh></v>
<v t="ekr.20220525082936.78"><vh>TypeQuery.visit_raw_expression_type</vh></v>
<v t="ekr.20220525082936.79"><vh>TypeQuery.visit_literal_type</vh></v>
<v t="ekr.20220525082936.80"><vh>TypeQuery.visit_star_type</vh></v>
<v t="ekr.20220525082936.81"><vh>TypeQuery.visit_union_type</vh></v>
<v t="ekr.20220525082936.82"><vh>TypeQuery.visit_overloaded</vh></v>
<v t="ekr.20220525082936.83"><vh>TypeQuery.visit_type_type</vh></v>
<v t="ekr.20220525082936.84"><vh>TypeQuery.visit_ellipsis_type</vh></v>
<v t="ekr.20220525082936.85"><vh>TypeQuery.visit_placeholder_type</vh></v>
<v t="ekr.20220525082936.86"><vh>TypeQuery.visit_type_alias_type</vh></v>
<v t="ekr.20220525082936.87"><vh>TypeQuery.query_types</vh></v>
</v>
</v>
<v t="ekr.20220525082936.88"><vh>@clean util.py</vh>
<v t="ekr.20220525082936.89"><vh>is_dunder</vh></v>
<v t="ekr.20220525082936.90"><vh>is_sunder</vh></v>
<v t="ekr.20220525082936.91"><vh>split_module_names</vh></v>
<v t="ekr.20220525082936.92"><vh>module_prefix</vh></v>
<v t="ekr.20220525082936.93"><vh>split_target</vh></v>
<v t="ekr.20220525082936.94"><vh>short_type</vh></v>
<v t="ekr.20220525082936.95"><vh>find_python_encoding</vh></v>
<v t="ekr.20220525082936.96"><vh>bytes_to_human_readable_repr</vh></v>
<v t="ekr.20220525082936.97"><vh>class DecodeError</vh></v>
<v t="ekr.20220525082936.98"><vh>decode_python_encoding</vh></v>
<v t="ekr.20220525082936.99"><vh>read_py_file</vh></v>
<v t="ekr.20220525082936.100"><vh>trim_source_line</vh></v>
<v t="ekr.20220525082936.101"><vh>get_mypy_comments</vh></v>
<v t="ekr.20220525082936.102"><vh>_python2_interpreter: Optional[str] = None</vh></v>
<v t="ekr.20220525082936.103"><vh>try_find_python2_interpreter</vh></v>
<v t="ekr.20220525082936.104"><vh>PASS_TEMPLATE: Final = """&lt;?xml version="1.0" encoding="utf-8"?&gt;</vh></v>
<v t="ekr.20220525082936.105"><vh>write_junit_xml</vh></v>
<v t="ekr.20220525082936.106"><vh>class IdMapper</vh>
<v t="ekr.20220525082936.107"><vh>IdMapper.__init__</vh></v>
<v t="ekr.20220525082936.108"><vh>IdMapper.id</vh></v>
</v>
<v t="ekr.20220525082936.109"><vh>get_prefix</vh></v>
<v t="ekr.20220525082936.110"><vh>get_top_two_prefixes</vh></v>
<v t="ekr.20220525082936.111"><vh>correct_relative_import</vh></v>
<v t="ekr.20220525082936.112"><vh>fields_cache: Final[Dict[Type[object], List[str]]] = {}</vh></v>
<v t="ekr.20220525082936.113"><vh>get_class_descriptors</vh></v>
<v t="ekr.20220525082936.114"><vh>replace_object_state</vh></v>
<v t="ekr.20220525082936.115"><vh>is_sub_path</vh></v>
<v t="ekr.20220525082936.116"><vh>hard_exit</vh></v>
<v t="ekr.20220525082936.117"><vh>unmangle</vh></v>
<v t="ekr.20220525082936.118"><vh>get_unique_redefinition_name</vh></v>
<v t="ekr.20220525082936.119"><vh>check_python_version</vh></v>
<v t="ekr.20220525082936.120"><vh>count_stats</vh></v>
<v t="ekr.20220525082936.121"><vh>split_words</vh></v>
<v t="ekr.20220525082936.122"><vh>get_terminal_width</vh></v>
<v t="ekr.20220525082936.123"><vh>soft_wrap</vh></v>
<v t="ekr.20220525082936.124"><vh>hash_digest</vh></v>
<v t="ekr.20220525082936.125"><vh>parse_gray_color</vh></v>
<v t="ekr.20220525082936.126"><vh>class FancyFormatter</vh>
<v t="ekr.20220525082936.127"><vh>FancyFormatter.__init__</vh></v>
<v t="ekr.20220525082936.128"><vh>FancyFormatter.initialize_win_colors</vh></v>
<v t="ekr.20220525082936.129"><vh>FancyFormatter.initialize_unix_colors</vh></v>
<v t="ekr.20220525082936.130"><vh>FancyFormatter.style</vh></v>
<v t="ekr.20220525082936.131"><vh>FancyFormatter.fit_in_terminal</vh></v>
<v t="ekr.20220525082936.132"><vh>FancyFormatter.colorize</vh></v>
<v t="ekr.20220525082936.133"><vh>FancyFormatter.highlight_quote_groups</vh></v>
<v t="ekr.20220525082936.134"><vh>FancyFormatter.underline_link</vh></v>
<v t="ekr.20220525082936.135"><vh>FancyFormatter.format_success</vh></v>
<v t="ekr.20220525082936.136"><vh>FancyFormatter.format_error</vh></v>
</v>
<v t="ekr.20220525082936.137"><vh>is_typeshed_file</vh></v>
<v t="ekr.20220525082936.138"><vh>is_stub_package_file</vh></v>
<v t="ekr.20220525082936.139"><vh>unnamed_function</vh></v>
<v t="ekr.20220525082936.140"><vh>TODO: replace with uses of perf_counter_ns when support for py3.6 is dropped</vh></v>
<v t="ekr.20220525082936.141"><vh>time_spent_us</vh></v>
</v>
<v t="ekr.20220525082936.142"><vh>@clean version.py</vh></v>
<v t="ekr.20220525082936.143"><vh>@clean visitor.py</vh>
<v t="ekr.20220525082936.144"><vh>class ExpressionVisitor</vh>
<v t="ekr.20220525082936.145"><vh>ExpressionVisitor.visit_int_expr</vh></v>
<v t="ekr.20220525082936.146"><vh>ExpressionVisitor.visit_str_expr</vh></v>
<v t="ekr.20220525082936.147"><vh>ExpressionVisitor.visit_bytes_expr</vh></v>
<v t="ekr.20220525082936.148"><vh>ExpressionVisitor.visit_unicode_expr</vh></v>
<v t="ekr.20220525082936.149"><vh>ExpressionVisitor.visit_float_expr</vh></v>
<v t="ekr.20220525082936.150"><vh>ExpressionVisitor.visit_complex_expr</vh></v>
<v t="ekr.20220525082936.151"><vh>ExpressionVisitor.visit_ellipsis</vh></v>
<v t="ekr.20220525082936.152"><vh>ExpressionVisitor.visit_star_expr</vh></v>
<v t="ekr.20220525082936.153"><vh>ExpressionVisitor.visit_name_expr</vh></v>
<v t="ekr.20220525082936.154"><vh>ExpressionVisitor.visit_member_expr</vh></v>
<v t="ekr.20220525082936.155"><vh>ExpressionVisitor.visit_yield_from_expr</vh></v>
<v t="ekr.20220525082936.156"><vh>ExpressionVisitor.visit_yield_expr</vh></v>
<v t="ekr.20220525082936.157"><vh>ExpressionVisitor.visit_call_expr</vh></v>
<v t="ekr.20220525082936.158"><vh>ExpressionVisitor.visit_op_expr</vh></v>
<v t="ekr.20220525082936.159"><vh>ExpressionVisitor.visit_comparison_expr</vh></v>
<v t="ekr.20220525082936.160"><vh>ExpressionVisitor.visit_cast_expr</vh></v>
<v t="ekr.20220525082936.161"><vh>ExpressionVisitor.visit_assert_type_expr</vh></v>
<v t="ekr.20220525082936.162"><vh>ExpressionVisitor.visit_reveal_expr</vh></v>
<v t="ekr.20220525082936.163"><vh>ExpressionVisitor.visit_super_expr</vh></v>
<v t="ekr.20220525082936.164"><vh>ExpressionVisitor.visit_unary_expr</vh></v>
<v t="ekr.20220525082936.165"><vh>ExpressionVisitor.visit_assignment_expr</vh></v>
<v t="ekr.20220525082936.166"><vh>ExpressionVisitor.visit_list_expr</vh></v>
<v t="ekr.20220525082936.167"><vh>ExpressionVisitor.visit_dict_expr</vh></v>
<v t="ekr.20220525082936.168"><vh>ExpressionVisitor.visit_tuple_expr</vh></v>
<v t="ekr.20220525082936.169"><vh>ExpressionVisitor.visit_set_expr</vh></v>
<v t="ekr.20220525082936.170"><vh>ExpressionVisitor.visit_index_expr</vh></v>
<v t="ekr.20220525082936.171"><vh>ExpressionVisitor.visit_type_application</vh></v>
<v t="ekr.20220525082936.172"><vh>ExpressionVisitor.visit_lambda_expr</vh></v>
<v t="ekr.20220525082936.173"><vh>ExpressionVisitor.visit_list_comprehension</vh></v>
<v t="ekr.20220525082936.174"><vh>ExpressionVisitor.visit_set_comprehension</vh></v>
<v t="ekr.20220525082936.175"><vh>ExpressionVisitor.visit_dictionary_comprehension</vh></v>
<v t="ekr.20220525082936.176"><vh>ExpressionVisitor.visit_generator_expr</vh></v>
<v t="ekr.20220525082936.177"><vh>ExpressionVisitor.visit_slice_expr</vh></v>
<v t="ekr.20220525082936.178"><vh>ExpressionVisitor.visit_conditional_expr</vh></v>
<v t="ekr.20220525082936.179"><vh>ExpressionVisitor.visit_backquote_expr</vh></v>
<v t="ekr.20220525082936.180"><vh>ExpressionVisitor.visit_type_var_expr</vh></v>
<v t="ekr.20220525082936.181"><vh>ExpressionVisitor.visit_paramspec_expr</vh></v>
<v t="ekr.20220525082936.182"><vh>ExpressionVisitor.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20220525082936.183"><vh>ExpressionVisitor.visit_type_alias_expr</vh></v>
<v t="ekr.20220525082936.184"><vh>ExpressionVisitor.visit_namedtuple_expr</vh></v>
<v t="ekr.20220525082936.185"><vh>ExpressionVisitor.visit_enum_call_expr</vh></v>
<v t="ekr.20220525082936.186"><vh>ExpressionVisitor.visit_typeddict_expr</vh></v>
<v t="ekr.20220525082936.187"><vh>ExpressionVisitor.visit_newtype_expr</vh></v>
<v t="ekr.20220525082936.188"><vh>ExpressionVisitor.visit__promote_expr</vh></v>
<v t="ekr.20220525082936.189"><vh>ExpressionVisitor.visit_await_expr</vh></v>
<v t="ekr.20220525082936.190"><vh>ExpressionVisitor.visit_temp_node</vh></v>
</v>
<v t="ekr.20220525082936.191"><vh>class StatementVisitor</vh>
<v t="ekr.20220525082936.192"><vh>StatementVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20220525082936.193"><vh>StatementVisitor.visit_for_stmt</vh></v>
<v t="ekr.20220525082936.194"><vh>StatementVisitor.visit_with_stmt</vh></v>
<v t="ekr.20220525082936.195"><vh>StatementVisitor.visit_del_stmt</vh></v>
<v t="ekr.20220525082936.196"><vh>StatementVisitor.visit_func_def</vh></v>
<v t="ekr.20220525082936.197"><vh>StatementVisitor.visit_overloaded_func_def</vh></v>
<v t="ekr.20220525082936.198"><vh>StatementVisitor.visit_class_def</vh></v>
<v t="ekr.20220525082936.199"><vh>StatementVisitor.visit_global_decl</vh></v>
<v t="ekr.20220525082936.200"><vh>StatementVisitor.visit_nonlocal_decl</vh></v>
<v t="ekr.20220525082936.201"><vh>StatementVisitor.visit_decorator</vh></v>
<v t="ekr.20220525082936.202"><vh>StatementVisitor.Module structure</vh></v>
<v t="ekr.20220525082936.203"><vh>StatementVisitor.visit_import</vh></v>
<v t="ekr.20220525082936.204"><vh>StatementVisitor.visit_import_from</vh></v>
<v t="ekr.20220525082936.205"><vh>StatementVisitor.visit_import_all</vh></v>
<v t="ekr.20220525082936.206"><vh>StatementVisitor.Statements</vh></v>
<v t="ekr.20220525082936.207"><vh>StatementVisitor.visit_block</vh></v>
<v t="ekr.20220525082936.208"><vh>StatementVisitor.visit_expression_stmt</vh></v>
<v t="ekr.20220525082936.209"><vh>StatementVisitor.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20220525082936.210"><vh>StatementVisitor.visit_while_stmt</vh></v>
<v t="ekr.20220525082936.211"><vh>StatementVisitor.visit_return_stmt</vh></v>
<v t="ekr.20220525082936.212"><vh>StatementVisitor.visit_assert_stmt</vh></v>
<v t="ekr.20220525082936.213"><vh>StatementVisitor.visit_if_stmt</vh></v>
<v t="ekr.20220525082936.214"><vh>StatementVisitor.visit_break_stmt</vh></v>
<v t="ekr.20220525082936.215"><vh>StatementVisitor.visit_continue_stmt</vh></v>
<v t="ekr.20220525082936.216"><vh>StatementVisitor.visit_pass_stmt</vh></v>
<v t="ekr.20220525082936.217"><vh>StatementVisitor.visit_raise_stmt</vh></v>
<v t="ekr.20220525082936.218"><vh>StatementVisitor.visit_try_stmt</vh></v>
<v t="ekr.20220525082936.219"><vh>StatementVisitor.visit_print_stmt</vh></v>
<v t="ekr.20220525082936.220"><vh>StatementVisitor.visit_exec_stmt</vh></v>
<v t="ekr.20220525082936.221"><vh>StatementVisitor.visit_match_stmt</vh></v>
</v>
<v t="ekr.20220525082936.222"><vh>class PatternVisitor</vh>
<v t="ekr.20220525082936.223"><vh>PatternVisitor.visit_as_pattern</vh></v>
<v t="ekr.20220525082936.224"><vh>PatternVisitor.visit_or_pattern</vh></v>
<v t="ekr.20220525082936.225"><vh>PatternVisitor.visit_value_pattern</vh></v>
<v t="ekr.20220525082936.226"><vh>PatternVisitor.visit_singleton_pattern</vh></v>
<v t="ekr.20220525082936.227"><vh>PatternVisitor.visit_sequence_pattern</vh></v>
<v t="ekr.20220525082936.228"><vh>PatternVisitor.visit_starred_pattern</vh></v>
<v t="ekr.20220525082936.229"><vh>PatternVisitor.visit_mapping_pattern</vh></v>
<v t="ekr.20220525082936.230"><vh>PatternVisitor.visit_class_pattern</vh></v>
</v>
<v t="ekr.20220525082936.231"><vh>class NodeVisitor</vh>
<v t="ekr.20220525082936.232"><vh>NodeVisitor.visit_mypy_file</vh></v>
<v t="ekr.20220525082936.233"><vh>NodeVisitor.visit_var</vh></v>
<v t="ekr.20220525082936.234"><vh>NodeVisitor.Module structure</vh></v>
<v t="ekr.20220525082936.235"><vh>NodeVisitor.visit_import</vh></v>
<v t="ekr.20220525082936.236"><vh>NodeVisitor.visit_import_from</vh></v>
<v t="ekr.20220525082936.237"><vh>NodeVisitor.visit_import_all</vh></v>
<v t="ekr.20220525082936.238"><vh>NodeVisitor.Definitions</vh></v>
<v t="ekr.20220525082936.239"><vh>NodeVisitor.visit_func_def</vh></v>
<v t="ekr.20220525082936.240"><vh>NodeVisitor.visit_overloaded_func_def</vh></v>
<v t="ekr.20220525082936.241"><vh>NodeVisitor.visit_class_def</vh></v>
<v t="ekr.20220525082936.242"><vh>NodeVisitor.visit_global_decl</vh></v>
<v t="ekr.20220525082936.243"><vh>NodeVisitor.visit_nonlocal_decl</vh></v>
<v t="ekr.20220525082936.244"><vh>NodeVisitor.visit_decorator</vh></v>
<v t="ekr.20220525082936.245"><vh>NodeVisitor.visit_type_alias</vh></v>
<v t="ekr.20220525082936.246"><vh>NodeVisitor.visit_placeholder_node</vh></v>
<v t="ekr.20220525082936.247"><vh>NodeVisitor.Statements</vh></v>
<v t="ekr.20220525082936.248"><vh>NodeVisitor.visit_block</vh></v>
<v t="ekr.20220525082936.249"><vh>NodeVisitor.visit_expression_stmt</vh></v>
<v t="ekr.20220525082936.250"><vh>NodeVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20220525082936.251"><vh>NodeVisitor.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20220525082936.252"><vh>NodeVisitor.visit_while_stmt</vh></v>
<v t="ekr.20220525082936.253"><vh>NodeVisitor.visit_for_stmt</vh></v>
<v t="ekr.20220525082936.254"><vh>NodeVisitor.visit_return_stmt</vh></v>
<v t="ekr.20220525082936.255"><vh>NodeVisitor.visit_assert_stmt</vh></v>
<v t="ekr.20220525082936.256"><vh>NodeVisitor.visit_del_stmt</vh></v>
<v t="ekr.20220525082936.257"><vh>NodeVisitor.visit_if_stmt</vh></v>
<v t="ekr.20220525082936.258"><vh>NodeVisitor.visit_break_stmt</vh></v>
<v t="ekr.20220525082936.259"><vh>NodeVisitor.visit_continue_stmt</vh></v>
<v t="ekr.20220525082936.260"><vh>NodeVisitor.visit_pass_stmt</vh></v>
<v t="ekr.20220525082936.261"><vh>NodeVisitor.visit_raise_stmt</vh></v>
<v t="ekr.20220525082936.262"><vh>NodeVisitor.visit_try_stmt</vh></v>
<v t="ekr.20220525082936.263"><vh>NodeVisitor.visit_with_stmt</vh></v>
<v t="ekr.20220525082936.264"><vh>NodeVisitor.visit_print_stmt</vh></v>
<v t="ekr.20220525082936.265"><vh>NodeVisitor.visit_exec_stmt</vh></v>
<v t="ekr.20220525082936.266"><vh>NodeVisitor.visit_match_stmt</vh></v>
<v t="ekr.20220525082936.267"><vh>NodeVisitor.Expressions (default no-op implementation)</vh></v>
<v t="ekr.20220525082936.268"><vh>NodeVisitor.visit_int_expr</vh></v>
<v t="ekr.20220525082936.269"><vh>NodeVisitor.visit_str_expr</vh></v>
<v t="ekr.20220525082936.270"><vh>NodeVisitor.visit_bytes_expr</vh></v>
<v t="ekr.20220525082936.271"><vh>NodeVisitor.visit_unicode_expr</vh></v>
<v t="ekr.20220525082936.272"><vh>NodeVisitor.visit_float_expr</vh></v>
<v t="ekr.20220525082936.273"><vh>NodeVisitor.visit_complex_expr</vh></v>
<v t="ekr.20220525082936.274"><vh>NodeVisitor.visit_ellipsis</vh></v>
<v t="ekr.20220525082936.275"><vh>NodeVisitor.visit_star_expr</vh></v>
<v t="ekr.20220525082936.276"><vh>NodeVisitor.visit_name_expr</vh></v>
<v t="ekr.20220525082936.277"><vh>NodeVisitor.visit_member_expr</vh></v>
<v t="ekr.20220525082936.278"><vh>NodeVisitor.visit_yield_from_expr</vh></v>
<v t="ekr.20220525082936.279"><vh>NodeVisitor.visit_yield_expr</vh></v>
<v t="ekr.20220525082936.280"><vh>NodeVisitor.visit_call_expr</vh></v>
<v t="ekr.20220525082936.281"><vh>NodeVisitor.visit_op_expr</vh></v>
<v t="ekr.20220525082936.282"><vh>NodeVisitor.visit_comparison_expr</vh></v>
<v t="ekr.20220525082936.283"><vh>NodeVisitor.visit_cast_expr</vh></v>
<v t="ekr.20220525082936.284"><vh>NodeVisitor.visit_assert_type_expr</vh></v>
<v t="ekr.20220525082936.285"><vh>NodeVisitor.visit_reveal_expr</vh></v>
<v t="ekr.20220525082936.286"><vh>NodeVisitor.visit_super_expr</vh></v>
<v t="ekr.20220525082936.287"><vh>NodeVisitor.visit_assignment_expr</vh></v>
<v t="ekr.20220525082936.288"><vh>NodeVisitor.visit_unary_expr</vh></v>
<v t="ekr.20220525082936.289"><vh>NodeVisitor.visit_list_expr</vh></v>
<v t="ekr.20220525082936.290"><vh>NodeVisitor.visit_dict_expr</vh></v>
<v t="ekr.20220525082936.291"><vh>NodeVisitor.visit_tuple_expr</vh></v>
<v t="ekr.20220525082936.292"><vh>NodeVisitor.visit_set_expr</vh></v>
<v t="ekr.20220525082936.293"><vh>NodeVisitor.visit_index_expr</vh></v>
<v t="ekr.20220525082936.294"><vh>NodeVisitor.visit_type_application</vh></v>
<v t="ekr.20220525082936.295"><vh>NodeVisitor.visit_lambda_expr</vh></v>
<v t="ekr.20220525082936.296"><vh>NodeVisitor.visit_list_comprehension</vh></v>
<v t="ekr.20220525082936.297"><vh>NodeVisitor.visit_set_comprehension</vh></v>
<v t="ekr.20220525082936.298"><vh>NodeVisitor.visit_dictionary_comprehension</vh></v>
<v t="ekr.20220525082936.299"><vh>NodeVisitor.visit_generator_expr</vh></v>
<v t="ekr.20220525082936.300"><vh>NodeVisitor.visit_slice_expr</vh></v>
<v t="ekr.20220525082936.301"><vh>NodeVisitor.visit_conditional_expr</vh></v>
<v t="ekr.20220525082936.302"><vh>NodeVisitor.visit_backquote_expr</vh></v>
<v t="ekr.20220525082936.303"><vh>NodeVisitor.visit_type_var_expr</vh></v>
<v t="ekr.20220525082936.304"><vh>NodeVisitor.visit_paramspec_expr</vh></v>
<v t="ekr.20220525082936.305"><vh>NodeVisitor.visit_type_var_tuple_expr</vh></v>
<v t="ekr.20220525082936.306"><vh>NodeVisitor.visit_type_alias_expr</vh></v>
<v t="ekr.20220525082936.307"><vh>NodeVisitor.visit_namedtuple_expr</vh></v>
<v t="ekr.20220525082936.308"><vh>NodeVisitor.visit_enum_call_expr</vh></v>
<v t="ekr.20220525082936.309"><vh>NodeVisitor.visit_typeddict_expr</vh></v>
<v t="ekr.20220525082936.310"><vh>NodeVisitor.visit_newtype_expr</vh></v>
<v t="ekr.20220525082936.311"><vh>NodeVisitor.visit__promote_expr</vh></v>
<v t="ekr.20220525082936.312"><vh>NodeVisitor.visit_await_expr</vh></v>
<v t="ekr.20220525082936.313"><vh>NodeVisitor.visit_temp_node</vh></v>
<v t="ekr.20220525082936.314"><vh>NodeVisitor.Patterns</vh></v>
<v t="ekr.20220525082936.315"><vh>NodeVisitor.visit_as_pattern</vh></v>
<v t="ekr.20220525082936.316"><vh>NodeVisitor.visit_or_pattern</vh></v>
<v t="ekr.20220525082936.317"><vh>NodeVisitor.visit_value_pattern</vh></v>
<v t="ekr.20220525082936.318"><vh>NodeVisitor.visit_singleton_pattern</vh></v>
<v t="ekr.20220525082936.319"><vh>NodeVisitor.visit_sequence_pattern</vh></v>
<v t="ekr.20220525082936.320"><vh>NodeVisitor.visit_starred_pattern</vh></v>
<v t="ekr.20220525082936.321"><vh>NodeVisitor.visit_mapping_pattern</vh></v>
<v t="ekr.20220525082936.322"><vh>NodeVisitor.visit_class_pattern</vh></v>
</v>
</v>
<v t="ekr.20220525082936.323"><vh>@clean __init__.py</vh></v>
<v t="ekr.20220525082936.324"><vh>@clean __main__.py</vh>
<v t="ekr.20220525082936.325"><vh>console_entry</vh></v>
</v>
<v t="ekr.20220525082936.327"><vh>@path dmypy</vh>
<v t="ekr.20220525082936.328"><vh>@clean client.py</vh>
<v t="ekr.20220525082936.329"><vh>class AugmentedHelpFormatter</vh></v>
<v t="ekr.20220525082936.330"><vh>parser = argparse.ArgumentParser(prog='dmypy',</vh></v>
<v t="ekr.20220525082936.331"><vh>class BadStatus</vh></v>
<v t="ekr.20220525082936.332"><vh>main</vh></v>
<v t="ekr.20220525082936.333"><vh>fail</vh></v>
<v t="ekr.20220525082936.334"><vh>ActionFunction = Callable[[argparse.Namespace], None]</vh></v>
<v t="ekr.20220525082936.335"><vh>action</vh></v>
<v t="ekr.20220525082936.336"><vh>Action functions (run in client from command line).</vh></v>
<v t="ekr.20220525082936.337"><vh>do_start</vh></v>
<v t="ekr.20220525082936.338"><vh>do_restart</vh></v>
<v t="ekr.20220525082936.339"><vh>restart_server</vh></v>
<v t="ekr.20220525082936.340"><vh>start_server</vh></v>
<v t="ekr.20220525082936.341"><vh>wait_for_server</vh></v>
<v t="ekr.20220525082936.342"><vh>do_run</vh></v>
<v t="ekr.20220525082936.343"><vh>do_status</vh></v>
<v t="ekr.20220525082936.344"><vh>do_stop</vh></v>
<v t="ekr.20220525082936.345"><vh>do_kill</vh></v>
<v t="ekr.20220525082936.346"><vh>do_check</vh></v>
<v t="ekr.20220525082936.347"><vh>do_recheck</vh></v>
<v t="ekr.20220525082936.348"><vh>do_suggest</vh></v>
<v t="ekr.20220525082936.349"><vh>check_output</vh></v>
<v t="ekr.20220525082936.350"><vh>show_stats</vh></v>
<v t="ekr.20220525082936.351"><vh>do_hang</vh></v>
<v t="ekr.20220525082936.352"><vh>do_daemon</vh></v>
<v t="ekr.20220525082936.353"><vh>do_help</vh></v>
<v t="ekr.20220525082936.354"><vh>Client-side infrastructure.</vh></v>
<v t="ekr.20220525082936.355"><vh>request</vh></v>
<v t="ekr.20220525082936.356"><vh>get_status</vh></v>
<v t="ekr.20220525082936.357"><vh>check_status</vh></v>
<v t="ekr.20220525082936.358"><vh>read_status</vh></v>
<v t="ekr.20220525082936.359"><vh>is_running</vh></v>
<v t="ekr.20220525082936.360"><vh>console_entry</vh></v>
</v>
<v t="ekr.20220525082936.362"><vh>@clean __main__.py</vh></v>
</v>
<v t="ekr.20220525082936.363"><vh>@path plugins</vh>
<v t="ekr.20220525082936.364"><vh>@clean attrs.py</vh>
<v t="ekr.20220525082936.365"><vh>class Converter</vh>
<v t="ekr.20220525082936.366"><vh>Converter.__init__</vh></v>
</v>
<v t="ekr.20220525082936.367"><vh>class Attribute</vh>
<v t="ekr.20220525082936.368"><vh>Attribute.__init__</vh></v>
<v t="ekr.20220525082936.369"><vh>Attribute.argument</vh></v>
<v t="ekr.20220525082936.370"><vh>Attribute.serialize</vh></v>
<v t="ekr.20220525082936.371"><vh>Attribute.deserialize</vh></v>
<v t="ekr.20220525082936.372"><vh>Attribute.expand_typevar_from_subtype</vh></v>
</v>
<v t="ekr.20220525082936.373"><vh>_determine_eq_order</vh></v>
<v t="ekr.20220525082936.374"><vh>_get_decorator_optional_bool_argument</vh></v>
<v t="ekr.20220525082936.375"><vh>attr_tag_callback</vh></v>
<v t="ekr.20220525082936.376"><vh>attr_class_maker_callback</vh></v>
<v t="ekr.20220525082936.377"><vh>_get_frozen</vh></v>
<v t="ekr.20220525082936.378"><vh>_analyze_class</vh></v>
<v t="ekr.20220525082936.379"><vh>_add_empty_metadata</vh></v>
<v t="ekr.20220525082936.380"><vh>_detect_auto_attribs</vh></v>
<v t="ekr.20220525082936.381"><vh>_attributes_from_assignment</vh></v>
<v t="ekr.20220525082936.382"><vh>_cleanup_decorator</vh></v>
<v t="ekr.20220525082936.383"><vh>_attribute_from_auto_attrib</vh></v>
<v t="ekr.20220525082936.384"><vh>_attribute_from_attrib_maker</vh></v>
<v t="ekr.20220525082936.385"><vh>_parse_converter</vh></v>
<v t="ekr.20220525082936.386"><vh>is_valid_overloaded_converter</vh></v>
<v t="ekr.20220525082936.387"><vh>_parse_assignments</vh></v>
<v t="ekr.20220525082936.388"><vh>_add_order</vh></v>
<v t="ekr.20220525082936.389"><vh>_make_frozen</vh></v>
<v t="ekr.20220525082936.390"><vh>_add_init</vh></v>
<v t="ekr.20220525082936.391"><vh>_add_attrs_magic_attribute</vh></v>
<v t="ekr.20220525082936.392"><vh>_add_slots</vh></v>
<v t="ekr.20220525082936.393"><vh>_add_match_args</vh></v>
<v t="ekr.20220525082936.394"><vh>class MethodAdder</vh>
<v t="ekr.20220525082936.395"><vh>MethodAdder.__init__</vh></v>
<v t="ekr.20220525082936.396"><vh>MethodAdder.add_method</vh></v>
</v>
</v>
<v t="ekr.20220525082936.397"><vh>@clean common.py</vh>
<v t="ekr.20220525082936.398"><vh>_get_decorator_bool_argument</vh></v>
<v t="ekr.20220525082936.399"><vh>_get_bool_argument</vh></v>
<v t="ekr.20220525082936.400"><vh>_get_argument</vh></v>
<v t="ekr.20220525082936.401"><vh>add_method</vh></v>
<v t="ekr.20220525082936.402"><vh>add_method_to_class</vh></v>
<v t="ekr.20220525082936.403"><vh>add_attribute_to_class</vh></v>
<v t="ekr.20220525082936.404"><vh>deserialize_and_fixup_type</vh></v>
</v>
<v t="ekr.20220525082936.405"><vh>@clean ctypes.py</vh>
<v t="ekr.20220525082936.406"><vh>_get_bytes_type</vh></v>
<v t="ekr.20220525082936.407"><vh>_get_text_type</vh></v>
<v t="ekr.20220525082936.408"><vh>_find_simplecdata_base_arg</vh></v>
<v t="ekr.20220525082936.409"><vh>_autoconvertible_to_cdata</vh></v>
<v t="ekr.20220525082936.410"><vh>_autounboxed_cdata</vh></v>
<v t="ekr.20220525082936.411"><vh>_get_array_element_type</vh></v>
<v t="ekr.20220525082936.412"><vh>array_constructor_callback</vh></v>
<v t="ekr.20220525082936.413"><vh>array_getitem_callback</vh></v>
<v t="ekr.20220525082936.414"><vh>array_setitem_callback</vh></v>
<v t="ekr.20220525082936.415"><vh>array_iter_callback</vh></v>
<v t="ekr.20220525082936.416"><vh>array_value_callback</vh></v>
<v t="ekr.20220525082936.417"><vh>array_raw_callback</vh></v>
</v>
<v t="ekr.20220525082936.418"><vh>@clean dataclasses.py</vh>
<v t="ekr.20220525082936.419"><vh>class DataclassAttribute</vh>
<v t="ekr.20220525082936.420"><vh>DataclassAttribute.__init__</vh></v>
<v t="ekr.20220525082936.421"><vh>DataclassAttribute.to_argument</vh></v>
<v t="ekr.20220525082936.422"><vh>DataclassAttribute.to_var</vh></v>
<v t="ekr.20220525082936.423"><vh>DataclassAttribute.serialize</vh></v>
<v t="ekr.20220525082936.424"><vh>DataclassAttribute.deserialize</vh></v>
<v t="ekr.20220525082936.425"><vh>DataclassAttribute.expand_typevar_from_subtype</vh></v>
</v>
<v t="ekr.20220525082936.426"><vh>class DataclassTransformer</vh>
<v t="ekr.20220525082936.427"><vh>DataclassTransformer.__init__</vh></v>
<v t="ekr.20220525082936.428"><vh>DataclassTransformer.transform</vh></v>
<v t="ekr.20220525082936.429"><vh>DataclassTransformer.add_slots</vh></v>
<v t="ekr.20220525082936.430"><vh>DataclassTransformer.reset_init_only_vars</vh></v>
<v t="ekr.20220525082936.431"><vh>DataclassTransformer.collect_attributes</vh></v>
<v t="ekr.20220525082936.432"><vh>DataclassTransformer._freeze</vh></v>
<v t="ekr.20220525082936.433"><vh>DataclassTransformer._propertize_callables</vh></v>
<v t="ekr.20220525082936.434"><vh>DataclassTransformer._is_kw_only_type</vh></v>
<v t="ekr.20220525082936.435"><vh>DataclassTransformer._add_dataclass_fields_magic_attribute</vh></v>
</v>
<v t="ekr.20220525082936.436"><vh>dataclass_tag_callback</vh></v>
<v t="ekr.20220525082936.437"><vh>dataclass_class_maker_callback</vh></v>
<v t="ekr.20220525082936.438"><vh>_collect_field_args</vh></v>
</v>
<v t="ekr.20220525082936.439"><vh>@clean default.py</vh>
<v t="ekr.20220525082936.440"><vh>class DefaultPlugin</vh>
<v t="ekr.20220525082936.441"><vh>DefaultPlugin.get_function_hook</vh></v>
<v t="ekr.20220525082936.442"><vh>DefaultPlugin.get_method_signature_hook</vh></v>
<v t="ekr.20220525082936.443"><vh>DefaultPlugin.get_method_hook</vh></v>
<v t="ekr.20220525082936.444"><vh>DefaultPlugin.get_attribute_hook</vh></v>
<v t="ekr.20220525082936.445"><vh>DefaultPlugin.get_class_decorator_hook</vh></v>
<v t="ekr.20220525082936.446"><vh>DefaultPlugin.get_class_decorator_hook_2</vh></v>
</v>
<v t="ekr.20220525082936.447"><vh>contextmanager_callback</vh></v>
<v t="ekr.20220525082936.448"><vh>typed_dict_get_signature_callback</vh></v>
<v t="ekr.20220525082936.449"><vh>typed_dict_get_callback</vh></v>
<v t="ekr.20220525082936.450"><vh>typed_dict_pop_signature_callback</vh></v>
<v t="ekr.20220525082936.451"><vh>typed_dict_pop_callback</vh></v>
<v t="ekr.20220525082936.452"><vh>typed_dict_setdefault_signature_callback</vh></v>
<v t="ekr.20220525082936.453"><vh>typed_dict_setdefault_callback</vh></v>
<v t="ekr.20220525082936.454"><vh>typed_dict_delitem_callback</vh></v>
<v t="ekr.20220525082936.455"><vh>typed_dict_update_signature_callback</vh></v>
<v t="ekr.20220525082936.456"><vh>int_pow_callback</vh></v>
<v t="ekr.20220525082936.457"><vh>int_neg_callback</vh></v>
<v t="ekr.20220525082936.458"><vh>tuple_mul_callback</vh></v>
</v>
<v t="ekr.20220525082936.459"><vh>@clean enums.py</vh>
<v t="ekr.20220525082936.460"><vh>enum_name_callback</vh></v>
<v t="ekr.20220525082936.461"><vh>_T = TypeVar('_T')</vh></v>
<v t="ekr.20220525082936.462"><vh>_first</vh></v>
<v t="ekr.20220525082936.463"><vh>_infer_value_type_with_auto_fallback</vh></v>
<v t="ekr.20220525082936.464"><vh>_implements_new</vh></v>
<v t="ekr.20220525082936.465"><vh>enum_value_callback</vh></v>
<v t="ekr.20220525082936.466"><vh>_extract_underlying_field_name</vh></v>
</v>
<v t="ekr.20220525082936.467"><vh>@clean functools.py</vh>
<v t="ekr.20220525082936.468"><vh>class _MethodInfo</vh></v>
<v t="ekr.20220525082936.469"><vh>functools_total_ordering_maker_callback</vh></v>
<v t="ekr.20220525082936.470"><vh>_find_other_type</vh></v>
<v t="ekr.20220525082936.471"><vh>_analyze_class</vh></v>
</v>
<v t="ekr.20220525082936.472"><vh>@clean singledispatch.py</vh>
<v t="ekr.20220525082936.473"><vh>class SingledispatchTypeVars</vh></v>
<v t="ekr.20220525082936.474"><vh>class RegisterCallableInfo</vh></v>
<v t="ekr.20220525082936.475"><vh>SINGLEDISPATCH_TYPE: Final = 'functools._SingleDispatchCallable'</vh></v>
<v t="ekr.20220525082936.476"><vh>get_singledispatch_info</vh></v>
<v t="ekr.20220525082936.477"><vh>T = TypeVar('T')</vh></v>
<v t="ekr.20220525082936.478"><vh>get_first_arg</vh></v>
<v t="ekr.20220525082936.479"><vh>REGISTER_RETURN_CLASS: Final = '_SingleDispatchRegisterCallable'</vh></v>
<v t="ekr.20220525082936.480"><vh>make_fake_register_class_instance</vh></v>
<v t="ekr.20220525082936.481"><vh>PluginContext = Union[FunctionContext, MethodContext]</vh></v>
<v t="ekr.20220525082936.482"><vh>fail</vh></v>
<v t="ekr.20220525082936.483"><vh>create_singledispatch_function_callback</vh></v>
<v t="ekr.20220525082936.484"><vh>singledispatch_register_callback</vh></v>
<v t="ekr.20220525082936.485"><vh>register_function</vh></v>
<v t="ekr.20220525082936.486"><vh>get_dispatch_type</vh></v>
<v t="ekr.20220525082936.487"><vh>call_singledispatch_function_after_register_argument</vh></v>
<v t="ekr.20220525082936.488"><vh>call_singledispatch_function_callback</vh></v>
</v>
</v>
<v t="ekr.20220525082936.491"><vh>@path server</vh>
<v t="ekr.20220525082936.492"><vh>@clean astdiff.py</vh>
<v t="ekr.20220525082936.493"><vh>compare_symbol_table_snapshots</vh></v>
<v t="ekr.20220525082936.494"><vh>snapshot_symbol_table</vh></v>
<v t="ekr.20220525082936.495"><vh>snapshot_definition</vh></v>
<v t="ekr.20220525082936.496"><vh>snapshot_type</vh></v>
<v t="ekr.20220525082936.497"><vh>snapshot_optional_type</vh></v>
<v t="ekr.20220525082936.498"><vh>snapshot_types</vh></v>
<v t="ekr.20220525082936.499"><vh>snapshot_simple_type</vh></v>
<v t="ekr.20220525082936.500"><vh>encode_optional_str</vh></v>
<v t="ekr.20220525082936.501"><vh>class SnapshotTypeVisitor</vh>
<v t="ekr.20220525082936.502"><vh>SnapshotTypeVisitor.visit_unbound_type</vh></v>
<v t="ekr.20220525082936.503"><vh>SnapshotTypeVisitor.visit_any</vh></v>
<v t="ekr.20220525082936.504"><vh>SnapshotTypeVisitor.visit_none_type</vh></v>
<v t="ekr.20220525082936.505"><vh>SnapshotTypeVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082936.506"><vh>SnapshotTypeVisitor.visit_erased_type</vh></v>
<v t="ekr.20220525082936.507"><vh>SnapshotTypeVisitor.visit_deleted_type</vh></v>
<v t="ekr.20220525082936.508"><vh>SnapshotTypeVisitor.visit_instance</vh></v>
<v t="ekr.20220525082936.509"><vh>SnapshotTypeVisitor.visit_type_var</vh></v>
<v t="ekr.20220525082936.510"><vh>SnapshotTypeVisitor.visit_param_spec</vh></v>
<v t="ekr.20220525082936.511"><vh>SnapshotTypeVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082936.512"><vh>SnapshotTypeVisitor.visit_unpack_type</vh></v>
<v t="ekr.20220525082936.513"><vh>SnapshotTypeVisitor.visit_parameters</vh></v>
<v t="ekr.20220525082936.514"><vh>SnapshotTypeVisitor.visit_callable_type</vh></v>
<v t="ekr.20220525082936.515"><vh>SnapshotTypeVisitor.visit_tuple_type</vh></v>
<v t="ekr.20220525082936.516"><vh>SnapshotTypeVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20220525082936.517"><vh>SnapshotTypeVisitor.visit_literal_type</vh></v>
<v t="ekr.20220525082936.518"><vh>SnapshotTypeVisitor.visit_union_type</vh></v>
<v t="ekr.20220525082936.519"><vh>SnapshotTypeVisitor.visit_overloaded</vh></v>
<v t="ekr.20220525082936.520"><vh>SnapshotTypeVisitor.visit_partial_type</vh></v>
<v t="ekr.20220525082936.521"><vh>SnapshotTypeVisitor.visit_type_type</vh></v>
<v t="ekr.20220525082936.522"><vh>SnapshotTypeVisitor.visit_type_alias_type</vh></v>
</v>
<v t="ekr.20220525082936.523"><vh>snapshot_untyped_signature</vh></v>
</v>
<v t="ekr.20220525082936.524"><vh>@clean astmerge.py</vh>
<v t="ekr.20220525082936.525"><vh>merge_asts</vh></v>
<v t="ekr.20220525082936.526"><vh>replacement_map_from_symbol_table</vh></v>
<v t="ekr.20220525082936.527"><vh>replace_nodes_in_ast</vh></v>
<v t="ekr.20220525082936.528"><vh>SN = TypeVar('SN', bound=SymbolNode)</vh></v>
<v t="ekr.20220525082936.529"><vh>class NodeReplaceVisitor</vh>
<v t="ekr.20220525082936.530"><vh>NodeReplaceVisitor.__init__</vh></v>
<v t="ekr.20220525082936.531"><vh>NodeReplaceVisitor.visit_mypy_file</vh></v>
<v t="ekr.20220525082936.532"><vh>NodeReplaceVisitor.visit_block</vh></v>
<v t="ekr.20220525082936.533"><vh>NodeReplaceVisitor.visit_func_def</vh></v>
<v t="ekr.20220525082936.534"><vh>NodeReplaceVisitor.visit_overloaded_func_def</vh></v>
<v t="ekr.20220525082936.535"><vh>NodeReplaceVisitor.visit_class_def</vh></v>
<v t="ekr.20220525082936.536"><vh>NodeReplaceVisitor.process_base_func</vh></v>
<v t="ekr.20220525082936.537"><vh>NodeReplaceVisitor.process_type_var_def</vh></v>
<v t="ekr.20220525082936.538"><vh>NodeReplaceVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20220525082936.539"><vh>NodeReplaceVisitor.Expressions</vh></v>
<v t="ekr.20220525082936.540"><vh>NodeReplaceVisitor.visit_name_expr</vh></v>
<v t="ekr.20220525082936.541"><vh>NodeReplaceVisitor.visit_member_expr</vh></v>
<v t="ekr.20220525082936.542"><vh>NodeReplaceVisitor.visit_ref_expr</vh></v>
<v t="ekr.20220525082936.543"><vh>NodeReplaceVisitor.visit_namedtuple_expr</vh></v>
<v t="ekr.20220525082936.544"><vh>NodeReplaceVisitor.visit_cast_expr</vh></v>
<v t="ekr.20220525082936.545"><vh>NodeReplaceVisitor.visit_assert_type_expr</vh></v>
<v t="ekr.20220525082936.546"><vh>NodeReplaceVisitor.visit_super_expr</vh></v>
<v t="ekr.20220525082936.547"><vh>NodeReplaceVisitor.visit_call_expr</vh></v>
<v t="ekr.20220525082936.548"><vh>NodeReplaceVisitor.visit_newtype_expr</vh></v>
<v t="ekr.20220525082936.549"><vh>NodeReplaceVisitor.visit_lambda_expr</vh></v>
<v t="ekr.20220525082936.550"><vh>NodeReplaceVisitor.visit_typeddict_expr</vh></v>
<v t="ekr.20220525082936.551"><vh>NodeReplaceVisitor.visit_enum_call_expr</vh></v>
<v t="ekr.20220525082936.552"><vh>NodeReplaceVisitor.visit_type_alias_expr</vh></v>
<v t="ekr.20220525082936.553"><vh>NodeReplaceVisitor.Others</vh></v>
<v t="ekr.20220525082936.554"><vh>NodeReplaceVisitor.visit_var</vh></v>
<v t="ekr.20220525082936.555"><vh>NodeReplaceVisitor.visit_type_alias</vh></v>
<v t="ekr.20220525082936.556"><vh>NodeReplaceVisitor.Helpers</vh></v>
<v t="ekr.20220525082936.557"><vh>NodeReplaceVisitor.fixup</vh></v>
<v t="ekr.20220525082936.558"><vh>NodeReplaceVisitor.fixup_and_reset_typeinfo</vh></v>
<v t="ekr.20220525082936.559"><vh>NodeReplaceVisitor.fixup_type</vh></v>
<v t="ekr.20220525082936.560"><vh>NodeReplaceVisitor.process_type_info</vh></v>
<v t="ekr.20220525082936.561"><vh>NodeReplaceVisitor.process_synthetic_type_info</vh></v>
<v t="ekr.20220525082936.562"><vh>NodeReplaceVisitor.replace_statements</vh></v>
</v>
<v t="ekr.20220525082936.563"><vh>class TypeReplaceVisitor</vh>
<v t="ekr.20220525082936.564"><vh>TypeReplaceVisitor.__init__</vh></v>
<v t="ekr.20220525082936.565"><vh>TypeReplaceVisitor.visit_instance</vh></v>
<v t="ekr.20220525082936.566"><vh>TypeReplaceVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20220525082936.567"><vh>TypeReplaceVisitor.visit_any</vh></v>
<v t="ekr.20220525082936.568"><vh>TypeReplaceVisitor.visit_none_type</vh></v>
<v t="ekr.20220525082936.569"><vh>TypeReplaceVisitor.visit_callable_type</vh></v>
<v t="ekr.20220525082936.570"><vh>TypeReplaceVisitor.visit_overloaded</vh></v>
<v t="ekr.20220525082936.571"><vh>TypeReplaceVisitor.visit_erased_type</vh></v>
<v t="ekr.20220525082936.572"><vh>TypeReplaceVisitor.visit_deleted_type</vh></v>
<v t="ekr.20220525082936.573"><vh>TypeReplaceVisitor.visit_partial_type</vh></v>
<v t="ekr.20220525082936.574"><vh>TypeReplaceVisitor.visit_tuple_type</vh></v>
<v t="ekr.20220525082936.575"><vh>TypeReplaceVisitor.visit_type_type</vh></v>
<v t="ekr.20220525082936.576"><vh>TypeReplaceVisitor.visit_type_var</vh></v>
<v t="ekr.20220525082936.577"><vh>TypeReplaceVisitor.visit_param_spec</vh></v>
<v t="ekr.20220525082936.578"><vh>TypeReplaceVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082936.579"><vh>TypeReplaceVisitor.visit_unpack_type</vh></v>
<v t="ekr.20220525082936.580"><vh>TypeReplaceVisitor.visit_parameters</vh></v>
<v t="ekr.20220525082936.581"><vh>TypeReplaceVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20220525082936.582"><vh>TypeReplaceVisitor.visit_raw_expression_type</vh></v>
<v t="ekr.20220525082936.583"><vh>TypeReplaceVisitor.visit_literal_type</vh></v>
<v t="ekr.20220525082936.584"><vh>TypeReplaceVisitor.visit_unbound_type</vh></v>
<v t="ekr.20220525082936.585"><vh>TypeReplaceVisitor.visit_type_list</vh></v>
<v t="ekr.20220525082936.586"><vh>TypeReplaceVisitor.visit_callable_argument</vh></v>
<v t="ekr.20220525082936.587"><vh>TypeReplaceVisitor.visit_ellipsis_type</vh></v>
<v t="ekr.20220525082936.588"><vh>TypeReplaceVisitor.visit_star_type</vh></v>
<v t="ekr.20220525082936.589"><vh>TypeReplaceVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082936.590"><vh>TypeReplaceVisitor.visit_union_type</vh></v>
<v t="ekr.20220525082936.591"><vh>TypeReplaceVisitor.visit_placeholder_type</vh></v>
<v t="ekr.20220525082936.592"><vh>TypeReplaceVisitor.Helpers</vh></v>
<v t="ekr.20220525082936.593"><vh>TypeReplaceVisitor.fixup</vh></v>
</v>
<v t="ekr.20220525082936.594"><vh>replace_nodes_in_symbol_table</vh></v>
</v>
<v t="ekr.20220525082936.595"><vh>@clean aststrip.py</vh>
<v t="ekr.20220525082936.596"><vh>strip_target</vh></v>
<v t="ekr.20220525082936.597"><vh>class NodeStripVisitor</vh>
<v t="ekr.20220525082936.598"><vh>NodeStripVisitor.__init__</vh></v>
<v t="ekr.20220525082936.599"><vh>NodeStripVisitor.strip_file_top_level</vh></v>
<v t="ekr.20220525082936.600"><vh>NodeStripVisitor.visit_block</vh></v>
<v t="ekr.20220525082936.601"><vh>NodeStripVisitor.visit_class_def</vh></v>
<v t="ekr.20220525082936.602"><vh>NodeStripVisitor.save_implicit_attributes</vh></v>
<v t="ekr.20220525082936.603"><vh>NodeStripVisitor.visit_func_def</vh></v>
<v t="ekr.20220525082936.604"><vh>NodeStripVisitor.visit_decorator</vh></v>
<v t="ekr.20220525082936.605"><vh>NodeStripVisitor.visit_overloaded_func_def</vh></v>
<v t="ekr.20220525082936.606"><vh>NodeStripVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20220525082936.607"><vh>NodeStripVisitor.visit_import_from</vh></v>
<v t="ekr.20220525082936.608"><vh>NodeStripVisitor.visit_import_all</vh></v>
<v t="ekr.20220525082936.609"><vh>NodeStripVisitor.visit_for_stmt</vh></v>
<v t="ekr.20220525082936.610"><vh>NodeStripVisitor.visit_name_expr</vh></v>
<v t="ekr.20220525082936.611"><vh>NodeStripVisitor.visit_member_expr</vh></v>
<v t="ekr.20220525082936.612"><vh>NodeStripVisitor.visit_index_expr</vh></v>
<v t="ekr.20220525082936.613"><vh>NodeStripVisitor.strip_ref_expr</vh></v>
<v t="ekr.20220525082936.614"><vh>NodeStripVisitor.visit_call_expr</vh></v>
<v t="ekr.20220525082936.615"><vh>NodeStripVisitor.visit_super_expr</vh></v>
<v t="ekr.20220525082936.616"><vh>NodeStripVisitor.process_lvalue_in_method</vh></v>
<v t="ekr.20220525082936.617"><vh>NodeStripVisitor.enter_class</vh></v>
<v t="ekr.20220525082936.618"><vh>NodeStripVisitor.enter_method</vh></v>
</v>
</v>
<v t="ekr.20220525082936.619"><vh>@clean deps.py</vh>
<v t="ekr.20220525082936.620"><vh>get_dependencies</vh></v>
<v t="ekr.20220525082936.621"><vh>get_dependencies_of_target</vh></v>
<v t="ekr.20220525082936.622"><vh>class DependencyVisitor</vh>
<v t="ekr.20220525082936.623"><vh>DependencyVisitor.__init__</vh></v>
<v t="ekr.20220525082936.624"><vh>DependencyVisitor.visit_mypy_file</vh></v>
<v t="ekr.20220525082936.625"><vh>DependencyVisitor.visit_func_def</vh></v>
<v t="ekr.20220525082936.626"><vh>DependencyVisitor.visit_decorator</vh></v>
<v t="ekr.20220525082936.627"><vh>DependencyVisitor.visit_class_def</vh></v>
<v t="ekr.20220525082936.628"><vh>DependencyVisitor.visit_newtype_expr</vh></v>
<v t="ekr.20220525082936.629"><vh>DependencyVisitor.process_type_info</vh></v>
<v t="ekr.20220525082936.630"><vh>DependencyVisitor.visit_import</vh></v>
<v t="ekr.20220525082936.631"><vh>DependencyVisitor.visit_import_from</vh></v>
<v t="ekr.20220525082936.632"><vh>DependencyVisitor.visit_import_all</vh></v>
<v t="ekr.20220525082936.633"><vh>DependencyVisitor.visit_block</vh></v>
<v t="ekr.20220525082936.634"><vh>DependencyVisitor.visit_assignment_stmt</vh></v>
<v t="ekr.20220525082936.635"><vh>DependencyVisitor.process_lvalue</vh></v>
<v t="ekr.20220525082936.636"><vh>DependencyVisitor.is_self_member_ref</vh></v>
<v t="ekr.20220525082936.637"><vh>DependencyVisitor.get_non_partial_lvalue_type</vh></v>
<v t="ekr.20220525082936.638"><vh>DependencyVisitor.visit_operator_assignment_stmt</vh></v>
<v t="ekr.20220525082936.639"><vh>DependencyVisitor.visit_for_stmt</vh></v>
<v t="ekr.20220525082936.640"><vh>DependencyVisitor.visit_with_stmt</vh></v>
<v t="ekr.20220525082936.641"><vh>DependencyVisitor.visit_print_stmt</vh></v>
<v t="ekr.20220525082936.642"><vh>DependencyVisitor.visit_del_stmt</vh></v>
<v t="ekr.20220525082936.643"><vh>DependencyVisitor.Expressions</vh></v>
<v t="ekr.20220525082936.644"><vh>DependencyVisitor.process_global_ref_expr</vh></v>
<v t="ekr.20220525082936.645"><vh>DependencyVisitor.visit_name_expr</vh></v>
<v t="ekr.20220525082936.646"><vh>DependencyVisitor.visit_member_expr</vh></v>
<v t="ekr.20220525082936.647"><vh>DependencyVisitor.get_unimported_fullname</vh></v>
<v t="ekr.20220525082936.648"><vh>DependencyVisitor.visit_super_expr</vh></v>
<v t="ekr.20220525082936.649"><vh>DependencyVisitor.visit_call_expr</vh></v>
<v t="ekr.20220525082936.650"><vh>DependencyVisitor.process_isinstance_call</vh></v>
<v t="ekr.20220525082936.651"><vh>DependencyVisitor.visit_cast_expr</vh></v>
<v t="ekr.20220525082936.652"><vh>DependencyVisitor.visit_assert_type_expr</vh></v>
<v t="ekr.20220525082936.653"><vh>DependencyVisitor.visit_type_application</vh></v>
<v t="ekr.20220525082936.654"><vh>DependencyVisitor.visit_index_expr</vh></v>
<v t="ekr.20220525082936.655"><vh>DependencyVisitor.visit_unary_expr</vh></v>
<v t="ekr.20220525082936.656"><vh>DependencyVisitor.visit_op_expr</vh></v>
<v t="ekr.20220525082936.657"><vh>DependencyVisitor.visit_comparison_expr</vh></v>
<v t="ekr.20220525082936.658"><vh>DependencyVisitor.process_binary_op</vh></v>
<v t="ekr.20220525082936.659"><vh>DependencyVisitor.add_operator_method_dependency</vh></v>
<v t="ekr.20220525082936.660"><vh>DependencyVisitor.add_operator_method_dependency_for_type</vh></v>
<v t="ekr.20220525082936.661"><vh>DependencyVisitor.visit_generator_expr</vh></v>
<v t="ekr.20220525082936.662"><vh>DependencyVisitor.visit_dictionary_comprehension</vh></v>
<v t="ekr.20220525082936.663"><vh>DependencyVisitor.visit_star_expr</vh></v>
<v t="ekr.20220525082936.664"><vh>DependencyVisitor.visit_yield_from_expr</vh></v>
<v t="ekr.20220525082936.665"><vh>DependencyVisitor.visit_await_expr</vh></v>
<v t="ekr.20220525082936.666"><vh>DependencyVisitor.Helpers</vh></v>
<v t="ekr.20220525082936.667"><vh>DependencyVisitor.add_type_alias_deps</vh></v>
<v t="ekr.20220525082936.668"><vh>DependencyVisitor.add_dependency</vh></v>
<v t="ekr.20220525082936.669"><vh>DependencyVisitor.add_type_dependencies</vh></v>
<v t="ekr.20220525082936.670"><vh>DependencyVisitor.add_attribute_dependency</vh></v>
<v t="ekr.20220525082936.671"><vh>DependencyVisitor.attribute_triggers</vh></v>
<v t="ekr.20220525082936.672"><vh>DependencyVisitor.add_attribute_dependency_for_expr</vh></v>
<v t="ekr.20220525082936.673"><vh>DependencyVisitor.add_iter_dependency</vh></v>
<v t="ekr.20220525082936.674"><vh>DependencyVisitor.use_logical_deps</vh></v>
<v t="ekr.20220525082936.675"><vh>DependencyVisitor.get_type_triggers</vh></v>
</v>
<v t="ekr.20220525082936.676"><vh>get_type_triggers</vh></v>
<v t="ekr.20220525082936.677"><vh>class TypeTriggersVisitor</vh>
<v t="ekr.20220525082936.678"><vh>TypeTriggersVisitor.__init__</vh></v>
<v t="ekr.20220525082936.679"><vh>TypeTriggersVisitor.get_type_triggers</vh></v>
<v t="ekr.20220525082936.680"><vh>TypeTriggersVisitor.visit_instance</vh></v>
<v t="ekr.20220525082936.681"><vh>TypeTriggersVisitor.visit_type_alias_type</vh></v>
<v t="ekr.20220525082936.682"><vh>TypeTriggersVisitor.visit_any</vh></v>
<v t="ekr.20220525082936.683"><vh>TypeTriggersVisitor.visit_none_type</vh></v>
<v t="ekr.20220525082936.684"><vh>TypeTriggersVisitor.visit_callable_type</vh></v>
<v t="ekr.20220525082936.685"><vh>TypeTriggersVisitor.visit_overloaded</vh></v>
<v t="ekr.20220525082936.686"><vh>TypeTriggersVisitor.visit_erased_type</vh></v>
<v t="ekr.20220525082936.687"><vh>TypeTriggersVisitor.visit_deleted_type</vh></v>
<v t="ekr.20220525082936.688"><vh>TypeTriggersVisitor.visit_partial_type</vh></v>
<v t="ekr.20220525082936.689"><vh>TypeTriggersVisitor.visit_tuple_type</vh></v>
<v t="ekr.20220525082936.690"><vh>TypeTriggersVisitor.visit_type_type</vh></v>
<v t="ekr.20220525082936.691"><vh>TypeTriggersVisitor.visit_type_var</vh></v>
<v t="ekr.20220525082936.692"><vh>TypeTriggersVisitor.visit_param_spec</vh></v>
<v t="ekr.20220525082936.693"><vh>TypeTriggersVisitor.visit_type_var_tuple</vh></v>
<v t="ekr.20220525082936.694"><vh>TypeTriggersVisitor.visit_unpack_type</vh></v>
<v t="ekr.20220525082936.695"><vh>TypeTriggersVisitor.visit_parameters</vh></v>
<v t="ekr.20220525082936.696"><vh>TypeTriggersVisitor.visit_typeddict_type</vh></v>
<v t="ekr.20220525082936.697"><vh>TypeTriggersVisitor.visit_literal_type</vh></v>
<v t="ekr.20220525082936.698"><vh>TypeTriggersVisitor.visit_unbound_type</vh></v>
<v t="ekr.20220525082936.699"><vh>TypeTriggersVisitor.visit_uninhabited_type</vh></v>
<v t="ekr.20220525082936.700"><vh>TypeTriggersVisitor.visit_union_type</vh></v>
</v>
<v t="ekr.20220525082936.701"><vh>merge_dependencies</vh></v>
<v t="ekr.20220525082936.702"><vh>non_trivial_bases</vh></v>
<v t="ekr.20220525082936.703"><vh>has_user_bases</vh></v>
<v t="ekr.20220525082936.704"><vh>dump_all_dependencies</vh></v>
</v>
<v t="ekr.20220525082936.705"><vh>@clean mergecheck.py</vh>
<v t="ekr.20220525082936.706"><vh>check_consistency</vh></v>
<v t="ekr.20220525082936.707"><vh>path_to_str</vh></v>
</v>
<v t="ekr.20220525082936.708"><vh>@clean objgraph.py</vh>
<v t="ekr.20220525082936.709"><vh>isproperty</vh></v>
<v t="ekr.20220525082936.710"><vh>get_edge_candidates</vh></v>
<v t="ekr.20220525082936.711"><vh>get_edges</vh></v>
<v t="ekr.20220525082936.712"><vh>get_reachable_graph</vh></v>
<v t="ekr.20220525082936.713"><vh>get_path</vh></v>
</v>
<v t="ekr.20220525082936.714"><vh>@clean subexpr.py</vh>
<v t="ekr.20220525082936.715"><vh>get_subexpressions</vh></v>
<v t="ekr.20220525082936.716"><vh>class SubexpressionFinder</vh>
<v t="ekr.20220525082936.717"><vh>SubexpressionFinder.__init__</vh></v>
<v t="ekr.20220525082936.718"><vh>SubexpressionFinder.visit_int_expr</vh></v>
<v t="ekr.20220525082936.719"><vh>SubexpressionFinder.visit_name_expr</vh></v>
<v t="ekr.20220525082936.720"><vh>SubexpressionFinder.visit_float_expr</vh></v>
<v t="ekr.20220525082936.721"><vh>SubexpressionFinder.visit_str_expr</vh></v>
<v t="ekr.20220525082936.722"><vh>SubexpressionFinder.visit_bytes_expr</vh></v>
<v t="ekr.20220525082936.723"><vh>SubexpressionFinder.visit_unicode_expr</vh></v>
<v t="ekr.20220525082936.724"><vh>SubexpressionFinder.visit_complex_expr</vh></v>
<v t="ekr.20220525082936.725"><vh>SubexpressionFinder.visit_ellipsis</vh></v>
<v t="ekr.20220525082936.726"><vh>SubexpressionFinder.visit_super_expr</vh></v>
<v t="ekr.20220525082936.727"><vh>SubexpressionFinder.visit_type_var_expr</vh></v>
<v t="ekr.20220525082936.728"><vh>SubexpressionFinder.visit_type_alias_expr</vh></v>
<v t="ekr.20220525082936.729"><vh>SubexpressionFinder.visit_namedtuple_expr</vh></v>
<v t="ekr.20220525082936.730"><vh>SubexpressionFinder.visit_typeddict_expr</vh></v>
<v t="ekr.20220525082936.731"><vh>SubexpressionFinder.visit__promote_expr</vh></v>
<v t="ekr.20220525082936.732"><vh>SubexpressionFinder.visit_newtype_expr</vh></v>
<v t="ekr.20220525082936.733"><vh>SubexpressionFinder.visit_member_expr</vh></v>
<v t="ekr.20220525082936.734"><vh>SubexpressionFinder.visit_yield_from_expr</vh></v>
<v t="ekr.20220525082936.735"><vh>SubexpressionFinder.visit_yield_expr</vh></v>
<v t="ekr.20220525082936.736"><vh>SubexpressionFinder.visit_call_expr</vh></v>
<v t="ekr.20220525082936.737"><vh>SubexpressionFinder.visit_op_expr</vh></v>
<v t="ekr.20220525082936.738"><vh>SubexpressionFinder.visit_comparison_expr</vh></v>
<v t="ekr.20220525082936.739"><vh>SubexpressionFinder.visit_slice_expr</vh></v>
<v t="ekr.20220525082936.740"><vh>SubexpressionFinder.visit_cast_expr</vh></v>
<v t="ekr.20220525082936.741"><vh>SubexpressionFinder.visit_assert_type_expr</vh></v>
<v t="ekr.20220525082936.742"><vh>SubexpressionFinder.visit_reveal_expr</vh></v>
<v t="ekr.20220525082936.743"><vh>SubexpressionFinder.visit_assignment_expr</vh></v>
<v t="ekr.20220525082936.744"><vh>SubexpressionFinder.visit_unary_expr</vh></v>
<v t="ekr.20220525082936.745"><vh>SubexpressionFinder.visit_list_expr</vh></v>
<v t="ekr.20220525082936.746"><vh>SubexpressionFinder.visit_tuple_expr</vh></v>
<v t="ekr.20220525082936.747"><vh>SubexpressionFinder.visit_dict_expr</vh></v>
<v t="ekr.20220525082936.748"><vh>SubexpressionFinder.visit_set_expr</vh></v>
<v t="ekr.20220525082936.749"><vh>SubexpressionFinder.visit_index_expr</vh></v>
<v t="ekr.20220525082936.750"><vh>SubexpressionFinder.visit_generator_expr</vh></v>
<v t="ekr.20220525082936.751"><vh>SubexpressionFinder.visit_dictionary_comprehension</vh></v>
<v t="ekr.20220525082936.752"><vh>SubexpressionFinder.visit_list_comprehension</vh></v>
<v t="ekr.20220525082936.753"><vh>SubexpressionFinder.visit_set_comprehension</vh></v>
<v t="ekr.20220525082936.754"><vh>SubexpressionFinder.visit_conditional_expr</vh></v>
<v t="ekr.20220525082936.755"><vh>SubexpressionFinder.visit_type_application</vh></v>
<v t="ekr.20220525082936.756"><vh>SubexpressionFinder.visit_lambda_expr</vh></v>
<v t="ekr.20220525082936.757"><vh>SubexpressionFinder.visit_star_expr</vh></v>
<v t="ekr.20220525082936.758"><vh>SubexpressionFinder.visit_backquote_expr</vh></v>
<v t="ekr.20220525082936.759"><vh>SubexpressionFinder.visit_await_expr</vh></v>
<v t="ekr.20220525082936.760"><vh>SubexpressionFinder.add</vh></v>
</v>
</v>
<v t="ekr.20220525082936.761"><vh>@clean target.py</vh></v>
<v t="ekr.20220525082936.762"><vh>@clean trigger.py</vh>
<v t="ekr.20220525082936.763"><vh>make_trigger</vh></v>
<v t="ekr.20220525082936.764"><vh>make_wildcard_trigger</vh></v>
</v>
<v t="ekr.20220525082936.765"><vh>@clean update.py</vh>
<v t="ekr.20220525082936.766"><vh>class FineGrainedBuildManager</vh>
<v t="ekr.20220525082936.767"><vh>FineGrainedBuildManager.__init__</vh></v>
<v t="ekr.20220525082936.768"><vh>FineGrainedBuildManager.update</vh></v>
<v t="ekr.20220525082936.769"><vh>FineGrainedBuildManager.trigger</vh></v>
<v t="ekr.20220525082936.770"><vh>FineGrainedBuildManager.flush_cache</vh></v>
<v t="ekr.20220525082936.771"><vh>FineGrainedBuildManager.update_one</vh></v>
<v t="ekr.20220525082936.772"><vh>FineGrainedBuildManager.update_module</vh></v>
</v>
<v t="ekr.20220525082936.773"><vh>find_unloaded_deps</vh></v>
<v t="ekr.20220525082936.774"><vh>ensure_deps_loaded</vh></v>
<v t="ekr.20220525082936.775"><vh>ensure_trees_loaded</vh></v>
<v t="ekr.20220525082936.776"><vh>class NormalUpdate</vh></v>
<v t="ekr.20220525082936.777"><vh>class BlockedUpdate</vh></v>
<v t="ekr.20220525082936.778"><vh>UpdateResult = Union[NormalUpdate, BlockedUpdate]</vh></v>
<v t="ekr.20220525082936.779"><vh>update_module_isolated</vh>
<v t="ekr.20220525082936.780"><vh>restore</vh></v>
</v>
<v t="ekr.20220525082936.781"><vh>find_relative_leaf_module</vh></v>
<v t="ekr.20220525082936.782"><vh>delete_module</vh></v>
<v t="ekr.20220525082936.783"><vh>dedupe_modules</vh></v>
<v t="ekr.20220525082936.784"><vh>get_module_to_path_map</vh></v>
<v t="ekr.20220525082936.785"><vh>get_sources</vh></v>
<v t="ekr.20220525082936.786"><vh>calculate_active_triggers</vh></v>
<v t="ekr.20220525082936.787"><vh>replace_modules_with_new_variants</vh></v>
<v t="ekr.20220525082936.788"><vh>propagate_changes_using_dependencies</vh></v>
<v t="ekr.20220525082936.789"><vh>find_targets_recursive</vh></v>
<v t="ekr.20220525082936.790"><vh>reprocess_nodes</vh>
<v t="ekr.20220525082936.791"><vh>key</vh></v>
</v>
<v t="ekr.20220525082936.792"><vh>find_symbol_tables_recursive</vh></v>
<v t="ekr.20220525082936.793"><vh>update_deps</vh></v>
<v t="ekr.20220525082936.794"><vh>lookup_target</vh>
<v t="ekr.20220525082936.795"><vh>not_found</vh></v>
</v>
<v t="ekr.20220525082936.796"><vh>is_verbose</vh></v>
<v t="ekr.20220525082936.797"><vh>target_from_node</vh></v>
<v t="ekr.20220525082936.798"><vh>if sys.platform != "win32":</vh></v>
<v t="ekr.20220525082936.799"><vh>refresh_suppressed_submodules</vh></v>
</v>
</v>
<v t="ekr.20220525082936.802"><vh>@path test</vh>
<v t="ekr.20220525082936.803"><vh>@clean config.py</vh></v>
<v t="ekr.20220525082936.804"><vh>@clean data.py</vh>
<v t="ekr.20220525082936.805"><vh>class UpdateFile</vh></v>
<v t="ekr.20220525082936.806"><vh>class DeleteFile</vh></v>
<v t="ekr.20220525082936.807"><vh>FileOperation = Union[UpdateFile, DeleteFile]</vh></v>
<v t="ekr.20220525082936.808"><vh>parse_test_case</vh></v>
<v t="ekr.20220525082936.809"><vh>class DataDrivenTestCase</vh>
<v t="ekr.20220525082936.810"><vh>DataDrivenTestCase.__init__</vh></v>
<v t="ekr.20220525082936.811"><vh>DataDrivenTestCase.runtest</vh></v>
<v t="ekr.20220525082936.812"><vh>DataDrivenTestCase.setup</vh></v>
<v t="ekr.20220525082936.813"><vh>DataDrivenTestCase.teardown</vh></v>
<v t="ekr.20220525082936.814"><vh>DataDrivenTestCase.reportinfo</vh></v>
<v t="ekr.20220525082936.815"><vh>DataDrivenTestCase.repr_failure</vh></v>
<v t="ekr.20220525082936.816"><vh>DataDrivenTestCase.find_steps</vh></v>
</v>
<v t="ekr.20220525082936.817"><vh>module_from_path</vh></v>
<v t="ekr.20220525082936.818"><vh>class TestItem</vh>
<v t="ekr.20220525082936.819"><vh>TestItem.__init__</vh></v>
</v>
<v t="ekr.20220525082936.820"><vh>parse_test_data</vh></v>
<v t="ekr.20220525082936.821"><vh>strip_list</vh></v>
<v t="ekr.20220525082936.822"><vh>collapse_line_continuation</vh></v>
<v t="ekr.20220525082936.823"><vh>expand_variables</vh></v>
<v t="ekr.20220525082936.824"><vh>expand_errors</vh></v>
<v t="ekr.20220525082936.825"><vh>fix_win_path</vh></v>
<v t="ekr.20220525082936.826"><vh>fix_cobertura_filename</vh></v>
<v t="ekr.20220525082936.827"><vh>pytest setup</vh></v>
<v t="ekr.20220525082936.828"><vh>pytest_addoption</vh></v>
<v t="ekr.20220525082936.829"><vh>pytest_pycollect_makeitem</vh></v>
<v t="ekr.20220525082936.830"><vh>split_test_cases</vh></v>
<v t="ekr.20220525082936.831"><vh>class DataSuiteCollector</vh>
<v t="ekr.20220525082936.832"><vh>DataSuiteCollector.collect</vh></v>
</v>
<v t="ekr.20220525082936.833"><vh>class DataFileCollector</vh>
<v t="ekr.20220525082936.834"><vh>DataFileCollector.from_parent</vh></v>
<v t="ekr.20220525082936.835"><vh>DataFileCollector.collect</vh></v>
</v>
<v t="ekr.20220525082936.836"><vh>add_test_name_suffix</vh></v>
<v t="ekr.20220525082936.837"><vh>is_incremental</vh></v>
<v t="ekr.20220525082936.838"><vh>has_stable_flags</vh></v>
<v t="ekr.20220525082936.839"><vh>class DataSuite</vh>
<v t="ekr.20220525082936.840"><vh>DataSuite.setup</vh></v>
<v t="ekr.20220525082936.841"><vh>DataSuite.run_case</vh></v>
</v>
</v>
<v t="ekr.20220525082936.842"><vh>@clean helpers.py</vh>
<v t="ekr.20220525082936.843"><vh>run_mypy</vh></v>
<v t="ekr.20220525082936.844"><vh>assert_string_arrays_equal</vh></v>
<v t="ekr.20220525082936.845"><vh>assert_module_equivalence</vh></v>
<v t="ekr.20220525082936.846"><vh>assert_target_equivalence</vh></v>
<v t="ekr.20220525082936.847"><vh>update_testcase_output</vh></v>
<v t="ekr.20220525082936.848"><vh>show_align_message</vh></v>
<v t="ekr.20220525082936.849"><vh>clean_up</vh></v>
<v t="ekr.20220525082936.850"><vh>local_sys_path_set</vh></v>
<v t="ekr.20220525082936.851"><vh>num_skipped_prefix_lines</vh></v>
<v t="ekr.20220525082936.852"><vh>num_skipped_suffix_lines</vh></v>
<v t="ekr.20220525082936.853"><vh>testfile_pyversion</vh></v>
<v t="ekr.20220525082936.854"><vh>testcase_pyversion</vh></v>
<v t="ekr.20220525082936.855"><vh>normalize_error_messages</vh></v>
<v t="ekr.20220525082936.856"><vh>retry_on_error</vh></v>
<v t="ekr.20220525082936.857"><vh>good_repr</vh></v>
<v t="ekr.20220525082936.858"><vh>assert_equal</vh></v>
<v t="ekr.20220525082936.859"><vh>typename</vh></v>
<v t="ekr.20220525082936.860"><vh>assert_type</vh></v>
<v t="ekr.20220525082936.861"><vh>parse_options</vh></v>
<v t="ekr.20220525082936.862"><vh>split_lines</vh></v>
<v t="ekr.20220525082936.863"><vh>write_and_fudge_mtime</vh></v>
<v t="ekr.20220525082936.864"><vh>perform_file_operations</vh></v>
<v t="ekr.20220525082936.865"><vh>check_test_output_files</vh></v>
<v t="ekr.20220525082936.866"><vh>normalize_file_output</vh></v>
</v>
<v t="ekr.20220525082936.867"><vh>@clean testapi.py</vh>
<v t="ekr.20220525082936.868"><vh>class APISuite</vh>
<v t="ekr.20220525082936.869"><vh>APISuite.setUp</vh></v>
<v t="ekr.20220525082936.870"><vh>APISuite.tearDown</vh></v>
<v t="ekr.20220525082936.871"><vh>APISuite.test_capture_bad_opt</vh></v>
<v t="ekr.20220525082936.872"><vh>APISuite.test_capture_empty</vh></v>
<v t="ekr.20220525082936.873"><vh>APISuite.test_capture_help</vh></v>
<v t="ekr.20220525082936.874"><vh>APISuite.test_capture_version</vh></v>
</v>
</v>
<v t="ekr.20220525082936.875"><vh>@clean testargs.py</vh>
<v t="ekr.20220525082936.876"><vh>class ArgSuite</vh>
<v t="ekr.20220525082936.877"><vh>ArgSuite.test_coherence</vh></v>
<v t="ekr.20220525082936.878"><vh>ArgSuite.test_executable_inference</vh></v>
</v>
</v>
<v t="ekr.20220525082936.879"><vh>@clean testcheck.py</vh>
<v t="ekr.20220525082936.880"><vh>class TypeCheckSuite</vh>
<v t="ekr.20220525082936.881"><vh>TypeCheckSuite.run_case</vh></v>
<v t="ekr.20220525082936.882"><vh>TypeCheckSuite.run_case_once</vh></v>
<v t="ekr.20220525082936.883"><vh>TypeCheckSuite.verify_cache</vh></v>
<v t="ekr.20220525082936.884"><vh>TypeCheckSuite.find_error_message_paths</vh></v>
<v t="ekr.20220525082936.885"><vh>TypeCheckSuite.find_module_files</vh></v>
<v t="ekr.20220525082936.886"><vh>TypeCheckSuite.find_missing_cache_files</vh></v>
<v t="ekr.20220525082936.887"><vh>TypeCheckSuite.parse_module</vh></v>
</v>
</v>
<v t="ekr.20220525082936.888"><vh>@clean testcmdline.py</vh>
<v t="ekr.20220525082936.889"><vh>class PythonCmdlineSuite</vh>
<v t="ekr.20220525082936.890"><vh>PythonCmdlineSuite.run_case</vh></v>
</v>
<v t="ekr.20220525082936.891"><vh>test_python_cmdline</vh></v>
<v t="ekr.20220525082936.892"><vh>parse_args</vh></v>
<v t="ekr.20220525082936.893"><vh>parse_cwd</vh></v>
</v>
<v t="ekr.20220525082936.894"><vh>@clean testdaemon.py</vh>
<v t="ekr.20220525082936.895"><vh>class DaemonSuite</vh>
<v t="ekr.20220525082936.896"><vh>DaemonSuite.run_case</vh></v>
</v>
<v t="ekr.20220525082936.897"><vh>test_daemon</vh></v>
<v t="ekr.20220525082936.898"><vh>parse_script</vh></v>
<v t="ekr.20220525082936.899"><vh>run_cmd</vh></v>
<v t="ekr.20220525082936.900"><vh>class DaemonUtilitySuite</vh>
<v t="ekr.20220525082936.901"><vh>DaemonUtilitySuite.test_filter_out_missing_top_level_packages</vh>
<v t="ekr.20220525082936.902"><vh>DaemonUtilitySuite.makepath</vh></v>
</v>
<v t="ekr.20220525082936.903"><vh>DaemonUtilitySuite.make_file</vh></v>
</v>
</v>
<v t="ekr.20220525082936.904"><vh>@clean testdeps.py</vh>
<v t="ekr.20220525082936.905"><vh>class GetDependenciesSuite</vh>
<v t="ekr.20220525082936.906"><vh>GetDependenciesSuite.run_case</vh></v>
<v t="ekr.20220525082936.907"><vh>GetDependenciesSuite.build</vh></v>
</v>
</v>
<v t="ekr.20220525082936.908"><vh>@clean testdiff.py</vh>
<v t="ekr.20220525082936.909"><vh>class ASTDiffSuite</vh>
<v t="ekr.20220525082936.910"><vh>ASTDiffSuite.run_case</vh></v>
<v t="ekr.20220525082936.911"><vh>ASTDiffSuite.build</vh></v>
</v>
</v>
<v t="ekr.20220525082936.912"><vh>@clean testerrorstream.py</vh>
<v t="ekr.20220525082936.913"><vh>class ErrorStreamSuite</vh></v>
<v t="ekr.20220525082936.914"><vh>test_error_stream</vh>
<v t="ekr.20220525082936.915"><vh>flush_errors</vh></v>
</v>
</v>
<v t="ekr.20220525082936.916"><vh>@clean testfinegrained.py</vh>
<v t="ekr.20220525082936.917"><vh>class FineGrainedSuite</vh>
<v t="ekr.20220525082936.918"><vh>FineGrainedSuite.should_skip</vh></v>
<v t="ekr.20220525082936.919"><vh>FineGrainedSuite.run_case</vh></v>
<v t="ekr.20220525082936.920"><vh>FineGrainedSuite.get_options</vh></v>
<v t="ekr.20220525082936.921"><vh>FineGrainedSuite.run_check</vh></v>
<v t="ekr.20220525082936.922"><vh>FineGrainedSuite.build</vh></v>
<v t="ekr.20220525082936.923"><vh>FineGrainedSuite.format_triggered</vh></v>
<v t="ekr.20220525082936.924"><vh>FineGrainedSuite.get_build_steps</vh></v>
<v t="ekr.20220525082936.925"><vh>FineGrainedSuite.perform_step</vh></v>
<v t="ekr.20220525082936.926"><vh>FineGrainedSuite.parse_sources</vh></v>
<v t="ekr.20220525082936.927"><vh>FineGrainedSuite.maybe_suggest</vh></v>
<v t="ekr.20220525082936.928"><vh>FineGrainedSuite.get_suggest</vh></v>
</v>
<v t="ekr.20220525082936.929"><vh>normalize_messages</vh></v>
</v>
<v t="ekr.20220525082936.930"><vh>@clean testfinegrainedcache.py</vh>
<v t="ekr.20220525082936.931"><vh>class FineGrainedCacheSuite</vh></v>
</v>
<v t="ekr.20220525082936.932"><vh>@clean testformatter.py</vh>
<v t="ekr.20220525082936.933"><vh>class FancyErrorFormattingTestCases</vh>
<v t="ekr.20220525082936.934"><vh>FancyErrorFormattingTestCases.test_trim_source</vh></v>
<v t="ekr.20220525082936.935"><vh>FancyErrorFormattingTestCases.test_split_words</vh></v>
</v>
</v>
<v t="ekr.20220525082936.936"><vh>@clean testfscache.py</vh>
<v t="ekr.20220525082936.937"><vh>class TestFileSystemCache</vh>
<v t="ekr.20220525082936.938"><vh>TestFileSystemCache.setUp</vh></v>
<v t="ekr.20220525082936.939"><vh>TestFileSystemCache.tearDown</vh></v>
<v t="ekr.20220525082936.940"><vh>TestFileSystemCache.test_isfile_case_1</vh></v>
<v t="ekr.20220525082936.941"><vh>TestFileSystemCache.test_isfile_case_2</vh></v>
<v t="ekr.20220525082936.942"><vh>TestFileSystemCache.test_isfile_case_3</vh></v>
<v t="ekr.20220525082936.943"><vh>TestFileSystemCache.test_isfile_case_other_directory</vh></v>
<v t="ekr.20220525082936.944"><vh>TestFileSystemCache.make_file</vh></v>
<v t="ekr.20220525082936.945"><vh>TestFileSystemCache.isfile_case</vh></v>
</v>
</v>
<v t="ekr.20220525082936.946"><vh>@clean testgraph.py</vh>
<v t="ekr.20220525082936.947"><vh>class GraphSuite</vh>
<v t="ekr.20220525082936.948"><vh>GraphSuite.test_topsort</vh></v>
<v t="ekr.20220525082936.949"><vh>GraphSuite.test_scc</vh></v>
<v t="ekr.20220525082936.950"><vh>GraphSuite._make_manager</vh></v>
<v t="ekr.20220525082936.951"><vh>GraphSuite.test_sorted_components</vh></v>
<v t="ekr.20220525082936.952"><vh>GraphSuite.test_order_ascc</vh></v>
</v>
</v>
<v t="ekr.20220525082936.953"><vh>@clean testinfer.py</vh>
<v t="ekr.20220525082936.954"><vh>class MapActualsToFormalsSuite</vh>
<v t="ekr.20220525082936.955"><vh>MapActualsToFormalsSuite.test_basic</vh></v>
<v t="ekr.20220525082936.956"><vh>MapActualsToFormalsSuite.test_positional_only</vh></v>
<v t="ekr.20220525082936.957"><vh>MapActualsToFormalsSuite.test_optional</vh></v>
<v t="ekr.20220525082936.958"><vh>MapActualsToFormalsSuite.test_callee_star</vh></v>
<v t="ekr.20220525082936.959"><vh>MapActualsToFormalsSuite.test_caller_star</vh></v>
<v t="ekr.20220525082936.960"><vh>MapActualsToFormalsSuite.test_too_many_caller_args</vh></v>
<v t="ekr.20220525082936.961"><vh>MapActualsToFormalsSuite.test_tuple_star</vh></v>
<v t="ekr.20220525082936.962"><vh>MapActualsToFormalsSuite.tuple</vh></v>
<v t="ekr.20220525082936.963"><vh>MapActualsToFormalsSuite.test_named_args</vh></v>
<v t="ekr.20220525082936.964"><vh>MapActualsToFormalsSuite.test_some_named_args</vh></v>
<v t="ekr.20220525082936.965"><vh>MapActualsToFormalsSuite.test_missing_named_arg</vh></v>
<v t="ekr.20220525082936.966"><vh>MapActualsToFormalsSuite.test_duplicate_named_arg</vh></v>
<v t="ekr.20220525082936.967"><vh>MapActualsToFormalsSuite.test_varargs_and_bare_asterisk</vh></v>
<v t="ekr.20220525082936.968"><vh>MapActualsToFormalsSuite.test_keyword_varargs</vh></v>
<v t="ekr.20220525082936.969"><vh>MapActualsToFormalsSuite.test_both_kinds_of_varargs</vh></v>
<v t="ekr.20220525082936.970"><vh>MapActualsToFormalsSuite.test_special_cases</vh></v>
<v t="ekr.20220525082936.971"><vh>MapActualsToFormalsSuite.assert_map</vh></v>
<v t="ekr.20220525082936.972"><vh>MapActualsToFormalsSuite.assert_vararg_map</vh></v>
</v>
<v t="ekr.20220525082936.973"><vh>expand_caller_kinds</vh></v>
<v t="ekr.20220525082936.974"><vh>expand_callee_kinds</vh></v>
<v t="ekr.20220525082936.975"><vh>class OperandDisjointDictSuite</vh>
<v t="ekr.20220525082936.976"><vh>OperandDisjointDictSuite.new</vh></v>
<v t="ekr.20220525082936.977"><vh>OperandDisjointDictSuite.test_independent_maps</vh></v>
<v t="ekr.20220525082936.978"><vh>OperandDisjointDictSuite.test_partial_merging</vh></v>
<v t="ekr.20220525082936.979"><vh>OperandDisjointDictSuite.test_full_merging</vh></v>
<v t="ekr.20220525082936.980"><vh>OperandDisjointDictSuite.test_merge_with_multiple_overlaps</vh></v>
</v>
<v t="ekr.20220525082936.981"><vh>class OperandComparisonGroupingSuite</vh>
<v t="ekr.20220525082936.982"><vh>OperandComparisonGroupingSuite.literal_keymap</vh></v>
<v t="ekr.20220525082936.983"><vh>OperandComparisonGroupingSuite.test_basic_cases</vh></v>
<v t="ekr.20220525082936.984"><vh>OperandComparisonGroupingSuite.test_multiple_groups</vh></v>
<v t="ekr.20220525082936.985"><vh>OperandComparisonGroupingSuite.test_multiple_groups_coalescing</vh></v>
<v t="ekr.20220525082936.986"><vh>OperandComparisonGroupingSuite.test_multiple_groups_different_operators</vh></v>
<v t="ekr.20220525082936.987"><vh>OperandComparisonGroupingSuite.test_single_pair</vh></v>
<v t="ekr.20220525082936.988"><vh>OperandComparisonGroupingSuite.test_empty_pair_list</vh></v>
</v>
</v>
<v t="ekr.20220525082936.989"><vh>@clean testipc.py</vh>
<v t="ekr.20220525082936.990"><vh>server</vh></v>
<v t="ekr.20220525082936.991"><vh>class IPCTests</vh>
<v t="ekr.20220525082936.992"><vh>IPCTests.test_transaction_large</vh></v>
<v t="ekr.20220525082936.993"><vh>IPCTests.test_connect_twice</vh></v>
<v t="ekr.20220525082936.994"><vh>IPCTests.test_connect_alot</vh></v>
</v>
</v>
<v t="ekr.20220525082936.995"><vh>@clean testmerge.py</vh>
<v t="ekr.20220525082936.996"><vh>class ASTMergeSuite</vh>
<v t="ekr.20220525082936.997"><vh>ASTMergeSuite.setup</vh></v>
<v t="ekr.20220525082936.998"><vh>ASTMergeSuite.run_case</vh></v>
<v t="ekr.20220525082936.999"><vh>ASTMergeSuite.build</vh></v>
<v t="ekr.20220525082936.1000"><vh>ASTMergeSuite.build_increment</vh></v>
<v t="ekr.20220525082936.1001"><vh>ASTMergeSuite.dump</vh></v>
<v t="ekr.20220525082936.1002"><vh>ASTMergeSuite.dump_asts</vh></v>
<v t="ekr.20220525082936.1003"><vh>ASTMergeSuite.dump_symbol_tables</vh></v>
<v t="ekr.20220525082936.1004"><vh>ASTMergeSuite.dump_symbol_table</vh></v>
<v t="ekr.20220525082936.1005"><vh>ASTMergeSuite.format_symbol_table_node</vh></v>
<v t="ekr.20220525082936.1006"><vh>ASTMergeSuite.dump_typeinfos</vh></v>
<v t="ekr.20220525082936.1007"><vh>ASTMergeSuite.dump_typeinfos_recursive</vh></v>
<v t="ekr.20220525082936.1008"><vh>ASTMergeSuite.dump_typeinfo</vh></v>
<v t="ekr.20220525082936.1009"><vh>ASTMergeSuite.dump_types</vh></v>
<v t="ekr.20220525082936.1010"><vh>ASTMergeSuite.format_type</vh></v>
</v>
<v t="ekr.20220525082936.1011"><vh>is_dumped_module</vh></v>
</v>
<v t="ekr.20220525082936.1012"><vh>@clean testmodulefinder.py</vh>
<v t="ekr.20220525082936.1013"><vh>class ModuleFinderSuite</vh>
<v t="ekr.20220525082936.1014"><vh>ModuleFinderSuite.setUp</vh></v>
<v t="ekr.20220525082936.1015"><vh>ModuleFinderSuite.test__no_namespace_packages__nsx</vh></v>
<v t="ekr.20220525082936.1016"><vh>ModuleFinderSuite.test__no_namespace_packages__nsx_a</vh></v>
<v t="ekr.20220525082936.1017"><vh>ModuleFinderSuite.test__no_namespace_packages__find_a_in_pkg1</vh></v>
<v t="ekr.20220525082936.1018"><vh>ModuleFinderSuite.test__no_namespace_packages__find_b_in_pkg2</vh></v>
<v t="ekr.20220525082936.1019"><vh>ModuleFinderSuite.test__find_nsx_as_namespace_pkg_in_pkg1</vh></v>
<v t="ekr.20220525082936.1020"><vh>ModuleFinderSuite.test__find_nsx_a_init_in_pkg1</vh></v>
<v t="ekr.20220525082936.1021"><vh>ModuleFinderSuite.test__find_nsx_b_init_in_pkg2</vh></v>
<v t="ekr.20220525082936.1022"><vh>ModuleFinderSuite.test__find_nsx_c_c_in_pkg3</vh></v>
<v t="ekr.20220525082936.1023"><vh>ModuleFinderSuite.test__find_nsy_a__init_pyi</vh></v>
<v t="ekr.20220525082936.1024"><vh>ModuleFinderSuite.test__find_nsy_b__init_py</vh></v>
<v t="ekr.20220525082936.1025"><vh>ModuleFinderSuite.test__find_nsy_c_pyi</vh></v>
<v t="ekr.20220525082936.1026"><vh>ModuleFinderSuite.test__find_a_in_pkg1</vh></v>
<v t="ekr.20220525082936.1027"><vh>ModuleFinderSuite.test__find_b_init_in_pkg2</vh></v>
<v t="ekr.20220525082936.1028"><vh>ModuleFinderSuite.test__find_d_nowhere</vh></v>
</v>
<v t="ekr.20220525082936.1029"><vh>class ModuleFinderSitePackagesSuite</vh>
<v t="ekr.20220525082936.1030"><vh>ModuleFinderSitePackagesSuite.setUp</vh></v>
<v t="ekr.20220525082936.1031"><vh>ModuleFinderSitePackagesSuite.path</vh></v>
<v t="ekr.20220525082936.1032"><vh>ModuleFinderSitePackagesSuite.test__packages_with_ns</vh></v>
<v t="ekr.20220525082936.1033"><vh>ModuleFinderSitePackagesSuite.test__packages_without_ns</vh></v>
</v>
</v>
<v t="ekr.20220525082936.1034"><vh>@clean testmypyc.py</vh>
<v t="ekr.20220525082936.1035"><vh>class MypycTest</vh></v>
</v>
<v t="ekr.20220525082936.1036"><vh>@clean testparse.py</vh>
<v t="ekr.20220525082936.1037"><vh>class ParserSuite</vh>
<v t="ekr.20220525082936.1038"><vh>ParserSuite.run_case</vh></v>
</v>
<v t="ekr.20220525082936.1039"><vh>test_parser</vh></v>
<v t="ekr.20220525082936.1040"><vh>The file name shown in test case output. This is displayed in error</vh></v>
<v t="ekr.20220525082936.1041"><vh>class ParseErrorSuite</vh></v>
<v t="ekr.20220525082936.1042"><vh>test_parse_error</vh></v>
</v>
<v t="ekr.20220525082936.1043"><vh>@clean testpep561.py</vh>
<v t="ekr.20220525082936.1044"><vh>class PEP561Suite</vh>
<v t="ekr.20220525082936.1045"><vh>PEP561Suite.run_case</vh></v>
</v>
<v t="ekr.20220525082936.1046"><vh>virtualenv</vh></v>
<v t="ekr.20220525082936.1047"><vh>install_package</vh></v>
<v t="ekr.20220525082936.1048"><vh>test_pep561</vh></v>
<v t="ekr.20220525082936.1049"><vh>parse_pkgs</vh></v>
<v t="ekr.20220525082936.1050"><vh>parse_mypy_args</vh></v>
<v t="ekr.20220525082936.1051"><vh>test_mypy_path_is_respected</vh></v>
</v>
<v t="ekr.20220525082936.1052"><vh>@clean testpythoneval.py</vh>
<v t="ekr.20220525082936.1053"><vh>class PythonEvaluationSuite</vh>
<v t="ekr.20220525082936.1054"><vh>PythonEvaluationSuite.run_case</vh></v>
</v>
<v t="ekr.20220525082936.1055"><vh>test_python_evaluation</vh></v>
<v t="ekr.20220525082936.1056"><vh>adapt_output</vh></v>
</v>
<v t="ekr.20220525082936.1057"><vh>@clean testreports.py</vh>
<v t="ekr.20220525082936.1058"><vh>class CoberturaReportSuite</vh>
<v t="ekr.20220525082936.1059"><vh>CoberturaReportSuite.test_get_line_rate</vh></v>
<v t="ekr.20220525082936.1060"><vh>CoberturaReportSuite.test_as_xml</vh></v>
</v>
</v>
<v t="ekr.20220525082936.1061"><vh>@clean testsemanal.py</vh>
<v t="ekr.20220525082936.1062"><vh>get_semanal_options</vh></v>
<v t="ekr.20220525082936.1063"><vh>class SemAnalSuite</vh></v>
<v t="ekr.20220525082936.1064"><vh>test_semanal</vh></v>
<v t="ekr.20220525082936.1065"><vh>Semantic analyzer error test cases</vh></v>
<v t="ekr.20220525082936.1066"><vh>class SemAnalErrorSuite</vh></v>
<v t="ekr.20220525082936.1067"><vh>test_semanal_error</vh></v>
<v t="ekr.20220525082936.1068"><vh>SymbolNode table export test cases</vh></v>
<v t="ekr.20220525082936.1069"><vh>class SemAnalSymtableSuite</vh>
<v t="ekr.20220525082936.1070"><vh>SemAnalSymtableSuite.run_case</vh></v>
</v>
<v t="ekr.20220525082936.1071"><vh>class SemAnalTypeInfoSuite</vh>
<v t="ekr.20220525082936.1072"><vh>SemAnalTypeInfoSuite.run_case</vh></v>
</v>
<v t="ekr.20220525082936.1073"><vh>class TypeInfoMap</vh>
<v t="ekr.20220525082936.1074"><vh>TypeInfoMap.__str__</vh></v>
</v>
</v>
<v t="ekr.20220525082936.1075"><vh>@clean testsolve.py</vh>
<v t="ekr.20220525082936.1076"><vh>class SolveSuite</vh>
<v t="ekr.20220525082936.1077"><vh>SolveSuite.setUp</vh></v>
<v t="ekr.20220525082936.1078"><vh>SolveSuite.test_empty_input</vh></v>
<v t="ekr.20220525082936.1079"><vh>SolveSuite.test_simple_supertype_constraints</vh></v>
<v t="ekr.20220525082936.1080"><vh>SolveSuite.test_simple_subtype_constraints</vh></v>
<v t="ekr.20220525082936.1081"><vh>SolveSuite.test_both_kinds_of_constraints</vh></v>
<v t="ekr.20220525082936.1082"><vh>SolveSuite.test_unsatisfiable_constraints</vh></v>
<v t="ekr.20220525082936.1083"><vh>SolveSuite.test_exactly_specified_result</vh></v>
<v t="ekr.20220525082936.1084"><vh>SolveSuite.test_multiple_variables</vh></v>
<v t="ekr.20220525082936.1085"><vh>SolveSuite.test_no_constraints_for_var</vh></v>
<v t="ekr.20220525082936.1086"><vh>SolveSuite.test_simple_constraints_with_dynamic_type</vh></v>
<v t="ekr.20220525082936.1087"><vh>SolveSuite.test_both_normal_and_any_types_in_results</vh></v>
<v t="ekr.20220525082936.1088"><vh>SolveSuite.assert_solve</vh></v>
<v t="ekr.20220525082936.1089"><vh>SolveSuite.supc</vh></v>
<v t="ekr.20220525082936.1090"><vh>SolveSuite.subc</vh></v>
</v>
</v>
<v t="ekr.20220525082936.1091"><vh>@clean teststubgen.py</vh>
<v t="ekr.20220525082936.1092"><vh>class StubgenCmdLineSuite</vh>
<v t="ekr.20220525082936.1093"><vh>StubgenCmdLineSuite.test_files_found</vh></v>
<v t="ekr.20220525082936.1094"><vh>StubgenCmdLineSuite.test_packages_found</vh></v>
<v t="ekr.20220525082936.1095"><vh>StubgenCmdLineSuite.test_module_not_found</vh></v>
<v t="ekr.20220525082936.1096"><vh>StubgenCmdLineSuite.make_file</vh></v>
<v t="ekr.20220525082936.1097"><vh>StubgenCmdLineSuite.run</vh></v>
</v>
<v t="ekr.20220525082936.1098"><vh>class StubgenCliParseSuite</vh>
<v t="ekr.20220525082936.1099"><vh>StubgenCliParseSuite.test_walk_packages</vh></v>
</v>
<v t="ekr.20220525082936.1100"><vh>class StubgenUtilSuite</vh>
<v t="ekr.20220525082936.1101"><vh>StubgenUtilSuite.test_parse_signature</vh></v>
<v t="ekr.20220525082936.1102"><vh>StubgenUtilSuite.test_parse_signature_with_args</vh></v>
<v t="ekr.20220525082936.1103"><vh>StubgenUtilSuite.test_parse_signature_with_optional_args</vh></v>
<v t="ekr.20220525082936.1104"><vh>StubgenUtilSuite.test_parse_signature_with_default_arg</vh></v>
<v t="ekr.20220525082936.1105"><vh>StubgenUtilSuite.test_parse_signature_with_qualified_function</vh></v>
<v t="ekr.20220525082936.1106"><vh>StubgenUtilSuite.test_parse_signature_with_kw_only_arg</vh></v>
<v t="ekr.20220525082936.1107"><vh>StubgenUtilSuite.test_parse_signature_with_star_arg</vh></v>
<v t="ekr.20220525082936.1108"><vh>StubgenUtilSuite.test_parse_signature_with_star_star_arg</vh></v>
<v t="ekr.20220525082936.1109"><vh>StubgenUtilSuite.assert_parse_signature</vh></v>
<v t="ekr.20220525082936.1110"><vh>StubgenUtilSuite.test_build_signature</vh></v>
<v t="ekr.20220525082936.1111"><vh>StubgenUtilSuite.test_parse_all_signatures</vh></v>
<v t="ekr.20220525082936.1112"><vh>StubgenUtilSuite.test_find_unique_signatures</vh></v>
<v t="ekr.20220525082936.1113"><vh>StubgenUtilSuite.test_infer_sig_from_docstring</vh></v>
<v t="ekr.20220525082936.1114"><vh>StubgenUtilSuite.test_infer_sig_from_docstring_duplicate_args</vh></v>
<v t="ekr.20220525082936.1115"><vh>StubgenUtilSuite.test_infer_sig_from_docstring_bad_indentation</vh></v>
<v t="ekr.20220525082936.1116"><vh>StubgenUtilSuite.test_infer_arg_sig_from_anon_docstring</vh></v>
<v t="ekr.20220525082936.1117"><vh>StubgenUtilSuite.test_infer_prop_type_from_docstring</vh></v>
<v t="ekr.20220525082936.1118"><vh>StubgenUtilSuite.test_infer_sig_from_docstring_square_brackets</vh></v>
<v t="ekr.20220525082936.1119"><vh>StubgenUtilSuite.test_remove_misplaced_type_comments_1</vh></v>
<v t="ekr.20220525082936.1120"><vh>StubgenUtilSuite.test_remove_misplaced_type_comments_2</vh></v>
<v t="ekr.20220525082936.1121"><vh>StubgenUtilSuite.test_remove_misplaced_type_comments_3</vh></v>
<v t="ekr.20220525082936.1122"><vh>StubgenUtilSuite.test_remove_misplaced_type_comments_4</vh></v>
<v t="ekr.20220525082936.1123"><vh>StubgenUtilSuite.test_remove_misplaced_type_comments_5</vh></v>
<v t="ekr.20220525082936.1124"><vh>StubgenUtilSuite.test_remove_misplaced_type_comments_bytes</vh></v>
<v t="ekr.20220525082936.1125"><vh>StubgenUtilSuite.@unittest.skipIf(sys.platform == 'win32',</vh></v>
<v t="ekr.20220525082936.1126"><vh>StubgenUtilSuite.test_common_dir_prefix_unix</vh></v>
<v t="ekr.20220525082936.1127"><vh>StubgenUtilSuite.@unittest.skipIf(sys.platform != 'win32',</vh></v>
<v t="ekr.20220525082936.1128"><vh>StubgenUtilSuite.test_common_dir_prefix_win</vh></v>
</v>
<v t="ekr.20220525082936.1129"><vh>class StubgenHelpersSuite</vh>
<v t="ekr.20220525082936.1130"><vh>StubgenHelpersSuite.test_is_blacklisted_path</vh></v>
<v t="ekr.20220525082936.1131"><vh>StubgenHelpersSuite.test_is_non_library_module</vh></v>
</v>
<v t="ekr.20220525082936.1132"><vh>class StubgenPythonSuite</vh>
<v t="ekr.20220525082936.1133"><vh>StubgenPythonSuite.run_case</vh></v>
<v t="ekr.20220525082936.1134"><vh>StubgenPythonSuite.run_case_inner</vh></v>
<v t="ekr.20220525082936.1135"><vh>StubgenPythonSuite.parse_flags</vh></v>
<v t="ekr.20220525082936.1136"><vh>StubgenPythonSuite.parse_modules</vh></v>
<v t="ekr.20220525082936.1137"><vh>StubgenPythonSuite.add_file</vh></v>
</v>
<v t="ekr.20220525082936.1138"><vh>self_arg = ArgSig(name='self')</vh></v>
<v t="ekr.20220525082936.1139"><vh>class TestBaseClass</vh></v>
<v t="ekr.20220525082936.1140"><vh>class TestClass</vh></v>
<v t="ekr.20220525082936.1141"><vh>class StubgencSuite</vh>
<v t="ekr.20220525082936.1142"><vh>StubgencSuite.test_infer_hash_sig</vh></v>
<v t="ekr.20220525082936.1143"><vh>StubgencSuite.test_infer_getitem_sig</vh></v>
<v t="ekr.20220525082936.1144"><vh>StubgencSuite.test_infer_setitem_sig</vh></v>
<v t="ekr.20220525082936.1145"><vh>StubgencSuite.test_infer_binary_op_sig</vh></v>
<v t="ekr.20220525082936.1146"><vh>StubgencSuite.test_infer_unary_op_sig</vh></v>
<v t="ekr.20220525082936.1147"><vh>StubgencSuite.test_generate_c_type_stub_no_crash_for_object</vh></v>
<v t="ekr.20220525082936.1148"><vh>StubgencSuite.test_generate_c_type_stub_variable_type_annotation</vh></v>
<v t="ekr.20220525082936.1149"><vh>StubgencSuite.test_generate_c_type_inheritance</vh></v>
<v t="ekr.20220525082936.1150"><vh>StubgencSuite.test_generate_c_type_inheritance_same_module</vh></v>
<v t="ekr.20220525082936.1151"><vh>StubgencSuite.test_generate_c_type_inheritance_other_module</vh></v>
<v t="ekr.20220525082936.1152"><vh>StubgencSuite.test_generate_c_type_inheritance_builtin_type</vh></v>
<v t="ekr.20220525082936.1153"><vh>StubgencSuite.test_generate_c_type_with_docstring</vh>
<v t="ekr.20220525082936.1154"><vh>StubgencSuite.test</vh></v>
</v>
<v t="ekr.20220525082936.1155"><vh>StubgencSuite.test_generate_c_type_with_docstring_no_self_arg</vh>
<v t="ekr.20220525082936.1156"><vh>StubgencSuite.test</vh></v>
</v>
<v t="ekr.20220525082936.1157"><vh>StubgencSuite.test_generate_c_type_classmethod</vh>
<v t="ekr.20220525082936.1158"><vh>StubgencSuite.test</vh></v>
</v>
<v t="ekr.20220525082936.1159"><vh>StubgencSuite.test_generate_c_type_with_docstring_empty_default</vh>
<v t="ekr.20220525082936.1160"><vh>StubgencSuite.test</vh></v>
</v>
<v t="ekr.20220525082936.1161"><vh>StubgencSuite.test_generate_c_function_other_module_arg</vh></v>
<v t="ekr.20220525082936.1162"><vh>StubgencSuite.test_generate_c_function_same_module_arg</vh></v>
<v t="ekr.20220525082936.1163"><vh>StubgencSuite.test_generate_c_function_other_module_ret</vh></v>
<v t="ekr.20220525082936.1164"><vh>StubgencSuite.test_generate_c_function_same_module_ret</vh></v>
<v t="ekr.20220525082936.1165"><vh>StubgencSuite.test_generate_c_property_with_pybind11</vh>
<v t="ekr.20220525082936.1166"><vh>StubgencSuite.get_attribute</vh></v>
</v>
<v t="ekr.20220525082936.1167"><vh>StubgencSuite.test_generate_c_property_with_rw_property</vh>
<v t="ekr.20220525082936.1168"><vh>StubgencSuite.__init__</vh></v>
<v t="ekr.20220525082936.1169"><vh>StubgencSuite.attribute</vh></v>
<v t="ekr.20220525082936.1170"><vh>StubgencSuite.attribute</vh></v>
</v>
<v t="ekr.20220525082936.1171"><vh>StubgencSuite.test_generate_c_type_with_single_arg_generic</vh>
<v t="ekr.20220525082936.1172"><vh>StubgencSuite.test</vh></v>
</v>
<v t="ekr.20220525082936.1173"><vh>StubgencSuite.test_generate_c_type_with_double_arg_generic</vh>
<v t="ekr.20220525082936.1174"><vh>StubgencSuite.test</vh></v>
</v>
<v t="ekr.20220525082936.1175"><vh>StubgencSuite.test_generate_c_type_with_nested_generic</vh>
<v t="ekr.20220525082936.1176"><vh>StubgencSuite.test</vh></v>
</v>
<v t="ekr.20220525082936.1177"><vh>StubgencSuite.test_generate_c_type_with_generic_using_other_module_first</vh>
<v t="ekr.20220525082936.1178"><vh>StubgencSuite.test</vh></v>
</v>
<v t="ekr.20220525082936.1179"><vh>StubgencSuite.test_generate_c_type_with_generic_using_other_module_last</vh>
<v t="ekr.20220525082936.1180"><vh>StubgencSuite.test</vh></v>
</v>
<v t="ekr.20220525082936.1181"><vh>StubgencSuite.test_generate_c_type_with_overload_pybind11</vh>
<v t="ekr.20220525082936.1182"><vh>StubgencSuite.__init__</vh></v>
</v>
</v>
<v t="ekr.20220525082936.1183"><vh>class ArgSigSuite</vh>
<v t="ekr.20220525082936.1184"><vh>ArgSigSuite.test_repr</vh></v>
</v>
<v t="ekr.20220525082936.1185"><vh>class IsValidTypeSuite</vh>
<v t="ekr.20220525082936.1186"><vh>IsValidTypeSuite.test_is_valid_type</vh></v>
</v>
<v t="ekr.20220525082936.1187"><vh>class ModuleInspectSuite</vh>
<v t="ekr.20220525082936.1188"><vh>ModuleInspectSuite.test_python_module</vh></v>
<v t="ekr.20220525082936.1189"><vh>ModuleInspectSuite.test_python_package</vh></v>
<v t="ekr.20220525082936.1190"><vh>ModuleInspectSuite.test_c_module</vh></v>
<v t="ekr.20220525082936.1191"><vh>ModuleInspectSuite.test_non_existent</vh></v>
</v>
<v t="ekr.20220525082936.1192"><vh>module_to_path</vh></v>
</v>
<v t="ekr.20220525082936.1193"><vh>@clean teststubinfo.py</vh>
<v t="ekr.20220525082936.1194"><vh>class TestStubInfo</vh>
<v t="ekr.20220525082936.1195"><vh>TestStubInfo.test_is_legacy_bundled_packages</vh></v>
</v>
</v>
<v t="ekr.20220525082936.1196"><vh>@clean teststubtest.py</vh>
<v t="ekr.20220525082936.1197"><vh>use_tmp_dir</vh></v>
<v t="ekr.20220525082936.1198"><vh>class TEST_MODULE_NAME = "test_module"</vh></v>
<v t="ekr.20220525082936.1199"><vh>run_stubtest</vh></v>
<v t="ekr.20220525082936.1200"><vh>class Case</vh></v>
<v t="ekr.20220525082936.1201"><vh>collect_cases</vh>
<v t="ekr.20220525082936.1202"><vh>test</vh></v>
</v>
<v t="ekr.20220525082936.1203"><vh>class StubtestUnit</vh>
<v t="ekr.20220525082936.1204"><vh>StubtestUnit.test_basic_good</vh></v>
<v t="ekr.20220525082936.1205"><vh>StubtestUnit.test_types</vh></v>
<v t="ekr.20220525082936.1206"><vh>StubtestUnit.test_coroutines</vh></v>
<v t="ekr.20220525082936.1207"><vh>StubtestUnit.test_arg_name</vh></v>
<v t="ekr.20220525082936.1208"><vh>StubtestUnit.test_arg_kind</vh></v>
<v t="ekr.20220525082936.1209"><vh>StubtestUnit.test_default_value</vh></v>
<v t="ekr.20220525082936.1210"><vh>StubtestUnit.test_static_class_method</vh></v>
<v t="ekr.20220525082936.1211"><vh>StubtestUnit.test_arg_mismatch</vh></v>
<v t="ekr.20220525082936.1212"><vh>StubtestUnit.test_varargs_varkwargs</vh></v>
<v t="ekr.20220525082936.1213"><vh>StubtestUnit.test_overload</vh></v>
<v t="ekr.20220525082936.1214"><vh>StubtestUnit.test_property</vh></v>
<v t="ekr.20220525082936.1215"><vh>StubtestUnit.test_var</vh></v>
<v t="ekr.20220525082936.1216"><vh>StubtestUnit.test_type_alias</vh></v>
<v t="ekr.20220525082936.1217"><vh>StubtestUnit.test_enum</vh></v>
<v t="ekr.20220525082936.1218"><vh>StubtestUnit.test_decorator</vh></v>
<v t="ekr.20220525082936.1219"><vh>StubtestUnit.test_missing</vh></v>
<v t="ekr.20220525082936.1220"><vh>StubtestUnit.test_missing_no_runtime_all</vh></v>
<v t="ekr.20220525082936.1221"><vh>StubtestUnit.test_non_public_1</vh></v>
<v t="ekr.20220525082936.1222"><vh>StubtestUnit.test_non_public_2</vh></v>
<v t="ekr.20220525082936.1223"><vh>StubtestUnit.test_dunders</vh></v>
<v t="ekr.20220525082936.1224"><vh>StubtestUnit.test_not_subclassable</vh></v>
<v t="ekr.20220525082936.1225"><vh>StubtestUnit.test_name_mangling</vh></v>
<v t="ekr.20220525082936.1226"><vh>StubtestUnit.test_mro</vh></v>
<v t="ekr.20220525082936.1227"><vh>StubtestUnit.test_good_literal</vh></v>
<v t="ekr.20220525082936.1228"><vh>StubtestUnit.test_bad_literal</vh></v>
<v t="ekr.20220525082936.1229"><vh>StubtestUnit.test_special_subtype</vh></v>
<v t="ekr.20220525082936.1230"><vh>StubtestUnit.test_protocol</vh></v>
</v>
<v t="ekr.20220525082936.1231"><vh>remove_color_code</vh></v>
<v t="ekr.20220525082936.1232"><vh>class StubtestMiscUnit</vh>
<v t="ekr.20220525082936.1233"><vh>StubtestMiscUnit.test_output</vh></v>
<v t="ekr.20220525082936.1234"><vh>StubtestMiscUnit.test_ignore_flags</vh></v>
<v t="ekr.20220525082936.1235"><vh>StubtestMiscUnit.test_allowlist</vh></v>
<v t="ekr.20220525082936.1236"><vh>StubtestMiscUnit.test_mypy_build</vh></v>
<v t="ekr.20220525082936.1237"><vh>StubtestMiscUnit.test_missing_stubs</vh></v>
<v t="ekr.20220525082936.1238"><vh>StubtestMiscUnit.test_get_typeshed_stdlib_modules</vh></v>
<v t="ekr.20220525082936.1239"><vh>StubtestMiscUnit.test_signature</vh></v>
<v t="ekr.20220525082936.1240"><vh>StubtestMiscUnit.test_config_file</vh></v>
</v>
</v>
<v t="ekr.20220525082936.1241"><vh>@clean testsubtypes.py</vh>
<v t="ekr.20220525082936.1242"><vh>class SubtypingSuite</vh>
<v t="ekr.20220525082936.1243"><vh>SubtypingSuite.setUp</vh></v>
<v t="ekr.20220525082936.1244"><vh>SubtypingSuite.test_trivial_cases</vh></v>
<v t="ekr.20220525082936.1245"><vh>SubtypingSuite.test_instance_subtyping</vh></v>
<v t="ekr.20220525082936.1246"><vh>SubtypingSuite.test_simple_generic_instance_subtyping_invariant</vh></v>
<v t="ekr.20220525082936.1247"><vh>SubtypingSuite.test_simple_generic_instance_subtyping_covariant</vh></v>
<v t="ekr.20220525082936.1248"><vh>SubtypingSuite.test_simple_generic_instance_subtyping_contravariant</vh></v>
<v t="ekr.20220525082936.1249"><vh>SubtypingSuite.test_generic_subtyping_with_inheritance_invariant</vh></v>
<v t="ekr.20220525082936.1250"><vh>SubtypingSuite.test_generic_subtyping_with_inheritance_covariant</vh></v>
<v t="ekr.20220525082936.1251"><vh>SubtypingSuite.test_generic_subtyping_with_inheritance_contravariant</vh></v>
<v t="ekr.20220525082936.1252"><vh>SubtypingSuite.test_interface_subtyping</vh></v>
<v t="ekr.20220525082936.1253"><vh>SubtypingSuite.test_generic_interface_subtyping</vh></v>
<v t="ekr.20220525082936.1254"><vh>SubtypingSuite.test_basic_callable_subtyping</vh></v>
<v t="ekr.20220525082936.1255"><vh>SubtypingSuite.test_default_arg_callable_subtyping</vh></v>
<v t="ekr.20220525082936.1256"><vh>SubtypingSuite.test_var_arg_callable_subtyping_1</vh></v>
<v t="ekr.20220525082936.1257"><vh>SubtypingSuite.test_var_arg_callable_subtyping_2</vh></v>
<v t="ekr.20220525082936.1258"><vh>SubtypingSuite.test_var_arg_callable_subtyping_3</vh></v>
<v t="ekr.20220525082936.1259"><vh>SubtypingSuite.test_var_arg_callable_subtyping_4</vh></v>
<v t="ekr.20220525082936.1260"><vh>SubtypingSuite.test_var_arg_callable_subtyping_5</vh></v>
<v t="ekr.20220525082936.1261"><vh>SubtypingSuite.test_var_arg_callable_subtyping_6</vh></v>
<v t="ekr.20220525082936.1262"><vh>SubtypingSuite.test_var_arg_callable_subtyping_7</vh></v>
<v t="ekr.20220525082936.1263"><vh>SubtypingSuite.test_var_arg_callable_subtyping_8</vh></v>
<v t="ekr.20220525082936.1264"><vh>SubtypingSuite.test_var_arg_callable_subtyping_9</vh></v>
<v t="ekr.20220525082936.1265"><vh>SubtypingSuite.test_type_callable_subtyping</vh></v>
<v t="ekr.20220525082936.1266"><vh>SubtypingSuite.IDEA: Maybe add these test cases (they are tested pretty well in type</vh></v>
<v t="ekr.20220525082936.1267"><vh>SubtypingSuite.assert_subtype</vh></v>
<v t="ekr.20220525082936.1268"><vh>SubtypingSuite.assert_not_subtype</vh></v>
<v t="ekr.20220525082936.1269"><vh>SubtypingSuite.assert_strict_subtype</vh></v>
<v t="ekr.20220525082936.1270"><vh>SubtypingSuite.assert_equivalent</vh></v>
<v t="ekr.20220525082936.1271"><vh>SubtypingSuite.assert_unrelated</vh></v>
</v>
</v>
<v t="ekr.20220525082936.1272"><vh>@clean testtransform.py</vh>
<v t="ekr.20220525082936.1273"><vh>class TransformSuite</vh>
<v t="ekr.20220525082936.1274"><vh>TransformSuite.run_case</vh></v>
</v>
<v t="ekr.20220525082936.1275"><vh>test_transform</vh></v>
</v>
<v t="ekr.20220525082936.1276"><vh>@clean testtypegen.py</vh>
<v t="ekr.20220525082936.1277"><vh>class TypeExportSuite</vh>
<v t="ekr.20220525082936.1278"><vh>TypeExportSuite.run_case</vh></v>
</v>
</v>
<v t="ekr.20220525082936.1279"><vh>@clean testtypes.py</vh>
<v t="ekr.20220525082936.1280"><vh>class TypesSuite</vh>
<v t="ekr.20220525082936.1281"><vh>TypesSuite.setUp</vh></v>
<v t="ekr.20220525082936.1282"><vh>TypesSuite.test_any</vh></v>
<v t="ekr.20220525082936.1283"><vh>TypesSuite.test_simple_unbound_type</vh></v>
<v t="ekr.20220525082936.1284"><vh>TypesSuite.test_generic_unbound_type</vh></v>
<v t="ekr.20220525082936.1285"><vh>TypesSuite.test_callable_type</vh></v>
<v t="ekr.20220525082936.1286"><vh>TypesSuite.test_callable_type_with_default_args</vh></v>
<v t="ekr.20220525082936.1287"><vh>TypesSuite.test_callable_type_with_var_args</vh></v>
<v t="ekr.20220525082936.1288"><vh>TypesSuite.test_tuple_type</vh></v>
<v t="ekr.20220525082936.1289"><vh>TypesSuite.test_type_variable_binding</vh></v>
<v t="ekr.20220525082936.1290"><vh>TypesSuite.test_generic_function_type</vh></v>
<v t="ekr.20220525082936.1291"><vh>TypesSuite.test_type_alias_expand_once</vh></v>
<v t="ekr.20220525082936.1292"><vh>TypesSuite.test_type_alias_expand_all</vh></v>
<v t="ekr.20220525082936.1293"><vh>TypesSuite.test_indirection_no_infinite_recursion</vh></v>
</v>
<v t="ekr.20220525082936.1294"><vh>class TypeOpsSuite</vh>
<v t="ekr.20220525082936.1295"><vh>TypeOpsSuite.setUp</vh></v>
<v t="ekr.20220525082936.1296"><vh>TypeOpsSuite.expand_type</vh></v>
<v t="ekr.20220525082936.1297"><vh>TypeOpsSuite.test_trivial_expand</vh></v>
<v t="ekr.20220525082936.1298"><vh>TypeOpsSuite.test_trivial_expand_recursive</vh></v>
<v t="ekr.20220525082936.1299"><vh>TypeOpsSuite.test_expand_naked_type_var</vh></v>
<v t="ekr.20220525082936.1300"><vh>TypeOpsSuite.test_expand_basic_generic_types</vh></v>
<v t="ekr.20220525082936.1301"><vh>TypeOpsSuite.IDEA: Add test cases for</vh></v>
<v t="ekr.20220525082936.1302"><vh>TypeOpsSuite.assert_expand</vh></v>
<v t="ekr.20220525082936.1303"><vh>TypeOpsSuite.erase_type</vh></v>
<v t="ekr.20220525082936.1304"><vh>TypeOpsSuite.test_trivial_erase</vh></v>
<v t="ekr.20220525082936.1305"><vh>TypeOpsSuite.test_erase_with_type_variable</vh></v>
<v t="ekr.20220525082936.1306"><vh>TypeOpsSuite.test_erase_with_generic_type</vh></v>
<v t="ekr.20220525082936.1307"><vh>TypeOpsSuite.test_erase_with_generic_type_recursive</vh></v>
<v t="ekr.20220525082936.1308"><vh>TypeOpsSuite.test_erase_with_tuple_type</vh></v>
<v t="ekr.20220525082936.1309"><vh>TypeOpsSuite.test_erase_with_function_type</vh></v>
<v t="ekr.20220525082936.1310"><vh>TypeOpsSuite.test_erase_with_type_object</vh></v>
<v t="ekr.20220525082936.1311"><vh>TypeOpsSuite.test_erase_with_type_type</vh></v>
<v t="ekr.20220525082936.1312"><vh>TypeOpsSuite.assert_erase</vh></v>
<v t="ekr.20220525082936.1313"><vh>TypeOpsSuite.is_more_precise</vh></v>
<v t="ekr.20220525082936.1314"><vh>TypeOpsSuite.test_is_more_precise</vh></v>
<v t="ekr.20220525082936.1315"><vh>TypeOpsSuite.is_proper_subtype</vh></v>
<v t="ekr.20220525082936.1316"><vh>TypeOpsSuite.test_is_proper_subtype</vh></v>
<v t="ekr.20220525082936.1317"><vh>TypeOpsSuite.test_is_proper_subtype_covariance</vh></v>
<v t="ekr.20220525082936.1318"><vh>TypeOpsSuite.test_is_proper_subtype_contravariance</vh></v>
<v t="ekr.20220525082936.1319"><vh>TypeOpsSuite.test_is_proper_subtype_invariance</vh></v>
<v t="ekr.20220525082936.1320"><vh>TypeOpsSuite.test_is_proper_subtype_and_subtype_literal_types</vh></v>
<v t="ekr.20220525082936.1321"><vh>TypeOpsSuite.test_subtype_aliases</vh></v>
<v t="ekr.20220525082936.1322"><vh>TypeOpsSuite.can_be_true / can_be_false</vh></v>
<v t="ekr.20220525082936.1323"><vh>TypeOpsSuite.test_empty_tuple_always_false</vh></v>
<v t="ekr.20220525082936.1324"><vh>TypeOpsSuite.test_nonempty_tuple_always_true</vh></v>
<v t="ekr.20220525082936.1325"><vh>TypeOpsSuite.test_union_can_be_true_if_any_true</vh></v>
<v t="ekr.20220525082936.1326"><vh>TypeOpsSuite.test_union_can_not_be_true_if_none_true</vh></v>
<v t="ekr.20220525082936.1327"><vh>TypeOpsSuite.test_union_can_be_false_if_any_false</vh></v>
<v t="ekr.20220525082936.1328"><vh>TypeOpsSuite.test_union_can_not_be_false_if_none_false</vh></v>
<v t="ekr.20220525082936.1329"><vh>TypeOpsSuite.true_only / false_only</vh></v>
<v t="ekr.20220525082936.1330"><vh>TypeOpsSuite.test_true_only_of_false_type_is_uninhabited</vh></v>
<v t="ekr.20220525082936.1331"><vh>TypeOpsSuite.test_true_only_of_true_type_is_idempotent</vh></v>
<v t="ekr.20220525082936.1332"><vh>TypeOpsSuite.test_true_only_of_instance</vh></v>
<v t="ekr.20220525082936.1333"><vh>TypeOpsSuite.test_true_only_of_union</vh></v>
<v t="ekr.20220525082936.1334"><vh>TypeOpsSuite.test_false_only_of_true_type_is_uninhabited</vh></v>
<v t="ekr.20220525082936.1335"><vh>TypeOpsSuite.test_false_only_tuple</vh></v>
<v t="ekr.20220525082936.1336"><vh>TypeOpsSuite.test_false_only_of_false_type_is_idempotent</vh></v>
<v t="ekr.20220525082936.1337"><vh>TypeOpsSuite.test_false_only_of_instance</vh></v>
<v t="ekr.20220525082936.1338"><vh>TypeOpsSuite.test_false_only_of_union</vh></v>
<v t="ekr.20220525082936.1339"><vh>TypeOpsSuite.test_simplified_union</vh></v>
<v t="ekr.20220525082936.1340"><vh>TypeOpsSuite.test_simplified_union_with_literals</vh></v>
<v t="ekr.20220525082936.1341"><vh>TypeOpsSuite.test_simplified_union_with_str_literals</vh></v>
<v t="ekr.20220525082936.1342"><vh>TypeOpsSuite.test_simplify_very_large_union</vh></v>
<v t="ekr.20220525082936.1343"><vh>TypeOpsSuite.test_simplified_union_with_str_instance_literals</vh></v>
<v t="ekr.20220525082936.1344"><vh>TypeOpsSuite.test_simplified_union_with_mixed_str_literals</vh></v>
<v t="ekr.20220525082936.1345"><vh>TypeOpsSuite.assert_simplified_union</vh></v>
<v t="ekr.20220525082936.1346"><vh>TypeOpsSuite.Helpers</vh></v>
<v t="ekr.20220525082936.1347"><vh>TypeOpsSuite.tuple</vh></v>
<v t="ekr.20220525082936.1348"><vh>TypeOpsSuite.callable</vh></v>
</v>
<v t="ekr.20220525082936.1349"><vh>class JoinSuite</vh>
<v t="ekr.20220525082936.1350"><vh>JoinSuite.setUp</vh></v>
<v t="ekr.20220525082936.1351"><vh>JoinSuite.test_trivial_cases</vh></v>
<v t="ekr.20220525082936.1352"><vh>JoinSuite.test_class_subtyping</vh></v>
<v t="ekr.20220525082936.1353"><vh>JoinSuite.test_tuples</vh></v>
<v t="ekr.20220525082936.1354"><vh>JoinSuite.test_var_tuples</vh></v>
<v t="ekr.20220525082936.1355"><vh>JoinSuite.test_function_types</vh></v>
<v t="ekr.20220525082936.1356"><vh>JoinSuite.test_type_vars</vh></v>
<v t="ekr.20220525082936.1357"><vh>JoinSuite.test_none</vh></v>
<v t="ekr.20220525082936.1358"><vh>JoinSuite.test_unbound_type</vh></v>
<v t="ekr.20220525082936.1359"><vh>JoinSuite.test_any_type</vh></v>
<v t="ekr.20220525082936.1360"><vh>JoinSuite.test_mixed_truth_restricted_type_simple</vh></v>
<v t="ekr.20220525082936.1361"><vh>JoinSuite.test_mixed_truth_restricted_type</vh></v>
<v t="ekr.20220525082936.1362"><vh>JoinSuite.test_other_mixed_types</vh></v>
<v t="ekr.20220525082936.1363"><vh>JoinSuite.test_simple_generics</vh></v>
<v t="ekr.20220525082936.1364"><vh>JoinSuite.test_generics_invariant</vh></v>
<v t="ekr.20220525082936.1365"><vh>JoinSuite.test_generics_covariant</vh></v>
<v t="ekr.20220525082936.1366"><vh>JoinSuite.test_generics_contravariant</vh></v>
<v t="ekr.20220525082936.1367"><vh>JoinSuite.test_generics_with_multiple_args</vh></v>
<v t="ekr.20220525082936.1368"><vh>JoinSuite.test_generics_with_inheritance</vh></v>
<v t="ekr.20220525082936.1369"><vh>JoinSuite.test_generics_with_inheritance_and_shared_supertype</vh></v>
<v t="ekr.20220525082936.1370"><vh>JoinSuite.test_generic_types_and_any</vh></v>
<v t="ekr.20220525082936.1371"><vh>JoinSuite.test_callables_with_any</vh></v>
<v t="ekr.20220525082936.1372"><vh>JoinSuite.test_overloaded</vh></v>
<v t="ekr.20220525082936.1373"><vh>JoinSuite.test_overloaded_with_any</vh></v>
<v t="ekr.20220525082936.1374"><vh>JoinSuite.test_join_interface_types</vh></v>
<v t="ekr.20220525082936.1375"><vh>JoinSuite.test_join_interface_and_class_types</vh></v>
<v t="ekr.20220525082936.1376"><vh>JoinSuite.test_join_class_types_with_interface_result</vh></v>
<v t="ekr.20220525082936.1377"><vh>JoinSuite.test_generic_interfaces</vh></v>
<v t="ekr.20220525082936.1378"><vh>JoinSuite.test_simple_type_objects</vh></v>
<v t="ekr.20220525082936.1379"><vh>JoinSuite.test_type_type</vh></v>
<v t="ekr.20220525082936.1380"><vh>JoinSuite.test_literal_type</vh></v>
<v t="ekr.20220525082936.1381"><vh>JoinSuite.There are additional test cases in check-inference.test.</vh></v>
<v t="ekr.20220525082936.1382"><vh>JoinSuite.assert_join</vh></v>
<v t="ekr.20220525082936.1383"><vh>JoinSuite.assert_simple_join</vh></v>
<v t="ekr.20220525082936.1384"><vh>JoinSuite.tuple</vh></v>
<v t="ekr.20220525082936.1385"><vh>JoinSuite.var_tuple</vh></v>
<v t="ekr.20220525082936.1386"><vh>JoinSuite.callable</vh></v>
<v t="ekr.20220525082936.1387"><vh>JoinSuite.type_callable</vh></v>
</v>
<v t="ekr.20220525082936.1388"><vh>class MeetSuite</vh>
<v t="ekr.20220525082936.1389"><vh>MeetSuite.setUp</vh></v>
<v t="ekr.20220525082936.1390"><vh>MeetSuite.test_trivial_cases</vh></v>
<v t="ekr.20220525082936.1391"><vh>MeetSuite.test_class_subtyping</vh></v>
<v t="ekr.20220525082936.1392"><vh>MeetSuite.test_tuples</vh></v>
<v t="ekr.20220525082936.1393"><vh>MeetSuite.test_function_types</vh></v>
<v t="ekr.20220525082936.1394"><vh>MeetSuite.test_type_vars</vh></v>
<v t="ekr.20220525082936.1395"><vh>MeetSuite.test_none</vh></v>
<v t="ekr.20220525082936.1396"><vh>MeetSuite.test_unbound_type</vh></v>
<v t="ekr.20220525082936.1397"><vh>MeetSuite.test_dynamic_type</vh></v>
<v t="ekr.20220525082936.1398"><vh>MeetSuite.test_simple_generics</vh></v>
<v t="ekr.20220525082936.1399"><vh>MeetSuite.test_generics_with_multiple_args</vh></v>
<v t="ekr.20220525082936.1400"><vh>MeetSuite.test_generics_with_inheritance</vh></v>
<v t="ekr.20220525082936.1401"><vh>MeetSuite.test_generics_with_inheritance_and_shared_supertype</vh></v>
<v t="ekr.20220525082936.1402"><vh>MeetSuite.test_generic_types_and_dynamic</vh></v>
<v t="ekr.20220525082936.1403"><vh>MeetSuite.test_callables_with_dynamic</vh></v>
<v t="ekr.20220525082936.1404"><vh>MeetSuite.test_meet_interface_types</vh></v>
<v t="ekr.20220525082936.1405"><vh>MeetSuite.test_meet_interface_and_class_types</vh></v>
<v t="ekr.20220525082936.1406"><vh>MeetSuite.test_meet_class_types_with_shared_interfaces</vh></v>
<v t="ekr.20220525082936.1407"><vh>MeetSuite.test_meet_with_generic_interfaces</vh></v>
<v t="ekr.20220525082936.1408"><vh>MeetSuite.test_type_type</vh></v>
<v t="ekr.20220525082936.1409"><vh>MeetSuite.test_literal_type</vh></v>
<v t="ekr.20220525082936.1410"><vh>MeetSuite.FIX generic interfaces + ranges</vh></v>
<v t="ekr.20220525082936.1411"><vh>MeetSuite.assert_meet_uninhabited</vh></v>
<v t="ekr.20220525082936.1412"><vh>MeetSuite.assert_meet</vh></v>
<v t="ekr.20220525082936.1413"><vh>MeetSuite.assert_simple_meet</vh></v>
<v t="ekr.20220525082936.1414"><vh>MeetSuite.tuple</vh></v>
<v t="ekr.20220525082936.1415"><vh>MeetSuite.callable</vh></v>
</v>
<v t="ekr.20220525082936.1416"><vh>class SameTypeSuite</vh>
<v t="ekr.20220525082936.1417"><vh>SameTypeSuite.setUp</vh></v>
<v t="ekr.20220525082936.1418"><vh>SameTypeSuite.test_literal_type</vh></v>
<v t="ekr.20220525082936.1419"><vh>SameTypeSuite.assert_same</vh></v>
<v t="ekr.20220525082936.1420"><vh>SameTypeSuite.assert_not_same</vh></v>
<v t="ekr.20220525082936.1421"><vh>SameTypeSuite.assert_simple_is_same</vh></v>
</v>
<v t="ekr.20220525082936.1422"><vh>class RemoveLastKnownValueSuite</vh>
<v t="ekr.20220525082936.1423"><vh>RemoveLastKnownValueSuite.setUp</vh></v>
<v t="ekr.20220525082936.1424"><vh>RemoveLastKnownValueSuite.test_optional</vh></v>
<v t="ekr.20220525082936.1425"><vh>RemoveLastKnownValueSuite.test_two_instances</vh></v>
<v t="ekr.20220525082936.1426"><vh>RemoveLastKnownValueSuite.test_multiple_same_instances</vh></v>
<v t="ekr.20220525082936.1427"><vh>RemoveLastKnownValueSuite.test_single_last_known_value</vh></v>
<v t="ekr.20220525082936.1428"><vh>RemoveLastKnownValueSuite.test_last_known_values_with_merge</vh></v>
<v t="ekr.20220525082936.1429"><vh>RemoveLastKnownValueSuite.test_generics</vh></v>
<v t="ekr.20220525082936.1430"><vh>RemoveLastKnownValueSuite.assert_union_result</vh></v>
</v>
</v>
<v t="ekr.20220525082936.1431"><vh>@clean testutil.py</vh>
<v t="ekr.20220525082936.1432"><vh>class TestGetTerminalSize</vh></v>
</v>
<v t="ekr.20220525082936.1433"><vh>@clean test_find_sources.py</vh>
<v t="ekr.20220525082936.1434"><vh>class FakeFSCache</vh>
<v t="ekr.20220525082936.1435"><vh>FakeFSCache.__init__</vh></v>
<v t="ekr.20220525082936.1436"><vh>FakeFSCache.isfile</vh></v>
<v t="ekr.20220525082936.1437"><vh>FakeFSCache.isdir</vh></v>
<v t="ekr.20220525082936.1438"><vh>FakeFSCache.listdir</vh></v>
<v t="ekr.20220525082936.1439"><vh>FakeFSCache.init_under_package_root</vh></v>
</v>
<v t="ekr.20220525082936.1440"><vh>normalise_path</vh></v>
<v t="ekr.20220525082936.1441"><vh>normalise_build_source_list</vh></v>
<v t="ekr.20220525082936.1442"><vh>crawl</vh></v>
<v t="ekr.20220525082936.1443"><vh>find_sources_in_dir</vh></v>
<v t="ekr.20220525082936.1444"><vh>find_sources</vh></v>
<v t="ekr.20220525082936.1445"><vh>class SourceFinderSuite</vh>
<v t="ekr.20220525082936.1446"><vh>SourceFinderSuite.setUp</vh></v>
<v t="ekr.20220525082936.1447"><vh>SourceFinderSuite.tearDown</vh></v>
<v t="ekr.20220525082936.1448"><vh>SourceFinderSuite.test_crawl_no_namespace</vh></v>
<v t="ekr.20220525082936.1449"><vh>SourceFinderSuite.test_crawl_namespace</vh></v>
<v t="ekr.20220525082936.1450"><vh>SourceFinderSuite.test_crawl_namespace_explicit_base</vh></v>
<v t="ekr.20220525082936.1451"><vh>SourceFinderSuite.test_crawl_namespace_multi_dir</vh></v>
<v t="ekr.20220525082936.1452"><vh>SourceFinderSuite.test_find_sources_in_dir_no_namespace</vh></v>
<v t="ekr.20220525082936.1453"><vh>SourceFinderSuite.test_find_sources_in_dir_namespace</vh></v>
<v t="ekr.20220525082936.1454"><vh>SourceFinderSuite.test_find_sources_in_dir_namespace_explicit_base</vh></v>
<v t="ekr.20220525082936.1455"><vh>SourceFinderSuite.test_find_sources_in_dir_namespace_multi_dir</vh></v>
<v t="ekr.20220525082936.1456"><vh>SourceFinderSuite.test_find_sources_exclude</vh></v>
</v>
</v>
<v t="ekr.20220525082936.1457"><vh>@clean typefixture.py</vh>
<v t="ekr.20220525082936.1458"><vh>class TypeFixture</vh>
<v t="ekr.20220525082936.1459"><vh>TypeFixture.__init__</vh></v>
<v t="ekr.20220525082936.1460"><vh>TypeFixture._add_bool_dunder</vh></v>
<v t="ekr.20220525082936.1461"><vh>TypeFixture.Helper methods</vh></v>
<v t="ekr.20220525082936.1462"><vh>TypeFixture.callable</vh></v>
<v t="ekr.20220525082936.1463"><vh>TypeFixture.callable_type</vh></v>
<v t="ekr.20220525082936.1464"><vh>TypeFixture.callable_default</vh></v>
<v t="ekr.20220525082936.1465"><vh>TypeFixture.callable_var_arg</vh></v>
<v t="ekr.20220525082936.1466"><vh>TypeFixture.make_type_info</vh></v>
<v t="ekr.20220525082936.1467"><vh>TypeFixture.def_alias_1</vh></v>
<v t="ekr.20220525082936.1468"><vh>TypeFixture.def_alias_2</vh></v>
<v t="ekr.20220525082936.1469"><vh>TypeFixture.non_rec_alias</vh></v>
</v>
<v t="ekr.20220525082936.1470"><vh>class InterfaceTypeFixture</vh>
<v t="ekr.20220525082936.1471"><vh>InterfaceTypeFixture.__init__</vh></v>
</v>
</v>
<v t="ekr.20220525082936.1472"><vh>@clean update.py</vh></v>
<v t="ekr.20220525082936.1473"><vh>@clean visitors.py</vh>
<v t="ekr.20220525082936.1474"><vh>class SkippedNodeSearcher</vh>
<v t="ekr.20220525082936.1475"><vh>SkippedNodeSearcher.__init__</vh></v>
<v t="ekr.20220525082936.1476"><vh>SkippedNodeSearcher.visit_mypy_file</vh></v>
<v t="ekr.20220525082936.1477"><vh>SkippedNodeSearcher.visit_assignment_stmt</vh></v>
<v t="ekr.20220525082936.1478"><vh>SkippedNodeSearcher.visit_name_expr</vh></v>
<v t="ekr.20220525082936.1479"><vh>SkippedNodeSearcher.visit_int_expr</vh></v>
<v t="ekr.20220525082936.1480"><vh>SkippedNodeSearcher.skip_if_typing</vh></v>
</v>
<v t="ekr.20220525082936.1481"><vh>ignore_node</vh></v>
<v t="ekr.20220525082936.1482"><vh>class TypeAssertTransformVisitor</vh></v>
</v>
</v>
<v t="ekr.20220525082936.1484"><vh>@path typeshed</vh>
<v t="ekr.20220525082936.1485"><vh>@path stdlib</vh>
<v t="ekr.20220525082936.1486"><vh>@path @python2</vh>
<v t="ekr.20220525082936.1491"><vh>@path distutils</vh></v>
<v t="ekr.20220525082936.1493"><vh>@path email</vh></v>
<v t="ekr.20220525082936.1497"><vh>@path lib2to3</vh></v>
<v t="ekr.20220525082936.1501"><vh>@path multiprocessing</vh></v>
<v t="ekr.20220525082936.1508"><vh>@path xml</vh>
<v t="ekr.20220525082936.1511"><vh>@path parsers</vh></v>
</v>
</v>
<v t="ekr.20220525082936.1517"><vh>@path concurrent</vh></v>
<v t="ekr.20220525082936.1522"><vh>@path distutils</vh></v>
<v t="ekr.20220525082936.1524"><vh>@path email</vh></v>
<v t="ekr.20220525082936.1530"><vh>@path importlib</vh></v>
<v t="ekr.20220525082936.1533"><vh>@path lib2to3</vh></v>
<v t="ekr.20220525082936.1537"><vh>@path multiprocessing</vh></v>
<v t="ekr.20220525082936.1548"><vh>@path xml</vh>
<v t="ekr.20220525082936.1551"><vh>@path parsers</vh></v>
</v>
</v>
<v t="ekr.20220525082936.1556"><vh>@path stubs</vh>
<v t="ekr.20220525082936.1557"><vh>@path mypy-extensions</vh></v>
</v>
</v>
</v>
</v>
</v>
<v t="ekr.20220320050830.1"><vh>Recent files</vh>
<v t="ekr.20220525082934.502"></v>
<v t="ekr.20220525082933.340"></v>
</v>
<v t="ekr.20220320050857.1"><vh>=== #12352: auto annotations</vh></v>
<v t="ekr.20220526075427.1"><vh>Recent code</vh></v>
<v t="ekr.20220224065603.211"><vh>TypeChecker.check_for_missing_annotations (to be changed)</vh></v>
<v t="ekr.20220319232947.1"><vh>--- calling sequence</vh>
<v t="ekr.20220525082933.373"></v>
<v t="ekr.20220224065603.211"></v>
<v t="ekr.20220525082933.377"></v>
<v t="ekr.20220525082933.116"></v>
</v>
<v t="ekr.20220526075541.1"><vh>--- handle args</vh>
<v t="ekr.20220525082934.835"></v>
<v t="ekr.20220525082934.528"></v>
<v t="ekr.20220525082933.889"></v>
<v t="ekr.20220525082933.893"></v>
</v>
<v t="ekr.20220526075516.1"><vh>--- 1 use of matches_exclude</vh>
<v t="ekr.20220525082934.834"></v>
</v>
</vnodes>
<tnodes>
<t tx="ekr.20220224065603.211">def check_for_missing_annotations(self, fdef: FuncItem) -&gt; None:

    # Check for functions with unspecified/not fully specified types.
    def is_unannotated_any(t: Type) -&gt; bool:
        if not isinstance(t, ProperType):
            return False
        return isinstance(t, AnyType) and t.type_of_any == TypeOfAny.unannotated
        
    def arg_is_unannotated_and_is_not_initialized(t: Type):
        if not is_unannotated_any(t):
            return False  # Passes legacy test.
        ### To do: return False if 
        return True  # Fail

    has_explicit_annotation = (isinstance(fdef.type, CallableType)
                               and any(not is_unannotated_any(t)
                                       for t in fdef.type.arg_types + [fdef.type.ret_type]))

    show_untyped = not self.is_typeshed_stub or self.options.warn_incomplete_stub
    check_incomplete_defs = self.options.disallow_incomplete_defs and has_explicit_annotation
    if show_untyped and (self.options.disallow_untyped_defs or check_incomplete_defs):
        if fdef.type is None and self.options.disallow_untyped_defs:
            if (not fdef.arguments or (len(fdef.arguments) == 1 and
                    (fdef.arg_names[0] == 'self' or fdef.arg_names[0] == 'cls'))):
                self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
                if not has_return_statement(fdef) and not fdef.is_generator:
                    self.note('Use "-&gt; None" if function does not return a value', fdef,
                              code=codes.NO_UNTYPED_DEF)
            else:
                self.fail(message_registry.FUNCTION_TYPE_EXPECTED, fdef)
        elif isinstance(fdef.type, CallableType):
            ret_type = get_proper_type(fdef.type.ret_type)
            if is_unannotated_any(ret_type):
                self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
            elif fdef.is_generator:
                if is_unannotated_any(self.get_generator_return_type(ret_type,
                                                                     fdef.is_coroutine)):
                    self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
            elif fdef.is_coroutine and isinstance(ret_type, Instance):
                if is_unannotated_any(self.get_coroutine_return_type(ret_type)):
                    self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
            # Separate test for arg types.  ###
            if 1:  ### Legacy test:
                if any(is_unannotated_any(t) for t in fdef.type.arg_types):
                    self.fail(message_registry.ARGUMENT_TYPE_EXPECTED, fdef)  ###
            else:
                pass  ### New: add to fdef.type ???
</t>
<t tx="ekr.20220319232947.1"></t>
<t tx="ekr.20220320050830.1"></t>
<t tx="ekr.20220320050857.1">@language rest

https://github.com/python/mypy/issues/12352
  
@language python

</t>
<t tx="ekr.20220525082746.1">'''Recursively import all python files in a directory and clean the result.'''
@tabwidth -4 # For a better match.
g.cls()
dir_ = r'C:\Repos\mypy'
# dir_ = r'C:\Users\Edward Ream\Python\python39\Lib\site-packages\pylint'
c.recursiveImport(
    add_context=True,  # Override setting only if True/False
    add_file_context=False,  # Override setting only if True/False
    dir_=dir_,
    kind = '@clean', # '@auto', '@clean', '@nosent','@file',
    add_path = True,
    recursive = True,
    safe_at_file = False,
    theTypes = ['.py'],
        # ['.ts', '.js', '.json'] # ['.html', '.js', '.json', '.py', '.rs', '.svg', '.ts', '.tsx']
    verbose = False,
)
if 1:
    last = c.lastTopLevel()
    # for p in last.self_and_subtree():
        # p.expand()
    # c.expandAllSubheads()
    c.redraw(last)
print('Done')</t>
<t tx="ekr.20220525082932.2"></t>
<t tx="ekr.20220525082932.3">@path C:/Repos/mypy/
@language python
@tabwidth -4
import os.path

pytest_plugins = [
    'mypy.test.data',
]


@others
</t>
<t tx="ekr.20220525082933.1">def pytest_configure(config):
    mypy_source_root = os.path.dirname(os.path.abspath(__file__))
    if os.getcwd() != mypy_source_root:
        os.chdir(mypy_source_root)


</t>
<t tx="ekr.20220525082933.100">def save_cache(cache: JsonDict, incremental_cache_path: str = CACHE_PATH) -&gt; None:
    with open(incremental_cache_path, 'w') as stream:
        json.dump(cache, stream, indent=2)


</t>
<t tx="ekr.20220525082933.1000">def fine_grained_increment(self,
                           sources: List[BuildSource],
                           remove: Optional[List[str]] = None,
                           update: Optional[List[str]] = None,
                           ) -&gt; List[str]:
    """Perform a fine-grained type checking increment.

    If remove and update are None, determine changed paths by using
    fswatcher. Otherwise, assume that only these files have changes.

    Args:
        sources: sources passed on the command line
        remove: paths of files that have been removed
        update: paths of files that have been changed or created
    """
    assert self.fine_grained_manager is not None
    manager = self.fine_grained_manager.manager

    t0 = time.time()
    if remove is None and update is None:
        # Use the fswatcher to determine which files were changed
        # (updated or added) or removed.
        self.update_sources(sources)
        changed, removed = self.find_changed(sources)
    else:
        # Use the remove/update lists to update fswatcher.
        # This avoids calling stat() for unchanged files.
        changed, removed = self.update_changed(sources, remove or [], update or [])
    changed += self.find_added_suppressed(self.fine_grained_manager.graph, set(),
                                          manager.search_paths)
    manager.search_paths = compute_search_paths(sources, manager.options, manager.data_dir)
    t1 = time.time()
    manager.log(f"fine-grained increment: find_changed: {t1 - t0:.3f}s")
    messages = self.fine_grained_manager.update(changed, removed)
    t2 = time.time()
    manager.log(f"fine-grained increment: update: {t2 - t1:.3f}s")
    manager.add_stats(
        find_changes_time=t1 - t0,
        fg_update_time=t2 - t1,
        files_changed=len(removed) + len(changed))

    self.previous_sources = sources
    return messages

</t>
<t tx="ekr.20220525082933.1001">def fine_grained_increment_follow_imports(self, sources: List[BuildSource]) -&gt; List[str]:
    """Like fine_grained_increment, but follow imports."""
    t0 = time.time()

    # TODO: Support file events

    assert self.fine_grained_manager is not None
    fine_grained_manager = self.fine_grained_manager
    graph = fine_grained_manager.graph
    manager = fine_grained_manager.manager

    orig_modules = list(graph.keys())

    self.update_sources(sources)
    changed_paths = self.fswatcher.find_changed()
    manager.search_paths = compute_search_paths(sources, manager.options, manager.data_dir)

    t1 = time.time()
    manager.log(f"fine-grained increment: find_changed: {t1 - t0:.3f}s")

    seen = {source.module for source in sources}

    # Find changed modules reachable from roots (or in roots) already in graph.
    changed, new_files = self.find_reachable_changed_modules(
        sources, graph, seen, changed_paths
    )
    sources.extend(new_files)

    # Process changes directly reachable from roots.
    messages = fine_grained_manager.update(changed, [])

    # Follow deps from changed modules (still within graph).
    worklist = changed[:]
    while worklist:
        module = worklist.pop()
        if module[0] not in graph:
            continue
        sources2 = self.direct_imports(module, graph)
        # Filter anything already seen before. This prevents
        # infinite looping if there are any self edges. (Self
        # edges are maybe a bug, but...)
        sources2 = [source for source in sources2 if source.module not in seen]
        changed, new_files = self.find_reachable_changed_modules(
            sources2, graph, seen, changed_paths
        )
        self.update_sources(new_files)
        messages = fine_grained_manager.update(changed, [])
        worklist.extend(changed)

    t2 = time.time()

    def refresh_file(module: str, path: str) -&gt; List[str]:
        return fine_grained_manager.update([(module, path)], [])

    for module_id, state in list(graph.items()):
        new_messages = refresh_suppressed_submodules(
            module_id, state.path, fine_grained_manager.deps, graph, self.fscache, refresh_file
        )
        if new_messages is not None:
            messages = new_messages

    t3 = time.time()

    # There may be new files that became available, currently treated as
    # suppressed imports. Process them.
    while True:
        new_unsuppressed = self.find_added_suppressed(graph, seen, manager.search_paths)
        if not new_unsuppressed:
            break
        new_files = [BuildSource(mod[1], mod[0]) for mod in new_unsuppressed]
        sources.extend(new_files)
        self.update_sources(new_files)
        messages = fine_grained_manager.update(new_unsuppressed, [])

        for module_id, path in new_unsuppressed:
            new_messages = refresh_suppressed_submodules(
                module_id, path,
                fine_grained_manager.deps,
                graph,
                self.fscache,
                refresh_file
            )
            if new_messages is not None:
                messages = new_messages

    t4 = time.time()

    # Find all original modules in graph that were not reached -- they are deleted.
    to_delete = []
    for module_id in orig_modules:
        if module_id not in graph:
            continue
        if module_id not in seen:
            module_path = graph[module_id].path
            assert module_path is not None
            to_delete.append((module_id, module_path))
    if to_delete:
        messages = fine_grained_manager.update([], to_delete)

    fix_module_deps(graph)

    self.previous_sources = find_all_sources_in_build(graph)
    self.update_sources(self.previous_sources)

    # Store current file state as side effect
    self.fswatcher.find_changed()

    t5 = time.time()

    manager.log(f"fine-grained increment: update: {t5 - t1:.3f}s")
    manager.add_stats(
        find_changes_time=t1 - t0,
        fg_update_time=t2 - t1,
        refresh_suppressed_time=t3 - t2,
        find_added_supressed_time=t4 - t3,
        cleanup_time=t5 - t4)

    return messages

</t>
<t tx="ekr.20220525082933.1002">def find_reachable_changed_modules(
        self,
        roots: List[BuildSource],
        graph: mypy.build.Graph,
        seen: Set[str],
        changed_paths: AbstractSet[str]) -&gt; Tuple[List[Tuple[str, str]],
                                                  List[BuildSource]]:
    """Follow imports within graph from given sources until hitting changed modules.

    If we find a changed module, we can't continue following imports as the imports
    may have changed.

    Args:
        roots: modules where to start search from
        graph: module graph to use for the search
        seen: modules we've seen before that won't be visited (mutated here!!)
        changed_paths: which paths have changed (stop search here and return any found)

    Return (encountered reachable changed modules,
            unchanged files not in sources_set traversed).
    """
    changed = []
    new_files = []
    worklist = roots[:]
    seen.update(source.module for source in worklist)
    while worklist:
        nxt = worklist.pop()
        if nxt.module not in seen:
            seen.add(nxt.module)
            new_files.append(nxt)
        if nxt.path in changed_paths:
            assert nxt.path is not None  # TODO
            changed.append((nxt.module, nxt.path))
        elif nxt.module in graph:
            state = graph[nxt.module]
            for dep in state.dependencies:
                if dep not in seen:
                    seen.add(dep)
                    worklist.append(BuildSource(graph[dep].path,
                                                graph[dep].id))
    return changed, new_files

</t>
<t tx="ekr.20220525082933.1003">def direct_imports(self,
                   module: Tuple[str, str],
                   graph: mypy.build.Graph) -&gt; List[BuildSource]:
    """Return the direct imports of module not included in seen."""
    state = graph[module[0]]
    return [BuildSource(graph[dep].path, dep)
            for dep in state.dependencies]

</t>
<t tx="ekr.20220525082933.1004">def find_added_suppressed(self,
                          graph: mypy.build.Graph,
                          seen: Set[str],
                          search_paths: SearchPaths) -&gt; List[Tuple[str, str]]:
    """Find suppressed modules that have been added (and not included in seen).

    Args:
        seen: reachable modules we've seen before (mutated here!!)

    Return suppressed, added modules.
    """
    all_suppressed = set()
    for state in graph.values():
        all_suppressed |= state.suppressed_set

    # Filter out things that shouldn't actually be considered suppressed.
    #
    # TODO: Figure out why these are treated as suppressed
    all_suppressed = {module
                      for module in all_suppressed
                      if module not in graph and not ignore_suppressed_imports(module)}

    # Optimization: skip top-level packages that are obviously not
    # there, to avoid calling the relatively slow find_module()
    # below too many times.
    packages = {module.split('.', 1)[0] for module in all_suppressed}
    packages = filter_out_missing_top_level_packages(packages, search_paths, self.fscache)

    # TODO: Namespace packages

    finder = FindModuleCache(search_paths, self.fscache, self.options)

    found = []

    for module in all_suppressed:
        top_level_pkg = module.split('.', 1)[0]
        if top_level_pkg not in packages:
            # Fast path: non-existent top-level package
            continue
        result = finder.find_module(module, fast_path=True)
        if isinstance(result, str) and module not in seen:
            # When not following imports, we only follow imports to .pyi files.
            if not self.following_imports() and not result.endswith('.pyi'):
                continue
            found.append((module, result))
            seen.add(module)

    return found

</t>
<t tx="ekr.20220525082933.1005">def increment_output(self,
                     messages: List[str],
                     sources: List[BuildSource],
                     is_tty: bool,
                     terminal_width: int) -&gt; Dict[str, Any]:
    status = 1 if messages else 0
    messages = self.pretty_messages(messages, len(sources), is_tty, terminal_width)
    return {'out': ''.join(s + '\n' for s in messages), 'err': '', 'status': status}

</t>
<t tx="ekr.20220525082933.1006">def pretty_messages(self, messages: List[str], n_sources: int,
                    is_tty: bool = False, terminal_width: Optional[int] = None) -&gt; List[str]:
    use_color = self.options.color_output and is_tty
    fit_width = self.options.pretty and is_tty
    if fit_width:
        messages = self.formatter.fit_in_terminal(messages,
                                                  fixed_terminal_width=terminal_width)
    if self.options.error_summary:
        summary: Optional[str] = None
        n_errors, n_notes, n_files = count_stats(messages)
        if n_errors:
            summary = self.formatter.format_error(n_errors, n_files, n_sources,
                                                  use_color=use_color)
        elif not messages or n_notes == len(messages):
            summary = self.formatter.format_success(n_sources, use_color)
        if summary:
            # Create new list to avoid appending multiple summaries on successive runs.
            messages = messages + [summary]
    if use_color:
        messages = [self.formatter.colorize(m) for m in messages]
    return messages

</t>
<t tx="ekr.20220525082933.1007">def update_sources(self, sources: List[BuildSource]) -&gt; None:
    paths = [source.path for source in sources if source.path is not None]
    if self.following_imports():
        # Filter out directories (used for namespace packages).
        paths = [path for path in paths if self.fscache.isfile(path)]
    self.fswatcher.add_watched_paths(paths)

</t>
<t tx="ekr.20220525082933.1008">def update_changed(self,
                   sources: List[BuildSource],
                   remove: List[str],
                   update: List[str],
                   ) -&gt; ChangesAndRemovals:

    changed_paths = self.fswatcher.update_changed(remove, update)
    return self._find_changed(sources, changed_paths)

</t>
<t tx="ekr.20220525082933.1009">def find_changed(self, sources: List[BuildSource]) -&gt; ChangesAndRemovals:
    changed_paths = self.fswatcher.find_changed()
    return self._find_changed(sources, changed_paths)

</t>
<t tx="ekr.20220525082933.101">def set_expected(commits: List[Tuple[str, str]],
                 cache: JsonDict,
                 temp_repo_path: str,
                 target_file_path: Optional[str],
                 mypy_cache_path: str,
                 mypy_script: Optional[str]) -&gt; None:
    """Populates the given `cache` with the expected results for all of the given `commits`.

    This function runs mypy on the `target_file_path` inside the `temp_repo_path`, and stores
    the result in the `cache`.

    If `cache` already contains results for a particular commit, this function will
    skip evaluating that commit and move on to the next."""
    for commit_id, message in commits:
        if commit_id in cache:
            print(f'Skipping commit (already cached): {commit_id}: "{message}"')
        else:
            print(f'Caching expected output for commit {commit_id}: "{message}"')
            execute(["git", "-C", temp_repo_path, "checkout", commit_id])
            runtime, output, stats = run_mypy(target_file_path, mypy_cache_path, mypy_script,
                                              incremental=False)
            cache[commit_id] = {'runtime': runtime, 'output': output}
            if output == "":
                print(f"    Clean output ({runtime:.3f} sec)")
            else:
                print(f"    Output ({runtime:.3f} sec)")
                print_offset(output, 8)
    print()


</t>
<t tx="ekr.20220525082933.1010">def _find_changed(self, sources: List[BuildSource],
                  changed_paths: AbstractSet[str]) -&gt; ChangesAndRemovals:
    # Find anything that has been added or modified
    changed = [(source.module, source.path)
               for source in sources
               if source.path and source.path in changed_paths]

    # Now find anything that has been removed from the build
    modules = {source.module for source in sources}
    omitted = [source for source in self.previous_sources if source.module not in modules]
    removed = []
    for source in omitted:
        path = source.path
        assert path
        removed.append((source.module, path))

    # Find anything that has had its module path change because of added or removed __init__s
    last = {s.path: s.module for s in self.previous_sources}
    for s in sources:
        assert s.path
        if s.path in last and last[s.path] != s.module:
            # Mark it as removed from its old name and changed at its new name
            removed.append((last[s.path], s.path))
            changed.append((s.module, s.path))

    return changed, removed

</t>
<t tx="ekr.20220525082933.1011">def cmd_suggest(self,
                function: str,
                callsites: bool,
                **kwargs: Any) -&gt; Dict[str, object]:
    """Suggest a signature for a function."""
    if not self.fine_grained_manager:
        return {
            'error': "Command 'suggest' is only valid after a 'check' command"
            " (that produces no parse errors)"}
    engine = SuggestionEngine(self.fine_grained_manager, **kwargs)
    try:
        if callsites:
            out = engine.suggest_callsites(function)
        else:
            out = engine.suggest(function)
    except SuggestionFailure as err:
        return {'error': str(err)}
    else:
        if not out:
            out = "No suggestions\n"
        elif not out.endswith("\n"):
            out += "\n"
        return {'out': out, 'err': "", 'status': 0}
    finally:
        self.flush_caches()

</t>
<t tx="ekr.20220525082933.1012">def cmd_hang(self) -&gt; Dict[str, object]:
    """Hang for 100 seconds, as a debug hack."""
    time.sleep(100)
    return {}


</t>
<t tx="ekr.20220525082933.1013"># Misc utilities.


MiB: Final = 2 ** 20


</t>
<t tx="ekr.20220525082933.1014">def get_meminfo() -&gt; Dict[str, Any]:
    res: Dict[str, Any] = {}
    try:
        import psutil  # type: ignore  # It's not in typeshed yet
    except ImportError:
        res['memory_psutil_missing'] = (
            'psutil not found, run pip install mypy[dmypy] '
            'to install the needed components for dmypy'
        )
    else:
        process = psutil.Process()
        meminfo = process.memory_info()
        res['memory_rss_mib'] = meminfo.rss / MiB
        res['memory_vms_mib'] = meminfo.vms / MiB
        if sys.platform == 'win32':
            res['memory_maxrss_mib'] = meminfo.peak_wset / MiB
        else:
            # See https://stackoverflow.com/questions/938733/total-memory-used-by-python-process
            import resource  # Since it doesn't exist on Windows.
            rusage = resource.getrusage(resource.RUSAGE_SELF)
            if sys.platform == 'darwin':
                factor = 1
            else:
                factor = 1024  # Linux
            res['memory_maxrss_mib'] = rusage.ru_maxrss * factor / MiB
    return res


</t>
<t tx="ekr.20220525082933.1015">def find_all_sources_in_build(graph: mypy.build.Graph,
                              extra: Sequence[BuildSource] = ()) -&gt; List[BuildSource]:
    result = list(extra)
    seen = {source.module for source in result}
    for module, state in graph.items():
        if module not in seen:
            result.append(BuildSource(state.path, module))
    return result


</t>
<t tx="ekr.20220525082933.1016">def fix_module_deps(graph: mypy.build.Graph) -&gt; None:
    """After an incremental update, update module dependencies to reflect the new state.

    This can make some suppressed dependencies non-suppressed, and vice versa (if modules
    have been added to or removed from the build).
    """
    for module, state in graph.items():
        new_suppressed = []
        new_dependencies = []
        for dep in state.dependencies + state.suppressed:
            if dep in graph:
                new_dependencies.append(dep)
            else:
                new_suppressed.append(dep)
        state.dependencies = new_dependencies
        state.dependencies_set = set(new_dependencies)
        state.suppressed = new_suppressed
        state.suppressed_set = set(new_suppressed)


</t>
<t tx="ekr.20220525082933.1017">def filter_out_missing_top_level_packages(packages: Set[str],
                                          search_paths: SearchPaths,
                                          fscache: FileSystemCache) -&gt; Set[str]:
    """Quickly filter out obviously missing top-level packages.

    Return packages with entries that can't be found removed.

    This is approximate: some packages that aren't actually valid may be
    included. However, all potentially valid packages must be returned.
    """
    # Start with a empty set and add all potential top-level packages.
    found = set()
    paths = (
        search_paths.python_path + search_paths.mypy_path + search_paths.package_path +
        search_paths.typeshed_path
    )
    paths += tuple(os.path.join(p, '@python2') for p in search_paths.typeshed_path)
    for p in paths:
        try:
            entries = fscache.listdir(p)
        except Exception:
            entries = []
        for entry in entries:
            # The code is hand-optimized for mypyc since this may be somewhat
            # performance-critical.
            if entry.endswith('.py'):
                entry = entry[:-3]
            elif entry.endswith('.pyi'):
                entry = entry[:-4]
            elif entry.endswith('-stubs'):
                # Possible PEP 561 stub package
                entry = entry[:-6]
                if entry.endswith('-python2'):
                    entry = entry[:-8]
            if entry in packages:
                found.add(entry)
    return found
</t>
<t tx="ekr.20220525082933.1018">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Shared code between dmypy.py and dmypy_server.py.

This should be pretty lightweight and not depend on other mypy code (other than ipc).
"""

import json

from typing import Any
from typing_extensions import Final

from mypy.ipc import IPCBase

DEFAULT_STATUS_FILE: Final = ".dmypy.json"


@others
</t>
<t tx="ekr.20220525082933.1019">def receive(connection: IPCBase) -&gt; Any:
    """Receive JSON data from a connection until EOF.

    Raise OSError if the data received is not valid JSON or if it is
    not a dict.
    """
    bdata = connection.read()
    if not bdata:
        raise OSError("No data received")
    try:
        data = json.loads(bdata.decode('utf8'))
    except Exception as e:
        raise OSError("Data received is not valid JSON") from e
    if not isinstance(data, dict):
        raise OSError(f"Data received is not a dict ({type(data)})")
    return data
</t>
<t tx="ekr.20220525082933.102">def test_incremental(commits: List[Tuple[str, str]],
                     cache: JsonDict,
                     temp_repo_path: str,
                     target_file_path: Optional[str],
                     mypy_cache_path: str,
                     *,
                     mypy_script: Optional[str] = None,
                     daemon: bool = False,
                     exit_on_error: bool = False) -&gt; None:
    """Runs incremental mode on all `commits` to verify the output matches the expected output.

    This function runs mypy on the `target_file_path` inside the `temp_repo_path`. The
    expected output must be stored inside of the given `cache`.
    """
    print("Note: first commit is evaluated twice to warm up cache")
    commits = [commits[0]] + commits
    overall_stats = {}  # type: Dict[str, float]
    for commit_id, message in commits:
        print(f'Now testing commit {commit_id}: "{message}"')
        execute(["git", "-C", temp_repo_path, "checkout", commit_id])
        runtime, output, stats = run_mypy(target_file_path, mypy_cache_path, mypy_script,
                                          incremental=True, daemon=daemon)
        relevant_stats = combine_stats(overall_stats, stats)
        expected_runtime = cache[commit_id]['runtime']  # type: float
        expected_output = cache[commit_id]['output']  # type: str
        if output != expected_output:
            print("    Output does not match expected result!")
            print(f"    Expected output ({expected_runtime:.3f} sec):")
            print_offset(expected_output, 8)
            print(f"    Actual output: ({runtime:.3f} sec):")
            print_offset(output, 8)
            if exit_on_error:
                break
        else:
            print("    Output matches expected result!")
            print(f"    Incremental: {runtime:.3f} sec")
            print(f"    Original:    {expected_runtime:.3f} sec")
            if relevant_stats:
                print(f"    Stats:       {relevant_stats}")
    if overall_stats:
        print("Overall stats:", overall_stats)


</t>
<t tx="ekr.20220525082933.1020">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from typing import Optional, Container, Callable, List, Dict, cast

from mypy.types import (
    Type, TypeVisitor, UnboundType, AnyType, NoneType, TypeVarId, Instance, TypeVarType,
    CallableType, TupleType, TypedDictType, UnionType, Overloaded, ErasedType, PartialType,
    DeletedType, TypeTranslator, UninhabitedType, TypeType, TypeOfAny, LiteralType, ProperType,
    get_proper_type, get_proper_types, TypeAliasType, ParamSpecType, Parameters, UnpackType,
    TypeVarTupleType
)
from mypy.nodes import ARG_STAR, ARG_STAR2


@others
</t>
<t tx="ekr.20220525082933.1021">def erase_type(typ: Type) -&gt; ProperType:
    """Erase any type variables from a type.

    Also replace tuple types with the corresponding concrete types.

    Examples:
      A -&gt; A
      B[X] -&gt; B[Any]
      Tuple[A, B] -&gt; tuple
      Callable[[A1, A2, ...], R] -&gt; Callable[..., Any]
      Type[X] -&gt; Type[Any]
    """
    typ = get_proper_type(typ)
    return typ.accept(EraseTypeVisitor())


</t>
<t tx="ekr.20220525082933.1022">class EraseTypeVisitor(TypeVisitor[ProperType]):

    @others
</t>
<t tx="ekr.20220525082933.1023">def visit_unbound_type(self, t: UnboundType) -&gt; ProperType:
    # TODO: replace with an assert after UnboundType can't leak from semantic analysis.
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20220525082933.1024">def visit_any(self, t: AnyType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20220525082933.1025">def visit_none_type(self, t: NoneType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20220525082933.1026">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20220525082933.1027">def visit_erased_type(self, t: ErasedType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20220525082933.1028">def visit_partial_type(self, t: PartialType) -&gt; ProperType:
    # Should not get here.
    raise RuntimeError()

</t>
<t tx="ekr.20220525082933.1029">def visit_deleted_type(self, t: DeletedType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20220525082933.103">def combine_stats(overall_stats: Dict[str, float],
                  new_stats: Dict[str, Any]) -&gt; Dict[str, float]:
    INTERESTING_KEYS = ['build_time', 'gc_time']
    # For now, we only support float keys
    relevant_stats = {}  # type: Dict[str, float]
    for key in INTERESTING_KEYS:
        if key in new_stats:
            value = float(new_stats[key])
            relevant_stats[key] = value
            overall_stats[key] = overall_stats.get(key, 0.0) + value
    return relevant_stats


</t>
<t tx="ekr.20220525082933.1030">def visit_instance(self, t: Instance) -&gt; ProperType:
    return Instance(t.type, [AnyType(TypeOfAny.special_form)] * len(t.args), t.line)

</t>
<t tx="ekr.20220525082933.1031">def visit_type_var(self, t: TypeVarType) -&gt; ProperType:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082933.1032">def visit_param_spec(self, t: ParamSpecType) -&gt; ProperType:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082933.1033">def visit_parameters(self, t: Parameters) -&gt; ProperType:
    raise RuntimeError("Parameters should have been bound to a class")

</t>
<t tx="ekr.20220525082933.1034">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; ProperType:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082933.1035">def visit_unpack_type(self, t: UnpackType) -&gt; ProperType:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082933.1036">def visit_callable_type(self, t: CallableType) -&gt; ProperType:
    # We must preserve the fallback type for overload resolution to work.
    any_type = AnyType(TypeOfAny.special_form)
    return CallableType(
        arg_types=[any_type, any_type],
        arg_kinds=[ARG_STAR, ARG_STAR2],
        arg_names=[None, None],
        ret_type=any_type,
        fallback=t.fallback,
        is_ellipsis_args=True,
        implicit=True,
    )

</t>
<t tx="ekr.20220525082933.1037">def visit_overloaded(self, t: Overloaded) -&gt; ProperType:
    return t.fallback.accept(self)

</t>
<t tx="ekr.20220525082933.1038">def visit_tuple_type(self, t: TupleType) -&gt; ProperType:
    return t.partial_fallback.accept(self)

</t>
<t tx="ekr.20220525082933.1039">def visit_typeddict_type(self, t: TypedDictType) -&gt; ProperType:
    return t.fallback.accept(self)

</t>
<t tx="ekr.20220525082933.104">def cleanup(temp_repo_path: str, mypy_cache_path: str) -&gt; None:
    delete_folder(temp_repo_path)
    delete_folder(mypy_cache_path)


</t>
<t tx="ekr.20220525082933.1040">def visit_literal_type(self, t: LiteralType) -&gt; ProperType:
    # The fallback for literal types should always be either
    # something like int or str, or an enum class -- types that
    # don't contain any TypeVars. So there's no need to visit it.
    return t

</t>
<t tx="ekr.20220525082933.1041">def visit_union_type(self, t: UnionType) -&gt; ProperType:
    erased_items = [erase_type(item) for item in t.items]
    from mypy.typeops import make_simplified_union
    return make_simplified_union(erased_items)

</t>
<t tx="ekr.20220525082933.1042">def visit_type_type(self, t: TypeType) -&gt; ProperType:
    return TypeType.make_normalized(t.item.accept(self), line=t.line)

</t>
<t tx="ekr.20220525082933.1043">def visit_type_alias_type(self, t: TypeAliasType) -&gt; ProperType:
    raise RuntimeError("Type aliases should be expanded before accepting this visitor")


</t>
<t tx="ekr.20220525082933.1044">def erase_typevars(t: Type, ids_to_erase: Optional[Container[TypeVarId]] = None) -&gt; Type:
    """Replace all type variables in a type with any,
    or just the ones in the provided collection.
    """
    @others
    return t.accept(TypeVarEraser(erase_id, AnyType(TypeOfAny.special_form)))


</t>
<t tx="ekr.20220525082933.1045">def erase_id(id: TypeVarId) -&gt; bool:
    if ids_to_erase is None:
        return True
    return id in ids_to_erase
</t>
<t tx="ekr.20220525082933.1046">def replace_meta_vars(t: Type, target_type: Type) -&gt; Type:
    """Replace unification variables in a type with the target type."""
    return t.accept(TypeVarEraser(lambda id: id.is_meta_var(), target_type))


</t>
<t tx="ekr.20220525082933.1047">class TypeVarEraser(TypeTranslator):
    """Implementation of type erasure"""

    @others
</t>
<t tx="ekr.20220525082933.1048">def __init__(self, erase_id: Callable[[TypeVarId], bool], replacement: Type) -&gt; None:
    self.erase_id = erase_id
    self.replacement = replacement

</t>
<t tx="ekr.20220525082933.1049">def visit_type_var(self, t: TypeVarType) -&gt; Type:
    if self.erase_id(t.id):
        return self.replacement
    return t

</t>
<t tx="ekr.20220525082933.105">def test_repo(target_repo_url: str, temp_repo_path: str,
              target_file_path: Optional[str],
              mypy_path: str, incremental_cache_path: str, mypy_cache_path: str,
              range_type: str, range_start: str, branch: str,
              params: Namespace) -&gt; None:
    """Tests incremental mode against the repo specified in `target_repo_url`.

    This algorithm runs in five main stages:

    1.  Clones `target_repo_url` into the `temp_repo_path` folder locally,
        checking out the specified `branch` if applicable.
    2.  Examines the repo's history to get the list of all commits to
        to test incremental mode on.
    3.  Runs mypy WITHOUT incremental mode against the `target_file_path` (which is
        assumed to be located inside the `temp_repo_path`), testing each commit
        discovered in stage two.
        -   If the results of running mypy WITHOUT incremental mode on a
            particular commit are already cached inside the `incremental_cache_path`,
            skip that commit to save time.
        -   Cache the results after finishing.
    4.  Rewind back to the first commit, and run mypy WITH incremental mode
        against the `target_file_path` commit-by-commit, and compare to the expected
        results found in stage 3.
    5.  Delete all unnecessary temp files.
    """
    # Stage 1: Clone repo and get ready to being testing
    ensure_environment_is_ready(mypy_path, temp_repo_path, mypy_cache_path)
    initialize_repo(target_repo_url, temp_repo_path, branch)

    # Stage 2: Get all commits we want to test
    if range_type == "last":
        start_commit = get_nth_commit(temp_repo_path, int(range_start))[0]
    elif range_type == "commit":
        start_commit = range_start
    else:
        raise RuntimeError(f"Invalid option: {range_type}")
    commits = get_commits_starting_at(temp_repo_path, start_commit)
    if params.limit:
        commits = commits[:params.limit]
    if params.sample:
        seed = params.seed or base64.urlsafe_b64encode(os.urandom(15)).decode('ascii')
        random.seed(seed)
        commits = random.sample(commits, params.sample)
        print("Sampled down to %d commits using random seed %s" % (len(commits), seed))

    # Stage 3: Find and cache expected results for each commit (without incremental mode)
    cache = load_cache(incremental_cache_path)
    set_expected(commits, cache, temp_repo_path, target_file_path, mypy_cache_path,
                 mypy_script=params.mypy_script)
    save_cache(cache, incremental_cache_path)

    # Stage 4: Rewind and re-run mypy (with incremental mode enabled)
    if params.daemon:
        print('Starting daemon')
        start_daemon(mypy_cache_path)
    test_incremental(commits, cache, temp_repo_path, target_file_path, mypy_cache_path,
                     mypy_script=params.mypy_script, daemon=params.daemon,
                     exit_on_error=params.exit_on_error)

    # Stage 5: Remove temp files, stop daemon
    if not params.keep_temporary_files:
        cleanup(temp_repo_path, mypy_cache_path)
    if params.daemon:
        print('Stopping daemon')
        stop_daemon()


</t>
<t tx="ekr.20220525082933.1050">def visit_param_spec(self, t: ParamSpecType) -&gt; Type:
    if self.erase_id(t.id):
        return self.replacement
    return t

</t>
<t tx="ekr.20220525082933.1051">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # Type alias target can't contain bound type variables, so
    # it is safe to just erase the arguments.
    return t.copy_modified(args=[a.accept(self) for a in t.args])


</t>
<t tx="ekr.20220525082933.1052">def remove_instance_last_known_values(t: Type) -&gt; Type:
    return t.accept(LastKnownValueEraser())


</t>
<t tx="ekr.20220525082933.1053">class LastKnownValueEraser(TypeTranslator):
    """Removes the Literal[...] type that may be associated with any
    Instance types."""

    @others
</t>
<t tx="ekr.20220525082933.1054">def visit_instance(self, t: Instance) -&gt; Type:
    if not t.last_known_value and not t.args:
        return t
    new_t = t.copy_modified(
        args=[a.accept(self) for a in t.args],
        last_known_value=None,
    )
    new_t.can_be_true = t.can_be_true
    new_t.can_be_false = t.can_be_false
    return new_t

</t>
<t tx="ekr.20220525082933.1055">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # Type aliases can't contain literal values, because they are
    # always constructed as explicit types.
    return t

</t>
<t tx="ekr.20220525082933.1056">def visit_union_type(self, t: UnionType) -&gt; Type:
    new = cast(UnionType, super().visit_union_type(t))
    # Erasure can result in many duplicate items; merge them.
    # Call make_simplified_union only on lists of instance types
    # that all have the same fullname, to avoid simplifying too
    # much.
    instances = [item for item in new.items
                 if isinstance(get_proper_type(item), Instance)]
    # Avoid merge in simple cases such as optional types.
    if len(instances) &gt; 1:
        instances_by_name: Dict[str, List[Instance]] = {}
        new_items = get_proper_types(new.items)
        for item in new_items:
            if isinstance(item, Instance) and not item.args:
                instances_by_name.setdefault(item.type.fullname, []).append(item)
        merged: List[Type] = []
        for item in new_items:
            if isinstance(item, Instance) and not item.args:
                types = instances_by_name.get(item.type.fullname)
                if types is not None:
                    if len(types) == 1:
                        merged.append(item)
                    else:
                        from mypy.typeops import make_simplified_union
                        merged.append(make_simplified_union(types))
                        del instances_by_name[item.type.fullname]
            else:
                merged.append(item)
        return UnionType.make_union(merged)
    return new
</t>
<t tx="ekr.20220525082933.1057">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Classification of possible errors mypy can detect.

These can be used for filtering specific errors.
"""

from typing import Dict, List
from typing_extensions import Final


# All created error codes are implicitly stored in this list.
all_error_codes: List["ErrorCode"] = []

error_codes: Dict[str, "ErrorCode"] = {}


@others
ATTR_DEFINED: Final = ErrorCode("attr-defined", "Check that attribute exists", "General")
NAME_DEFINED: Final = ErrorCode("name-defined", "Check that name is defined", "General")
CALL_ARG: Final[ErrorCode] = ErrorCode(
    "call-arg", "Check number, names and kinds of arguments in calls", "General"
)
ARG_TYPE: Final = ErrorCode("arg-type", "Check argument types in calls", "General")
CALL_OVERLOAD: Final = ErrorCode(
    "call-overload", "Check that an overload variant matches arguments", "General"
)
VALID_TYPE: Final = ErrorCode("valid-type", "Check that type (annotation) is valid", "General")
VAR_ANNOTATED: Final = ErrorCode(
    "var-annotated", "Require variable annotation if type can't be inferred", "General"
)
OVERRIDE: Final = ErrorCode(
    "override", "Check that method override is compatible with base class", "General"
)
RETURN: Final[ErrorCode] = ErrorCode(
    "return", "Check that function always returns a value", "General"
)
RETURN_VALUE: Final[ErrorCode] = ErrorCode(
    "return-value", "Check that return value is compatible with signature", "General"
)
ASSIGNMENT: Final = ErrorCode(
    "assignment", "Check that assigned value is compatible with target", "General"
)
TYPE_ARG: Final = ErrorCode("type-arg", "Check that generic type arguments are present", "General")
TYPE_VAR: Final = ErrorCode("type-var", "Check that type variable values are valid", "General")
UNION_ATTR: Final = ErrorCode(
    "union-attr", "Check that attribute exists in each item of a union", "General"
)
INDEX: Final = ErrorCode("index", "Check indexing operations", "General")
OPERATOR: Final = ErrorCode("operator", "Check that operator is valid for operands", "General")
LIST_ITEM: Final = ErrorCode(
    "list-item", "Check list items in a list expression [item, ...]", "General"
)
DICT_ITEM: Final = ErrorCode(
    "dict-item", "Check dict items in a dict expression {key: value, ...}", "General"
)
TYPEDDICT_ITEM: Final = ErrorCode(
    "typeddict-item", "Check items when constructing TypedDict", "General"
)
HAS_TYPE: Final = ErrorCode(
    "has-type", "Check that type of reference can be determined", "General"
)
IMPORT: Final = ErrorCode(
    "import", "Require that imported module can be found or has stubs", "General"
)
NO_REDEF: Final = ErrorCode("no-redef", "Check that each name is defined once", "General")
FUNC_RETURNS_VALUE: Final = ErrorCode(
    "func-returns-value", "Check that called function returns a value in value context", "General"
)
ABSTRACT: Final = ErrorCode(
    "abstract", "Prevent instantiation of classes with abstract attributes", "General"
)
VALID_NEWTYPE: Final = ErrorCode(
    "valid-newtype", "Check that argument 2 to NewType is valid", "General"
)
STRING_FORMATTING: Final = ErrorCode(
    "str-format", "Check that string formatting/interpolation is type-safe", "General"
)
STR_BYTES_PY3: Final = ErrorCode(
    "str-bytes-safe", "Warn about dangerous coercions related to bytes and string types", "General"
)
EXIT_RETURN: Final = ErrorCode(
    "exit-return", "Warn about too general return type for '__exit__'", "General"
)
LITERAL_REQ: Final = ErrorCode(
    "literal-required", "Check that value is a literal", 'General'
)
UNUSED_COROUTINE: Final = ErrorCode(
    "unused-coroutine", "Ensure that all coroutines are used", "General"
)

# These error codes aren't enabled by default.
NO_UNTYPED_DEF: Final[ErrorCode] = ErrorCode(
    "no-untyped-def", "Check that every function has an annotation", "General"
)
NO_UNTYPED_CALL: Final = ErrorCode(
    "no-untyped-call",
    "Disallow calling functions without type annotations from annotated functions",
    "General",
)
REDUNDANT_CAST: Final = ErrorCode(
    "redundant-cast", "Check that cast changes type of expression", "General"
)
ASSERT_TYPE: Final = ErrorCode(
    "assert-type", "Check that assert_type() call succeeds", "General"
)
COMPARISON_OVERLAP: Final = ErrorCode(
    "comparison-overlap", "Check that types in comparisons and 'in' expressions overlap", "General"
)
NO_ANY_UNIMPORTED: Final = ErrorCode(
    "no-any-unimported", 'Reject "Any" types from unfollowed imports', "General"
)
NO_ANY_RETURN: Final = ErrorCode(
    "no-any-return",
    'Reject returning value with "Any" type if return type is not "Any"',
    "General",
)
UNREACHABLE: Final = ErrorCode(
    "unreachable", "Warn about unreachable statements or expressions", "General"
)
REDUNDANT_EXPR: Final = ErrorCode(
    "redundant-expr", "Warn about redundant expressions", "General", default_enabled=False
)
TRUTHY_BOOL: Final[ErrorCode] = ErrorCode(
    "truthy-bool",
    "Warn about expressions that could always evaluate to true in boolean contexts",
    "General",
    default_enabled=False,
)
NAME_MATCH: Final = ErrorCode(
    "name-match", "Check that type definition has consistent naming", "General"
)
NO_OVERLOAD_IMPL: Final = ErrorCode(
    "no-overload-impl",
    "Check that overloaded functions outside stub files have an implementation",
    "General",
)
IGNORE_WITHOUT_CODE: Final = ErrorCode(
    "ignore-without-code",
    "Warn about '# type: ignore' comments which do not have error codes",
    "General",
    default_enabled=False,
)
UNUSED_AWAITABLE: Final = ErrorCode(
    "unused-awaitable",
    "Ensure that all awaitable values are used",
    "General",
    default_enabled=False,
)


# Syntax errors are often blocking.
SYNTAX: Final = ErrorCode("syntax", "Report syntax errors", "General")

# This is an internal marker code for a whole-file ignore. It is not intended to
# be user-visible.
FILE: Final = ErrorCode("file", "Internal marker for a whole file being ignored", "General")
del error_codes[FILE.code]

# This is a catch-all for remaining uncategorized errors.
MISC: Final = ErrorCode("misc", "Miscellaneous other checks", "General")
</t>
<t tx="ekr.20220525082933.1058">class ErrorCode:
    @others
</t>
<t tx="ekr.20220525082933.1059">def __init__(self, code: str,
             description: str,
             category: str,
             default_enabled: bool = True) -&gt; None:
    self.code = code
    self.description = description
    self.category = category
    self.default_enabled = default_enabled
    error_codes[code] = self

</t>
<t tx="ekr.20220525082933.106">def main() -&gt; None:
    help_factory = (lambda prog: RawDescriptionHelpFormatter(prog=prog, max_help_position=32))  # type: Any
    parser = ArgumentParser(
        prog='incremental_checker',
        description=__doc__,
        formatter_class=help_factory)

    parser.add_argument("range_type", metavar="START_TYPE", choices=["last", "commit"],
                        help="must be one of 'last' or 'commit'")
    parser.add_argument("range_start", metavar="COMMIT_ID_OR_NUMBER",
                        help="the commit id to start from, or the number of "
                        "commits to move back (see above)")
    parser.add_argument("-r", "--repo_url", default=MYPY_REPO_URL, metavar="URL",
                        help="the repo to clone and run tests on")
    parser.add_argument("-f", "--file-path", default=MYPY_TARGET_FILE, metavar="FILE",
                        help="the name of the file or directory to typecheck")
    parser.add_argument("-x", "--exit-on-error", action='store_true',
                        help="Exits as soon as an error occurs")
    parser.add_argument("--keep-temporary-files", action='store_true',
                        help="Keep temporary files on exit")
    parser.add_argument("--cache-path", default=CACHE_PATH, metavar="DIR",
                        help="sets a custom location to store cache data")
    parser.add_argument("--branch", default=None, metavar="NAME",
                        help="check out and test a custom branch"
                        "uses the default if not specified")
    parser.add_argument("--sample", type=int, help="use a random sample of size SAMPLE")
    parser.add_argument("--seed", type=str, help="random seed")
    parser.add_argument("--limit", type=int,
                        help="maximum number of commits to use (default until end)")
    parser.add_argument("--mypy-script", type=str, help="alternate mypy script to run")
    parser.add_argument("--daemon", action='store_true',
                        help="use mypy daemon instead of incremental (highly experimental)")

    if len(sys.argv[1:]) == 0:
        parser.print_help()
        parser.exit()

    params = parser.parse_args(sys.argv[1:])

    # Make all paths absolute so we avoid having to worry about being in the right folder

    # The path to this specific script (incremental_checker.py).
    script_path = os.path.abspath(sys.argv[0])

    # The path to the mypy repo.
    mypy_path = os.path.abspath(os.path.dirname(os.path.dirname(script_path)))

    # The folder the cloned repo will reside in.
    temp_repo_path = os.path.abspath(os.path.join(mypy_path, "tmp_repo"))

    # The particular file or package to typecheck inside the repo.
    if params.file_path:
        target_file_path = os.path.abspath(os.path.join(temp_repo_path, params.file_path))
    else:
        # Allow `-f ''` to clear target_file_path.
        target_file_path = None

    # The path to where the incremental checker cache data is stored.
    incremental_cache_path = os.path.abspath(params.cache_path)

    # The path to store the mypy incremental mode cache data
    mypy_cache_path = os.path.abspath(os.path.join(mypy_path, "misc", ".mypy_cache"))

    print(f"Assuming mypy is located at {mypy_path}")
    print(f"Temp repo will be cloned at {temp_repo_path}")
    print(f"Testing file/dir located at {target_file_path}")
    print(f"Using cache data located at {incremental_cache_path}")
    print()

    test_repo(params.repo_url, temp_repo_path, target_file_path,
              mypy_path, incremental_cache_path, mypy_cache_path,
              params.range_type, params.range_start, params.branch,
              params)


</t>
<t tx="ekr.20220525082933.1060">def __str__(self) -&gt; str:
    return f'&lt;ErrorCode {self.code}&gt;'


</t>
<t tx="ekr.20220525082933.1061">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
import os.path
import sys
import traceback

from mypy.backports import OrderedDict
from collections import defaultdict

from typing import Tuple, List, TypeVar, Set, Dict, Optional, TextIO, Callable, Union
from typing_extensions import Final, Literal, NoReturn

from mypy.scope import Scope
from mypy.options import Options
from mypy.version import __version__ as mypy_version
from mypy.errorcodes import ErrorCode, IMPORT
from mypy.message_registry import ErrorMessage
from mypy import errorcodes as codes
from mypy.util import DEFAULT_SOURCE_OFFSET, is_typeshed_file

T = TypeVar("T")

allowed_duplicates: Final = ["@overload", "Got:", "Expected:"]

# Keep track of the original error code when the error code of a message is changed.
# This is used to give notes about out-of-date "type: ignore" comments.
original_error_codes: Final = {codes.LITERAL_REQ: codes.MISC}


@others
</t>
<t tx="ekr.20220525082933.1062">class ErrorInfo:
    """Representation of a single error message."""

    # Description of a sequence of imports that refer to the source file
    # related to this error. Each item is a (path, line number) tuple.
    import_ctx: List[Tuple[str, int]]

    # The path to source file that was the source of this error.
    file = ''

    # The fully-qualified id of the source module for this error.
    module: Optional[str] = None

    # The name of the type in which this error is located at.
    type: Optional[str] = ""  # Unqualified, may be None

    # The name of the function or member in which this error is located at.
    function_or_member: Optional[str] = ""  # Unqualified, may be None

    # The line number related to this error within file.
    line = 0     # -1 if unknown

    # The column number related to this error with file.
    column = 0   # -1 if unknown

    # Either 'error' or 'note'
    severity = ''

    # The error message.
    message = ''

    # The error code.
    code: Optional[ErrorCode] = None

    # If True, we should halt build after the file that generated this error.
    blocker = False

    # Only report this particular messages once per program.
    only_once = False

    # Do not remove duplicate copies of this message (ignored if only_once is True).
    allow_dups = False

    # Actual origin of the error message as tuple (path, line number, end line number)
    # If end line number is unknown, use line number.
    origin: Tuple[str, int, int]

    # Fine-grained incremental target where this was reported
    target: Optional[str] = None

    # If True, don't show this message in output, but still record the error (needed
    # by mypy daemon)
    hidden = False

    @others
</t>
<t tx="ekr.20220525082933.1063">def __init__(self,
             import_ctx: List[Tuple[str, int]],
             file: str,
             module: Optional[str],
             typ: Optional[str],
             function_or_member: Optional[str],
             line: int,
             column: int,
             severity: str,
             message: str,
             code: Optional[ErrorCode],
             blocker: bool,
             only_once: bool,
             allow_dups: bool,
             origin: Optional[Tuple[str, int, int]] = None,
             target: Optional[str] = None) -&gt; None:
    self.import_ctx = import_ctx
    self.file = file
    self.module = module
    self.type = typ
    self.function_or_member = function_or_member
    self.line = line
    self.column = column
    self.severity = severity
    self.message = message
    self.code = code
    self.blocker = blocker
    self.only_once = only_once
    self.allow_dups = allow_dups
    self.origin = origin or (file, line, line)
    self.target = target


</t>
<t tx="ekr.20220525082933.1064"># Type used internally to represent errors:
#   (path, line, column, severity, message, allow_dups, code)
ErrorTuple = Tuple[Optional[str],
                   int,
                   int,
                   str,
                   str,
                   bool,
                   Optional[ErrorCode]]


</t>
<t tx="ekr.20220525082933.1065">class ErrorWatcher:
    """Context manager that can be used to keep track of new errors recorded
    around a given operation.

    Errors maintain a stack of such watchers. The handler is called starting
    at the top of the stack, and is propagated down the stack unless filtered
    out by one of the ErrorWatcher instances.
    """
    @others
</t>
<t tx="ekr.20220525082933.1066">def __init__(self, errors: 'Errors', *,
             filter_errors: Union[bool, Callable[[str, ErrorInfo], bool]] = False,
             save_filtered_errors: bool = False):
    self.errors = errors
    self._has_new_errors = False
    self._filter = filter_errors
    self._filtered: Optional[List[ErrorInfo]] = [] if save_filtered_errors else None

</t>
<t tx="ekr.20220525082933.1067">def __enter__(self) -&gt; 'ErrorWatcher':
    self.errors._watchers.append(self)
    return self

</t>
<t tx="ekr.20220525082933.1068">def __exit__(self, exc_type: object, exc_val: object, exc_tb: object) -&gt; Literal[False]:
    assert self == self.errors._watchers.pop()
    return False

</t>
<t tx="ekr.20220525082933.1069">def on_error(self, file: str, info: ErrorInfo) -&gt; bool:
    """Handler called when a new error is recorded.

    The default implementation just sets the has_new_errors flag

    Return True to filter out the error, preventing it from being seen by other
    ErrorWatcher further down the stack and from being recorded by Errors
    """
    self._has_new_errors = True
    if isinstance(self._filter, bool):
        should_filter = self._filter
    elif callable(self._filter):
        should_filter = self._filter(file, info)
    else:
        raise AssertionError(f"invalid error filter: {type(self._filter)}")
    if should_filter and self._filtered is not None:
        self._filtered.append(info)

    return should_filter

</t>
<t tx="ekr.20220525082933.107">@path C:/Repos/mypy/misc/
@language python
@tabwidth -4
#!/usr/bin/env python3

from typing import Callable, List, Tuple

import os
import shutil
import statistics
import subprocess
import textwrap
import time


@others
if __name__ == '__main__':
    main()

</t>
<t tx="ekr.20220525082933.1070">def has_new_errors(self) -&gt; bool:
    return self._has_new_errors

</t>
<t tx="ekr.20220525082933.1071">def filtered_errors(self) -&gt; List[ErrorInfo]:
    assert self._filtered is not None
    return self._filtered


</t>
<t tx="ekr.20220525082933.1072">class Errors:
    """Container for compile errors.

    This class generates and keeps tracks of compile errors and the
    current error context (nested imports).
    """

    # Map from files to generated error messages. Is an OrderedDict so
    # that it can be used to order messages based on the order the
    # files were processed.
    error_info_map: Dict[str, List[ErrorInfo]]

    # optimization for legacy codebases with many files with errors
    has_blockers: Set[str]

    # Files that we have reported the errors for
    flushed_files: Set[str]

    # Current error context: nested import context/stack, as a list of (path, line) pairs.
    import_ctx: List[Tuple[str, int]]

    # Path name prefix that is removed from all paths, if set.
    ignore_prefix: Optional[str] = None

    # Path to current file.
    file: str = ""

    # Ignore some errors on these lines of each file
    # (path -&gt; line -&gt; error-codes)
    ignored_lines: Dict[str, Dict[int, List[str]]]

    # Lines on which an error was actually ignored.
    used_ignored_lines: Dict[str, Dict[int, List[str]]]

    # Files where all errors should be ignored.
    ignored_files: Set[str]

    # Collection of reported only_once messages.
    only_once_messages: Set[str]

    # Set to True to show "In function "foo":" messages.
    show_error_context: bool = False

    # Set to True to show column numbers in error messages.
    show_column_numbers: bool = False

    # Set to True to show absolute file paths in error messages.
    show_absolute_path: bool = False

    # State for keeping track of the current fine-grained incremental mode target.
    # (See mypy.server.update for more about targets.)
    # Current module id.
    target_module: Optional[str] = None
    scope: Optional[Scope] = None

    # Have we seen an import-related error so far? If yes, we filter out other messages
    # in some cases to avoid reporting huge numbers of errors.
    seen_import_error = False

    _watchers: List[ErrorWatcher] = []

    @others
</t>
<t tx="ekr.20220525082933.1073">def __init__(self,
             show_error_context: bool = False,
             show_column_numbers: bool = False,
             show_error_codes: bool = False,
             pretty: bool = False,
             read_source: Optional[Callable[[str], Optional[List[str]]]] = None,
             show_absolute_path: bool = False,
             enabled_error_codes: Optional[Set[ErrorCode]] = None,
             disabled_error_codes: Optional[Set[ErrorCode]] = None,
             many_errors_threshold: int = -1) -&gt; None:
    self.show_error_context = show_error_context
    self.show_column_numbers = show_column_numbers
    self.show_error_codes = show_error_codes
    self.show_absolute_path = show_absolute_path
    self.pretty = pretty
    # We use fscache to read source code when showing snippets.
    self.read_source = read_source
    self.enabled_error_codes = enabled_error_codes or set()
    self.disabled_error_codes = disabled_error_codes or set()
    self.many_errors_threshold = many_errors_threshold
    self.initialize()

</t>
<t tx="ekr.20220525082933.1074">def initialize(self) -&gt; None:
    self.error_info_map = OrderedDict()
    self.flushed_files = set()
    self.import_ctx = []
    self.function_or_member = [None]
    self.ignored_lines = OrderedDict()
    self.used_ignored_lines = defaultdict(lambda: defaultdict(list))
    self.ignored_files = set()
    self.only_once_messages = set()
    self.has_blockers = set()
    self.scope = None
    self.target_module = None
    self.seen_import_error = False

</t>
<t tx="ekr.20220525082933.1075">def reset(self) -&gt; None:
    self.initialize()

</t>
<t tx="ekr.20220525082933.1076">def set_ignore_prefix(self, prefix: str) -&gt; None:
    """Set path prefix that will be removed from all paths."""
    prefix = os.path.normpath(prefix)
    # Add separator to the end, if not given.
    if os.path.basename(prefix) != '':
        prefix += os.sep
    self.ignore_prefix = prefix

</t>
<t tx="ekr.20220525082933.1077">def simplify_path(self, file: str) -&gt; str:
    if self.show_absolute_path:
        return os.path.abspath(file)
    else:
        file = os.path.normpath(file)
        return remove_path_prefix(file, self.ignore_prefix)

</t>
<t tx="ekr.20220525082933.1078">def set_file(self, file: str,
             module: Optional[str],
             scope: Optional[Scope] = None) -&gt; None:
    """Set the path and module id of the current file."""
    # The path will be simplified later, in render_messages. That way
    #  * 'file' is always a key that uniquely identifies a source file
    #    that mypy read (simplified paths might not be unique); and
    #  * we only have to simplify in one place, while still supporting
    #    reporting errors for files other than the one currently being
    #    processed.
    self.file = file
    self.target_module = module
    self.scope = scope

</t>
<t tx="ekr.20220525082933.1079">def set_file_ignored_lines(self, file: str,
                           ignored_lines: Dict[int, List[str]],
                           ignore_all: bool = False) -&gt; None:
    self.ignored_lines[file] = ignored_lines
    if ignore_all:
        self.ignored_files.add(file)

</t>
<t tx="ekr.20220525082933.108">class Command:
    def __init__(self, setup: Callable[[], None], command: Callable[[], None]) -&gt; None:
        self.setup = setup
        self.command = command


</t>
<t tx="ekr.20220525082933.1080">def current_target(self) -&gt; Optional[str]:
    """Retrieves the current target from the associated scope.

    If there is no associated scope, use the target module."""
    if self.scope is not None:
        return self.scope.current_target()
    return self.target_module

</t>
<t tx="ekr.20220525082933.1081">def current_module(self) -&gt; Optional[str]:
    return self.target_module

</t>
<t tx="ekr.20220525082933.1082">def import_context(self) -&gt; List[Tuple[str, int]]:
    """Return a copy of the import context."""
    return self.import_ctx[:]

</t>
<t tx="ekr.20220525082933.1083">def set_import_context(self, ctx: List[Tuple[str, int]]) -&gt; None:
    """Replace the entire import context with a new value."""
    self.import_ctx = ctx[:]

</t>
<t tx="ekr.20220525082933.1084">def report(self,
           line: int,
           column: Optional[int],
           message: str,
           code: Optional[ErrorCode] = None,
           *,
           blocker: bool = False,
           severity: str = 'error',
           file: Optional[str] = None,
           only_once: bool = False,
           allow_dups: bool = False,
           origin_line: Optional[int] = None,
           offset: int = 0,
           end_line: Optional[int] = None) -&gt; None:
    """Report message at the given line using the current error context.

    Args:
        line: line number of error
        column: column number of error
        message: message to report
        code: error code (defaults to 'misc'; not shown for notes)
        blocker: if True, don't continue analysis after this error
        severity: 'error' or 'note'
        file: if non-None, override current file as context
        only_once: if True, only report this exact message once per build
        allow_dups: if True, allow duplicate copies of this message (ignored if only_once)
        origin_line: if non-None, override current context as origin
        end_line: if non-None, override current context as end
    """
    if self.scope:
        type = self.scope.current_type_name()
        if self.scope.ignored &gt; 0:
            type = None  # Omit type context if nested function
        function = self.scope.current_function_name()
    else:
        type = None
        function = None

    if column is None:
        column = -1
    if file is None:
        file = self.file
    if offset:
        message = " " * offset + message

    if origin_line is None:
        origin_line = line

    if end_line is None:
        end_line = origin_line

    code = code or (codes.MISC if not blocker else None)

    info = ErrorInfo(self.import_context(), file, self.current_module(), type,
                     function, line, column, severity, message, code,
                     blocker, only_once, allow_dups,
                     origin=(self.file, origin_line, end_line),
                     target=self.current_target())
    self.add_error_info(info)

</t>
<t tx="ekr.20220525082933.1085">def _add_error_info(self, file: str, info: ErrorInfo) -&gt; None:
    assert file not in self.flushed_files
    # process the stack of ErrorWatchers before modifying any internal state
    # in case we need to filter out the error entirely
    if self._filter_error(file, info):
        return
    if file not in self.error_info_map:
        self.error_info_map[file] = []
    self.error_info_map[file].append(info)
    if info.blocker:
        self.has_blockers.add(file)
    if info.code is IMPORT:
        self.seen_import_error = True

</t>
<t tx="ekr.20220525082933.1086">def _filter_error(self, file: str, info: ErrorInfo) -&gt; bool:
    """
    process ErrorWatcher stack from top to bottom,
    stopping early if error needs to be filtered out
    """
    i = len(self._watchers)
    while i &gt; 0:
        i -= 1
        w = self._watchers[i]
        if w.on_error(file, info):
            return True
    return False

</t>
<t tx="ekr.20220525082933.1087">def add_error_info(self, info: ErrorInfo) -&gt; None:
    file, line, end_line = info.origin
    # process the stack of ErrorWatchers before modifying any internal state
    # in case we need to filter out the error entirely
    # NB: we need to do this both here and in _add_error_info, otherwise we
    # might incorrectly update the sets of ignored or only_once messages
    if self._filter_error(file, info):
        return
    if not info.blocker:  # Blockers cannot be ignored
        if file in self.ignored_lines:
            # It's okay if end_line is *before* line.
            # Function definitions do this, for example, because the correct
            # error reporting line is at the *end* of the ignorable range
            # (for compatibility reasons). If so, just flip 'em!
            if end_line &lt; line:
                line, end_line = end_line, line
            # Check each line in this context for "type: ignore" comments.
            # line == end_line for most nodes, so we only loop once.
            for scope_line in range(line, end_line + 1):
                if self.is_ignored_error(scope_line, info, self.ignored_lines[file]):
                    # Annotation requests us to ignore all errors on this line.
                    self.used_ignored_lines[file][scope_line].append(
                        (info.code or codes.MISC).code)
                    return
        if file in self.ignored_files:
            return
    if info.only_once:
        if info.message in self.only_once_messages:
            return
        self.only_once_messages.add(info.message)
    if self.seen_import_error and info.code is not IMPORT and self.has_many_errors():
        # Missing stubs can easily cause thousands of errors about
        # Any types, especially when upgrading to mypy 0.900,
        # which no longer bundles third-party library stubs. Avoid
        # showing too many errors to make it easier to see
        # import-related errors.
        info.hidden = True
        self.report_hidden_errors(info)
    self._add_error_info(file, info)
    ignored_codes = self.ignored_lines.get(file, {}).get(info.line, [])
    if ignored_codes and info.code:
        # Something is ignored on the line, but not this error, so maybe the error
        # code is incorrect.
        msg = f'Error code "{info.code.code}" not covered by "type: ignore" comment'
        if info.code in original_error_codes:
            # If there seems to be a "type: ignore" with a stale error
            # code, report a more specific note.
            old_code = original_error_codes[info.code].code
            if old_code in ignored_codes:
                msg = (f'Error code changed to {info.code.code}; "type: ignore" comment ' +
                       'may be out of date')
        note = ErrorInfo(
            info.import_ctx, info.file, info.module, info.type, info.function_or_member,
            info.line, info.column, 'note', msg,
            code=None, blocker=False, only_once=False, allow_dups=False
        )
        self._add_error_info(file, note)

</t>
<t tx="ekr.20220525082933.1088">def has_many_errors(self) -&gt; bool:
    if self.many_errors_threshold &lt; 0:
        return False
    if len(self.error_info_map) &gt;= self.many_errors_threshold:
        return True
    if sum(len(errors)
           for errors in self.error_info_map.values()) &gt;= self.many_errors_threshold:
        return True
    return False

</t>
<t tx="ekr.20220525082933.1089">def report_hidden_errors(self, info: ErrorInfo) -&gt; None:
    message = (
        '(Skipping most remaining errors due to unresolved imports or missing stubs; ' +
        'fix these first)'
    )
    if message in self.only_once_messages:
        return
    self.only_once_messages.add(message)
    new_info = ErrorInfo(
        import_ctx=info.import_ctx,
        file=info.file,
        module=info.module,
        typ=None,
        function_or_member=None,
        line=info.line,
        column=info.line,
        severity='note',
        message=message,
        code=None,
        blocker=False,
        only_once=True,
        allow_dups=False,
        origin=info.origin,
        target=info.target,
    )
    self._add_error_info(info.origin[0], new_info)

</t>
<t tx="ekr.20220525082933.109">def print_offset(text: str, indent_length: int = 4) -&gt; None:
    print()
    print(textwrap.indent(text, ' ' * indent_length))
    print()


</t>
<t tx="ekr.20220525082933.1090">def is_ignored_error(self, line: int, info: ErrorInfo, ignores: Dict[int, List[str]]) -&gt; bool:
    if info.blocker:
        # Blocking errors can never be ignored
        return False
    if info.code and self.is_error_code_enabled(info.code) is False:
        return True
    if line not in ignores:
        return False
    if not ignores[line]:
        # Empty list means that we ignore all errors
        return True
    if info.code and self.is_error_code_enabled(info.code) is True:
        return info.code.code in ignores[line]
    return False

</t>
<t tx="ekr.20220525082933.1091">def is_error_code_enabled(self, error_code: ErrorCode) -&gt; bool:
    if error_code in self.disabled_error_codes:
        return False
    elif error_code in self.enabled_error_codes:
        return True
    else:
        return error_code.default_enabled

</t>
<t tx="ekr.20220525082933.1092">def clear_errors_in_targets(self, path: str, targets: Set[str]) -&gt; None:
    """Remove errors in specific fine-grained targets within a file."""
    if path in self.error_info_map:
        new_errors = []
        has_blocker = False
        for info in self.error_info_map[path]:
            if info.target not in targets:
                new_errors.append(info)
                has_blocker |= info.blocker
            elif info.only_once:
                self.only_once_messages.remove(info.message)
        self.error_info_map[path] = new_errors
        if not has_blocker and path in self.has_blockers:
            self.has_blockers.remove(path)

</t>
<t tx="ekr.20220525082933.1093">def generate_unused_ignore_errors(self, file: str) -&gt; None:
    ignored_lines = self.ignored_lines[file]
    if not is_typeshed_file(file) and file not in self.ignored_files:
        ignored_lines = self.ignored_lines[file]
        used_ignored_lines = self.used_ignored_lines[file]
        for line, ignored_codes in ignored_lines.items():
            used_ignored_codes = used_ignored_lines[line]
            unused_ignored_codes = set(ignored_codes) - set(used_ignored_codes)
            # `ignore` is used
            if len(ignored_codes) == 0 and len(used_ignored_codes) &gt; 0:
                continue
            # All codes appearing in `ignore[...]` are used
            if len(ignored_codes) &gt; 0 and len(unused_ignored_codes) == 0:
                continue
            # Display detail only when `ignore[...]` specifies more than one error code
            unused_codes_message = ""
            if len(ignored_codes) &gt; 1 and len(unused_ignored_codes) &gt; 0:
                unused_codes_message = f"[{', '.join(sorted(unused_ignored_codes))}]"
            message = f'Unused "type: ignore{unused_codes_message}" comment'
            # Don't use report since add_error_info will ignore the error!
            info = ErrorInfo(self.import_context(), file, self.current_module(), None,
                             None, line, -1, 'error', message,
                             None, False, False, False)
            self._add_error_info(file, info)

</t>
<t tx="ekr.20220525082933.1094">def generate_ignore_without_code_errors(self,
                                        file: str,
                                        is_warning_unused_ignores: bool) -&gt; None:
    if is_typeshed_file(file) or file in self.ignored_files:
        return

    used_ignored_lines = self.used_ignored_lines[file]

    # If the whole file is ignored, ignore it.
    if used_ignored_lines:
        _, used_codes = min(used_ignored_lines.items())
        if codes.FILE.code in used_codes:
            return

    for line, ignored_codes in self.ignored_lines[file].items():
        if ignored_codes:
            continue

        # If the ignore is itself unused and that would be warned about, let
        # that error stand alone
        if is_warning_unused_ignores and not used_ignored_lines[line]:
            continue

        codes_hint = ''
        ignored_codes = sorted(set(used_ignored_lines[line]))
        if ignored_codes:
            codes_hint = f' (consider "type: ignore[{", ".join(ignored_codes)}]" instead)'

        message = f'"type: ignore" comment without error code{codes_hint}'
        # Don't use report since add_error_info will ignore the error!
        info = ErrorInfo(self.import_context(), file, self.current_module(), None,
                         None, line, -1, 'error', message, codes.IGNORE_WITHOUT_CODE,
                         False, False, False)
        self._add_error_info(file, info)

</t>
<t tx="ekr.20220525082933.1095">def num_messages(self) -&gt; int:
    """Return the number of generated messages."""
    return sum(len(x) for x in self.error_info_map.values())

</t>
<t tx="ekr.20220525082933.1096">def is_errors(self) -&gt; bool:
    """Are there any generated messages?"""
    return bool(self.error_info_map)

</t>
<t tx="ekr.20220525082933.1097">def is_blockers(self) -&gt; bool:
    """Are the any errors that are blockers?"""
    return bool(self.has_blockers)

</t>
<t tx="ekr.20220525082933.1098">def blocker_module(self) -&gt; Optional[str]:
    """Return the module with a blocking error, or None if not possible."""
    for path in self.has_blockers:
        for err in self.error_info_map[path]:
            if err.blocker:
                return err.module
    return None

</t>
<t tx="ekr.20220525082933.1099">def is_errors_for_file(self, file: str) -&gt; bool:
    """Are there any errors for the given file?"""
    return file in self.error_info_map

</t>
<t tx="ekr.20220525082933.110">def delete_folder(folder_path: str) -&gt; None:
    if os.path.exists(folder_path):
        shutil.rmtree(folder_path)


</t>
<t tx="ekr.20220525082933.1100">def raise_error(self, use_stdout: bool = True) -&gt; NoReturn:
    """Raise a CompileError with the generated messages.

    Render the messages suitable for displaying.
    """
    # self.new_messages() will format all messages that haven't already
    # been returned from a file_messages() call.
    raise CompileError(self.new_messages(),
                       use_stdout=use_stdout,
                       module_with_blocker=self.blocker_module())

</t>
<t tx="ekr.20220525082933.1101">def format_messages(self, error_info: List[ErrorInfo],
                    source_lines: Optional[List[str]]) -&gt; List[str]:
    """Return a string list that represents the error messages.

    Use a form suitable for displaying to the user. If self.pretty
    is True also append a relevant trimmed source code line (only for
    severity 'error').
    """
    a: List[str] = []
    error_info = [info for info in error_info if not info.hidden]
    errors = self.render_messages(self.sort_messages(error_info))
    errors = self.remove_duplicates(errors)
    for file, line, column, severity, message, allow_dups, code in errors:
        s = ''
        if file is not None:
            if self.show_column_numbers and line &gt;= 0 and column &gt;= 0:
                srcloc = f'{file}:{line}:{1 + column}'
            elif line &gt;= 0:
                srcloc = f'{file}:{line}'
            else:
                srcloc = file
            s = f'{srcloc}: {severity}: {message}'
        else:
            s = message
        if self.show_error_codes and code and severity != 'note':
            # If note has an error code, it is related to a previous error. Avoid
            # displaying duplicate error codes.
            s = f'{s}  [{code.code}]'
        a.append(s)
        if self.pretty:
            # Add source code fragment and a location marker.
            if severity == 'error' and source_lines and line &gt; 0:
                source_line = source_lines[line - 1]
                source_line_expanded = source_line.expandtabs()
                if column &lt; 0:
                    # Something went wrong, take first non-empty column.
                    column = len(source_line) - len(source_line.lstrip())

                # Shifts column after tab expansion
                column = len(source_line[:column].expandtabs())

                # Note, currently coloring uses the offset to detect source snippets,
                # so these offsets should not be arbitrary.
                a.append(' ' * DEFAULT_SOURCE_OFFSET + source_line_expanded)
                a.append(' ' * (DEFAULT_SOURCE_OFFSET + column) + '^')
    return a

</t>
<t tx="ekr.20220525082933.1102">def file_messages(self, path: str) -&gt; List[str]:
    """Return a string list of new error messages from a given file.

    Use a form suitable for displaying to the user.
    """
    if path not in self.error_info_map:
        return []
    self.flushed_files.add(path)
    source_lines = None
    if self.pretty:
        assert self.read_source
        source_lines = self.read_source(path)
    return self.format_messages(self.error_info_map[path], source_lines)

</t>
<t tx="ekr.20220525082933.1103">def new_messages(self) -&gt; List[str]:
    """Return a string list of new error messages.

    Use a form suitable for displaying to the user.
    Errors from different files are ordered based on the order in which
    they first generated an error.
    """
    msgs = []
    for path in self.error_info_map.keys():
        if path not in self.flushed_files:
            msgs.extend(self.file_messages(path))
    return msgs

</t>
<t tx="ekr.20220525082933.1104">def targets(self) -&gt; Set[str]:
    """Return a set of all targets that contain errors."""
    # TODO: Make sure that either target is always defined or that not being defined
    #       is okay for fine-grained incremental checking.
    return {
        info.target
        for errs in self.error_info_map.values()
        for info in errs
        if info.target
    }

</t>
<t tx="ekr.20220525082933.1105">def render_messages(self,
                    errors: List[ErrorInfo]) -&gt; List[ErrorTuple]:
    """Translate the messages into a sequence of tuples.

    Each tuple is of form (path, line, col, severity, message, allow_dups, code).
    The rendered sequence includes information about error contexts.
    The path item may be None. If the line item is negative, the
    line number is not defined for the tuple.
    """
    result: List[ErrorTuple] = []
    prev_import_context: List[Tuple[str, int]] = []
    prev_function_or_member: Optional[str] = None
    prev_type: Optional[str] = None

    for e in errors:
        # Report module import context, if different from previous message.
        if not self.show_error_context:
            pass
        elif e.import_ctx != prev_import_context:
            last = len(e.import_ctx) - 1
            i = last
            while i &gt;= 0:
                path, line = e.import_ctx[i]
                fmt = '{}:{}: note: In module imported here'
                if i &lt; last:
                    fmt = '{}:{}: note: ... from here'
                if i &gt; 0:
                    fmt += ','
                else:
                    fmt += ':'
                # Remove prefix to ignore from path (if present) to
                # simplify path.
                path = remove_path_prefix(path, self.ignore_prefix)
                result.append((None, -1, -1, 'note',
                               fmt.format(path, line), e.allow_dups, None))
                i -= 1

        file = self.simplify_path(e.file)

        # Report context within a source file.
        if not self.show_error_context:
            pass
        elif (e.function_or_member != prev_function_or_member or
                e.type != prev_type):
            if e.function_or_member is None:
                if e.type is None:
                    result.append((file, -1, -1, 'note', 'At top level:', e.allow_dups, None))
                else:
                    result.append((file, -1, -1, 'note', 'In class "{}":'.format(
                        e.type), e.allow_dups, None))
            else:
                if e.type is None:
                    result.append((file, -1, -1, 'note',
                                   'In function "{}":'.format(
                                       e.function_or_member), e.allow_dups, None))
                else:
                    result.append((file, -1, -1, 'note',
                                   'In member "{}" of class "{}":'.format(
                                       e.function_or_member, e.type), e.allow_dups, None))
        elif e.type != prev_type:
            if e.type is None:
                result.append((file, -1, -1, 'note', 'At top level:', e.allow_dups, None))
            else:
                result.append((file, -1, -1, 'note',
                               f'In class "{e.type}":', e.allow_dups, None))

        if isinstance(e.message, ErrorMessage):
            result.append(
                (file, e.line, e.column, e.severity, e.message.value, e.allow_dups, e.code))
        else:
            result.append(
                (file, e.line, e.column, e.severity, e.message, e.allow_dups, e.code))

        prev_import_context = e.import_ctx
        prev_function_or_member = e.function_or_member
        prev_type = e.type

    return result

</t>
<t tx="ekr.20220525082933.1106">def sort_messages(self, errors: List[ErrorInfo]) -&gt; List[ErrorInfo]:
    """Sort an array of error messages locally by line number.

    I.e., sort a run of consecutive messages with the same
    context by line number, but otherwise retain the general
    ordering of the messages.
    """
    result: List[ErrorInfo] = []
    i = 0
    while i &lt; len(errors):
        i0 = i
        # Find neighbouring errors with the same context and file.
        while (i + 1 &lt; len(errors) and
                errors[i + 1].import_ctx == errors[i].import_ctx and
                errors[i + 1].file == errors[i].file):
            i += 1
        i += 1

        # Sort the errors specific to a file according to line number and column.
        a = sorted(errors[i0:i], key=lambda x: (x.line, x.column))
        result.extend(a)
    return result

</t>
<t tx="ekr.20220525082933.1107">def remove_duplicates(self, errors: List[ErrorTuple]) -&gt; List[ErrorTuple]:
    """Remove duplicates from a sorted error list."""
    res: List[ErrorTuple] = []
    i = 0
    while i &lt; len(errors):
        dup = False
        # Use slightly special formatting for member conflicts reporting.
        conflicts_notes = False
        j = i - 1
        # Find duplicates, unless duplicates are allowed.
        if not errors[i][5]:
            while j &gt;= 0 and errors[j][0] == errors[i][0]:
                if errors[j][4].strip() == 'Got:':
                    conflicts_notes = True
                j -= 1
            j = i - 1
            while (j &gt;= 0 and errors[j][0] == errors[i][0] and
                    errors[j][1] == errors[i][1]):
                if (errors[j][3] == errors[i][3] and
                        # Allow duplicate notes in overload conflicts reporting.
                        not ((errors[i][3] == 'note' and
                            errors[i][4].strip() in allowed_duplicates)
                            or (errors[i][4].strip().startswith('def ') and
                                conflicts_notes)) and
                        errors[j][4] == errors[i][4]):  # ignore column
                    dup = True
                    break
                j -= 1
        if not dup:
            res.append(errors[i])
        i += 1
    return res


</t>
<t tx="ekr.20220525082933.1108">class CompileError(Exception):
    """Exception raised when there is a compile error.

    It can be a parse, semantic analysis, type check or other
    compilation-related error.

    CompileErrors raised from an errors object carry all of the
    messages that have not been reported out by error streaming.
    This is patched up by build.build to contain either all error
    messages (if errors were streamed) or none (if they were not).

    """

    messages: List[str]
    use_stdout = False
    # Can be set in case there was a module with a blocking error
    module_with_blocker: Optional[str] = None

    @others
</t>
<t tx="ekr.20220525082933.1109">def __init__(self,
             messages: List[str],
             use_stdout: bool = False,
             module_with_blocker: Optional[str] = None) -&gt; None:
    super().__init__('\n'.join(messages))
    self.messages = messages
    self.use_stdout = use_stdout
    self.module_with_blocker = module_with_blocker


</t>
<t tx="ekr.20220525082933.111">def execute(command: List[str]) -&gt; None:
    proc = subprocess.Popen(
        ' '.join(command),
        stderr=subprocess.PIPE,
        stdout=subprocess.PIPE,
        shell=True)
    stdout_bytes, stderr_bytes = proc.communicate()  # type: Tuple[bytes, bytes]
    stdout, stderr = stdout_bytes.decode('utf-8'), stderr_bytes.decode('utf-8')
    if proc.returncode != 0:
        print('EXECUTED COMMAND:', repr(command))
        print('RETURN CODE:', proc.returncode)
        print()
        print('STDOUT:')
        print_offset(stdout)
        print('STDERR:')
        print_offset(stderr)
        raise RuntimeError('Unexpected error from external tool.')


</t>
<t tx="ekr.20220525082933.1110">def remove_path_prefix(path: str, prefix: Optional[str]) -&gt; str:
    """If path starts with prefix, return copy of path with the prefix removed.
    Otherwise, return path. If path is None, return None.
    """
    if prefix is not None and path.startswith(prefix):
        return path[len(prefix):]
    else:
        return path


</t>
<t tx="ekr.20220525082933.1111">def report_internal_error(err: Exception,
                          file: Optional[str],
                          line: int,
                          errors: Errors,
                          options: Options,
                          stdout: Optional[TextIO] = None,
                          stderr: Optional[TextIO] = None,
                          ) -&gt; NoReturn:
    """Report internal error and exit.

    This optionally starts pdb or shows a traceback.
    """
    stdout = (stdout or sys.stdout)
    stderr = (stderr or sys.stderr)
    # Dump out errors so far, they often provide a clue.
    # But catch unexpected errors rendering them.
    try:
        for msg in errors.new_messages():
            print(msg)
    except Exception as e:
        print("Failed to dump errors:", repr(e), file=stderr)

    # Compute file:line prefix for official-looking error messages.
    if file:
        if line:
            prefix = f'{file}:{line}: '
        else:
            prefix = f'{file}: '
    else:
        prefix = ''

    # Print "INTERNAL ERROR" message.
    print(f'{prefix}error: INTERNAL ERROR --',
          'Please try using mypy master on GitHub:\n'
          'https://mypy.readthedocs.io/en/stable/common_issues.html'
          '#using-a-development-mypy-build',
          file=stderr)
    if options.show_traceback:
        print('Please report a bug at https://github.com/python/mypy/issues',
            file=stderr)
    else:
        print('If this issue continues with mypy master, '
              'please report a bug at https://github.com/python/mypy/issues',
            file=stderr)
    print(f'version: {mypy_version}',
          file=stderr)

    # If requested, drop into pdb. This overrides show_tb.
    if options.pdb:
        print('Dropping into pdb', file=stderr)
        import pdb
        pdb.post_mortem(sys.exc_info()[2])

    # If requested, print traceback, else print note explaining how to get one.
    if options.raise_exceptions:
        raise err
    if not options.show_traceback:
        if not options.pdb:
            print('{}: note: please use --show-traceback to print a traceback '
                  'when reporting a bug'.format(prefix),
                  file=stderr)
    else:
        tb = traceback.extract_stack()[:-2]
        tb2 = traceback.extract_tb(sys.exc_info()[2])
        print('Traceback (most recent call last):')
        for s in traceback.format_list(tb + tb2):
            print(s.rstrip('\n'))
        print(f'{type(err).__name__}: {err}', file=stdout)
        print(f'{prefix}: note: use --pdb to drop into pdb', file=stderr)

    # Exit.  The caller has nothing more to say.
    # We use exit code 2 to signal that this is no ordinary error.
    raise SystemExit(2)
</t>
<t tx="ekr.20220525082933.1112">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from typing import Dict, Iterable, List, TypeVar, Mapping, cast, Union, Optional

from mypy.types import (
    Type, Instance, CallableType, TypeVisitor, UnboundType, AnyType,
    NoneType, Overloaded, TupleType, TypedDictType, UnionType,
    ErasedType, PartialType, DeletedType, UninhabitedType, TypeType, TypeVarId,
    FunctionLike, TypeVarType, LiteralType, get_proper_type, ProperType,
    TypeAliasType, ParamSpecType, TypeVarLikeType, Parameters, ParamSpecFlavor,
    UnpackType, TypeVarTupleType
)


@others
</t>
<t tx="ekr.20220525082933.112">def trial(num_trials: int, command: Command) -&gt; List[float]:
    trials = []
    for i in range(num_trials):
        command.setup()
        start = time.time()
        command.command()
        delta = time.time() - start
        trials.append(delta)
    return trials


</t>
<t tx="ekr.20220525082933.113">def report(name: str, times: List[float]) -&gt; None:
    print(f"{name}:")
    print(f"  Times: {times}")
    print(f"  Mean:  {statistics.mean(times)}")
    print(f"  Stdev: {statistics.stdev(times)}")
    print()


</t>
<t tx="ekr.20220525082933.114">def main() -&gt; None:
    trials = 3

    print("Testing baseline")
    baseline = trial(trials, Command(
        lambda: None,
        lambda: execute(["python3", "-m", "mypy", "mypy"])))
    report("Baseline", baseline)

    print("Testing cold cache")
    cold_cache = trial(trials, Command(
        lambda: delete_folder(".mypy_cache"),
        lambda: execute(["python3", "-m", "mypy", "-i", "mypy"])))
    report("Cold cache", cold_cache)

    print("Testing warm cache")
    execute(["python3", "-m", "mypy", "-i", "mypy"])
    warm_cache = trial(trials, Command(
        lambda: None,
        lambda: execute(["python3", "-m", "mypy", "-i", "mypy"])))
    report("Warm cache", warm_cache)


</t>
<t tx="ekr.20220525082933.115">@path C:/Repos/mypy/misc/
@language python
@tabwidth -4
from mypy.plugin import Plugin, FunctionContext
from mypy.types import (
    FunctionLike, Type, Instance, CallableType, UnionType, get_proper_type, ProperType,
    get_proper_types, TupleType, NoneTyp, AnyType
)
from mypy.nodes import TypeInfo
from mypy.subtypes import is_proper_subtype

from typing_extensions import Type as typing_Type
from typing import Optional, Callable


@others
</t>
<t tx="ekr.20220525082933.116">class ProperTypePlugin(Plugin):
    """
    A plugin to ensure that every type is expanded before doing any special-casing.

    This solves the problem that we have hundreds of call sites like:

        if isinstance(typ, UnionType):
            ...  # special-case union

    But after introducing a new type TypeAliasType (and removing immediate expansion)
    all these became dangerous because typ may be e.g. an alias to union.
    """
    @others
</t>
<t tx="ekr.20220525082933.117">def get_function_hook(self, fullname: str
                      ) -&gt; Optional[Callable[[FunctionContext], Type]]:
    if fullname == 'builtins.isinstance':
        return isinstance_proper_hook
    if fullname == 'mypy.types.get_proper_type':
        return proper_type_hook
    if fullname == 'mypy.types.get_proper_types':
        return proper_types_hook
    return None


</t>
<t tx="ekr.20220525082933.118">def isinstance_proper_hook(ctx: FunctionContext) -&gt; Type:
    if len(ctx.arg_types) != 2 or not ctx.arg_types[1]:
        return ctx.default_return_type

    right = get_proper_type(ctx.arg_types[1][0])
    for arg in ctx.arg_types[0]:
        if (is_improper_type(arg) or
                isinstance(get_proper_type(arg), AnyType) and is_dangerous_target(right)):
            if is_special_target(right):
                return ctx.default_return_type
            ctx.api.fail('Never apply isinstance() to unexpanded types;'
                         ' use mypy.types.get_proper_type() first', ctx.context)
            ctx.api.note('If you pass on the original type'  # type: ignore[attr-defined]
                         ' after the check, always use its unexpanded version', ctx.context)
    return ctx.default_return_type


</t>
<t tx="ekr.20220525082933.119">def is_special_target(right: ProperType) -&gt; bool:
    """Whitelist some special cases for use in isinstance() with improper types."""
    if isinstance(right, FunctionLike) and right.is_type_obj():
        if right.type_object().fullname == 'builtins.tuple':
            # Used with Union[Type, Tuple[Type, ...]].
            return True
        if right.type_object().fullname in (
            'mypy.types.Type',
            'mypy.types.ProperType',
            'mypy.types.TypeAliasType'
        ):
            # Special case: things like assert isinstance(typ, ProperType) are always OK.
            return True
        if right.type_object().fullname in (
            'mypy.types.UnboundType',
            'mypy.types.TypeVarType',
            'mypy.types.ParamSpecType',
            'mypy.types.RawExpressionType',
            'mypy.types.EllipsisType',
            'mypy.types.StarType',
            'mypy.types.TypeList',
            'mypy.types.CallableArgument',
            'mypy.types.PartialType',
            'mypy.types.ErasedType'
        ):
            # Special case: these are not valid targets for a type alias and thus safe.
            # TODO: introduce a SyntheticType base to simplify this?
            return True
    elif isinstance(right, TupleType):
        return all(is_special_target(t) for t in get_proper_types(right.items))
    return False


</t>
<t tx="ekr.20220525082933.120">def is_improper_type(typ: Type) -&gt; bool:
    """Is this a type that is not a subtype of ProperType?"""
    typ = get_proper_type(typ)
    if isinstance(typ, Instance):
        info = typ.type
        return info.has_base('mypy.types.Type') and not info.has_base('mypy.types.ProperType')
    if isinstance(typ, UnionType):
        return any(is_improper_type(t) for t in typ.items)
    return False


</t>
<t tx="ekr.20220525082933.121">def is_dangerous_target(typ: ProperType) -&gt; bool:
    """Is this a dangerous target (right argument) for an isinstance() check?"""
    if isinstance(typ, TupleType):
        return any(is_dangerous_target(get_proper_type(t)) for t in typ.items)
    if isinstance(typ, CallableType) and typ.is_type_obj():
        return typ.type_object().has_base('mypy.types.Type')
    return False


</t>
<t tx="ekr.20220525082933.122">def proper_type_hook(ctx: FunctionContext) -&gt; Type:
    """Check if this get_proper_type() call is not redundant."""
    arg_types = ctx.arg_types[0]
    if arg_types:
        arg_type = get_proper_type(arg_types[0])
        proper_type = get_proper_type_instance(ctx)
        if is_proper_subtype(arg_type, UnionType.make_union([NoneTyp(), proper_type])):
            # Minimize amount of spurious errors from overload machinery.
            # TODO: call the hook on the overload as a whole?
            if isinstance(arg_type, (UnionType, Instance)):
                ctx.api.fail('Redundant call to get_proper_type()', ctx.context)
    return ctx.default_return_type


</t>
<t tx="ekr.20220525082933.123">def proper_types_hook(ctx: FunctionContext) -&gt; Type:
    """Check if this get_proper_types() call is not redundant."""
    arg_types = ctx.arg_types[0]
    if arg_types:
        arg_type = arg_types[0]
        proper_type = get_proper_type_instance(ctx)
        item_type = UnionType.make_union([NoneTyp(), proper_type])
        ok_type = ctx.api.named_generic_type('typing.Iterable', [item_type])
        if is_proper_subtype(arg_type, ok_type):
            ctx.api.fail('Redundant call to get_proper_types()', ctx.context)
    return ctx.default_return_type


</t>
<t tx="ekr.20220525082933.124">def get_proper_type_instance(ctx: FunctionContext) -&gt; Instance:
    types = ctx.api.modules['mypy.types']  # type: ignore
    proper_type_info = types.names['ProperType']
    assert isinstance(proper_type_info.node, TypeInfo)
    return Instance(proper_type_info.node, [])


</t>
<t tx="ekr.20220525082933.125">def plugin(version: str) -&gt; typing_Type[ProperTypePlugin]:
    return ProperTypePlugin
</t>
<t tx="ekr.20220525082933.126">@path C:/Repos/mypy/misc/
@language python
@tabwidth -4
"""Sync stdlib stubs (and a few other files) from typeshed.

Usage:

  python3 misc/sync-typeshed.py [--commit hash] [--typeshed-dir dir]

By default, sync to the latest typeshed commit.
"""

import argparse
import os
import shutil
import subprocess
import sys
import tempfile
import textwrap
from typing import Optional


@others
if __name__ == '__main__':
    main()
</t>
<t tx="ekr.20220525082933.127">def check_state() -&gt; None:
    if not os.path.isfile('README.md'):
        sys.exit('error: The current working directory must be the mypy repository root')
    out = subprocess.check_output(['git', 'status', '-s', os.path.join('mypy', 'typeshed')])
    if out:
        # If there are local changes under mypy/typeshed, they would be lost.
        sys.exit('error: Output of "git status -s mypy/typeshed" must be empty')


</t>
<t tx="ekr.20220525082933.128">def update_typeshed(typeshed_dir: str, commit: Optional[str]) -&gt; str:
    """Update contents of local typeshed copy.

    Return the normalized typeshed commit hash.
    """
    assert os.path.isdir(os.path.join(typeshed_dir, 'stdlib'))
    assert os.path.isdir(os.path.join(typeshed_dir, 'stubs'))
    if commit:
        subprocess.run(['git', 'checkout', commit], check=True, cwd=typeshed_dir)
    commit = git_head_commit(typeshed_dir)
    stdlib_dir = os.path.join('mypy', 'typeshed', 'stdlib')
    # Remove existing stubs.
    shutil.rmtree(stdlib_dir)
    # Copy new stdlib stubs.
    shutil.copytree(os.path.join(typeshed_dir, 'stdlib'), stdlib_dir)
    # Copy mypy_extensions stubs. We don't want to use a stub package, since it's
    # treated specially by mypy and we make assumptions about what's there.
    stubs_dir = os.path.join('mypy', 'typeshed', 'stubs')
    shutil.rmtree(stubs_dir)
    os.makedirs(stubs_dir)
    shutil.copytree(os.path.join(typeshed_dir, 'stubs', 'mypy-extensions'),
                    os.path.join(stubs_dir, 'mypy-extensions'))
    shutil.copy(os.path.join(typeshed_dir, 'LICENSE'), os.path.join('mypy', 'typeshed'))
    return commit


</t>
<t tx="ekr.20220525082933.129">def git_head_commit(repo: str) -&gt; str:
    commit = subprocess.check_output(['git', 'rev-parse', 'HEAD'], cwd=repo).decode('ascii')
    return commit.strip()


</t>
<t tx="ekr.20220525082933.130">def main() -&gt; None:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--commit", default=None,
        help="Typeshed commit (default to latest master if using a repository clone)"
    )
    parser.add_argument(
        "--typeshed-dir", default=None,
        help="Location of typeshed (default to a temporary repository clone)"
    )
    args = parser.parse_args()
    check_state()
    print('Update contents of mypy/typeshed from typeshed? [yN] ', end='')
    answer = input()
    if answer.lower() != 'y':
        sys.exit('Aborting')

    if not args.typeshed_dir:
        # Clone typeshed repo if no directory given.
        with tempfile.TemporaryDirectory() as tempdir:
            print(f'Cloning typeshed in {tempdir}...')
            subprocess.run(['git', 'clone', 'https://github.com/python/typeshed.git'],
                           check=True, cwd=tempdir)
            repo = os.path.join(tempdir, 'typeshed')
            commit = update_typeshed(repo, args.commit)
    else:
        commit = update_typeshed(args.typeshed_dir, args.commit)

    assert commit

    # Create a commit
    message = textwrap.dedent("""\
        Sync typeshed

        Source commit:
        https://github.com/python/typeshed/commit/{commit}
        """.format(commit=commit))
    subprocess.run(['git', 'add', '--all', os.path.join('mypy', 'typeshed')], check=True)
    subprocess.run(['git', 'commit', '-m', message], check=True)
    print('Created typeshed sync commit.')


</t>
<t tx="ekr.20220525082933.131">@path C:/Repos/mypy/misc/
@language python
@tabwidth -4
from typing import Iterator, List
import sys
import os
import os.path


@others
if __name__ == '__main__':
    main()
</t>
<t tx="ekr.20220525082933.132">class Chunk:
    def __init__(self, header_type: str, args: str) -&gt; None:
        self.header_type = header_type
        self.args = args
        self.lines = []  # type: List[str]


</t>
<t tx="ekr.20220525082933.133">def is_header(line: str) -&gt; bool:
    return line.startswith('[') and line.endswith(']')


</t>
<t tx="ekr.20220525082933.134">def normalize(lines: Iterator[str]) -&gt; Iterator[str]:
    return (line.rstrip() for line in lines)


</t>
<t tx="ekr.20220525082933.135">def produce_chunks(lines: Iterator[str]) -&gt; Iterator[Chunk]:
    current_chunk = None  # type: Chunk
    for line in normalize(lines):
        if is_header(line):
            if current_chunk is not None:
                yield current_chunk
            parts = line[1:-1].split(' ', 1)
            args = parts[1] if len(parts) &gt; 1 else ''
            current_chunk = Chunk(parts[0], args)
        else:
            current_chunk.lines.append(line)
    if current_chunk is not None:
        yield current_chunk


</t>
<t tx="ekr.20220525082933.136">def write_out(filename: str, lines: List[str]) -&gt; None:
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    with open(filename, 'w') as stream:
        stream.write('\n'.join(lines))


</t>
<t tx="ekr.20220525082933.137">def write_tree(root: str, chunks: Iterator[Chunk]) -&gt; None:
    init = next(chunks)
    assert init.header_type == 'case'
    
    root = os.path.join(root, init.args)
    write_out(os.path.join(root, 'main.py'), init.lines)

    for chunk in chunks:
        if chunk.header_type == 'file' and chunk.args.endswith('.py'):
            write_out(os.path.join(root, chunk.args), chunk.lines)


</t>
<t tx="ekr.20220525082933.138">def help() -&gt; None:
    print("Usage: python misc/test_case_to_actual.py test_file.txt root_path")


</t>
<t tx="ekr.20220525082933.139">def main() -&gt; None:
    if len(sys.argv) != 3:
        help()
        return

    test_file_path, root_path = sys.argv[1], sys.argv[2]
    with open(test_file_path) as stream:
        chunks = produce_chunks(iter(stream))
        write_tree(root_path, chunks)


</t>
<t tx="ekr.20220525082933.140">@path C:/Repos/mypy/misc/
@language python
@tabwidth -4
#!/usr/bin/env python3

from typing import Callable, List, Tuple, Optional

import sys
import glob
import os
import shutil
import statistics
import subprocess
import textwrap
import time


@others
if __name__ == '__main__':
    main()

</t>
<t tx="ekr.20220525082933.141">def print_offset(text: str, indent_length: int = 4) -&gt; None:
    print()
    print(textwrap.indent(text, ' ' * indent_length))
    print()


</t>
<t tx="ekr.20220525082933.142">def delete_folder(folder_path: str) -&gt; None:
    if os.path.exists(folder_path):
        shutil.rmtree(folder_path)


</t>
<t tx="ekr.20220525082933.143">def execute(command: List[str]) -&gt; None:
    proc = subprocess.Popen(
        ' '.join(command),
        stderr=subprocess.PIPE,
        stdout=subprocess.PIPE,
        shell=True)
    stdout_bytes, stderr_bytes = proc.communicate()  # type: Tuple[bytes, bytes]
    stdout, stderr = stdout_bytes.decode('utf-8'), stderr_bytes.decode('utf-8')
    if proc.returncode != 0:
        print('EXECUTED COMMAND:', repr(command))
        print('RETURN CODE:', proc.returncode)
        print()
        print('STDOUT:')
        print_offset(stdout)
        print('STDERR:')
        print_offset(stderr)
        print()


</t>
<t tx="ekr.20220525082933.144">Command = Callable[[], None]


</t>
<t tx="ekr.20220525082933.145">def test(setup: Command, command: Command, teardown: Command) -&gt; float:
    setup()
    start = time.time()
    command()
    end = time.time() - start
    teardown()
    return end


</t>
<t tx="ekr.20220525082933.146">def make_touch_wrappers(filename: str) -&gt; Tuple[Command, Command]:
    def setup() -&gt; None:
        execute(["touch", filename])
    def teardown() -&gt; None:
        pass
    return setup, teardown


</t>
<t tx="ekr.20220525082933.147">def make_change_wrappers(filename: str) -&gt; Tuple[Command, Command]:
    copy = None  # type: Optional[str]

    @others
    return setup, teardown

</t>
<t tx="ekr.20220525082933.148">def setup() -&gt; None:
    nonlocal copy
    with open(filename) as stream:
        copy = stream.read()
    with open(filename, 'a') as stream:
        stream.write('\n\nfoo = 3')

</t>
<t tx="ekr.20220525082933.149">def teardown() -&gt; None:
    assert copy is not None
    with open(filename, 'w') as stream:
        stream.write(copy)

    # Re-run to reset cache
    execute(["python3", "-m", "mypy", "-i", "mypy"]),

</t>
<t tx="ekr.20220525082933.150">def main() -&gt; None:
    if len(sys.argv) != 2 or sys.argv[1] not in {'touch', 'change'}:
        print("First argument should be 'touch' or 'change'")
        return

    if sys.argv[1] == 'touch':
        make_wrappers = make_touch_wrappers
        verb = "Touching"
    elif sys.argv[1] == 'change':
        make_wrappers = make_change_wrappers
        verb = "Changing"
    else:
        raise AssertionError()

    print("Setting up...")

    baseline = test(
        lambda: None,
        lambda: execute(["python3", "-m", "mypy", "mypy"]),
        lambda: None)
    print(f"Baseline:   {baseline}")

    cold = test(
        lambda: delete_folder(".mypy_cache"),
        lambda: execute(["python3", "-m", "mypy", "-i", "mypy"]),
        lambda: None)
    print(f"Cold cache: {cold}")

    warm = test(
        lambda: None,
        lambda: execute(["python3", "-m", "mypy", "-i", "mypy"]),
        lambda: None)
    print(f"Warm cache: {warm}")

    print()

    deltas = []
    for filename in glob.iglob("mypy/**/*.py", recursive=True):
        print(f"{verb} {filename}")
        
        setup, teardown = make_wrappers(filename)
        delta = test(
            setup,
            lambda: execute(["python3", "-m", "mypy", "-i", "mypy"]),
            teardown)
        print(f"    Time: {delta}")
        deltas.append(delta)
    print()

    print("Initial:")
    print(f"    Baseline:   {baseline}")
    print(f"    Cold cache: {cold}")
    print(f"    Warm cache: {warm}")
    print()
    print("Aggregate:")
    print(f"    Times:      {deltas}")
    print(f"    Mean:       {statistics.mean(deltas)}")
    print(f"    Median:     {statistics.median(deltas)}")
    print(f"    Stdev:      {statistics.stdev(deltas)}")
    print(f"    Min:        {min(deltas)}")
    print(f"    Max:        {max(deltas)}")
    print(f"    Total:      {sum(deltas)}")
    print()

</t>
<t tx="ekr.20220525082933.151">@path C:/Repos/mypy/misc/
@language python
@tabwidth -4
#!/usr/bin/env python3
"""Upload mypy packages to PyPI.

You must first tag the release, use `git push --tags` and wait for the wheel build in CI to complete.

"""

import argparse
import contextlib
import json
import re
import shutil
import subprocess
import tarfile
import tempfile
import venv
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Any, Dict, Iterator, List
from urllib.request import urlopen

BASE = "https://api.github.com/repos"
REPO = "mypyc/mypy_mypyc-wheels"


@others
if __name__ == "__main__":
    main()
</t>
<t tx="ekr.20220525082933.152">def is_whl_or_tar(name: str) -&gt; bool:
    return name.endswith(".tar.gz") or name.endswith(".whl")


</t>
<t tx="ekr.20220525082933.153">def get_release_for_tag(tag: str) -&gt; Dict[str, Any]:
    with urlopen(f"{BASE}/{REPO}/releases/tags/{tag}") as f:
        data = json.load(f)
    assert data["tag_name"] == tag
    return data


</t>
<t tx="ekr.20220525082933.154">def download_asset(asset: Dict[str, Any], dst: Path) -&gt; Path:
    name = asset["name"]
    download_url = asset["browser_download_url"]
    assert is_whl_or_tar(name)
    with urlopen(download_url) as src_file:
        with open(dst / name, "wb") as dst_file:
            shutil.copyfileobj(src_file, dst_file)
    return dst / name


</t>
<t tx="ekr.20220525082933.155">def download_all_release_assets(release: Dict[str, Any], dst: Path) -&gt; None:
    print(f"Downloading assets...")
    with ThreadPoolExecutor() as e:
        for asset in e.map(lambda asset: download_asset(asset, dst), release["assets"]):
            print(f"Downloaded {asset}")


</t>
<t tx="ekr.20220525082933.156">def check_sdist(dist: Path, version: str) -&gt; None:
    tarfiles = list(dist.glob("*.tar.gz"))
    assert len(tarfiles) == 1
    sdist = tarfiles[0]
    assert version in sdist.name
    with tarfile.open(sdist) as f:
        version_py = f.extractfile(f"{sdist.name[:-len('.tar.gz')]}/mypy/version.py")
        assert version_py is not None
        version_py_contents = version_py.read().decode("utf-8")

        # strip a git hash from our version, if necessary, since that's not present in version.py
        match = re.match(r"(.*\+dev).*$", version)
        hashless_version = match.group(1) if match else version

        assert (
            f"'{hashless_version}'" in version_py_contents
        ), "Version does not match version.py in sdist"


</t>
<t tx="ekr.20220525082933.157">def spot_check_dist(dist: Path, version: str) -&gt; None:
    items = [item for item in dist.iterdir() if is_whl_or_tar(item.name)]
    assert len(items) &gt; 10
    assert all(version in item.name for item in items)
    assert any(item.name.endswith("py3-none-any.whl") for item in items)


</t>
<t tx="ekr.20220525082933.158">@contextlib.contextmanager
def tmp_twine() -&gt; Iterator[Path]:
    with tempfile.TemporaryDirectory() as tmp_dir:
        tmp_venv_dir = Path(tmp_dir) / "venv"
        venv.create(tmp_venv_dir, with_pip=True)
        pip_exe = tmp_venv_dir / "bin" / "pip"
        subprocess.check_call([pip_exe, "install", "twine"])
        yield tmp_venv_dir / "bin" / "twine"


</t>
<t tx="ekr.20220525082933.159">def upload_dist(dist: Path, dry_run: bool = True) -&gt; None:
    with tmp_twine() as twine:
        files = [item for item in dist.iterdir() if is_whl_or_tar(item.name)]
        cmd: List[Any] = [twine, "upload"]
        cmd += files
        if dry_run:
            print("[dry run] " + " ".join(map(str, cmd)))
        else:
            print(" ".join(map(str, cmd)))
            subprocess.check_call(cmd)


</t>
<t tx="ekr.20220525082933.160">def upload_to_pypi(version: str, dry_run: bool = True) -&gt; None:
    assert re.match(r"v?0\.[0-9]{3}(\+\S+)?$", version)
    if "dev" in version:
        assert dry_run, "Must use --dry-run with dev versions of mypy"
    if version.startswith("v"):
        version = version[1:]

    target_dir = tempfile.mkdtemp()
    dist = Path(target_dir) / "dist"
    dist.mkdir()
    print(f"Temporary target directory: {target_dir}")

    release = get_release_for_tag(f"v{version}")
    download_all_release_assets(release, dist)

    spot_check_dist(dist, version)
    check_sdist(dist, version)
    upload_dist(dist, dry_run)
    print("&lt;&lt; All done! &gt;&gt;")


</t>
<t tx="ekr.20220525082933.161">def main() -&gt; None:
    parser = argparse.ArgumentParser(description="PyPI mypy package uploader")
    parser.add_argument(
        "--dry-run", action="store_true", default=False, help="Don't actually upload packages"
    )
    parser.add_argument("version", help="mypy version to release")
    args = parser.parse_args()

    upload_to_pypi(args.version, args.dry_run)


</t>
<t tx="ekr.20220525082933.162">@path C:/Repos/mypy/misc/
@language python
@tabwidth -4
"""Example of code generation approach to variadics.

See https://github.com/python/typing/issues/193#issuecomment-236383893
"""

LIMIT = 5
BOUND = 'object'

@others
main()
</t>
<t tx="ekr.20220525082933.163">def prelude(limit: int, bound: str) -&gt; None:
    print('from typing import Callable, Iterable, Iterator, Tuple, TypeVar, overload')
    print(f"Ts = TypeVar('Ts', bound={bound})")
    print("R = TypeVar('R')")
    for i in range(LIMIT):
        print('T{i} = TypeVar(\'T{i}\', bound={bound})'.format(i=i+1, bound=bound))

</t>
<t tx="ekr.20220525082933.164">def expand_template(template: str,
                    arg_template: str = 'arg{i}: {Ts}',
                    lower: int = 0,
                    limit: int = LIMIT) -&gt; None:
    print()
    for i in range(lower, limit):
        tvs = ', '.join(f'T{j+1}' for j in range(i))
        args = ', '.join(arg_template.format(i=j+1, Ts=f'T{j+1}')
                         for j in range(i))
        print('@overload')
        s = template.format(Ts=tvs, argsTs=args)
        s = s.replace('Tuple[]', 'Tuple[()]')
        print(s)
    args_l = [arg_template.format(i=j+1, Ts='Ts') for j in range(limit)]
    args_l.append('*' + (arg_template.format(i='s', Ts='Ts')))
    args = ', '.join(args_l)
    s = template.format(Ts='Ts, ...', argsTs=args)
    s = s.replace('Callable[[Ts, ...]', 'Callable[...')
    print('@overload')
    print(s)

</t>
<t tx="ekr.20220525082933.165">def main():
    prelude(LIMIT, BOUND)

    # map()
    expand_template('def map(func: Callable[[{Ts}], R], {argsTs}) -&gt; R: ...',
                    lower=1)
    # zip()
    expand_template('def zip({argsTs}) -&gt; Tuple[{Ts}]: ...')

    # Naomi's examples
    expand_template('def my_zip({argsTs}) -&gt; Iterator[Tuple[{Ts}]]: ...',
                    'arg{i}: Iterable[{Ts}]')
    expand_template('def make_check({argsTs}) -&gt; Callable[[{Ts}], bool]: ...')
    expand_template('def my_map(f: Callable[[{Ts}], R], {argsTs}) -&gt; Iterator[R]: ...',
                    'arg{i}: Iterable[{Ts}]')


</t>
<t tx="ekr.20220525082933.166"></t>
<t tx="ekr.20220525082933.167">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""This module makes it possible to use mypy as part of a Python application.

Since mypy still changes, the API was kept utterly simple and non-intrusive.
It just mimics command line activation without starting a new interpreter.
So the normal docs about the mypy command line apply.
Changes in the command line version of mypy will be immediately usable.

Just import this module and then call the 'run' function with a parameter of
type List[str], containing what normally would have been the command line
arguments to mypy.

Function 'run' returns a Tuple[str, str, int], namely
(&lt;normal_report&gt;, &lt;error_report&gt;, &lt;exit_status&gt;),
in which &lt;normal_report&gt; is what mypy normally writes to sys.stdout,
&lt;error_report&gt; is what mypy normally writes to sys.stderr and exit_status is
the exit status mypy normally returns to the operating system.

Any pretty formatting is left to the caller.

The 'run_dmypy' function is similar, but instead mimics invocation of
dmypy. Note that run_dmypy is not thread-safe and modifies sys.stdout
and sys.stderr during its invocation.

Note that these APIs don't support incremental generation of error
messages.

Trivial example of code using this module:

import sys
from mypy import api

result = api.run(sys.argv[1:])

if result[0]:
    print('\nType checking report:\n')
    print(result[0])  # stdout

if result[1]:
    print('\nError report:\n')
    print(result[1])  # stderr

print('\nExit status:', result[2])

"""

import sys

from io import StringIO
from typing import List, Tuple, TextIO, Callable


@others
</t>
<t tx="ekr.20220525082933.168">def _run(main_wrapper: Callable[[TextIO, TextIO], None]) -&gt; Tuple[str, str, int]:

    stdout = StringIO()
    stderr = StringIO()

    try:
        main_wrapper(stdout, stderr)
        exit_status = 0
    except SystemExit as system_exit:
        exit_status = system_exit.code

    return stdout.getvalue(), stderr.getvalue(), exit_status


</t>
<t tx="ekr.20220525082933.169">def run(args: List[str]) -&gt; Tuple[str, str, int]:
    # Lazy import to avoid needing to import all of mypy to call run_dmypy
    from mypy.main import main
    return _run(lambda stdout, stderr: main(None, args=args,
                                            stdout=stdout, stderr=stderr, clean_exit=True))


</t>
<t tx="ekr.20220525082933.17"></t>
<t tx="ekr.20220525082933.170">def run_dmypy(args: List[str]) -&gt; Tuple[str, str, int]:
    from mypy.dmypy.client import main

    @others
    return _run(f)
</t>
<t tx="ekr.20220525082933.171"># A bunch of effort has been put into threading stdout and stderr
# through the main API to avoid the threadsafety problems of
# modifying sys.stdout/sys.stderr, but that hasn't been done for
# the dmypy client, so we just do the non-threadsafe thing.
def f(stdout: TextIO, stderr: TextIO) -&gt; None:
    old_stdout = sys.stdout
    old_stderr = sys.stderr
    try:
        sys.stdout = stdout
        sys.stderr = stderr
        main(args)
    finally:
        sys.stdout = old_stdout
        sys.stderr = old_stderr

</t>
<t tx="ekr.20220525082933.172">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from typing import Dict, Sequence, Optional, Callable

import mypy.subtypes
import mypy.sametypes
from mypy.expandtype import expand_type
from mypy.types import (
    Type, TypeVarId, TypeVarType, CallableType, AnyType, PartialType, get_proper_types,
    TypeVarLikeType, ProperType, ParamSpecType, Parameters, get_proper_type,
    TypeVarTupleType,
)
from mypy.nodes import Context


@others
</t>
<t tx="ekr.20220525082933.173">def get_target_type(
    tvar: TypeVarLikeType,
    type: ProperType,
    callable: CallableType,
    report_incompatible_typevar_value: Callable[[CallableType, Type, str, Context], None],
    context: Context,
    skip_unsatisfied: bool
) -&gt; Optional[Type]:
    if isinstance(tvar, ParamSpecType):
        return type
    if isinstance(tvar, TypeVarTupleType):
        return type
    assert isinstance(tvar, TypeVarType)
    values = get_proper_types(tvar.values)
    if values:
        if isinstance(type, AnyType):
            return type
        if isinstance(type, TypeVarType) and type.values:
            # Allow substituting T1 for T if every allowed value of T1
            # is also a legal value of T.
            if all(any(mypy.sametypes.is_same_type(v, v1) for v in values)
                   for v1 in type.values):
                return type
        matching = []
        for value in values:
            if mypy.subtypes.is_subtype(type, value):
                matching.append(value)
        if matching:
            best = matching[0]
            # If there are more than one matching value, we select the narrowest
            for match in matching[1:]:
                if mypy.subtypes.is_subtype(match, best):
                    best = match
            return best
        if skip_unsatisfied:
            return None
        report_incompatible_typevar_value(callable, type, tvar.name, context)
    else:
        upper_bound = tvar.upper_bound
        if not mypy.subtypes.is_subtype(type, upper_bound):
            if skip_unsatisfied:
                return None
            report_incompatible_typevar_value(callable, type, tvar.name, context)
    return type


</t>
<t tx="ekr.20220525082933.174">def apply_generic_arguments(
        callable: CallableType, orig_types: Sequence[Optional[Type]],
        report_incompatible_typevar_value: Callable[[CallableType, Type, str, Context], None],
        context: Context,
        skip_unsatisfied: bool = False) -&gt; CallableType:
    """Apply generic type arguments to a callable type.

    For example, applying [int] to 'def [T] (T) -&gt; T' results in
    'def (int) -&gt; int'.

    Note that each type can be None; in this case, it will not be applied.

    If `skip_unsatisfied` is True, then just skip the types that don't satisfy type variable
    bound or constraints, instead of giving an error.
    """
    tvars = callable.variables
    assert len(tvars) == len(orig_types)
    # Check that inferred type variable values are compatible with allowed
    # values and bounds.  Also, promote subtype values to allowed values.
    types = get_proper_types(orig_types)

    # Create a map from type variable id to target type.
    id_to_type: Dict[TypeVarId, Type] = {}

    for tvar, type in zip(tvars, types):
        assert not isinstance(type, PartialType), "Internal error: must never apply partial type"
        if type is None:
            continue

        target_type = get_target_type(
            tvar, type, callable, report_incompatible_typevar_value, context, skip_unsatisfied
        )
        if target_type is not None:
            id_to_type[tvar.id] = target_type

    param_spec = callable.param_spec()
    if param_spec is not None:
        nt = id_to_type.get(param_spec.id)
        if nt is not None:
            nt = get_proper_type(nt)
            if isinstance(nt, CallableType) or isinstance(nt, Parameters):
                callable = callable.expand_param_spec(nt)

    # Apply arguments to argument types.
    arg_types = [expand_type(at, id_to_type) for at in callable.arg_types]

    # Apply arguments to TypeGuard if any.
    if callable.type_guard is not None:
        type_guard = expand_type(callable.type_guard, id_to_type)
    else:
        type_guard = None

    # The callable may retain some type vars if only some were applied.
    remaining_tvars = [tv for tv in tvars if tv.id not in id_to_type]

    return callable.copy_modified(
        arg_types=arg_types,
        ret_type=expand_type(callable.ret_type, id_to_type),
        variables=remaining_tvars,
        type_guard=type_guard,
    )
</t>
<t tx="ekr.20220525082933.175">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Utilities for mapping between actual and formal arguments (and their types)."""

from typing import TYPE_CHECKING, List, Optional, Sequence, Callable, Set

from mypy.maptype import map_instance_to_supertype
from mypy.types import (
    Type, Instance, TupleType, AnyType, TypeOfAny, TypedDictType, ParamSpecType, get_proper_type
)
from mypy import nodes

if TYPE_CHECKING:
    from mypy.infer import ArgumentInferContext


@others
</t>
<t tx="ekr.20220525082933.176">def map_actuals_to_formals(actual_kinds: List[nodes.ArgKind],
                           actual_names: Optional[Sequence[Optional[str]]],
                           formal_kinds: List[nodes.ArgKind],
                           formal_names: Sequence[Optional[str]],
                           actual_arg_type: Callable[[int],
                                                     Type]) -&gt; List[List[int]]:
    """Calculate mapping between actual (caller) args and formals.

    The result contains a list of caller argument indexes mapping to each
    callee argument index, indexed by callee index.

    The caller_arg_type argument should evaluate to the type of the actual
    argument type with the given index.
    """
    nformals = len(formal_kinds)
    formal_to_actual: List[List[int]] = [[] for i in range(nformals)]
    ambiguous_actual_kwargs: List[int] = []
    fi = 0
    for ai, actual_kind in enumerate(actual_kinds):
        if actual_kind == nodes.ARG_POS:
            if fi &lt; nformals:
                if not formal_kinds[fi].is_star():
                    formal_to_actual[fi].append(ai)
                    fi += 1
                elif formal_kinds[fi] == nodes.ARG_STAR:
                    formal_to_actual[fi].append(ai)
        elif actual_kind == nodes.ARG_STAR:
            # We need to know the actual type to map varargs.
            actualt = get_proper_type(actual_arg_type(ai))
            if isinstance(actualt, TupleType):
                # A tuple actual maps to a fixed number of formals.
                for _ in range(len(actualt.items)):
                    if fi &lt; nformals:
                        if formal_kinds[fi] != nodes.ARG_STAR2:
                            formal_to_actual[fi].append(ai)
                        else:
                            break
                        if formal_kinds[fi] != nodes.ARG_STAR:
                            fi += 1
            else:
                # Assume that it is an iterable (if it isn't, there will be
                # an error later).
                while fi &lt; nformals:
                    if formal_kinds[fi].is_named(star=True):
                        break
                    else:
                        formal_to_actual[fi].append(ai)
                    if formal_kinds[fi] == nodes.ARG_STAR:
                        break
                    fi += 1
        elif actual_kind.is_named():
            assert actual_names is not None, "Internal error: named kinds without names given"
            name = actual_names[ai]
            if name in formal_names:
                formal_to_actual[formal_names.index(name)].append(ai)
            elif nodes.ARG_STAR2 in formal_kinds:
                formal_to_actual[formal_kinds.index(nodes.ARG_STAR2)].append(ai)
        else:
            assert actual_kind == nodes.ARG_STAR2
            actualt = get_proper_type(actual_arg_type(ai))
            if isinstance(actualt, TypedDictType):
                for name in actualt.items:
                    if name in formal_names:
                        formal_to_actual[formal_names.index(name)].append(ai)
                    elif nodes.ARG_STAR2 in formal_kinds:
                        formal_to_actual[formal_kinds.index(nodes.ARG_STAR2)].append(ai)
            else:
                # We don't exactly know which **kwargs are provided by the
                # caller, so we'll defer until all the other unambiguous
                # actuals have been processed
                ambiguous_actual_kwargs.append(ai)

    if ambiguous_actual_kwargs:
        # Assume the ambiguous kwargs will fill the remaining arguments.
        #
        # TODO: If there are also tuple varargs, we might be missing some potential
        #       matches if the tuple was short enough to not match everything.
        unmatched_formals = [fi for fi in range(nformals)
                             if (formal_names[fi]
                                 and (not formal_to_actual[fi]
                                      or actual_kinds[formal_to_actual[fi][0]] == nodes.ARG_STAR)
                                 and formal_kinds[fi] != nodes.ARG_STAR)
                             or formal_kinds[fi] == nodes.ARG_STAR2]
        for ai in ambiguous_actual_kwargs:
            for fi in unmatched_formals:
                formal_to_actual[fi].append(ai)

    return formal_to_actual


</t>
<t tx="ekr.20220525082933.177">def map_formals_to_actuals(actual_kinds: List[nodes.ArgKind],
                           actual_names: Optional[Sequence[Optional[str]]],
                           formal_kinds: List[nodes.ArgKind],
                           formal_names: List[Optional[str]],
                           actual_arg_type: Callable[[int],
                                                     Type]) -&gt; List[List[int]]:
    """Calculate the reverse mapping of map_actuals_to_formals."""
    formal_to_actual = map_actuals_to_formals(actual_kinds,
                                              actual_names,
                                              formal_kinds,
                                              formal_names,
                                              actual_arg_type)
    # Now reverse the mapping.
    actual_to_formal: List[List[int]] = [[] for _ in actual_kinds]
    for formal, actuals in enumerate(formal_to_actual):
        for actual in actuals:
            actual_to_formal[actual].append(formal)
    return actual_to_formal


</t>
<t tx="ekr.20220525082933.178">class ArgTypeExpander:
    """Utility class for mapping actual argument types to formal arguments.

    One of the main responsibilities is to expand caller tuple *args and TypedDict
    **kwargs, and to keep track of which tuple/TypedDict items have already been
    consumed.

    Example:

       def f(x: int, *args: str) -&gt; None: ...
       f(*(1, 'x', 1.1))

    We'd call expand_actual_type three times:

      1. The first call would provide 'int' as the actual type of 'x' (from '1').
      2. The second call would provide 'str' as one of the actual types for '*args'.
      2. The third call would provide 'float' as one of the actual types for '*args'.

    A single instance can process all the arguments for a single call. Each call
    needs a separate instance since instances have per-call state.
    """

    @others
</t>
<t tx="ekr.20220525082933.179">def __init__(self, context: 'ArgumentInferContext') -&gt; None:
    # Next tuple *args index to use.
    self.tuple_index = 0
    # Keyword arguments in TypedDict **kwargs used.
    self.kwargs_used: Set[str] = set()
    # Type context for `*` and `**` arg kinds.
    self.context = context

</t>
<t tx="ekr.20220525082933.18">@path C:/Repos/mypy/misc/
@language python
@tabwidth -4
#!/usr/bin/env python3
import os
import shutil
from typing import Tuple, Any
try:
    import click
except ImportError:
    print("You need the module \'click\'")
    exit(1)

base_path = os.getcwd()

@others
if __name__ == '__main__':
    main()
</t>
<t tx="ekr.20220525082933.180">def expand_actual_type(self,
                       actual_type: Type,
                       actual_kind: nodes.ArgKind,
                       formal_name: Optional[str],
                       formal_kind: nodes.ArgKind) -&gt; Type:
    """Return the actual (caller) type(s) of a formal argument with the given kinds.

    If the actual argument is a tuple *args, return the next individual tuple item that
    maps to the formal arg.

    If the actual argument is a TypedDict **kwargs, return the next matching typed dict
    value type based on formal argument name and kind.

    This is supposed to be called for each formal, in order. Call multiple times per
    formal if multiple actuals map to a formal.
    """
    actual_type = get_proper_type(actual_type)
    if actual_kind == nodes.ARG_STAR:
        if isinstance(actual_type, Instance) and actual_type.args:
            from mypy.subtypes import is_subtype
            if is_subtype(actual_type, self.context.iterable_type):
                return map_instance_to_supertype(
                    actual_type,
                    self.context.iterable_type.type,
                ).args[0]
            else:
                # We cannot properly unpack anything other
                # than `Iterable` type with `*`.
                # Just return `Any`, other parts of code would raise
                # a different error for improper use.
                return AnyType(TypeOfAny.from_error)
        elif isinstance(actual_type, TupleType):
            # Get the next tuple item of a tuple *arg.
            if self.tuple_index &gt;= len(actual_type.items):
                # Exhausted a tuple -- continue to the next *args.
                self.tuple_index = 1
            else:
                self.tuple_index += 1
            return actual_type.items[self.tuple_index - 1]
        elif isinstance(actual_type, ParamSpecType):
            # ParamSpec is valid in *args but it can't be unpacked.
            return actual_type
        else:
            return AnyType(TypeOfAny.from_error)
    elif actual_kind == nodes.ARG_STAR2:
        from mypy.subtypes import is_subtype
        if isinstance(actual_type, TypedDictType):
            if formal_kind != nodes.ARG_STAR2 and formal_name in actual_type.items:
                # Lookup type based on keyword argument name.
                assert formal_name is not None
            else:
                # Pick an arbitrary item if no specified keyword is expected.
                formal_name = (set(actual_type.items.keys()) - self.kwargs_used).pop()
            self.kwargs_used.add(formal_name)
            return actual_type.items[formal_name]
        elif (
            isinstance(actual_type, Instance) and
            len(actual_type.args) &gt; 1 and
            is_subtype(actual_type, self.context.mapping_type)
        ):
            # Only `Mapping` type can be unpacked with `**`.
            # Other types will produce an error somewhere else.
            return map_instance_to_supertype(
                actual_type,
                self.context.mapping_type.type,
            ).args[1]
        elif isinstance(actual_type, ParamSpecType):
            # ParamSpec is valid in **kwargs but it can't be unpacked.
            return actual_type
        else:
            return AnyType(TypeOfAny.from_error)
    else:
        # No translation for other kinds -- 1:1 mapping.
        return actual_type
</t>
<t tx="ekr.20220525082933.181">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
import sys
from contextlib import contextmanager
from typing import Iterator

if sys.version_info &lt; (3, 6):
    from collections import OrderedDict as OrderedDict  # noqa: F401
else:
    # OrderedDict is kind of slow, so for most of our uses in Python 3.6
    # and later we'd rather just use dict
    OrderedDict = dict


if sys.version_info &lt; (3, 7):
    @contextmanager
    def nullcontext() -&gt; Iterator[None]:
        yield
else:
    from contextlib import nullcontext as nullcontext  # noqa: F401
</t>
<t tx="ekr.20220525082933.182">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from contextlib import contextmanager
from collections import defaultdict

from typing import Dict, List, Set, Iterator, Union, Optional, Tuple, cast
from typing_extensions import DefaultDict, TypeAlias as _TypeAlias

from mypy.types import (
    Type, AnyType, PartialType, UnionType, TypeOfAny, NoneType, get_proper_type
)
from mypy.subtypes import is_subtype
from mypy.join import join_simple
from mypy.sametypes import is_same_type
from mypy.erasetype import remove_instance_last_known_values
from mypy.nodes import Expression, Var, RefExpr
from mypy.literals import Key, literal, literal_hash, subkeys
from mypy.nodes import IndexExpr, MemberExpr, AssignmentExpr, NameExpr


BindableExpression: _TypeAlias = Union[IndexExpr, MemberExpr, AssignmentExpr, NameExpr]


@others
</t>
<t tx="ekr.20220525082933.183">class Frame:
    """A Frame represents a specific point in the execution of a program.
    It carries information about the current types of expressions at
    that point, arising either from assignments to those expressions
    or the result of isinstance checks. It also records whether it is
    possible to reach that point at all.

    This information is not copied into a new Frame when it is pushed
    onto the stack, so a given Frame only has information about types
    that were assigned in that frame.
    """

    @others
</t>
<t tx="ekr.20220525082933.184">def __init__(self, id: int, conditional_frame: bool = False) -&gt; None:
    self.id = id
    self.types: Dict[Key, Type] = {}
    self.unreachable = False
    self.conditional_frame = conditional_frame

    # Should be set only if we're entering a frame where it's not
    # possible to accurately determine whether or not contained
    # statements will be unreachable or not.
    #
    # Long-term, we should improve mypy to the point where we no longer
    # need this field.
    self.suppress_unreachable_warnings = False


</t>
<t tx="ekr.20220525082933.185">Assigns = DefaultDict[Expression, List[Tuple[Type, Optional[Type]]]]


</t>
<t tx="ekr.20220525082933.186">class ConditionalTypeBinder:
    """Keep track of conditional types of variables.

    NB: Variables are tracked by literal expression, so it is possible
    to confuse the binder; for example,

    ```
    class A:
        a = None          # type: Union[int, str]
    x = A()
    lst = [x]
    reveal_type(x.a)      # Union[int, str]
    x.a = 1
    reveal_type(x.a)      # int
    reveal_type(lst[0].a) # Union[int, str]
    lst[0].a = 'a'
    reveal_type(x.a)      # int
    reveal_type(lst[0].a) # str
    ```
    """
    # Stored assignments for situations with tuple/list lvalue and rvalue of union type.
    # This maps an expression to a list of bound types for every item in the union type.
    type_assignments: Optional[Assigns] = None

    @others
</t>
<t tx="ekr.20220525082933.187">def __init__(self) -&gt; None:
    self.next_id = 1

    # The stack of frames currently used.  These map
    # literal_hash(expr) -- literals like 'foo.bar' --
    # to types. The last element of this list is the
    # top-most, current frame. Each earlier element
    # records the state as of when that frame was last
    # on top of the stack.
    self.frames = [Frame(self._get_id())]

    # For frames higher in the stack, we record the set of
    # Frames that can escape there, either by falling off
    # the end of the frame or by a loop control construct
    # or raised exception. The last element of self.frames
    # has no corresponding element in this list.
    self.options_on_return: List[List[Frame]] = []

    # Maps literal_hash(expr) to get_declaration(expr)
    # for every expr stored in the binder
    self.declarations: Dict[Key, Optional[Type]] = {}
    # Set of other keys to invalidate if a key is changed, e.g. x -&gt; {x.a, x[0]}
    # Whenever a new key (e.g. x.a.b) is added, we update this
    self.dependencies: Dict[Key, Set[Key]] = {}

    # Whether the last pop changed the newly top frame on exit
    self.last_pop_changed = False

    self.try_frames: Set[int] = set()
    self.break_frames: List[int] = []
    self.continue_frames: List[int] = []

</t>
<t tx="ekr.20220525082933.188">def _get_id(self) -&gt; int:
    self.next_id += 1
    return self.next_id

</t>
<t tx="ekr.20220525082933.189">def _add_dependencies(self, key: Key, value: Optional[Key] = None) -&gt; None:
    if value is None:
        value = key
    else:
        self.dependencies.setdefault(key, set()).add(value)
    for elt in subkeys(key):
        self._add_dependencies(elt, value)

</t>
<t tx="ekr.20220525082933.19"># I don't know how to set callables with different args
def apply_all(func: Any, directory: str, extension: str,
            to_extension: str='', exclude: Tuple[str]=('',),
            recursive: bool=True, debug: bool=False) -&gt; None:
    excluded = [x+extension for x in exclude] if exclude else []
    for p, d, files in os.walk(os.path.join(base_path, directory)):
        for f in files:
            if f in excluded:
                continue
            inner_path = os.path.join(p, f)
            if not inner_path.endswith(extension):
                continue
            if to_extension:
                new_path = f"{inner_path[:-len(extension)]}{to_extension}"
                func(inner_path,new_path)
            else:
                func(inner_path)
        if not recursive:
            break

</t>
<t tx="ekr.20220525082933.190">def push_frame(self, conditional_frame: bool = False) -&gt; Frame:
    """Push a new frame into the binder."""
    f = Frame(self._get_id(), conditional_frame)
    self.frames.append(f)
    self.options_on_return.append([])
    return f

</t>
<t tx="ekr.20220525082933.191">def _put(self, key: Key, type: Type, index: int = -1) -&gt; None:
    self.frames[index].types[key] = type

</t>
<t tx="ekr.20220525082933.192">def _get(self, key: Key, index: int = -1) -&gt; Optional[Type]:
    if index &lt; 0:
        index += len(self.frames)
    for i in range(index, -1, -1):
        if key in self.frames[i].types:
            return self.frames[i].types[key]
    return None

</t>
<t tx="ekr.20220525082933.193">def put(self, expr: Expression, typ: Type) -&gt; None:
    if not isinstance(expr, (IndexExpr, MemberExpr, AssignmentExpr, NameExpr)):
        return
    if not literal(expr):
        return
    key = literal_hash(expr)
    assert key is not None, 'Internal error: binder tried to put non-literal'
    if key not in self.declarations:
        self.declarations[key] = get_declaration(expr)
        self._add_dependencies(key)
    self._put(key, typ)

</t>
<t tx="ekr.20220525082933.194">def unreachable(self) -&gt; None:
    self.frames[-1].unreachable = True

</t>
<t tx="ekr.20220525082933.195">def suppress_unreachable_warnings(self) -&gt; None:
    self.frames[-1].suppress_unreachable_warnings = True

</t>
<t tx="ekr.20220525082933.196">def get(self, expr: Expression) -&gt; Optional[Type]:
    key = literal_hash(expr)
    assert key is not None, 'Internal error: binder tried to get non-literal'
    return self._get(key)

</t>
<t tx="ekr.20220525082933.197">def is_unreachable(self) -&gt; bool:
    # TODO: Copy the value of unreachable into new frames to avoid
    # this traversal on every statement?
    return any(f.unreachable for f in self.frames)

</t>
<t tx="ekr.20220525082933.198">def is_unreachable_warning_suppressed(self) -&gt; bool:
    # TODO: See todo in 'is_unreachable'
    return any(f.suppress_unreachable_warnings for f in self.frames)

</t>
<t tx="ekr.20220525082933.199">def cleanse(self, expr: Expression) -&gt; None:
    """Remove all references to a Node from the binder."""
    key = literal_hash(expr)
    assert key is not None, 'Internal error: binder tried cleanse non-literal'
    self._cleanse_key(key)

</t>
<t tx="ekr.20220525082933.2"># This function name is special to pytest.  See
# http://doc.pytest.org/en/latest/writing_plugins.html#initialization-command-line-and-configuration-hooks
def pytest_addoption(parser) -&gt; None:
    parser.addoption('--bench', action='store_true', default=False,
                     help='Enable the benchmark test runs')
</t>
<t tx="ekr.20220525082933.20">def confirm(resp: bool=False, **kargs) -&gt; bool:
    kargs['rest'] = "to this {f2}/*{e2}".format(**kargs) if kargs.get('f2') else ''
    prompt = "{act} all files {rec}matching this expression {f1}/*{e1} {rest}".format(**kargs)
    prompt.format(**kargs)
    prompt = "{} [{}]|{}: ".format(prompt, 'Y' if resp else 'N', 'n' if resp else 'y')
    while True:
        ans = input(prompt).lower()
        if not ans:
            return resp
        if ans not in ['y','n']:
            print( 'Please, enter (y) or (n).')
            continue
        if ans == 'y':
            return True
        else:
            return False

</t>
<t tx="ekr.20220525082933.200">def _cleanse_key(self, key: Key) -&gt; None:
    """Remove all references to a key from the binder."""
    for frame in self.frames:
        if key in frame.types:
            del frame.types[key]

</t>
<t tx="ekr.20220525082933.201">def update_from_options(self, frames: List[Frame]) -&gt; bool:
    """Update the frame to reflect that each key will be updated
    as in one of the frames.  Return whether any item changes.

    If a key is declared as AnyType, only update it if all the
    options are the same.
    """

    frames = [f for f in frames if not f.unreachable]
    changed = False
    keys = {key for f in frames for key in f.types}

    for key in keys:
        current_value = self._get(key)
        resulting_values = [f.types.get(key, current_value) for f in frames]
        if any(x is None for x in resulting_values):
            # We didn't know anything about key before
            # (current_value must be None), and we still don't
            # know anything about key in at least one possible frame.
            continue

        type = resulting_values[0]
        assert type is not None
        declaration_type = get_proper_type(self.declarations.get(key))
        if isinstance(declaration_type, AnyType):
            # At this point resulting values can't contain None, see continue above
            if not all(is_same_type(type, cast(Type, t)) for t in resulting_values[1:]):
                type = AnyType(TypeOfAny.from_another_any, source_any=declaration_type)
        else:
            for other in resulting_values[1:]:
                assert other is not None
                type = join_simple(self.declarations[key], type, other)
        if current_value is None or not is_same_type(type, current_value):
            self._put(key, type)
            changed = True

    self.frames[-1].unreachable = not frames

    return changed

</t>
<t tx="ekr.20220525082933.202">def pop_frame(self, can_skip: bool, fall_through: int) -&gt; Frame:
    """Pop a frame and return it.

    See frame_context() for documentation of fall_through.
    """

    if fall_through &gt; 0:
        self.allow_jump(-fall_through)

    result = self.frames.pop()
    options = self.options_on_return.pop()

    if can_skip:
        options.insert(0, self.frames[-1])

    self.last_pop_changed = self.update_from_options(options)

    return result

</t>
<t tx="ekr.20220525082933.203">@contextmanager
def accumulate_type_assignments(self) -&gt; 'Iterator[Assigns]':
    """Push a new map to collect assigned types in multiassign from union.

    If this map is not None, actual binding is deferred until all items in
    the union are processed (a union of collected items is later bound
    manually by the caller).
    """
    old_assignments = None
    if self.type_assignments is not None:
        old_assignments = self.type_assignments
    self.type_assignments = defaultdict(list)
    yield self.type_assignments
    self.type_assignments = old_assignments

</t>
<t tx="ekr.20220525082933.204">def assign_type(self, expr: Expression,
                type: Type,
                declared_type: Optional[Type],
                restrict_any: bool = False) -&gt; None:
    # We should erase last known value in binder, because if we are using it,
    # it means that the target is not final, and therefore can't hold a literal.
    type = remove_instance_last_known_values(type)

    type = get_proper_type(type)
    declared_type = get_proper_type(declared_type)

    if self.type_assignments is not None:
        # We are in a multiassign from union, defer the actual binding,
        # just collect the types.
        self.type_assignments[expr].append((type, declared_type))
        return
    if not isinstance(expr, (IndexExpr, MemberExpr, NameExpr)):
        return None
    if not literal(expr):
        return
    self.invalidate_dependencies(expr)

    if declared_type is None:
        # Not sure why this happens.  It seems to mainly happen in
        # member initialization.
        return
    if not is_subtype(type, declared_type):
        # Pretty sure this is only happens when there's a type error.

        # Ideally this function wouldn't be called if the
        # expression has a type error, though -- do other kinds of
        # errors cause this function to get called at invalid
        # times?
        return

    enclosing_type = get_proper_type(self.most_recent_enclosing_type(expr, type))
    if isinstance(enclosing_type, AnyType) and not restrict_any:
        # If x is Any and y is int, after x = y we do not infer that x is int.
        # This could be changed.
        # Instead, since we narrowed type from Any in a recent frame (probably an
        # isinstance check), but now it is reassigned, we broaden back
        # to Any (which is the most recent enclosing type)
        self.put(expr, enclosing_type)
    # As a special case, when assigning Any to a variable with a
    # declared Optional type that has been narrowed to None,
    # replace all the Nones in the declared Union type with Any.
    # This overrides the normal behavior of ignoring Any assignments to variables
    # in order to prevent false positives.
    # (See discussion in #3526)
    elif (isinstance(type, AnyType)
          and isinstance(declared_type, UnionType)
          and any(isinstance(get_proper_type(item), NoneType) for item in declared_type.items)
          and isinstance(get_proper_type(self.most_recent_enclosing_type(expr, NoneType())),
                         NoneType)):
        # Replace any Nones in the union type with Any
        new_items = [type if isinstance(get_proper_type(item), NoneType) else item
                     for item in declared_type.items]
        self.put(expr, UnionType(new_items))
    elif (isinstance(type, AnyType)
          and not (isinstance(declared_type, UnionType)
                   and any(isinstance(get_proper_type(item), AnyType)
                           for item in declared_type.items))):
        # Assigning an Any value doesn't affect the type to avoid false negatives, unless
        # there is an Any item in a declared union type.
        self.put(expr, declared_type)
    else:
        self.put(expr, type)

    for i in self.try_frames:
        # XXX This should probably not copy the entire frame, but
        # just copy this variable into a single stored frame.
        self.allow_jump(i)

</t>
<t tx="ekr.20220525082933.205">def invalidate_dependencies(self, expr: BindableExpression) -&gt; None:
    """Invalidate knowledge of types that include expr, but not expr itself.

    For example, when expr is foo.bar, invalidate foo.bar.baz.

    It is overly conservative: it invalidates globally, including
    in code paths unreachable from here.
    """
    key = literal_hash(expr)
    assert key is not None
    for dep in self.dependencies.get(key, set()):
        self._cleanse_key(dep)

</t>
<t tx="ekr.20220525082933.206">def most_recent_enclosing_type(self, expr: BindableExpression, type: Type) -&gt; Optional[Type]:
    type = get_proper_type(type)
    if isinstance(type, AnyType):
        return get_declaration(expr)
    key = literal_hash(expr)
    assert key is not None
    enclosers = ([get_declaration(expr)] +
                 [f.types[key] for f in self.frames
                  if key in f.types and is_subtype(type, f.types[key])])
    return enclosers[-1]

</t>
<t tx="ekr.20220525082933.207">def allow_jump(self, index: int) -&gt; None:
    # self.frames and self.options_on_return have different lengths
    # so make sure the index is positive
    if index &lt; 0:
        index += len(self.options_on_return)
    frame = Frame(self._get_id())
    for f in self.frames[index + 1:]:
        frame.types.update(f.types)
        if f.unreachable:
            frame.unreachable = True
    self.options_on_return[index].append(frame)

</t>
<t tx="ekr.20220525082933.208">def handle_break(self) -&gt; None:
    self.allow_jump(self.break_frames[-1])
    self.unreachable()

</t>
<t tx="ekr.20220525082933.209">def handle_continue(self) -&gt; None:
    self.allow_jump(self.continue_frames[-1])
    self.unreachable()

</t>
<t tx="ekr.20220525082933.21">actions = ['cp', 'mv', 'rm']
</t>
<t tx="ekr.20220525082933.210">@contextmanager
def frame_context(self, *, can_skip: bool, fall_through: int = 1,
                  break_frame: int = 0, continue_frame: int = 0,
                  conditional_frame: bool = False,
                  try_frame: bool = False) -&gt; Iterator[Frame]:
    """Return a context manager that pushes/pops frames on enter/exit.

    If can_skip is True, control flow is allowed to bypass the
    newly-created frame.

    If fall_through &gt; 0, then it will allow control flow that
    falls off the end of the frame to escape to its ancestor
    `fall_through` levels higher. Otherwise control flow ends
    at the end of the frame.

    If break_frame &gt; 0, then 'break' statements within this frame
    will jump out to the frame break_frame levels higher than the
    frame created by this call to frame_context. Similarly for
    continue_frame and 'continue' statements.

    If try_frame is true, then execution is allowed to jump at any
    point within the newly created frame (or its descendants) to
    its parent (i.e., to the frame that was on top before this
    call to frame_context).

    After the context manager exits, self.last_pop_changed indicates
    whether any types changed in the newly-topmost frame as a result
    of popping this frame.
    """
    assert len(self.frames) &gt; 1

    if break_frame:
        self.break_frames.append(len(self.frames) - break_frame)
    if continue_frame:
        self.continue_frames.append(len(self.frames) - continue_frame)
    if try_frame:
        self.try_frames.add(len(self.frames) - 1)

    new_frame = self.push_frame(conditional_frame)
    if try_frame:
        # An exception may occur immediately
        self.allow_jump(-1)
    yield new_frame
    self.pop_frame(can_skip, fall_through)

    if break_frame:
        self.break_frames.pop()
    if continue_frame:
        self.continue_frames.pop()
    if try_frame:
        self.try_frames.remove(len(self.frames) - 1)

</t>
<t tx="ekr.20220525082933.211">@contextmanager
def top_frame_context(self) -&gt; Iterator[Frame]:
    """A variant of frame_context for use at the top level of
    a namespace (module, function, or class).
    """
    assert len(self.frames) == 1
    yield self.push_frame()
    self.pop_frame(True, 0)


</t>
<t tx="ekr.20220525082933.212">def get_declaration(expr: BindableExpression) -&gt; Optional[Type]:
    if isinstance(expr, RefExpr) and isinstance(expr.node, Var):
        type = get_proper_type(expr.node.type)
        if not isinstance(type, PartialType):
            return type
    return None
</t>
<t tx="ekr.20220525082933.213">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""A Bogus[T] type alias for marking when we subvert the type system

We need this for compiling with mypyc, which inserts runtime
typechecks that cause problems when we subvert the type system. So
when compiling with mypyc, we turn those places into Any, while
keeping the types around for normal typechecks.

Since this causes the runtime types to be Any, this is best used
in places where efficient access to properties is not important.
For those cases some other technique should be used.
"""

from mypy_extensions import FlexibleAlias
from typing import TypeVar, Any

T = TypeVar('T')

# This won't ever be true at runtime, but we consider it true during
# mypyc compilations.
MYPYC = False
if MYPYC:
    Bogus = FlexibleAlias[T, Any]
else:
    Bogus = FlexibleAlias[T, T]
</t>
<t tx="ekr.20220525082933.214">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Facilities to analyze entire programs, including imported modules.

Parse and analyze the source files of a program in the correct order
(based on file dependencies), and collect the results.

This module only directs a build, which is performed in multiple passes per
file.  The individual passes are implemented in separate modules.

The function build() is the main interface to this module.
"""
# TODO: More consistent terminology, e.g. path/fnam, module/id, state/file

import contextlib
import errno
import gc
import json
import os
import platform
import re
import stat
import sys
import time
import types

from typing import (AbstractSet, Any, Dict, Iterable, Iterator, List, Sequence,
                    Mapping, NamedTuple, Optional, Set, Tuple, TypeVar, Union, Callable, TextIO)
from typing_extensions import ClassVar, NoReturn, Final, TYPE_CHECKING, TypeAlias as _TypeAlias
from mypy_extensions import TypedDict

from mypy.nodes import MypyFile, ImportBase, Import, ImportFrom, ImportAll, SymbolTable
from mypy.semanal_pass1 import SemanticAnalyzerPreAnalysis
from mypy.semanal import SemanticAnalyzer
import mypy.semanal_main
from mypy.checker import TypeChecker
from mypy.indirection import TypeIndirectionVisitor
from mypy.errors import Errors, CompileError, ErrorInfo, report_internal_error
from mypy.util import (
    DecodeError, decode_python_encoding, is_sub_path, get_mypy_comments, module_prefix,
    read_py_file, hash_digest, is_typeshed_file, is_stub_package_file, get_top_two_prefixes,
    time_ref, time_spent_us
)
if TYPE_CHECKING:
    from mypy.report import Reports  # Avoid unconditional slow import
from mypy.fixup import fixup_module
from mypy.modulefinder import (
    BuildSource, BuildSourceSet, compute_search_paths, FindModuleCache, SearchPaths,
    ModuleSearchResult, ModuleNotFoundReason
)
from mypy.nodes import Expression
from mypy.options import Options
from mypy.parse import parse
from mypy.stats import dump_type_stats
from mypy.types import Type
from mypy.version import __version__
from mypy.plugin import Plugin, ChainedPlugin, ReportConfigContext
from mypy.plugins.default import DefaultPlugin
from mypy.fscache import FileSystemCache
from mypy.metastore import MetadataStore, FilesystemMetadataStore, SqliteMetadataStore
from mypy.typestate import TypeState, reset_global_state
from mypy.renaming import VariableRenameVisitor, LimitedVariableRenameVisitor
from mypy.config_parser import parse_mypy_comments
from mypy.freetree import free_tree
from mypy.stubinfo import legacy_bundled_packages, is_legacy_bundled_package
from mypy import errorcodes as codes


# Switch to True to produce debug output related to fine-grained incremental
# mode only that is useful during development. This produces only a subset of
# output compared to --verbose output. We use a global flag to enable this so
# that it's easy to enable this when running tests.
DEBUG_FINE_GRAINED: Final = False

# These modules are special and should always come from typeshed.
CORE_BUILTIN_MODULES: Final = {
    'builtins',
    'typing',
    'types',
    'typing_extensions',
    'mypy_extensions',
    '_importlib_modulespec',
    'sys',
    'abc',
}


Graph: _TypeAlias = Dict[str, 'State']


@others
</t>
<t tx="ekr.20220525082933.215"># TODO: Get rid of BuildResult.  We might as well return a BuildManager.
class BuildResult:
    """The result of a successful build.

    Attributes:
      manager: The build manager.
      files:   Dictionary from module name to related AST node.
      types:   Dictionary from parse tree node to its inferred type.
      used_cache: Whether the build took advantage of a pre-existing cache
      errors:  List of error messages.
    """

    @others
</t>
<t tx="ekr.20220525082933.216">def __init__(self, manager: 'BuildManager', graph: Graph) -&gt; None:
    self.manager = manager
    self.graph = graph
    self.files = manager.modules
    self.types = manager.all_types  # Non-empty if export_types True in options
    self.used_cache = manager.cache_enabled
    self.errors: List[str] = []  # Filled in by build if desired


</t>
<t tx="ekr.20220525082933.217">def build(sources: List[BuildSource],
          options: Options,
          alt_lib_path: Optional[str] = None,
          flush_errors: Optional[Callable[[List[str], bool], None]] = None,
          fscache: Optional[FileSystemCache] = None,
          stdout: Optional[TextIO] = None,
          stderr: Optional[TextIO] = None,
          extra_plugins: Optional[Sequence[Plugin]] = None,
          ) -&gt; BuildResult:
    """Analyze a program.

    A single call to build performs parsing, semantic analysis and optionally
    type checking for the program *and* all imported modules, recursively.

    Return BuildResult if successful or only non-blocking errors were found;
    otherwise raise CompileError.

    If a flush_errors callback is provided, all error messages will be
    passed to it and the errors and messages fields of BuildResult and
    CompileError (respectively) will be empty. Otherwise those fields will
    report any error messages.

    Args:
      sources: list of sources to build
      options: build options
      alt_lib_path: an additional directory for looking up library modules
        (takes precedence over other directories)
      flush_errors: optional function to flush errors after a file is processed
      fscache: optionally a file-system cacher

    """
    # If we were not given a flush_errors, we use one that will populate those
    # fields for callers that want the traditional API.
    messages = []

    @others
    flush_errors = flush_errors or default_flush_errors
    stdout = stdout or sys.stdout
    stderr = stderr or sys.stderr
    extra_plugins = extra_plugins or []

    try:
        result = _build(
            sources, options, alt_lib_path, flush_errors, fscache, stdout, stderr, extra_plugins
        )
        result.errors = messages
        return result
    except CompileError as e:
        # CompileErrors raised from an errors object carry all of the
        # messages that have not been reported out by error streaming.
        # Patch it up to contain either none or all none of the messages,
        # depending on whether we are flushing errors.
        serious = not e.use_stdout
        flush_errors(e.messages, serious)
        e.messages = messages
        raise


</t>
<t tx="ekr.20220525082933.218">def default_flush_errors(new_messages: List[str], is_serious: bool) -&gt; None:
    messages.extend(new_messages)

</t>
<t tx="ekr.20220525082933.219">def _build(sources: List[BuildSource],
           options: Options,
           alt_lib_path: Optional[str],
           flush_errors: Callable[[List[str], bool], None],
           fscache: Optional[FileSystemCache],
           stdout: TextIO,
           stderr: TextIO,
           extra_plugins: Sequence[Plugin],
           ) -&gt; BuildResult:
    if platform.python_implementation() == 'CPython':
        # This seems the most reasonable place to tune garbage collection.
        gc.set_threshold(150 * 1000)

    data_dir = default_data_dir()
    fscache = fscache or FileSystemCache()

    search_paths = compute_search_paths(sources, options, data_dir, alt_lib_path)

    reports = None
    if options.report_dirs:
        # Import lazily to avoid slowing down startup.
        from mypy.report import Reports  # noqa
        reports = Reports(data_dir, options.report_dirs)

    source_set = BuildSourceSet(sources)
    cached_read = fscache.read
    errors = Errors(options.show_error_context,
                    options.show_column_numbers,
                    options.show_error_codes,
                    options.pretty,
                    lambda path: read_py_file(path, cached_read, options.python_version),
                    options.show_absolute_path,
                    options.enabled_error_codes,
                    options.disabled_error_codes,
                    options.many_errors_threshold)
    plugin, snapshot = load_plugins(options, errors, stdout, extra_plugins)

    # Add catch-all .gitignore to cache dir if we created it
    cache_dir_existed = os.path.isdir(options.cache_dir)

    # Construct a build manager object to hold state during the build.
    #
    # Ignore current directory prefix in error messages.
    manager = BuildManager(data_dir, search_paths,
                           ignore_prefix=os.getcwd(),
                           source_set=source_set,
                           reports=reports,
                           options=options,
                           version_id=__version__,
                           plugin=plugin,
                           plugins_snapshot=snapshot,
                           errors=errors,
                           flush_errors=flush_errors,
                           fscache=fscache,
                           stdout=stdout,
                           stderr=stderr)
    manager.trace(repr(options))

    reset_global_state()
    try:
        graph = dispatch(sources, manager, stdout)
        if not options.fine_grained_incremental:
            TypeState.reset_all_subtype_caches()
        if options.timing_stats is not None:
            dump_timing_stats(options.timing_stats, graph)
        return BuildResult(manager, graph)
    finally:
        t0 = time.time()
        manager.metastore.commit()
        manager.add_stats(cache_commit_time=time.time() - t0)
        manager.log("Build finished in %.3f seconds with %d modules, and %d errors" %
                    (time.time() - manager.start_time,
                     len(manager.modules),
                     manager.errors.num_messages()))
        manager.dump_stats()
        if reports is not None:
            # Finish the HTML or XML reports even if CompileError was raised.
            reports.finish()
        if not cache_dir_existed and os.path.isdir(options.cache_dir):
            add_catch_all_gitignore(options.cache_dir)
            exclude_from_backups(options.cache_dir)
        if os.path.isdir(options.cache_dir):
            record_missing_stub_packages(options.cache_dir, manager.missing_stub_packages)


</t>
<t tx="ekr.20220525082933.22">@click.command(context_settings=dict(help_option_names=['-h', '--help']))
@click.option('--action', '-a', type=click.Choice(actions), required=True, help="What do I have to do :-)")
@click.option('--dir', '-d', 'directory', default='stubs', help="Directory to start search!")
@click.option('--ext', '-e', 'extension', default='.py', help="Extension \"from\" will be applied the action. Default .py")
@click.option('--to', '-t', 'to_extension', default='.pyi', help="Extension \"to\" will be applied the action if can. Default .pyi")
@click.option('--exclude', '-x', multiple=True, default=('__init__',), help="For every appear, will ignore this files. (can set multiples times)")
@click.option('--not-recursive', '-n', default=True, is_flag=True, help="Set if don't want to walk recursively.")
def main(action: str, directory: str, extension: str, to_extension: str,
    exclude: Tuple[str], not_recursive: bool) -&gt; None:
    """
    This script helps to copy/move/remove files based on their extension.

    The three actions will ask you for confirmation.

    Examples (by default the script search in stubs directory):

    - Change extension of all stubs from .py to .pyi:

        python &lt;script.py&gt; -a mv

    - Revert the previous action.

        python &lt;script.py&gt; -a mv -e .pyi -t .py

    - If you want to ignore "awesome.py" files.

        python &lt;script.py&gt; -a [cp|mv|rm] -x awesome

    - If you want to ignore "awesome.py" and "__init__.py" files.

        python &lt;script.py&gt; -a [cp|mv|rm] -x awesome -x __init__

    - If you want to remove all ".todo" files in "todo" directory, but not recursively:

        python &lt;script.py&gt; -a rm -e .todo -d todo -r

    """
    if action not in actions:
        print("Your action have to be one of this: {}".format(', '.join(actions)))
        return

    rec = "[Recursively] " if not_recursive else ''
    if not extension.startswith('.'):
        extension = f".{extension}"
    if not to_extension.startswith('.'):
        to_extension = f".{to_extension}"
    if directory.endswith('/'):
        directory = directory[:-1]
    if action == 'cp':
        if confirm(act='Copy',rec=rec, f1=directory, e1=extension, f2=directory, e2=to_extension):
            apply_all(shutil.copy, directory, extension, to_extension, exclude, not_recursive)
    elif action == 'rm':
        if confirm(act='Remove',rec=rec, f1=directory, e1=extension):
            apply_all(os.remove, directory, extension, exclude=exclude, recursive=not_recursive)
    elif action == 'mv':
        if confirm(act='Move',rec=rec, f1=directory, e1=extension, f2=directory, e2=to_extension):
            apply_all(shutil.move, directory, extension, to_extension, exclude, not_recursive)


</t>
<t tx="ekr.20220525082933.220">def default_data_dir() -&gt; str:
    """Returns directory containing typeshed directory."""
    return os.path.dirname(__file__)


</t>
<t tx="ekr.20220525082933.221">def normpath(path: str, options: Options) -&gt; str:
    """Convert path to absolute; but to relative in bazel mode.

    (Bazel's distributed cache doesn't like filesystem metadata to
    end up in output files.)
    """
    # TODO: Could we always use relpath?  (A worry in non-bazel
    # mode would be that a moved file may change its full module
    # name without changing its size, mtime or hash.)
    if options.bazel:
        return os.path.relpath(path)
    else:
        return os.path.abspath(path)


</t>
<t tx="ekr.20220525082933.222">class CacheMeta(NamedTuple):
    id: str
    path: str
    mtime: int
    size: int
    hash: str
    dependencies: List[str]  # names of imported modules
    data_mtime: int  # mtime of data_json
    data_json: str  # path of &lt;id&gt;.data.json
    suppressed: List[str]  # dependencies that weren't imported
    options: Optional[Dict[str, object]]  # build options
    # dep_prios and dep_lines are in parallel with dependencies + suppressed
    dep_prios: List[int]
    dep_lines: List[int]
    interface_hash: str  # hash representing the public interface
    version_id: str  # mypy version for cache invalidation
    ignore_all: bool  # if errors were ignored
    plugin_data: Any  # config data from plugins

</t>
<t tx="ekr.20220525082933.223"># NOTE: dependencies + suppressed == all reachable imports;
# suppressed contains those reachable imports that were prevented by
# silent mode or simply not found.


# Metadata for the fine-grained dependencies file associated with a module.
FgDepMeta = TypedDict('FgDepMeta', {'path': str, 'mtime': int})


</t>
<t tx="ekr.20220525082933.224">def cache_meta_from_dict(meta: Dict[str, Any], data_json: str) -&gt; CacheMeta:
    """Build a CacheMeta object from a json metadata dictionary

    Args:
      meta: JSON metadata read from the metadata cache file
      data_json: Path to the .data.json file containing the AST trees
    """
    sentinel: Any = None  # Values to be validated by the caller
    return CacheMeta(
        meta.get('id', sentinel),
        meta.get('path', sentinel),
        int(meta['mtime']) if 'mtime' in meta else sentinel,
        meta.get('size', sentinel),
        meta.get('hash', sentinel),
        meta.get('dependencies', []),
        int(meta['data_mtime']) if 'data_mtime' in meta else sentinel,
        data_json,
        meta.get('suppressed', []),
        meta.get('options'),
        meta.get('dep_prios', []),
        meta.get('dep_lines', []),
        meta.get('interface_hash', ''),
        meta.get('version_id', sentinel),
        meta.get('ignore_all', True),
        meta.get('plugin_data', None),
    )


</t>
<t tx="ekr.20220525082933.225"># Priorities used for imports.  (Here, top-level includes inside a class.)
# These are used to determine a more predictable order in which the
# nodes in an import cycle are processed.
PRI_HIGH: Final = 5  # top-level "from X import blah"
PRI_MED: Final = 10  # top-level "import X"
PRI_LOW: Final = 20  # either form inside a function
PRI_MYPY: Final = 25  # inside "if MYPY" or "if TYPE_CHECKING"
PRI_INDIRECT: Final = 30  # an indirect dependency
PRI_ALL: Final = 99  # include all priorities


</t>
<t tx="ekr.20220525082933.226">def import_priority(imp: ImportBase, toplevel_priority: int) -&gt; int:
    """Compute import priority from an import node."""
    if not imp.is_top_level:
        # Inside a function
        return PRI_LOW
    if imp.is_mypy_only:
        # Inside "if MYPY" or "if typing.TYPE_CHECKING"
        return max(PRI_MYPY, toplevel_priority)
    # A regular import; priority determined by argument.
    return toplevel_priority


</t>
<t tx="ekr.20220525082933.227">def load_plugins_from_config(
    options: Options, errors: Errors, stdout: TextIO
) -&gt; Tuple[List[Plugin], Dict[str, str]]:
    """Load all configured plugins.

    Return a list of all the loaded plugins from the config file.
    The second return value is a snapshot of versions/hashes of loaded user
    plugins (for cache validation).
    """
    import importlib

    snapshot: Dict[str, str] = {}

    if not options.config_file:
        return [], snapshot

    line = find_config_file_line_number(options.config_file, 'mypy', 'plugins')
    if line == -1:
        line = 1  # We need to pick some line number that doesn't look too confusing

    @others
    custom_plugins: List[Plugin] = []
    errors.set_file(options.config_file, None)
    for plugin_path in options.plugins:
        func_name = 'plugin'
        plugin_dir: Optional[str] = None
        if ':' in os.path.basename(plugin_path):
            plugin_path, func_name = plugin_path.rsplit(':', 1)
        if plugin_path.endswith('.py'):
            # Plugin paths can be relative to the config file location.
            plugin_path = os.path.join(os.path.dirname(options.config_file), plugin_path)
            if not os.path.isfile(plugin_path):
                plugin_error(f'Can\'t find plugin "{plugin_path}"')
            # Use an absolute path to avoid populating the cache entry
            # for 'tmp' during tests, since it will be different in
            # different tests.
            plugin_dir = os.path.abspath(os.path.dirname(plugin_path))
            fnam = os.path.basename(plugin_path)
            module_name = fnam[:-3]
            sys.path.insert(0, plugin_dir)
        elif re.search(r'[\\/]', plugin_path):
            fnam = os.path.basename(plugin_path)
            plugin_error(f'Plugin "{fnam}" does not have a .py extension')
        else:
            module_name = plugin_path

        try:
            module = importlib.import_module(module_name)
        except Exception as exc:
            plugin_error(f'Error importing plugin "{plugin_path}": {exc}')
        finally:
            if plugin_dir is not None:
                assert sys.path[0] == plugin_dir
                del sys.path[0]

        if not hasattr(module, func_name):
            plugin_error('Plugin "{}" does not define entry point function "{}"'.format(
                plugin_path, func_name))

        try:
            plugin_type = getattr(module, func_name)(__version__)
        except Exception:
            print(f'Error calling the plugin(version) entry point of {plugin_path}\n',
                  file=stdout)
            raise  # Propagate to display traceback

        if not isinstance(plugin_type, type):
            plugin_error(
                'Type object expected as the return value of "plugin"; got {!r} (in {})'.format(
                    plugin_type, plugin_path))
        if not issubclass(plugin_type, Plugin):
            plugin_error(
                'Return value of "plugin" must be a subclass of "mypy.plugin.Plugin" '
                '(in {})'.format(plugin_path))
        try:
            custom_plugins.append(plugin_type(options))
            snapshot[module_name] = take_module_snapshot(module)
        except Exception:
            print(f'Error constructing plugin instance of {plugin_type.__name__}\n',
                  file=stdout)
            raise  # Propagate to display traceback

    return custom_plugins, snapshot


</t>
<t tx="ekr.20220525082933.228">def plugin_error(message: str) -&gt; NoReturn:
    errors.report(line, 0, message)
    errors.raise_error(use_stdout=False)

</t>
<t tx="ekr.20220525082933.229">def load_plugins(options: Options,
                 errors: Errors,
                 stdout: TextIO,
                 extra_plugins: Sequence[Plugin],
                 ) -&gt; Tuple[Plugin, Dict[str, str]]:
    """Load all configured plugins.

    Return a plugin that encapsulates all plugins chained together. Always
    at least include the default plugin (it's last in the chain).
    The second return value is a snapshot of versions/hashes of loaded user
    plugins (for cache validation).
    """
    custom_plugins, snapshot = load_plugins_from_config(options, errors, stdout)

    custom_plugins += extra_plugins

    default_plugin: Plugin = DefaultPlugin(options)
    if not custom_plugins:
        return default_plugin, snapshot

    # Custom plugins take precedence over the default plugin.
    return ChainedPlugin(options, custom_plugins + [default_plugin]), snapshot


</t>
<t tx="ekr.20220525082933.23">@path C:/Repos/mypy/misc/
@language python
@tabwidth -4
#!/usr/bin/env python

from typing import Any, Dict, Iterable, List, Optional
from collections import Counter

import os
import os.path
import json

ROOT = ".mypy_cache/3.5"

JsonDict = Dict[str, Any]

@others
if __name__ == '__main__':
    main()
</t>
<t tx="ekr.20220525082933.230">def take_module_snapshot(module: types.ModuleType) -&gt; str:
    """Take plugin module snapshot by recording its version and hash.

    We record _both_ hash and the version to detect more possible changes
    (e.g. if there is a change in modules imported by a plugin).
    """
    if hasattr(module, '__file__'):
        assert module.__file__ is not None
        with open(module.__file__, 'rb') as f:
            digest = hash_digest(f.read())
    else:
        digest = 'unknown'
    ver = getattr(module, '__version__', 'none')
    return f'{ver}:{digest}'


</t>
<t tx="ekr.20220525082933.231">def find_config_file_line_number(path: str, section: str, setting_name: str) -&gt; int:
    """Return the approximate location of setting_name within mypy config file.

    Return -1 if can't determine the line unambiguously.
    """
    in_desired_section = False
    try:
        results = []
        with open(path, encoding="UTF-8") as f:
            for i, line in enumerate(f):
                line = line.strip()
                if line.startswith('[') and line.endswith(']'):
                    current_section = line[1:-1].strip()
                    in_desired_section = (current_section == section)
                elif in_desired_section and re.match(fr'{setting_name}\s*=', line):
                    results.append(i + 1)
        if len(results) == 1:
            return results[0]
    except OSError:
        pass
    return -1


</t>
<t tx="ekr.20220525082933.232">class BuildManager:
    """This class holds shared state for building a mypy program.

    It is used to coordinate parsing, import processing, semantic
    analysis and type checking.  The actual build steps are carried
    out by dispatch().

    Attributes:
      data_dir:        Mypy data directory (contains stubs)
      search_paths:    SearchPaths instance indicating where to look for modules
      modules:         Mapping of module ID to MypyFile (shared by the passes)
      semantic_analyzer:
                       Semantic analyzer, pass 2
      all_types:       Map {Expression: Type} from all modules (enabled by export_types)
      options:         Build options
      missing_modules: Set of modules that could not be imported encountered so far
      stale_modules:   Set of modules that needed to be rechecked (only used by tests)
      fg_deps_meta:    Metadata for fine-grained dependencies caches associated with modules
      fg_deps:         A fine-grained dependency map
      version_id:      The current mypy version (based on commit id when possible)
      plugin:          Active mypy plugin(s)
      plugins_snapshot:
                       Snapshot of currently active user plugins (versions and hashes)
      old_plugins_snapshot:
                       Plugins snapshot from previous incremental run (or None in
                       non-incremental mode and if cache was not found)
      errors:          Used for reporting all errors
      flush_errors:    A function for processing errors after each SCC
      cache_enabled:   Whether cache is being read. This is set based on options,
                       but is disabled if fine-grained cache loading fails
                       and after an initial fine-grained load. This doesn't
                       determine whether we write cache files or not.
      quickstart_state:
                       A cache of filename -&gt; mtime/size/hash info used to avoid
                       needing to hash source files when using a cache with mismatching mtimes
      stats:           Dict with various instrumentation numbers, it is used
                       not only for debugging, but also required for correctness,
                       in particular to check consistency of the fine-grained dependency cache.
      fscache:         A file system cacher
      ast_cache:       AST cache to speed up mypy daemon
    """

    @others
</t>
<t tx="ekr.20220525082933.233">def __init__(self, data_dir: str,
             search_paths: SearchPaths,
             ignore_prefix: str,
             source_set: BuildSourceSet,
             reports: 'Optional[Reports]',
             options: Options,
             version_id: str,
             plugin: Plugin,
             plugins_snapshot: Dict[str, str],
             errors: Errors,
             flush_errors: Callable[[List[str], bool], None],
             fscache: FileSystemCache,
             stdout: TextIO,
             stderr: TextIO,
             ) -&gt; None:
    self.stats: Dict[str, Any] = {}  # Values are ints or floats
    self.stdout = stdout
    self.stderr = stderr
    self.start_time = time.time()
    self.data_dir = data_dir
    self.errors = errors
    self.errors.set_ignore_prefix(ignore_prefix)
    self.search_paths = search_paths
    self.source_set = source_set
    self.reports = reports
    self.options = options
    self.version_id = version_id
    self.modules: Dict[str, MypyFile] = {}
    self.missing_modules: Set[str] = set()
    self.fg_deps_meta: Dict[str, FgDepMeta] = {}
    # fg_deps holds the dependencies of every module that has been
    # processed. We store this in BuildManager so that we can compute
    # dependencies as we go, which allows us to free ASTs and type information,
    # saving a ton of memory on net.
    self.fg_deps: Dict[str, Set[str]] = {}
    # Always convert the plugin to a ChainedPlugin so that it can be manipulated if needed
    if not isinstance(plugin, ChainedPlugin):
        plugin = ChainedPlugin(options, [plugin])
    self.plugin = plugin
    # Set of namespaces (module or class) that are being populated during semantic
    # analysis and may have missing definitions.
    self.incomplete_namespaces: Set[str] = set()
    self.semantic_analyzer = SemanticAnalyzer(
        self.modules,
        self.missing_modules,
        self.incomplete_namespaces,
        self.errors,
        self.plugin)
    self.all_types: Dict[Expression, Type] = {}  # Enabled by export_types
    self.indirection_detector = TypeIndirectionVisitor()
    self.stale_modules: Set[str] = set()
    self.rechecked_modules: Set[str] = set()
    self.flush_errors = flush_errors
    has_reporters = reports is not None and reports.reporters
    self.cache_enabled = (options.incremental
                          and (not options.fine_grained_incremental
                               or options.use_fine_grained_cache)
                          and not has_reporters)
    self.fscache = fscache
    self.find_module_cache = FindModuleCache(self.search_paths, self.fscache, self.options,
                                             source_set=self.source_set)
    self.metastore = create_metastore(options)

    # a mapping from source files to their corresponding shadow files
    # for efficient lookup
    self.shadow_map: Dict[str, str] = {}
    if self.options.shadow_file is not None:
        self.shadow_map = {source_file: shadow_file
                           for (source_file, shadow_file)
                           in self.options.shadow_file}
    # a mapping from each file being typechecked to its possible shadow file
    self.shadow_equivalence_map: Dict[str, Optional[str]] = {}
    self.plugin = plugin
    self.plugins_snapshot = plugins_snapshot
    self.old_plugins_snapshot = read_plugins_snapshot(self)
    self.quickstart_state = read_quickstart_file(options, self.stdout)
    # Fine grained targets (module top levels and top level functions) processed by
    # the semantic analyzer, used only for testing. Currently used only by the new
    # semantic analyzer.
    self.processed_targets: List[str] = []
    # Missing stub packages encountered.
    self.missing_stub_packages: Set[str] = set()
    # Cache for mypy ASTs that have completed semantic analysis
    # pass 1. When multiple files are added to the build in a
    # single daemon increment, only one of the files gets added
    # per step and the others are discarded. This gets repeated
    # until all the files have been added. This means that a
    # new file can be processed O(n**2) times. This cache
    # avoids most of this redundant work.
    self.ast_cache: Dict[str, Tuple[MypyFile, List[ErrorInfo]]] = {}

</t>
<t tx="ekr.20220525082933.234">def dump_stats(self) -&gt; None:
    if self.options.dump_build_stats:
        print("Stats:")
        for key, value in sorted(self.stats_summary().items()):
            print(f"{key + ':':24}{value}")

</t>
<t tx="ekr.20220525082933.235">def use_fine_grained_cache(self) -&gt; bool:
    return self.cache_enabled and self.options.use_fine_grained_cache

</t>
<t tx="ekr.20220525082933.236">def maybe_swap_for_shadow_path(self, path: str) -&gt; str:
    if not self.shadow_map:
        return path

    path = normpath(path, self.options)

    previously_checked = path in self.shadow_equivalence_map
    if not previously_checked:
        for source, shadow in self.shadow_map.items():
            if self.fscache.samefile(path, source):
                self.shadow_equivalence_map[path] = shadow
                break
            else:
                self.shadow_equivalence_map[path] = None

    shadow_file = self.shadow_equivalence_map.get(path)
    return shadow_file if shadow_file else path

</t>
<t tx="ekr.20220525082933.237">def get_stat(self, path: str) -&gt; os.stat_result:
    return self.fscache.stat(self.maybe_swap_for_shadow_path(path))

</t>
<t tx="ekr.20220525082933.238">def getmtime(self, path: str) -&gt; int:
    """Return a file's mtime; but 0 in bazel mode.

    (Bazel's distributed cache doesn't like filesystem metadata to
    end up in output files.)
    """
    if self.options.bazel:
        return 0
    else:
        return int(self.metastore.getmtime(path))

</t>
<t tx="ekr.20220525082933.239">def all_imported_modules_in_file(self,
                                 file: MypyFile) -&gt; List[Tuple[int, str, int]]:
    """Find all reachable import statements in a file.

    Return list of tuples (priority, module id, import line number)
    for all modules imported in file; lower numbers == higher priority.

    Can generate blocking errors on bogus relative imports.
    """

    def correct_rel_imp(imp: Union[ImportFrom, ImportAll]) -&gt; str:
        """Function to correct for relative imports."""
        file_id = file.fullname
        rel = imp.relative
        if rel == 0:
            return imp.id
        if os.path.basename(file.path).startswith('__init__.'):
            rel -= 1
        if rel != 0:
            file_id = ".".join(file_id.split(".")[:-rel])
        new_id = file_id + "." + imp.id if imp.id else file_id

        if not new_id:
            self.errors.set_file(file.path, file.name)
            self.errors.report(imp.line, 0,
                               "No parent module -- cannot perform relative import",
                               blocker=True)

        return new_id

    res: List[Tuple[int, str, int]] = []
    delayed_res: List[Tuple[int, str, int]] = []
    for imp in file.imports:
        if not imp.is_unreachable:
            if isinstance(imp, Import):
                pri = import_priority(imp, PRI_MED)
                ancestor_pri = import_priority(imp, PRI_LOW)
                for id, _ in imp.ids:
                    # We append the target (e.g. foo.bar.baz) before the ancestors (e.g. foo
                    # and foo.bar) so that, if FindModuleCache finds the target module in a
                    # package marked with py.typed underneath a namespace package installed in
                    # site-packages, (gasp), that cache's knowledge of the ancestors
                    # (aka FindModuleCache.ns_ancestors) can be primed when it is asked to find
                    # the parent.
                    res.append((pri, id, imp.line))
                    ancestor_parts = id.split(".")[:-1]
                    ancestors = []
                    for part in ancestor_parts:
                        ancestors.append(part)
                        res.append((ancestor_pri, ".".join(ancestors), imp.line))
            elif isinstance(imp, ImportFrom):
                cur_id = correct_rel_imp(imp)
                any_are_submodules = False
                all_are_submodules = True
                # Also add any imported names that are submodules.
                pri = import_priority(imp, PRI_MED)
                for name, __ in imp.names:
                    sub_id = cur_id + '.' + name
                    if self.is_module(sub_id):
                        res.append((pri, sub_id, imp.line))
                        any_are_submodules = True
                    else:
                        all_are_submodules = False
                # Add cur_id as a dependency, even if all of the
                # imports are submodules. Processing import from will try
                # to look through cur_id, so we should depend on it.
                # As a workaround for for some bugs in cycle handling (#4498),
                # if all of the imports are submodules, do the import at a lower
                # priority.
                pri = import_priority(imp, PRI_HIGH if not all_are_submodules else PRI_LOW)
                # The imported module goes in after the submodules, for the same namespace
                # related reasons discussed in the Import case.
                # There is an additional twist: if none of the submodules exist,
                # we delay the import in case other imports of other submodules succeed.
                if any_are_submodules:
                    res.append((pri, cur_id, imp.line))
                else:
                    delayed_res.append((pri, cur_id, imp.line))
            elif isinstance(imp, ImportAll):
                pri = import_priority(imp, PRI_HIGH)
                res.append((pri, correct_rel_imp(imp), imp.line))

    res.extend(delayed_res)
    return res

</t>
<t tx="ekr.20220525082933.24">class CacheData:
    @others
</t>
<t tx="ekr.20220525082933.240">def is_module(self, id: str) -&gt; bool:
    """Is there a file in the file system corresponding to module id?"""
    return find_module_simple(id, self) is not None

</t>
<t tx="ekr.20220525082933.241">def parse_file(self, id: str, path: str, source: str, ignore_errors: bool,
               options: Options) -&gt; MypyFile:
    """Parse the source of a file with the given name.

    Raise CompileError if there is a parse error.
    """
    t0 = time.time()
    tree = parse(source, path, id, self.errors, options=options)
    tree._fullname = id
    self.add_stats(files_parsed=1,
                   modules_parsed=int(not tree.is_stub),
                   stubs_parsed=int(tree.is_stub),
                   parse_time=time.time() - t0)

    if self.errors.is_blockers():
        self.log("Bailing due to parse errors")
        self.errors.raise_error()

    self.errors.set_file_ignored_lines(path, tree.ignored_lines, ignore_errors)
    return tree

</t>
<t tx="ekr.20220525082933.242">def load_fine_grained_deps(self, id: str) -&gt; Dict[str, Set[str]]:
    t0 = time.time()
    if id in self.fg_deps_meta:
        # TODO: Assert deps file wasn't changed.
        deps = json.loads(self.metastore.read(self.fg_deps_meta[id]['path']))
    else:
        deps = {}
    val = {k: set(v) for k, v in deps.items()}
    self.add_stats(load_fg_deps_time=time.time() - t0)
    return val

</t>
<t tx="ekr.20220525082933.243">def report_file(self,
                file: MypyFile,
                type_map: Dict[Expression, Type],
                options: Options) -&gt; None:
    if self.reports is not None and self.source_set.is_source(file):
        self.reports.file(file, self.modules, type_map, options)

</t>
<t tx="ekr.20220525082933.244">def verbosity(self) -&gt; int:
    return self.options.verbosity

</t>
<t tx="ekr.20220525082933.245">def log(self, *message: str) -&gt; None:
    if self.verbosity() &gt;= 1:
        if message:
            print('LOG: ', *message, file=self.stderr)
        else:
            print(file=self.stderr)
        self.stderr.flush()

</t>
<t tx="ekr.20220525082933.246">def log_fine_grained(self, *message: str) -&gt; None:
    import mypy.build
    if self.verbosity() &gt;= 1:
        self.log('fine-grained:', *message)
    elif mypy.build.DEBUG_FINE_GRAINED:
        # Output log in a simplified format that is quick to browse.
        if message:
            print(*message, file=self.stderr)
        else:
            print(file=self.stderr)
        self.stderr.flush()

</t>
<t tx="ekr.20220525082933.247">def trace(self, *message: str) -&gt; None:
    if self.verbosity() &gt;= 2:
        print('TRACE:', *message, file=self.stderr)
        self.stderr.flush()

</t>
<t tx="ekr.20220525082933.248">def add_stats(self, **kwds: Any) -&gt; None:
    for key, value in kwds.items():
        if key in self.stats:
            self.stats[key] += value
        else:
            self.stats[key] = value

</t>
<t tx="ekr.20220525082933.249">def stats_summary(self) -&gt; Mapping[str, object]:
    return self.stats


</t>
<t tx="ekr.20220525082933.25">def __init__(self, filename: str, data_json: JsonDict, meta_json: JsonDict,
             data_size: int, meta_size: int) -&gt; None:
    self.filename = filename
    self.data = data_json
    self.meta = meta_json
    self.data_size = data_size
    self.meta_size = meta_size

</t>
<t tx="ekr.20220525082933.250">def deps_to_json(x: Dict[str, Set[str]]) -&gt; str:
    return json.dumps({k: list(v) for k, v in x.items()})


</t>
<t tx="ekr.20220525082933.251"># File for storing metadata about all the fine-grained dependency caches
DEPS_META_FILE: Final = "@deps.meta.json"
# File for storing fine-grained dependencies that didn't a parent in the build
DEPS_ROOT_FILE: Final = "@root.deps.json"

# The name of the fake module used to store fine-grained dependencies that
# have no other place to go.
FAKE_ROOT_MODULE: Final = "@root"


</t>
<t tx="ekr.20220525082933.252">def write_deps_cache(rdeps: Dict[str, Dict[str, Set[str]]],
                     manager: BuildManager, graph: Graph) -&gt; None:
    """Write cache files for fine-grained dependencies.

    Serialize fine-grained dependencies map for fine grained mode.

    Dependencies on some module 'm' is stored in the dependency cache
    file m.deps.json.  This entails some spooky action at a distance:
    if module 'n' depends on 'm', that produces entries in m.deps.json.
    When there is a dependency on a module that does not exist in the
    build, it is stored with its first existing parent module. If no
    such module exists, it is stored with the fake module FAKE_ROOT_MODULE.

    This means that the validity of the fine-grained dependency caches
    are a global property, so we store validity checking information for
    fine-grained dependencies in a global cache file:
     * We take a snapshot of current sources to later check consistency
       between the fine-grained dependency cache and module cache metadata
     * We store the mtime of all of the dependency files to verify they
       haven't changed
    """
    metastore = manager.metastore

    error = False

    fg_deps_meta = manager.fg_deps_meta.copy()

    for id in rdeps:
        if id != FAKE_ROOT_MODULE:
            _, _, deps_json = get_cache_names(id, graph[id].xpath, manager.options)
        else:
            deps_json = DEPS_ROOT_FILE
        assert deps_json
        manager.log("Writing deps cache", deps_json)
        if not manager.metastore.write(deps_json, deps_to_json(rdeps[id])):
            manager.log(f"Error writing fine-grained deps JSON file {deps_json}")
            error = True
        else:
            fg_deps_meta[id] = {'path': deps_json, 'mtime': manager.getmtime(deps_json)}

    meta_snapshot: Dict[str, str] = {}
    for id, st in graph.items():
        # If we didn't parse a file (so it doesn't have a
        # source_hash), then it must be a module with a fresh cache,
        # so use the hash from that.
        if st.source_hash:
            hash = st.source_hash
        else:
            assert st.meta, "Module must be either parsed or cached"
            hash = st.meta.hash
        meta_snapshot[id] = hash

    meta = {'snapshot': meta_snapshot, 'deps_meta': fg_deps_meta}

    if not metastore.write(DEPS_META_FILE, json.dumps(meta)):
        manager.log(f"Error writing fine-grained deps meta JSON file {DEPS_META_FILE}")
        error = True

    if error:
        manager.errors.set_file(_cache_dir_prefix(manager.options), None)
        manager.errors.report(0, 0, "Error writing fine-grained dependencies cache",
                              blocker=True)


</t>
<t tx="ekr.20220525082933.253">def invert_deps(deps: Dict[str, Set[str]],
                graph: Graph) -&gt; Dict[str, Dict[str, Set[str]]]:
    """Splits fine-grained dependencies based on the module of the trigger.

    Returns a dictionary from module ids to all dependencies on that
    module. Dependencies not associated with a module in the build will be
    associated with the nearest parent module that is in the build, or the
    fake module FAKE_ROOT_MODULE if none are.
    """
    # Lazy import to speed up startup
    from mypy.server.target import trigger_to_target

    # Prepopulate the map for all the modules that have been processed,
    # so that we always generate files for processed modules (even if
    # there aren't any dependencies to them.)
    rdeps: Dict[str, Dict[str, Set[str]]] = {id: {} for id, st in graph.items() if st.tree}
    for trigger, targets in deps.items():
        module = module_prefix(graph, trigger_to_target(trigger))
        if not module or not graph[module].tree:
            module = FAKE_ROOT_MODULE

        mod_rdeps = rdeps.setdefault(module, {})
        mod_rdeps.setdefault(trigger, set()).update(targets)

    return rdeps


</t>
<t tx="ekr.20220525082933.254">def generate_deps_for_cache(manager: BuildManager,
                            graph: Graph) -&gt; Dict[str, Dict[str, Set[str]]]:
    """Generate fine-grained dependencies into a form suitable for serializing.

    This does a couple things:
    1. Splits fine-grained deps based on the module of the trigger
    2. For each module we generated fine-grained deps for, load any previous
       deps and merge them in.

    Returns a dictionary from module ids to all dependencies on that
    module. Dependencies not associated with a module in the build will be
    associated with the nearest parent module that is in the build, or the
    fake module FAKE_ROOT_MODULE if none are.
    """
    from mypy.server.deps import merge_dependencies  # Lazy import to speed up startup

    # Split the dependencies out into based on the module that is depended on.
    rdeps = invert_deps(manager.fg_deps, graph)

    # We can't just clobber existing dependency information, so we
    # load the deps for every module we've generated new dependencies
    # to and merge the new deps into them.
    for module, mdeps in rdeps.items():
        old_deps = manager.load_fine_grained_deps(module)
        merge_dependencies(old_deps, mdeps)

    return rdeps


</t>
<t tx="ekr.20220525082933.255">PLUGIN_SNAPSHOT_FILE: Final = "@plugins_snapshot.json"


</t>
<t tx="ekr.20220525082933.256">def write_plugins_snapshot(manager: BuildManager) -&gt; None:
    """Write snapshot of versions and hashes of currently active plugins."""
    if not manager.metastore.write(PLUGIN_SNAPSHOT_FILE, json.dumps(manager.plugins_snapshot)):
        manager.errors.set_file(_cache_dir_prefix(manager.options), None)
        manager.errors.report(0, 0, "Error writing plugins snapshot",
                              blocker=True)


</t>
<t tx="ekr.20220525082933.257">def read_plugins_snapshot(manager: BuildManager) -&gt; Optional[Dict[str, str]]:
    """Read cached snapshot of versions and hashes of plugins from previous run."""
    snapshot = _load_json_file(PLUGIN_SNAPSHOT_FILE, manager,
                               log_success='Plugins snapshot ',
                               log_error='Could not load plugins snapshot: ')
    if snapshot is None:
        return None
    if not isinstance(snapshot, dict):
        manager.log('Could not load plugins snapshot: cache is not a dict: {}'
                    .format(type(snapshot)))
        return None
    return snapshot


</t>
<t tx="ekr.20220525082933.258">def read_quickstart_file(options: Options,
                         stdout: TextIO,
                         ) -&gt; Optional[Dict[str, Tuple[float, int, str]]]:
    quickstart: Optional[Dict[str, Tuple[float, int, str]]] = None
    if options.quickstart_file:
        # This is very "best effort". If the file is missing or malformed,
        # just ignore it.
        raw_quickstart: Dict[str, Any] = {}
        try:
            with open(options.quickstart_file) as f:
                raw_quickstart = json.load(f)

            quickstart = {}
            for file, (x, y, z) in raw_quickstart.items():
                quickstart[file] = (x, y, z)
        except Exception as e:
            print(f"Warning: Failed to load quickstart file: {str(e)}\n", file=stdout)
    return quickstart


</t>
<t tx="ekr.20220525082933.259">def read_deps_cache(manager: BuildManager,
                    graph: Graph) -&gt; Optional[Dict[str, FgDepMeta]]:
    """Read and validate the fine-grained dependencies cache.

    See the write_deps_cache documentation for more information on
    the details of the cache.

    Returns None if the cache was invalid in some way.
    """
    deps_meta = _load_json_file(DEPS_META_FILE, manager,
                                log_success='Deps meta ',
                                log_error='Could not load fine-grained dependency metadata: ')
    if deps_meta is None:
        return None
    meta_snapshot = deps_meta['snapshot']
    # Take a snapshot of the source hashes from all of the metas we found.
    # (Including the ones we rejected because they were out of date.)
    # We use this to verify that they match up with the proto_deps.
    current_meta_snapshot = {id: st.meta_source_hash for id, st in graph.items()
                             if st.meta_source_hash is not None}

    common = set(meta_snapshot.keys()) &amp; set(current_meta_snapshot.keys())
    if any(meta_snapshot[id] != current_meta_snapshot[id] for id in common):
        # TODO: invalidate also if options changed (like --strict-optional)?
        manager.log('Fine-grained dependencies cache inconsistent, ignoring')
        return None

    module_deps_metas = deps_meta['deps_meta']
    if not manager.options.skip_cache_mtime_checks:
        for id, meta in module_deps_metas.items():
            try:
                matched = manager.getmtime(meta['path']) == meta['mtime']
            except FileNotFoundError:
                matched = False
            if not matched:
                manager.log(f"Invalid or missing fine-grained deps cache: {meta['path']}")
                return None

    return module_deps_metas


</t>
<t tx="ekr.20220525082933.26">@property
def total_size(self):
    return self.data_size + self.meta_size


</t>
<t tx="ekr.20220525082933.260">def _load_json_file(file: str, manager: BuildManager,
                    log_success: str, log_error: str) -&gt; Optional[Dict[str, Any]]:
    """A simple helper to read a JSON file with logging."""
    t0 = time.time()
    try:
        data = manager.metastore.read(file)
    except OSError:
        manager.log(log_error + file)
        return None
    manager.add_stats(metastore_read_time=time.time() - t0)
    # Only bother to compute the log message if we are logging it, since it could be big
    if manager.verbosity() &gt;= 2:
        manager.trace(log_success + data.rstrip())
    try:
        t1 = time.time()
        result = json.loads(data)
        manager.add_stats(data_json_load_time=time.time() - t1)
    except json.JSONDecodeError:
        manager.errors.set_file(file, None)
        manager.errors.report(-1, -1,
                              "Error reading JSON file;"
                              " you likely have a bad cache.\n"
                              "Try removing the {cache_dir} directory"
                              " and run mypy again.".format(
                                  cache_dir=manager.options.cache_dir
                              ),
                              blocker=True)
        return None
    else:
        return result


</t>
<t tx="ekr.20220525082933.261">def _cache_dir_prefix(options: Options) -&gt; str:
    """Get current cache directory (or file if id is given)."""
    if options.bazel:
        # This is needed so the cache map works.
        return os.curdir
    cache_dir = options.cache_dir
    pyversion = options.python_version
    base = os.path.join(cache_dir, '%d.%d' % pyversion)
    return base


</t>
<t tx="ekr.20220525082933.262">def add_catch_all_gitignore(target_dir: str) -&gt; None:
    """Add catch-all .gitignore to an existing directory.

    No-op if the .gitignore already exists.
    """
    gitignore = os.path.join(target_dir, ".gitignore")
    try:
        with open(gitignore, "x") as f:
            print("# Automatically created by mypy", file=f)
            print("*", file=f)
    except FileExistsError:
        pass


</t>
<t tx="ekr.20220525082933.263">def exclude_from_backups(target_dir: str) -&gt; None:
    """Exclude the directory from various archives and backups supporting CACHEDIR.TAG.

    If the CACHEDIR.TAG file exists the function is a no-op.
    """
    cachedir_tag = os.path.join(target_dir, "CACHEDIR.TAG")
    try:
        with open(cachedir_tag, "x") as f:
            f.write("""Signature: 8a477f597d28d172789f06886806bc55
# This file is a cache directory tag automatically created by mypy.
# For information about cache directory tags see https://bford.info/cachedir/
""")
    except FileExistsError:
        pass


</t>
<t tx="ekr.20220525082933.264">def create_metastore(options: Options) -&gt; MetadataStore:
    """Create the appropriate metadata store."""
    if options.sqlite_cache:
        mds: MetadataStore = SqliteMetadataStore(_cache_dir_prefix(options))
    else:
        mds = FilesystemMetadataStore(_cache_dir_prefix(options))
    return mds


</t>
<t tx="ekr.20220525082933.265">def get_cache_names(id: str, path: str, options: Options) -&gt; Tuple[str, str, Optional[str]]:
    """Return the file names for the cache files.

    Args:
      id: module ID
      path: module path
      cache_dir: cache directory
      pyversion: Python version (major, minor)

    Returns:
      A tuple with the file names to be used for the meta JSON, the
      data JSON, and the fine-grained deps JSON, respectively.
    """
    if options.cache_map:
        pair = options.cache_map.get(normpath(path, options))
    else:
        pair = None
    if pair is not None:
        # The cache map paths were specified relative to the base directory,
        # but the filesystem metastore APIs operates relative to the cache
        # prefix directory.
        # Solve this by rewriting the paths as relative to the root dir.
        # This only makes sense when using the filesystem backed cache.
        root = _cache_dir_prefix(options)
        return (os.path.relpath(pair[0], root), os.path.relpath(pair[1], root), None)
    prefix = os.path.join(*id.split('.'))
    is_package = os.path.basename(path).startswith('__init__.py')
    if is_package:
        prefix = os.path.join(prefix, '__init__')

    deps_json = None
    if options.cache_fine_grained:
        deps_json = prefix + '.deps.json'
    return (prefix + '.meta.json', prefix + '.data.json', deps_json)


</t>
<t tx="ekr.20220525082933.266">def find_cache_meta(id: str, path: str, manager: BuildManager) -&gt; Optional[CacheMeta]:
    """Find cache data for a module.

    Args:
      id: module ID
      path: module path
      manager: the build manager (for pyversion, log/trace, and build options)

    Returns:
      A CacheMeta instance if the cache data was found and appears
      valid; otherwise None.
    """
    # TODO: May need to take more build options into account
    meta_json, data_json, _ = get_cache_names(id, path, manager.options)
    manager.trace(f'Looking for {id} at {meta_json}')
    t0 = time.time()
    meta = _load_json_file(meta_json, manager,
                           log_success=f'Meta {id} ',
                           log_error=f'Could not load cache for {id}: ')
    t1 = time.time()
    if meta is None:
        return None
    if not isinstance(meta, dict):
        manager.log('Could not load cache for {}: meta cache is not a dict: {}'
                    .format(id, repr(meta)))
        return None
    m = cache_meta_from_dict(meta, data_json)
    t2 = time.time()
    manager.add_stats(load_meta_time=t2 - t0,
                      load_meta_load_time=t1 - t0,
                      load_meta_from_dict_time=t2 - t1)

    # Don't check for path match, that is dealt with in validate_meta().
    if (m.id != id or
            m.mtime is None or m.size is None or
            m.dependencies is None or m.data_mtime is None):
        manager.log(f'Metadata abandoned for {id}: attributes are missing')
        return None

    # Ignore cache if generated by an older mypy version.
    if ((m.version_id != manager.version_id and not manager.options.skip_version_check)
            or m.options is None
            or len(m.dependencies) + len(m.suppressed) != len(m.dep_prios)
            or len(m.dependencies) + len(m.suppressed) != len(m.dep_lines)):
        manager.log(f'Metadata abandoned for {id}: new attributes are missing')
        return None

    # Ignore cache if (relevant) options aren't the same.
    # Note that it's fine to mutilate cached_options since it's only used here.
    cached_options = m.options
    current_options = manager.options.clone_for_module(id).select_options_affecting_cache()
    if manager.options.skip_version_check:
        # When we're lax about version we're also lax about platform.
        cached_options['platform'] = current_options['platform']
    if 'debug_cache' in cached_options:
        # Older versions included debug_cache, but it's silly to compare it.
        del cached_options['debug_cache']
    if cached_options != current_options:
        manager.log(f'Metadata abandoned for {id}: options differ')
        if manager.options.verbosity &gt;= 2:
            for key in sorted(set(cached_options) | set(current_options)):
                if cached_options.get(key) != current_options.get(key):
                    manager.trace('    {}: {} != {}'
                                  .format(key, cached_options.get(key), current_options.get(key)))
        return None
    if manager.old_plugins_snapshot and manager.plugins_snapshot:
        # Check if plugins are still the same.
        if manager.plugins_snapshot != manager.old_plugins_snapshot:
            manager.log(f'Metadata abandoned for {id}: plugins differ')
            return None
    # So that plugins can return data with tuples in it without
    # things silently always invalidating modules, we round-trip
    # the config data. This isn't beautiful.
    plugin_data = json.loads(json.dumps(
        manager.plugin.report_config_data(ReportConfigContext(id, path, is_check=True))
    ))
    if m.plugin_data != plugin_data:
        manager.log(f'Metadata abandoned for {id}: plugin configuration differs')
        return None

    manager.add_stats(fresh_metas=1)
    return m


</t>
<t tx="ekr.20220525082933.267">def validate_meta(meta: Optional[CacheMeta], id: str, path: Optional[str],
                  ignore_all: bool, manager: BuildManager) -&gt; Optional[CacheMeta]:
    '''Checks whether the cached AST of this module can be used.

    Returns:
      None, if the cached AST is unusable.
      Original meta, if mtime/size matched.
      Meta with mtime updated to match source file, if hash/size matched but mtime/path didn't.
    '''
    # This requires two steps. The first one is obvious: we check that the module source file
    # contents is the same as it was when the cache data file was created. The second one is not
    # too obvious: we check that the cache data file mtime has not changed; it is needed because
    # we use cache data file mtime to propagate information about changes in the dependencies.

    if meta is None:
        manager.log(f'Metadata not found for {id}')
        return None

    if meta.ignore_all and not ignore_all:
        manager.log(f'Metadata abandoned for {id}: errors were previously ignored')
        return None

    t0 = time.time()
    bazel = manager.options.bazel
    assert path is not None, "Internal error: meta was provided without a path"
    if not manager.options.skip_cache_mtime_checks:
        # Check data_json; assume if its mtime matches it's good.
        try:
            data_mtime = manager.getmtime(meta.data_json)
        except OSError:
            manager.log(f'Metadata abandoned for {id}: failed to stat data_json')
            return None
        if data_mtime != meta.data_mtime:
            manager.log(f'Metadata abandoned for {id}: data cache is modified')
            return None

    if bazel:
        # Normalize path under bazel to make sure it isn't absolute
        path = normpath(path, manager.options)
    try:
        st = manager.get_stat(path)
    except OSError:
        return None
    if not (stat.S_ISREG(st.st_mode) or stat.S_ISDIR(st.st_mode)):
        manager.log(f'Metadata abandoned for {id}: file {path} does not exist')
        return None

    manager.add_stats(validate_stat_time=time.time() - t0)

    # When we are using a fine-grained cache, we want our initial
    # build() to load all of the cache information and then do a
    # fine-grained incremental update to catch anything that has
    # changed since the cache was generated. We *don't* want to do a
    # coarse-grained incremental rebuild, so we accept the cache
    # metadata even if it doesn't match the source file.
    #
    # We still *do* the mtime/hash checks, however, to enable
    # fine-grained mode to take advantage of the mtime-updating
    # optimization when mtimes differ but hashes match.  There is
    # essentially no extra time cost to computing the hash here, since
    # it will be cached and will be needed for finding changed files
    # later anyways.
    fine_grained_cache = manager.use_fine_grained_cache()

    size = st.st_size
    # Bazel ensures the cache is valid.
    if size != meta.size and not bazel and not fine_grained_cache:
        manager.log(f'Metadata abandoned for {id}: file {path} has different size')
        return None

    # Bazel ensures the cache is valid.
    mtime = 0 if bazel else int(st.st_mtime)
    if not bazel and (mtime != meta.mtime or path != meta.path):
        if manager.quickstart_state and path in manager.quickstart_state:
            # If the mtime and the size of the file recorded in the quickstart dump matches
            # what we see on disk, we know (assume) that the hash matches the quickstart
            # data as well. If that hash matches the hash in the metadata, then we know
            # the file is up to date even though the mtime is wrong, without needing to hash it.
            qmtime, qsize, qhash = manager.quickstart_state[path]
            if int(qmtime) == mtime and qsize == size and qhash == meta.hash:
                manager.log(f'Metadata fresh (by quickstart) for {id}: file {path}')
                meta = meta._replace(mtime=mtime, path=path)
                return meta

        t0 = time.time()
        try:
            # dir means it is a namespace package
            if stat.S_ISDIR(st.st_mode):
                source_hash = ''
            else:
                source_hash = manager.fscache.hash_digest(path)
        except (OSError, UnicodeDecodeError, DecodeError):
            return None
        manager.add_stats(validate_hash_time=time.time() - t0)
        if source_hash != meta.hash:
            if fine_grained_cache:
                manager.log(f'Using stale metadata for {id}: file {path}')
                return meta
            else:
                manager.log('Metadata abandoned for {}: file {} has different hash'.format(
                    id, path))
                return None
        else:
            t0 = time.time()
            # Optimization: update mtime and path (otherwise, this mismatch will reappear).
            meta = meta._replace(mtime=mtime, path=path)
            # Construct a dict we can pass to json.dumps() (compare to write_cache()).
            meta_dict = {
                'id': id,
                'path': path,
                'mtime': mtime,
                'size': size,
                'hash': source_hash,
                'data_mtime': meta.data_mtime,
                'dependencies': meta.dependencies,
                'suppressed': meta.suppressed,
                'options': (manager.options.clone_for_module(id)
                            .select_options_affecting_cache()),
                'dep_prios': meta.dep_prios,
                'dep_lines': meta.dep_lines,
                'interface_hash': meta.interface_hash,
                'version_id': manager.version_id,
                'ignore_all': meta.ignore_all,
                'plugin_data': meta.plugin_data,
            }
            if manager.options.debug_cache:
                meta_str = json.dumps(meta_dict, indent=2, sort_keys=True)
            else:
                meta_str = json.dumps(meta_dict)
            meta_json, _, _ = get_cache_names(id, path, manager.options)
            manager.log('Updating mtime for {}: file {}, meta {}, mtime {}'
                        .format(id, path, meta_json, meta.mtime))
            t1 = time.time()
            manager.metastore.write(meta_json, meta_str)  # Ignore errors, just an optimization.
            manager.add_stats(validate_update_time=time.time() - t1,
                              validate_munging_time=t1 - t0)
            return meta

    # It's a match on (id, path, size, hash, mtime).
    manager.log(f'Metadata fresh for {id}: file {path}')
    return meta


</t>
<t tx="ekr.20220525082933.268">def compute_hash(text: str) -&gt; str:
    # We use a crypto hash instead of the builtin hash(...) function
    # because the output of hash(...)  can differ between runs due to
    # hash randomization (enabled by default in Python 3.3).  See the
    # note in
    # https://docs.python.org/3/reference/datamodel.html#object.__hash__.
    return hash_digest(text.encode('utf-8'))


</t>
<t tx="ekr.20220525082933.269">def json_dumps(obj: Any, debug_cache: bool) -&gt; str:
    if debug_cache:
        return json.dumps(obj, indent=2, sort_keys=True)
    else:
        return json.dumps(obj, sort_keys=True)


</t>
<t tx="ekr.20220525082933.27">def extract_classes(chunks: Iterable[CacheData]) -&gt; Iterable[JsonDict]:
    @others
    yield from extract([chunk.data for chunk in chunks])


</t>
<t tx="ekr.20220525082933.270">def write_cache(id: str, path: str, tree: MypyFile,
                dependencies: List[str], suppressed: List[str],
                dep_prios: List[int], dep_lines: List[int],
                old_interface_hash: str, source_hash: str,
                ignore_all: bool, manager: BuildManager) -&gt; Tuple[str, Optional[CacheMeta]]:
    """Write cache files for a module.

    Note that this mypy's behavior is still correct when any given
    write_cache() call is replaced with a no-op, so error handling
    code that bails without writing anything is okay.

    Args:
      id: module ID
      path: module path
      tree: the fully checked module data
      dependencies: module IDs on which this module depends
      suppressed: module IDs which were suppressed as dependencies
      dep_prios: priorities (parallel array to dependencies)
      dep_lines: import line locations (parallel array to dependencies)
      old_interface_hash: the hash from the previous version of the data cache file
      source_hash: the hash of the source code
      ignore_all: the ignore_all flag for this module
      manager: the build manager (for pyversion, log/trace)

    Returns:
      A tuple containing the interface hash and CacheMeta
      corresponding to the metadata that was written (the latter may
      be None if the cache could not be written).
    """
    metastore = manager.metastore
    # For Bazel we use relative paths and zero mtimes.
    bazel = manager.options.bazel

    # Obtain file paths.
    meta_json, data_json, _ = get_cache_names(id, path, manager.options)
    manager.log(f'Writing {id} {path} {meta_json} {data_json}')

    # Update tree.path so that in bazel mode it's made relative (since
    # sometimes paths leak out).
    if bazel:
        tree.path = path

    # Serialize data and analyze interface
    data = tree.serialize()
    data_str = json_dumps(data, manager.options.debug_cache)
    interface_hash = compute_hash(data_str)

    plugin_data = manager.plugin.report_config_data(ReportConfigContext(id, path, is_check=False))

    # Obtain and set up metadata
    try:
        st = manager.get_stat(path)
    except OSError as err:
        manager.log(f"Cannot get stat for {path}: {err}")
        # Remove apparently-invalid cache files.
        # (This is purely an optimization.)
        for filename in [data_json, meta_json]:
            try:
                os.remove(filename)
            except OSError:
                pass
        # Still return the interface hash we computed.
        return interface_hash, None

    # Write data cache file, if applicable
    # Note that for Bazel we don't record the data file's mtime.
    if old_interface_hash == interface_hash:
        manager.trace(f"Interface for {id} is unchanged")
    else:
        manager.trace(f"Interface for {id} has changed")
        if not metastore.write(data_json, data_str):
            # Most likely the error is the replace() call
            # (see https://github.com/python/mypy/issues/3215).
            manager.log(f"Error writing data JSON file {data_json}")
            # Let's continue without writing the meta file.  Analysis:
            # If the replace failed, we've changed nothing except left
            # behind an extraneous temporary file; if the replace
            # worked but the getmtime() call failed, the meta file
            # will be considered invalid on the next run because the
            # data_mtime field won't match the data file's mtime.
            # Both have the effect of slowing down the next run a
            # little bit due to an out-of-date cache file.
            return interface_hash, None

    try:
        data_mtime = manager.getmtime(data_json)
    except OSError:
        manager.log(f"Error in os.stat({data_json!r}), skipping cache write")
        return interface_hash, None

    mtime = 0 if bazel else int(st.st_mtime)
    size = st.st_size
    # Note that the options we store in the cache are the options as
    # specified by the command line/config file and *don't* reflect
    # updates made by inline config directives in the file. This is
    # important, or otherwise the options would never match when
    # verifying the cache.
    options = manager.options.clone_for_module(id)
    assert source_hash is not None
    meta = {'id': id,
            'path': path,
            'mtime': mtime,
            'size': size,
            'hash': source_hash,
            'data_mtime': data_mtime,
            'dependencies': dependencies,
            'suppressed': suppressed,
            'options': options.select_options_affecting_cache(),
            'dep_prios': dep_prios,
            'dep_lines': dep_lines,
            'interface_hash': interface_hash,
            'version_id': manager.version_id,
            'ignore_all': ignore_all,
            'plugin_data': plugin_data,
            }

    # Write meta cache file
    meta_str = json_dumps(meta, manager.options.debug_cache)
    if not metastore.write(meta_json, meta_str):
        # Most likely the error is the replace() call
        # (see https://github.com/python/mypy/issues/3215).
        # The next run will simply find the cache entry out of date.
        manager.log(f"Error writing meta JSON file {meta_json}")

    return interface_hash, cache_meta_from_dict(meta, data_json)


</t>
<t tx="ekr.20220525082933.271">def delete_cache(id: str, path: str, manager: BuildManager) -&gt; None:
    """Delete cache files for a module.

    The cache files for a module are deleted when mypy finds errors there.
    This avoids inconsistent states with cache files from different mypy runs,
    see #4043 for an example.
    """
    # We don't delete .deps files on errors, since the dependencies
    # are mostly generated from other files and the metadata is
    # tracked separately.
    meta_path, data_path, _ = get_cache_names(id, path, manager.options)
    cache_paths = [meta_path, data_path]
    manager.log(f"Deleting {id} {path} {' '.join(x for x in cache_paths if x)}")

    for filename in cache_paths:
        try:
            manager.metastore.remove(filename)
        except OSError as e:
            if e.errno != errno.ENOENT:
                manager.log(f"Error deleting cache file {filename}: {e.strerror}")


</t>
<t tx="ekr.20220525082933.272">"""Dependency manager.

Design
======

Ideally
-------

A. Collapse cycles (each SCC -- strongly connected component --
   becomes one "supernode").

B. Topologically sort nodes based on dependencies.

C. Process from leaves towards roots.

Wrinkles
--------

a. Need to parse source modules to determine dependencies.

b. Processing order for modules within an SCC.

c. Must order mtimes of files to decide whether to re-process; depends
   on clock never resetting.

d. from P import M; checks filesystem whether module P.M exists in
   filesystem.

e. Race conditions, where somebody modifies a file while we're
   processing. Solved by using a FileSystemCache.


Steps
-----

1. For each explicitly given module find the source file location.

2. For each such module load and check the cache metadata, and decide
   whether it's valid.

3. Now recursively (or iteratively) find dependencies and add those to
   the graph:

   - for cached nodes use the list of dependencies from the cache
     metadata (this will be valid even if we later end up re-parsing
     the same source);

   - for uncached nodes parse the file and process all imports found,
     taking care of (a) above.

Step 3 should also address (d) above.

Once step 3 terminates we have the entire dependency graph, and for
each module we've either loaded the cache metadata or parsed the
source code.  (However, we may still need to parse those modules for
which we have cache metadata but that depend, directly or indirectly,
on at least one module for which the cache metadata is stale.)

Now we can execute steps A-C from the first section.  Finding SCCs for
step A shouldn't be hard; there's a recipe here:
http://code.activestate.com/recipes/578507/.  There's also a plethora
of topsort recipes, e.g. http://code.activestate.com/recipes/577413/.

For single nodes, processing is simple.  If the node was cached, we
deserialize the cache data and fix up cross-references.  Otherwise, we
do semantic analysis followed by type checking.  We also handle (c)
above; if a module has valid cache data *but* any of its
dependencies was processed from source, then the module should be
processed from source.

A relatively simple optimization (outside SCCs) we might do in the
future is as follows: if a node's cache data is valid, but one or more
of its dependencies are out of date so we have to re-parse the node
from source, once we have fully type-checked the node, we can decide
whether its symbol table actually changed compared to the cache data
(by reading the cache data and comparing it to the data we would be
writing).  If there is no change we can declare the node up to date,
and any node that depends (and for which we have cached data, and
whose other dependencies are up to date) on it won't need to be
re-parsed from source.

Import cycles
-------------

Finally we have to decide how to handle (c), import cycles.  Here
we'll need a modified version of the original state machine
(build.py), but we only need to do this per SCC, and we won't have to
deal with changes to the list of nodes while we're processing it.

If all nodes in the SCC have valid cache metadata and all dependencies
outside the SCC are still valid, we can proceed as follows:

  1. Load cache data for all nodes in the SCC.

  2. Fix up cross-references for all nodes in the SCC.

Otherwise, the simplest (but potentially slow) way to proceed is to
invalidate all cache data in the SCC and re-parse all nodes in the SCC
from source.  We can do this as follows:

  1. Parse source for all nodes in the SCC.

  2. Semantic analysis for all nodes in the SCC.

  3. Type check all nodes in the SCC.

(If there are more passes the process is the same -- each pass should
be done for all nodes before starting the next pass for any nodes in
the SCC.)

We could process the nodes in the SCC in any order.  For sentimental
reasons, I've decided to process them in the reverse order in which we
encountered them when originally constructing the graph.  That's how
the old build.py deals with cycles, and at least this reproduces the
previous implementation more accurately.

Can we do better than re-parsing all nodes in the SCC when any of its
dependencies are out of date?  It's doubtful.  The optimization
mentioned at the end of the previous section would require re-parsing
and type-checking a node and then comparing its symbol table to the
cached data; but because the node is part of a cycle we can't
technically type-check it until the semantic analysis of all other
nodes in the cycle has completed.  (This is an important issue because
Dropbox has a very large cycle in production code.  But I'd like to
deal with it later.)

Additional wrinkles
-------------------

During implementation more wrinkles were found.

- When a submodule of a package (e.g. x.y) is encountered, the parent
  package (e.g. x) must also be loaded, but it is not strictly a
  dependency.  See State.add_ancestors() below.
"""


</t>
<t tx="ekr.20220525082933.273">class ModuleNotFound(Exception):
    """Control flow exception to signal that a module was not found."""


</t>
<t tx="ekr.20220525082933.274">class State:
    """The state for a module.

    The source is only used for the -c command line option; in that
    case path is None.  Otherwise source is None and path isn't.
    """

    manager: BuildManager
    order_counter: ClassVar[int] = 0
    order: int  # Order in which modules were encountered
    id: str  # Fully qualified module name
    path: Optional[str] = None  # Path to module source
    abspath: Optional[str] = None  # Absolute path to module source
    xpath: str  # Path or '&lt;string&gt;'
    source: Optional[str] = None  # Module source code
    source_hash: Optional[str] = None  # Hash calculated based on the source code
    meta_source_hash: Optional[str] = None  # Hash of the source given in the meta, if any
    meta: Optional[CacheMeta] = None
    data: Optional[str] = None
    tree: Optional[MypyFile] = None
    # We keep both a list and set of dependencies. A set because it makes it efficient to
    # prevent duplicates and the list because I am afraid of changing the order of
    # iteration over dependencies.
    # They should be managed with add_dependency and suppress_dependency.
    dependencies: List[str]  # Modules directly imported by the module
    dependencies_set: Set[str]  # The same but as a set for deduplication purposes
    suppressed: List[str]  # Suppressed/missing dependencies
    suppressed_set: Set[str]  # Suppressed/missing dependencies
    priorities: Dict[str, int]

    # Map each dependency to the line number where it is first imported
    dep_line_map: Dict[str, int]

    # Parent package, its parent, etc.
    ancestors: Optional[List[str]] = None

    # List of (path, line number) tuples giving context for import
    import_context: List[Tuple[str, int]]

    # The State from which this module was imported, if any
    caller_state: Optional["State"] = None

    # If caller_state is set, the line number in the caller where the import occurred
    caller_line = 0

    # If True, indicate that the public interface of this module is unchanged
    externally_same = True

    # Contains a hash of the public interface in incremental mode
    interface_hash: str = ""

    # Options, specialized for this file
    options: Options

    # Whether to ignore all errors
    ignore_all = False

    # Whether the module has an error or any of its dependencies have one.
    transitive_error = False

    # Errors reported before semantic analysis, to allow fine-grained
    # mode to keep reporting them.
    early_errors: List[ErrorInfo]

    # Type checker used for checking this file.  Use type_checker() for
    # access and to construct this on demand.
    _type_checker: Optional[TypeChecker] = None

    fine_grained_deps_loaded = False

    # Cumulative time spent on this file, in microseconds (for profiling stats)
    time_spent_us: int = 0

    @others
</t>
<t tx="ekr.20220525082933.275">def __init__(self,
             id: Optional[str],
             path: Optional[str],
             source: Optional[str],
             manager: BuildManager,
             caller_state: 'Optional[State]' = None,
             caller_line: int = 0,
             ancestor_for: 'Optional[State]' = None,
             root_source: bool = False,
             # If `temporary` is True, this State is being created to just
             # quickly parse/load the tree, without an intention to further
             # process it. With this flag, any changes to external state as well
             # as error reporting should be avoided.
             temporary: bool = False,
             ) -&gt; None:
    if not temporary:
        assert id or path or source is not None, "Neither id, path nor source given"
    self.manager = manager
    State.order_counter += 1
    self.order = State.order_counter
    self.caller_state = caller_state
    self.caller_line = caller_line
    if caller_state:
        self.import_context = caller_state.import_context[:]
        self.import_context.append((caller_state.xpath, caller_line))
    else:
        self.import_context = []
    self.id = id or '__main__'
    self.options = manager.options.clone_for_module(self.id)
    self.early_errors = []
    self._type_checker = None
    if not path and source is None:
        assert id is not None
        try:
            path, follow_imports = find_module_and_diagnose(
                manager, id, self.options, caller_state, caller_line,
                ancestor_for, root_source, skip_diagnose=temporary)
        except ModuleNotFound:
            if not temporary:
                manager.missing_modules.add(id)
            raise
        if follow_imports == 'silent':
            self.ignore_all = True
    self.path = path
    if path:
        self.abspath = os.path.abspath(path)
    self.xpath = path or '&lt;string&gt;'
    if path and source is None and self.manager.cache_enabled:
        self.meta = find_cache_meta(self.id, path, manager)
        # TODO: Get mtime if not cached.
        if self.meta is not None:
            self.interface_hash = self.meta.interface_hash
            self.meta_source_hash = self.meta.hash
    if path and source is None and self.manager.fscache.isdir(path):
        source = ''
    self.source = source
    self.add_ancestors()
    t0 = time.time()
    self.meta = validate_meta(self.meta, self.id, self.path, self.ignore_all, manager)
    self.manager.add_stats(validate_meta_time=time.time() - t0)
    if self.meta:
        # Make copies, since we may modify these and want to
        # compare them to the originals later.
        self.dependencies = list(self.meta.dependencies)
        self.dependencies_set = set(self.dependencies)
        self.suppressed = list(self.meta.suppressed)
        self.suppressed_set = set(self.suppressed)
        all_deps = self.dependencies + self.suppressed
        assert len(all_deps) == len(self.meta.dep_prios)
        self.priorities = {id: pri
                           for id, pri in zip(all_deps, self.meta.dep_prios)}
        assert len(all_deps) == len(self.meta.dep_lines)
        self.dep_line_map = {id: line
                             for id, line in zip(all_deps, self.meta.dep_lines)}
        if temporary:
            self.load_tree(temporary=True)
        if not manager.use_fine_grained_cache():
            # Special case: if there were a previously missing package imported here
            # and it is not present, then we need to re-calculate dependencies.
            # This is to support patterns like this:
            #     from missing_package import missing_module  # type: ignore
            # At first mypy doesn't know that `missing_module` is a module
            # (it may be a variable, a class, or a function), so it is not added to
            # suppressed dependencies. Therefore, when the package with module is added,
            # we need to re-calculate dependencies.
            # NOTE: see comment below for why we skip this in fine grained mode.
            if exist_added_packages(self.suppressed, manager, self.options):
                self.parse_file()  # This is safe because the cache is anyway stale.
                self.compute_dependencies()
    else:
        # When doing a fine-grained cache load, pretend we only
        # know about modules that have cache information and defer
        # handling new modules until the fine-grained update.
        if manager.use_fine_grained_cache():
            manager.log(f"Deferring module to fine-grained update {path} ({id})")
            raise ModuleNotFound

        # Parse the file (and then some) to get the dependencies.
        self.parse_file()
        self.compute_dependencies()

</t>
<t tx="ekr.20220525082933.276">@property
def xmeta(self) -&gt; CacheMeta:
    assert self.meta, "missing meta on allegedly fresh module"
    return self.meta

</t>
<t tx="ekr.20220525082933.277">def add_ancestors(self) -&gt; None:
    if self.path is not None:
        _, name = os.path.split(self.path)
        base, _ = os.path.splitext(name)
        if '.' in base:
            # This is just a weird filename, don't add anything
            self.ancestors = []
            return
    # All parent packages are new ancestors.
    ancestors = []
    parent = self.id
    while '.' in parent:
        parent, _ = parent.rsplit('.', 1)
        ancestors.append(parent)
    self.ancestors = ancestors

</t>
<t tx="ekr.20220525082933.278">def is_fresh(self) -&gt; bool:
    """Return whether the cache data for this file is fresh."""
    # NOTE: self.dependencies may differ from
    # self.meta.dependencies when a dependency is dropped due to
    # suppression by silent mode.  However when a suppressed
    # dependency is added back we find out later in the process.
    return (self.meta is not None
            and self.is_interface_fresh()
            and self.dependencies == self.meta.dependencies)

</t>
<t tx="ekr.20220525082933.279">def is_interface_fresh(self) -&gt; bool:
    return self.externally_same

</t>
<t tx="ekr.20220525082933.28">def extract(chunks: Iterable[JsonDict]) -&gt; Iterable[JsonDict]:
    for chunk in chunks:
        if isinstance(chunk, dict):
            yield chunk
            yield from extract(chunk.values())
        elif isinstance(chunk, list):
            yield from extract(chunk)
</t>
<t tx="ekr.20220525082933.280">def mark_as_rechecked(self) -&gt; None:
    """Marks this module as having been fully re-analyzed by the type-checker."""
    self.manager.rechecked_modules.add(self.id)

</t>
<t tx="ekr.20220525082933.281">def mark_interface_stale(self, *, on_errors: bool = False) -&gt; None:
    """Marks this module as having a stale public interface, and discards the cache data."""
    self.externally_same = False
    if not on_errors:
        self.manager.stale_modules.add(self.id)

</t>
<t tx="ekr.20220525082933.282">def check_blockers(self) -&gt; None:
    """Raise CompileError if a blocking error is detected."""
    if self.manager.errors.is_blockers():
        self.manager.log("Bailing due to blocking errors")
        self.manager.errors.raise_error()

</t>
<t tx="ekr.20220525082933.283">@contextlib.contextmanager
def wrap_context(self, check_blockers: bool = True) -&gt; Iterator[None]:
    """Temporarily change the error import context to match this state.

    Also report an internal error if an unexpected exception was raised
    and raise an exception on a blocking error, unless
    check_blockers is False. Skipping blocking error reporting is used
    in the semantic analyzer so that we can report all blocking errors
    for a file (across multiple targets) to maintain backward
    compatibility.
    """
    save_import_context = self.manager.errors.import_context()
    self.manager.errors.set_import_context(self.import_context)
    try:
        yield
    except CompileError:
        raise
    except Exception as err:
        report_internal_error(err, self.path, 0, self.manager.errors,
                              self.options, self.manager.stdout, self.manager.stderr)
    self.manager.errors.set_import_context(save_import_context)
    # TODO: Move this away once we've removed the old semantic analyzer?
    if check_blockers:
        self.check_blockers()

</t>
<t tx="ekr.20220525082933.284">def load_fine_grained_deps(self) -&gt; Dict[str, Set[str]]:
    return self.manager.load_fine_grained_deps(self.id)

</t>
<t tx="ekr.20220525082933.285">def load_tree(self, temporary: bool = False) -&gt; None:
    assert self.meta is not None, "Internal error: this method must be called only" \
                                  " for cached modules"

    data = _load_json_file(self.meta.data_json, self.manager, "Load tree ",
                           "Could not load tree: ")
    if data is None:
        return None

    t0 = time.time()
    # TODO: Assert data file wasn't changed.
    self.tree = MypyFile.deserialize(data)
    t1 = time.time()
    self.manager.add_stats(deserialize_time=t1 - t0)
    if not temporary:
        self.manager.modules[self.id] = self.tree
        self.manager.add_stats(fresh_trees=1)

</t>
<t tx="ekr.20220525082933.286">def fix_cross_refs(self) -&gt; None:
    assert self.tree is not None, "Internal error: method must be called on parsed file only"
    # We need to set allow_missing when doing a fine grained cache
    # load because we need to gracefully handle missing modules.
    fixup_module(self.tree, self.manager.modules,
                 self.options.use_fine_grained_cache)

</t>
<t tx="ekr.20220525082933.287"># Methods for processing modules from source code.

</t>
<t tx="ekr.20220525082933.288">def parse_file(self) -&gt; None:
    """Parse file and run first pass of semantic analysis.

    Everything done here is local to the file. Don't depend on imported
    modules in any way. Also record module dependencies based on imports.
    """
    if self.tree is not None:
        # The file was already parsed (in __init__()).
        return

    manager = self.manager

    # Can we reuse a previously parsed AST? This avoids redundant work in daemon.
    cached = self.id in manager.ast_cache
    modules = manager.modules
    if not cached:
        manager.log(f"Parsing {self.xpath} ({self.id})")
    else:
        manager.log(f"Using cached AST for {self.xpath} ({self.id})")

    t0 = time_ref()

    with self.wrap_context():
        source = self.source
        self.source = None  # We won't need it again.
        if self.path and source is None:
            try:
                path = manager.maybe_swap_for_shadow_path(self.path)
                source = decode_python_encoding(manager.fscache.read(path),
                                                manager.options.python_version)
                self.source_hash = manager.fscache.hash_digest(path)
            except OSError as ioerr:
                # ioerr.strerror differs for os.stat failures between Windows and
                # other systems, but os.strerror(ioerr.errno) does not, so we use that.
                # (We want the error messages to be platform-independent so that the
                # tests have predictable output.)
                raise CompileError([
                    "mypy: can't read file '{}': {}".format(
                        self.path, os.strerror(ioerr.errno))],
                    module_with_blocker=self.id) from ioerr
            except (UnicodeDecodeError, DecodeError) as decodeerr:
                if self.path.endswith('.pyd'):
                    err = f"mypy: stubgen does not support .pyd files: '{self.path}'"
                else:
                    err = f"mypy: can't decode file '{self.path}': {str(decodeerr)}"
                raise CompileError([err], module_with_blocker=self.id) from decodeerr
        elif self.path and self.manager.fscache.isdir(self.path):
            source = ''
            self.source_hash = ''
        else:
            assert source is not None
            self.source_hash = compute_hash(source)

        self.parse_inline_configuration(source)
        if not cached:
            self.tree = manager.parse_file(self.id, self.xpath, source,
                                           self.ignore_all or self.options.ignore_errors,
                                           self.options)

        else:
            # Reuse a cached AST
            self.tree = manager.ast_cache[self.id][0]
            manager.errors.set_file_ignored_lines(
                self.xpath,
                self.tree.ignored_lines,
                self.ignore_all or self.options.ignore_errors)

    self.time_spent_us += time_spent_us(t0)

    if not cached:
        # Make a copy of any errors produced during parse time so that
        # fine-grained mode can repeat them when the module is
        # reprocessed.
        self.early_errors = list(manager.errors.error_info_map.get(self.xpath, []))
    else:
        self.early_errors = manager.ast_cache[self.id][1]

    modules[self.id] = self.tree

    if not cached:
        self.semantic_analysis_pass1()

    self.check_blockers()

    manager.ast_cache[self.id] = (self.tree, self.early_errors)

</t>
<t tx="ekr.20220525082933.289">def parse_inline_configuration(self, source: str) -&gt; None:
    """Check for inline mypy: options directive and parse them."""
    flags = get_mypy_comments(source)
    if flags:
        changes, config_errors = parse_mypy_comments(flags, self.options)
        self.options = self.options.apply_changes(changes)
        self.manager.errors.set_file(self.xpath, self.id)
        for lineno, error in config_errors:
            self.manager.errors.report(lineno, 0, error)

</t>
<t tx="ekr.20220525082933.29">def load_json(data_path: str, meta_path: str) -&gt; CacheData:
    with open(data_path) as ds:
        data_json = json.load(ds)

    with open(meta_path) as ms:
        meta_json = json.load(ms)

    data_size = os.path.getsize(data_path)
    meta_size = os.path.getsize(meta_path)

    return CacheData(data_path.replace(".data.json", ".*.json"),
                     data_json, meta_json, data_size, meta_size)


</t>
<t tx="ekr.20220525082933.290">def semantic_analysis_pass1(self) -&gt; None:
    """Perform pass 1 of semantic analysis, which happens immediately after parsing.

    This pass can't assume that any other modules have been processed yet.
    """
    options = self.options
    assert self.tree is not None

    t0 = time_ref()

    # Do the first pass of semantic analysis: analyze the reachability
    # of blocks and import statements. We must do this before
    # processing imports, since this may mark some import statements as
    # unreachable.
    #
    # TODO: This should not be considered as a semantic analysis
    #     pass -- it's an independent pass.
    analyzer = SemanticAnalyzerPreAnalysis()
    with self.wrap_context():
        analyzer.visit_file(self.tree, self.xpath, self.id, options)
    # TODO: Do this while constructing the AST?
    self.tree.names = SymbolTable()
    if not self.tree.is_stub:
        # Always perform some low-key variable renaming
        self.tree.accept(LimitedVariableRenameVisitor())
        if options.allow_redefinition:
            # Perform more renaming across the AST to allow variable redefinitions
            self.tree.accept(VariableRenameVisitor())
    self.time_spent_us += time_spent_us(t0)

</t>
<t tx="ekr.20220525082933.291">def add_dependency(self, dep: str) -&gt; None:
    if dep not in self.dependencies_set:
        self.dependencies.append(dep)
        self.dependencies_set.add(dep)
    if dep in self.suppressed_set:
        self.suppressed.remove(dep)
        self.suppressed_set.remove(dep)

</t>
<t tx="ekr.20220525082933.292">def suppress_dependency(self, dep: str) -&gt; None:
    if dep in self.dependencies_set:
        self.dependencies.remove(dep)
        self.dependencies_set.remove(dep)
    if dep not in self.suppressed_set:
        self.suppressed.append(dep)
        self.suppressed_set.add(dep)

</t>
<t tx="ekr.20220525082933.293">def compute_dependencies(self) -&gt; None:
    """Compute a module's dependencies after parsing it.

    This is used when we parse a file that we didn't have
    up-to-date cache information for. When we have an up-to-date
    cache, we just use the cached info.
    """
    manager = self.manager
    assert self.tree is not None

    # Compute (direct) dependencies.
    # Add all direct imports (this is why we needed the first pass).
    # Also keep track of each dependency's source line.
    # Missing dependencies will be moved from dependencies to
    # suppressed when they fail to be loaded in load_graph.

    self.dependencies = []
    self.dependencies_set = set()
    self.suppressed = []
    self.suppressed_set = set()
    self.priorities = {}  # id -&gt; priority
    self.dep_line_map = {}  # id -&gt; line
    dep_entries = (manager.all_imported_modules_in_file(self.tree) +
                   self.manager.plugin.get_additional_deps(self.tree))
    for pri, id, line in dep_entries:
        self.priorities[id] = min(pri, self.priorities.get(id, PRI_ALL))
        if id == self.id:
            continue
        self.add_dependency(id)
        if id not in self.dep_line_map:
            self.dep_line_map[id] = line
    # Every module implicitly depends on builtins.
    if self.id != 'builtins':
        self.add_dependency('builtins')

    self.check_blockers()  # Can fail due to bogus relative imports

</t>
<t tx="ekr.20220525082933.294">def type_check_first_pass(self) -&gt; None:
    if self.options.semantic_analysis_only:
        return
    t0 = time_ref()
    with self.wrap_context():
        self.type_checker().check_first_pass()
    self.time_spent_us += time_spent_us(t0)

</t>
<t tx="ekr.20220525082933.295">def type_checker(self) -&gt; TypeChecker:
    if not self._type_checker:
        assert self.tree is not None, "Internal error: must be called on parsed file only"
        manager = self.manager
        self._type_checker = TypeChecker(
            manager.errors, manager.modules, self.options,
            self.tree, self.xpath, manager.plugin,
        )
    return self._type_checker

</t>
<t tx="ekr.20220525082933.296">def type_map(self) -&gt; Dict[Expression, Type]:
    # We can extract the master type map directly since at this
    # point no temporary type maps can be active.
    assert len(self.type_checker()._type_maps) == 1
    return self.type_checker()._type_maps[0]

</t>
<t tx="ekr.20220525082933.297">def type_check_second_pass(self) -&gt; bool:
    if self.options.semantic_analysis_only:
        return False
    t0 = time_ref()
    with self.wrap_context():
        return self.type_checker().check_second_pass()
    self.time_spent_us += time_spent_us(t0)

</t>
<t tx="ekr.20220525082933.298">def finish_passes(self) -&gt; None:
    assert self.tree is not None, "Internal error: method must be called on parsed file only"
    manager = self.manager
    if self.options.semantic_analysis_only:
        return
    t0 = time_ref()
    with self.wrap_context():
        # Some tests (and tools) want to look at the set of all types.
        options = manager.options
        if options.export_types:
            manager.all_types.update(self.type_map())

        # We should always patch indirect dependencies, even in full (non-incremental) builds,
        # because the cache still may be written, and it must be correct.
        self._patch_indirect_dependencies(self.type_checker().module_refs, self.type_map())

        if self.options.dump_inference_stats:
            dump_type_stats(self.tree,
                            self.xpath,
                            modules=self.manager.modules,
                            inferred=True,
                            typemap=self.type_map())
        manager.report_file(self.tree, self.type_map(), self.options)

        self.update_fine_grained_deps(self.manager.fg_deps)
        self.free_state()
        if not manager.options.fine_grained_incremental and not manager.options.preserve_asts:
            free_tree(self.tree)
    self.time_spent_us += time_spent_us(t0)

</t>
<t tx="ekr.20220525082933.299">def free_state(self) -&gt; None:
    if self._type_checker:
        self._type_checker.reset()
        self._type_checker = None

</t>
<t tx="ekr.20220525082933.3">@path C:/Repos/mypy/
@language python
@tabwidth -4
#!/usr/bin/env python3
import subprocess
from subprocess import Popen
from sys import argv, exit, executable

# Slow test suites
CMDLINE = 'PythonCmdline'
SAMPLES = 'SamplesSuite'
TYPESHED = 'TypeshedSuite'
PEP561 = 'PEP561Suite'
EVALUATION = 'PythonEvaluation'
DAEMON = 'testdaemon'
STUBGEN_CMD = 'StubgenCmdLine'
STUBGEN_PY = 'StubgenPythonSuite'
MYPYC_RUN = 'TestRun'
MYPYC_RUN_MULTI = 'TestRunMultiFile'
MYPYC_EXTERNAL = 'TestExternal'
MYPYC_COMMAND_LINE = 'TestCommandLine'
ERROR_STREAM = 'ErrorStreamSuite'


ALL_NON_FAST = [
    CMDLINE,
    SAMPLES,
    TYPESHED,
    PEP561,
    EVALUATION,
    DAEMON,
    STUBGEN_CMD,
    STUBGEN_PY,
    MYPYC_RUN,
    MYPYC_RUN_MULTI,
    MYPYC_EXTERNAL,
    MYPYC_COMMAND_LINE,
    ERROR_STREAM,
]


# These must be enabled by explicitly including 'mypyc-extra' on the command line.
MYPYC_OPT_IN = [MYPYC_RUN, MYPYC_RUN_MULTI]

# We split the pytest run into three parts to improve test
# parallelization. Each run should have tests that each take a roughly similar
# time to run.
cmds = {
    # Self type check
    'self': [executable, '-m', 'mypy', '--config-file', 'mypy_self_check.ini', '-p', 'mypy'],
    # Lint
    'lint': ['flake8', '-j0'],
    # Fast test cases only (this is the bulk of the test suite)
    'pytest-fast': ['pytest', '-q', '-k', 'not (%s)' % ' or '.join(ALL_NON_FAST)],
    # Test cases that invoke mypy (with small inputs)
    'pytest-cmdline': ['pytest', '-q', '-k', ' or '.join([CMDLINE,
                                                          EVALUATION,
                                                          STUBGEN_CMD,
                                                          STUBGEN_PY])],
    # Test cases that may take seconds to run each
    'pytest-slow': ['pytest', '-q', '-k', ' or '.join(
        [SAMPLES,
         TYPESHED,
         DAEMON,
         MYPYC_EXTERNAL,
         MYPYC_COMMAND_LINE,
         ERROR_STREAM])],

    # Test cases that might take minutes to run
    'pytest-slower': ['pytest', '-q', '-k', ' or '.join(
        [PEP561])],

    # Test cases to run in typeshed CI
    'typeshed-ci': ['pytest', '-q', '-k', ' or '.join([CMDLINE,
                                                       EVALUATION,
                                                       SAMPLES,
                                                       TYPESHED])],
    # Mypyc tests that aren't run by default, since they are slow and rarely
    # fail for commits that don't touch mypyc
    'mypyc-extra': ['pytest', '-q', '-k', ' or '.join(MYPYC_OPT_IN)],
}

# Stop run immediately if these commands fail
FAST_FAIL = ['self', 'lint']

DEFAULT_COMMANDS = [cmd for cmd in cmds if cmd not in ('mypyc-extra', 'typeshed-ci')]

assert all(cmd in cmds for cmd in FAST_FAIL)


@others
if __name__ == '__main__':
    main()
</t>
<t tx="ekr.20220525082933.30">def get_files(root: str) -&gt; Iterable[CacheData]:
    for (dirpath, dirnames, filenames) in os.walk(root):
        for filename in filenames:
            if filename.endswith(".data.json"):
                meta_filename = filename.replace(".data.json", ".meta.json")
                yield load_json(
                        os.path.join(dirpath, filename),
                        os.path.join(dirpath, meta_filename))


</t>
<t tx="ekr.20220525082933.300">def _patch_indirect_dependencies(self,
                                 module_refs: Set[str],
                                 type_map: Dict[Expression, Type]) -&gt; None:
    types = set(type_map.values())
    assert None not in types
    valid = self.valid_references()

    encountered = self.manager.indirection_detector.find_modules(types) | module_refs
    extra = encountered - valid

    for dep in sorted(extra):
        if dep not in self.manager.modules:
            continue
        if dep not in self.suppressed_set and dep not in self.manager.missing_modules:
            self.add_dependency(dep)
            self.priorities[dep] = PRI_INDIRECT
        elif dep not in self.suppressed_set and dep in self.manager.missing_modules:
            self.suppress_dependency(dep)

</t>
<t tx="ekr.20220525082933.301">def compute_fine_grained_deps(self) -&gt; Dict[str, Set[str]]:
    assert self.tree is not None
    if self.id in ('builtins', 'typing', 'types', 'sys', '_typeshed'):
        # We don't track changes to core parts of typeshed -- the
        # assumption is that they are only changed as part of mypy
        # updates, which will invalidate everything anyway. These
        # will always be processed in the initial non-fine-grained
        # build. Other modules may be brought in as a result of an
        # fine-grained increment, and we may need these
        # dependencies then to handle cyclic imports.
        return {}
    from mypy.server.deps import get_dependencies  # Lazy import to speed up startup
    return get_dependencies(target=self.tree,
                            type_map=self.type_map(),
                            python_version=self.options.python_version,
                            options=self.manager.options)

</t>
<t tx="ekr.20220525082933.302">def update_fine_grained_deps(self, deps: Dict[str, Set[str]]) -&gt; None:
    options = self.manager.options
    if options.cache_fine_grained or options.fine_grained_incremental:
        from mypy.server.deps import merge_dependencies  # Lazy import to speed up startup
        merge_dependencies(self.compute_fine_grained_deps(), deps)
        TypeState.update_protocol_deps(deps)

</t>
<t tx="ekr.20220525082933.303">def valid_references(self) -&gt; Set[str]:
    assert self.ancestors is not None
    valid_refs = set(self.dependencies + self.suppressed + self.ancestors)
    valid_refs.add(self.id)

    if "os" in valid_refs:
        valid_refs.add("os.path")

    return valid_refs

</t>
<t tx="ekr.20220525082933.304">def write_cache(self) -&gt; None:
    assert self.tree is not None, "Internal error: method must be called on parsed file only"
    # We don't support writing cache files in fine-grained incremental mode.
    if (not self.path
            or self.options.cache_dir == os.devnull
            or self.options.fine_grained_incremental):
        return
    is_errors = self.transitive_error
    if is_errors:
        delete_cache(self.id, self.path, self.manager)
        self.meta = None
        self.mark_interface_stale(on_errors=True)
        return
    dep_prios = self.dependency_priorities()
    dep_lines = self.dependency_lines()
    assert self.source_hash is not None
    assert len(set(self.dependencies)) == len(self.dependencies), (
        f"Duplicates in dependencies list for {self.id} ({self.dependencies})")
    new_interface_hash, self.meta = write_cache(
        self.id, self.path, self.tree,
        list(self.dependencies), list(self.suppressed),
        dep_prios, dep_lines, self.interface_hash, self.source_hash, self.ignore_all,
        self.manager)
    if new_interface_hash == self.interface_hash:
        self.manager.log(f"Cached module {self.id} has same interface")
    else:
        self.manager.log(f"Cached module {self.id} has changed interface")
        self.mark_interface_stale()
        self.interface_hash = new_interface_hash

</t>
<t tx="ekr.20220525082933.305">def verify_dependencies(self, suppressed_only: bool = False) -&gt; None:
    """Report errors for import targets in modules that don't exist.

    If suppressed_only is set, only check suppressed dependencies.
    """
    manager = self.manager
    assert self.ancestors is not None
    if suppressed_only:
        all_deps = self.suppressed
    else:
        # Strip out indirect dependencies. See comment in build.load_graph().
        dependencies = [dep for dep in self.dependencies
                        if self.priorities.get(dep) != PRI_INDIRECT]
        all_deps = dependencies + self.suppressed + self.ancestors
    for dep in all_deps:
        if dep in manager.modules:
            continue
        options = manager.options.clone_for_module(dep)
        if options.ignore_missing_imports:
            continue
        line = self.dep_line_map.get(dep, 1)
        try:
            if dep in self.ancestors:
                state, ancestor = None, self  # type: (Optional[State], Optional[State])
            else:
                state, ancestor = self, None
            # Called just for its side effects of producing diagnostics.
            find_module_and_diagnose(
                manager, dep, options,
                caller_state=state, caller_line=line,
                ancestor_for=ancestor)
        except (ModuleNotFound, CompileError):
            # Swallow up any ModuleNotFounds or CompilerErrors while generating
            # a diagnostic. CompileErrors may get generated in
            # fine-grained mode when an __init__.py is deleted, if a module
            # that was in that package has targets reprocessed before
            # it is renamed.
            pass

</t>
<t tx="ekr.20220525082933.306">def dependency_priorities(self) -&gt; List[int]:
    return [self.priorities.get(dep, PRI_HIGH) for dep in self.dependencies + self.suppressed]

</t>
<t tx="ekr.20220525082933.307">def dependency_lines(self) -&gt; List[int]:
    return [self.dep_line_map.get(dep, 1) for dep in self.dependencies + self.suppressed]

</t>
<t tx="ekr.20220525082933.308">def generate_unused_ignore_notes(self) -&gt; None:
    if self.options.warn_unused_ignores:
        # If this file was initially loaded from the cache, it may have suppressed
        # dependencies due to imports with ignores on them. We need to generate
        # those errors to avoid spuriously flagging them as unused ignores.
        if self.meta:
            self.verify_dependencies(suppressed_only=True)
        self.manager.errors.generate_unused_ignore_errors(self.xpath)

</t>
<t tx="ekr.20220525082933.309">def generate_ignore_without_code_notes(self) -&gt; None:
    if self.manager.errors.is_error_code_enabled(codes.IGNORE_WITHOUT_CODE):
        self.manager.errors.generate_ignore_without_code_errors(
            self.xpath,
            self.options.warn_unused_ignores,
        )


</t>
<t tx="ekr.20220525082933.31">def pluck(name: str, chunks: Iterable[JsonDict]) -&gt; Iterable[JsonDict]:
    return (chunk for chunk in chunks if chunk['.class'] == name)


</t>
<t tx="ekr.20220525082933.310"># Module import and diagnostic glue


</t>
<t tx="ekr.20220525082933.311">def find_module_and_diagnose(manager: BuildManager,
                             id: str,
                             options: Options,
                             caller_state: 'Optional[State]' = None,
                             caller_line: int = 0,
                             ancestor_for: 'Optional[State]' = None,
                             root_source: bool = False,
                             skip_diagnose: bool = False) -&gt; Tuple[str, str]:
    """Find a module by name, respecting follow_imports and producing diagnostics.

    If the module is not found, then the ModuleNotFound exception is raised.

    Args:
      id: module to find
      options: the options for the module being loaded
      caller_state: the state of the importing module, if applicable
      caller_line: the line number of the import
      ancestor_for: the child module this is an ancestor of, if applicable
      root_source: whether this source was specified on the command line
      skip_diagnose: skip any error diagnosis and reporting (but ModuleNotFound is
          still raised if the module is missing)

    The specified value of follow_imports for a module can be overridden
    if the module is specified on the command line or if it is a stub,
    so we compute and return the "effective" follow_imports of the module.

    Returns a tuple containing (file path, target's effective follow_imports setting)
    """
    file_id = id
    if id == 'builtins' and options.python_version[0] == 2:
        # The __builtin__ module is called internally by mypy
        # 'builtins' in Python 2 mode (similar to Python 3),
        # but the stub file is __builtin__.pyi.  The reason is
        # that a lot of code hard-codes 'builtins.x' and it's
        # easier to work it around like this.  It also means
        # that the implementation can mostly ignore the
        # difference and just assume 'builtins' everywhere,
        # which simplifies code.
        file_id = '__builtin__'
    result = find_module_with_reason(file_id, manager)
    if isinstance(result, str):
        # For non-stubs, look at options.follow_imports:
        # - normal (default) -&gt; fully analyze
        # - silent -&gt; analyze but silence errors
        # - skip -&gt; don't analyze, make the type Any
        follow_imports = options.follow_imports
        if (root_source  # Honor top-level modules
                or (not result.endswith('.py')  # Stubs are always normal
                    and not options.follow_imports_for_stubs)  # except when they aren't
                or id in mypy.semanal_main.core_modules):  # core is always normal
            follow_imports = 'normal'
        if skip_diagnose:
            pass
        elif follow_imports == 'silent':
            # Still import it, but silence non-blocker errors.
            manager.log(f"Silencing {result} ({id})")
        elif follow_imports == 'skip' or follow_imports == 'error':
            # In 'error' mode, produce special error messages.
            if id not in manager.missing_modules:
                manager.log(f"Skipping {result} ({id})")
            if follow_imports == 'error':
                if ancestor_for:
                    skipping_ancestor(manager, id, result, ancestor_for)
                else:
                    skipping_module(manager, caller_line, caller_state,
                                    id, result)
            raise ModuleNotFound
        if not manager.options.no_silence_site_packages:
            for dir in manager.search_paths.package_path + manager.search_paths.typeshed_path:
                if is_sub_path(result, dir):
                    # Silence errors in site-package dirs and typeshed
                    follow_imports = 'silent'
        if (id in CORE_BUILTIN_MODULES
                and not is_typeshed_file(result)
                and not is_stub_package_file(result)
                and not options.use_builtins_fixtures
                and not options.custom_typeshed_dir):
            raise CompileError([
                f'mypy: "{os.path.relpath(result)}" shadows library module "{id}"',
                f'note: A user-defined top-level module with name "{id}" is not supported'
            ])
        return (result, follow_imports)
    else:
        # Could not find a module.  Typically the reason is a
        # misspelled module name, missing stub, module not in
        # search path or the module has not been installed.

        ignore_missing_imports = options.ignore_missing_imports
        top_level, second_level = get_top_two_prefixes(file_id)
        # Don't honor a global (not per-module) ignore_missing_imports
        # setting for modules that used to have bundled stubs, as
        # otherwise updating mypy can silently result in new false
        # negatives. (Unless there are stubs but they are incomplete.)
        global_ignore_missing_imports = manager.options.ignore_missing_imports
        py_ver = options.python_version[0]
        if ((is_legacy_bundled_package(top_level, py_ver)
                or is_legacy_bundled_package(second_level, py_ver))
                and global_ignore_missing_imports
                and not options.ignore_missing_imports_per_module
                and result is ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED):
            ignore_missing_imports = False

        if skip_diagnose:
            raise ModuleNotFound
        if caller_state:
            if not (ignore_missing_imports or in_partial_package(id, manager)):
                module_not_found(manager, caller_line, caller_state, id, result)
            raise ModuleNotFound
        elif root_source:
            # If we can't find a root source it's always fatal.
            # TODO: This might hide non-fatal errors from
            # root sources processed earlier.
            raise CompileError([f"mypy: can't find module '{id}'"])
        else:
            raise ModuleNotFound


</t>
<t tx="ekr.20220525082933.312">def exist_added_packages(suppressed: List[str],
                         manager: BuildManager, options: Options) -&gt; bool:
    """Find if there are any newly added packages that were previously suppressed.

    Exclude everything not in build for follow-imports=skip.
    """
    for dep in suppressed:
        if dep in manager.source_set.source_modules:
            # We don't need to add any special logic for this. If a module
            # is added to build, importers will be invalidated by normal mechanism.
            continue
        path = find_module_simple(dep, manager)
        if not path:
            continue
        if (options.follow_imports == 'skip' and
                (not path.endswith('.pyi') or options.follow_imports_for_stubs)):
            continue
        if '__init__.py' in path:
            # It is better to have a bit lenient test, this will only slightly reduce
            # performance, while having a too strict test may affect correctness.
            return True
    return False


</t>
<t tx="ekr.20220525082933.313">def find_module_simple(id: str, manager: BuildManager) -&gt; Optional[str]:
    """Find a filesystem path for module `id` or `None` if not found."""
    x = find_module_with_reason(id, manager)
    if isinstance(x, ModuleNotFoundReason):
        return None
    return x


</t>
<t tx="ekr.20220525082933.314">def find_module_with_reason(id: str, manager: BuildManager) -&gt; ModuleSearchResult:
    """Find a filesystem path for module `id` or the reason it can't be found."""
    t0 = time.time()
    x = manager.find_module_cache.find_module(id)
    manager.add_stats(find_module_time=time.time() - t0, find_module_calls=1)
    return x


</t>
<t tx="ekr.20220525082933.315">def in_partial_package(id: str, manager: BuildManager) -&gt; bool:
    """Check if a missing module can potentially be a part of a package.

    This checks if there is any existing parent __init__.pyi stub that
    defines a module-level __getattr__ (a.k.a. partial stub package).
    """
    while '.' in id:
        parent, _ = id.rsplit('.', 1)
        if parent in manager.modules:
            parent_mod: Optional[MypyFile] = manager.modules[parent]
        else:
            # Parent is not in build, try quickly if we can find it.
            try:
                parent_st = State(id=parent, path=None, source=None, manager=manager,
                                  temporary=True)
            except (ModuleNotFound, CompileError):
                parent_mod = None
            else:
                parent_mod = parent_st.tree
        if parent_mod is not None:
            if parent_mod.is_partial_stub_package:
                return True
            else:
                # Bail out soon, complete subpackage found
                return False
        id = parent
    return False


</t>
<t tx="ekr.20220525082933.316">def module_not_found(manager: BuildManager, line: int, caller_state: State,
                     target: str, reason: ModuleNotFoundReason) -&gt; None:
    errors = manager.errors
    save_import_context = errors.import_context()
    errors.set_import_context(caller_state.import_context)
    errors.set_file(caller_state.xpath, caller_state.id)
    if target == 'builtins':
        errors.report(line, 0, "Cannot find 'builtins' module. Typeshed appears broken!",
                      blocker=True)
        errors.raise_error()
    else:
        daemon = manager.options.fine_grained_incremental
        msg, notes = reason.error_message_templates(daemon)
        pyver = '%d.%d' % manager.options.python_version
        errors.report(line, 0, msg.format(module=target, pyver=pyver), code=codes.IMPORT)
        top_level, second_level = get_top_two_prefixes(target)
        if second_level in legacy_bundled_packages:
            top_level = second_level
        for note in notes:
            if '{stub_dist}' in note:
                note = note.format(stub_dist=legacy_bundled_packages[top_level].name)
            errors.report(line, 0, note, severity='note', only_once=True, code=codes.IMPORT)
        if reason is ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED:
            manager.missing_stub_packages.add(legacy_bundled_packages[top_level].name)
    errors.set_import_context(save_import_context)


</t>
<t tx="ekr.20220525082933.317">def skipping_module(manager: BuildManager, line: int, caller_state: Optional[State],
                    id: str, path: str) -&gt; None:
    """Produce an error for an import ignored due to --follow_imports=error"""
    assert caller_state, (id, path)
    save_import_context = manager.errors.import_context()
    manager.errors.set_import_context(caller_state.import_context)
    manager.errors.set_file(caller_state.xpath, caller_state.id)
    manager.errors.report(line, 0,
                          f'Import of "{id}" ignored',
                          severity='error')
    manager.errors.report(line, 0,
                          "(Using --follow-imports=error, module not passed on command line)",
                          severity='note', only_once=True)
    manager.errors.set_import_context(save_import_context)


</t>
<t tx="ekr.20220525082933.318">def skipping_ancestor(manager: BuildManager, id: str, path: str, ancestor_for: 'State') -&gt; None:
    """Produce an error for an ancestor ignored due to --follow_imports=error"""
    # TODO: Read the path (the __init__.py file) and return
    # immediately if it's empty or only contains comments.
    # But beware, some package may be the ancestor of many modules,
    # so we'd need to cache the decision.
    manager.errors.set_import_context([])
    manager.errors.set_file(ancestor_for.xpath, ancestor_for.id)
    manager.errors.report(-1, -1, f'Ancestor package "{id}" ignored',
                          severity='error', only_once=True)
    manager.errors.report(-1, -1,
                          "(Using --follow-imports=error, submodule passed on command line)",
                          severity='note', only_once=True)


</t>
<t tx="ekr.20220525082933.319">def log_configuration(manager: BuildManager, sources: List[BuildSource]) -&gt; None:
    """Output useful configuration information to LOG and TRACE"""

    manager.log()
    configuration_vars = [
        ("Mypy Version", __version__),
        ("Config File", (manager.options.config_file or "Default")),
        ("Configured Executable", manager.options.python_executable or "None"),
        ("Current Executable", sys.executable),
        ("Cache Dir", manager.options.cache_dir),
        ("Compiled", str(not __file__.endswith(".py"))),
        ("Exclude", manager.options.exclude),
    ]

    for conf_name, conf_value in configuration_vars:
        manager.log(f"{conf_name + ':':24}{conf_value}")

    for source in sources:
        manager.log(f"{'Found source:':24}{source}")

    # Complete list of searched paths can get very long, put them under TRACE
    for path_type, paths in manager.search_paths._asdict().items():
        if not paths:
            manager.trace(f"No {path_type}")
            continue

        manager.trace(f"{path_type}:")

        for pth in paths:
            manager.trace(f"    {pth}")


</t>
<t tx="ekr.20220525082933.32">def report_counter(counter: Counter, amount: Optional[int] = None) -&gt; None:
    for name, count in counter.most_common(amount):
        print(f'    {count: &lt;8} {name}')
    print()


</t>
<t tx="ekr.20220525082933.320"># The driver


</t>
<t tx="ekr.20220525082933.321">def dispatch(sources: List[BuildSource],
             manager: BuildManager,
             stdout: TextIO,
             ) -&gt; Graph:
    log_configuration(manager, sources)

    t0 = time.time()
    graph = load_graph(sources, manager)

    # This is a kind of unfortunate hack to work around some of fine-grained's
    # fragility: if we have loaded less than 50% of the specified files from
    # cache in fine-grained cache mode, load the graph again honestly.
    # In this case, we just turn the cache off entirely, so we don't need
    # to worry about some files being loaded and some from cache and so
    # that fine-grained mode never *writes* to the cache.
    if manager.use_fine_grained_cache() and len(graph) &lt; 0.50 * len(sources):
        manager.log("Redoing load_graph without cache because too much was missing")
        manager.cache_enabled = False
        graph = load_graph(sources, manager)

    t1 = time.time()
    manager.add_stats(graph_size=len(graph),
                      stubs_found=sum(g.path is not None and g.path.endswith('.pyi')
                                      for g in graph.values()),
                      graph_load_time=(t1 - t0),
                      fm_cache_size=len(manager.find_module_cache.results),
                      )
    if not graph:
        print("Nothing to do?!", file=stdout)
        return graph
    manager.log(f"Loaded graph with {len(graph)} nodes ({t1 - t0:.3f} sec)")
    if manager.options.dump_graph:
        dump_graph(graph, stdout)
        return graph

    # Fine grained dependencies that didn't have an associated module in the build
    # are serialized separately, so we read them after we load the graph.
    # We need to read them both for running in daemon mode and if we are generating
    # a fine-grained cache (so that we can properly update them incrementally).
    # The `read_deps_cache` will also validate
    # the deps cache against the loaded individual cache files.
    if manager.options.cache_fine_grained or manager.use_fine_grained_cache():
        t2 = time.time()
        fg_deps_meta = read_deps_cache(manager, graph)
        manager.add_stats(load_fg_deps_time=time.time() - t2)
        if fg_deps_meta is not None:
            manager.fg_deps_meta = fg_deps_meta
        elif manager.stats.get('fresh_metas', 0) &gt; 0:
            # Clear the stats so we don't infinite loop because of positive fresh_metas
            manager.stats.clear()
            # There were some cache files read, but no fine-grained dependencies loaded.
            manager.log("Error reading fine-grained dependencies cache -- aborting cache load")
            manager.cache_enabled = False
            manager.log("Falling back to full run -- reloading graph...")
            return dispatch(sources, manager, stdout)

    # If we are loading a fine-grained incremental mode cache, we
    # don't want to do a real incremental reprocess of the
    # graph---we'll handle it all later.
    if not manager.use_fine_grained_cache():
        process_graph(graph, manager)
        # Update plugins snapshot.
        write_plugins_snapshot(manager)
        manager.old_plugins_snapshot = manager.plugins_snapshot
        if manager.options.cache_fine_grained or manager.options.fine_grained_incremental:
            # If we are running a daemon or are going to write cache for further fine grained use,
            # then we need to collect fine grained protocol dependencies.
            # Since these are a global property of the program, they are calculated after we
            # processed the whole graph.
            TypeState.add_all_protocol_deps(manager.fg_deps)
            if not manager.options.fine_grained_incremental:
                rdeps = generate_deps_for_cache(manager, graph)
                write_deps_cache(rdeps, manager, graph)

    if manager.options.dump_deps:
        # This speeds up startup a little when not using the daemon mode.
        from mypy.server.deps import dump_all_dependencies
        dump_all_dependencies(manager.modules, manager.all_types,
                              manager.options.python_version, manager.options)
    return graph


</t>
<t tx="ekr.20220525082933.322">class NodeInfo:
    """Some info about a node in the graph of SCCs."""

    @others
</t>
<t tx="ekr.20220525082933.323">def __init__(self, index: int, scc: List[str]) -&gt; None:
    self.node_id = "n%d" % index
    self.scc = scc
    self.sizes: Dict[str, int] = {}  # mod -&gt; size in bytes
    self.deps: Dict[str, int] = {}  # node_id -&gt; pri

</t>
<t tx="ekr.20220525082933.324">def dumps(self) -&gt; str:
    """Convert to JSON string."""
    total_size = sum(self.sizes.values())
    return "[{}, {}, {},\n     {},\n     {}]".format(json.dumps(self.node_id),
                                                 json.dumps(total_size),
                                                 json.dumps(self.scc),
                                                 json.dumps(self.sizes),
                                                 json.dumps(self.deps))


</t>
<t tx="ekr.20220525082933.325">def dump_timing_stats(path: str, graph: Graph) -&gt; None:
    """
    Dump timing stats for each file in the given graph
    """
    with open(path, 'w') as f:
        for k in sorted(graph.keys()):
            v = graph[k]
            f.write(f'{v.id} {v.time_spent_us}\n')


</t>
<t tx="ekr.20220525082933.326">def dump_graph(graph: Graph, stdout: Optional[TextIO] = None) -&gt; None:
    """Dump the graph as a JSON string to stdout.

    This copies some of the work by process_graph()
    (sorted_components() and order_ascc()).
    """
    stdout = stdout or sys.stdout
    nodes = []
    sccs = sorted_components(graph)
    for i, ascc in enumerate(sccs):
        scc = order_ascc(graph, ascc)
        node = NodeInfo(i, scc)
        nodes.append(node)
    inv_nodes = {}  # module -&gt; node_id
    for node in nodes:
        for mod in node.scc:
            inv_nodes[mod] = node.node_id
    for node in nodes:
        for mod in node.scc:
            state = graph[mod]
            size = 0
            if state.path:
                try:
                    size = os.path.getsize(state.path)
                except os.error:
                    pass
            node.sizes[mod] = size
            for dep in state.dependencies:
                if dep in state.priorities:
                    pri = state.priorities[dep]
                    if dep in inv_nodes:
                        dep_id = inv_nodes[dep]
                        if (dep_id != node.node_id and
                                (dep_id not in node.deps or pri &lt; node.deps[dep_id])):
                            node.deps[dep_id] = pri
    print("[" + ",\n ".join(node.dumps() for node in nodes) + "\n]", file=stdout)


</t>
<t tx="ekr.20220525082933.327">def load_graph(sources: List[BuildSource], manager: BuildManager,
               old_graph: Optional[Graph] = None,
               new_modules: Optional[List[State]] = None) -&gt; Graph:
    """Given some source files, load the full dependency graph.

    If an old_graph is passed in, it is used as the starting point and
    modified during graph loading.

    If a new_modules is passed in, any modules that are loaded are
    added to the list. This is an argument and not a return value
    so that the caller can access it even if load_graph fails.

    As this may need to parse files, this can raise CompileError in case
    there are syntax errors.
    """

    graph: Graph = old_graph if old_graph is not None else {}

    # The deque is used to implement breadth-first traversal.
    # TODO: Consider whether to go depth-first instead.  This may
    # affect the order in which we process files within import cycles.
    new = new_modules if new_modules is not None else []
    entry_points: Set[str] = set()
    # Seed the graph with the initial root sources.
    for bs in sources:
        try:
            st = State(id=bs.module, path=bs.path, source=bs.text, manager=manager,
                       root_source=True)
        except ModuleNotFound:
            continue
        if st.id in graph:
            manager.errors.set_file(st.xpath, st.id)
            manager.errors.report(
                -1, -1,
                f'Duplicate module named "{st.id}" (also at "{graph[st.id].xpath}")',
                blocker=True,
            )
            manager.errors.report(
                -1, -1,
                "See https://mypy.readthedocs.io/en/stable/running_mypy.html#mapping-file-paths-to-modules "  # noqa: E501
                "for more info",
                severity='note',
            )
            manager.errors.report(
                -1, -1,
                "Common resolutions include: a) using `--exclude` to avoid checking one of them, "
                "b) adding `__init__.py` somewhere, c) using `--explicit-package-bases` or "
                "adjusting MYPYPATH",
                severity='note'
            )

            manager.errors.raise_error()
        graph[st.id] = st
        new.append(st)
        entry_points.add(bs.module)

    # Note: Running this each time could be slow in the daemon. If it's a problem, we
    # can do more work to maintain this incrementally.
    seen_files = {st.abspath: st for st in graph.values() if st.path}

    # Collect dependencies.  We go breadth-first.
    # More nodes might get added to new as we go, but that's fine.
    for st in new:
        assert st.ancestors is not None
        # Strip out indirect dependencies.  These will be dealt with
        # when they show up as direct dependencies, and there's a
        # scenario where they hurt:
        # - Suppose A imports B and B imports C.
        # - Suppose on the next round:
        #   - C is deleted;
        #   - B is updated to remove the dependency on C;
        #   - A is unchanged.
        # - In this case A's cached *direct* dependencies are still valid
        #   (since direct dependencies reflect the imports found in the source)
        #   but A's cached *indirect* dependency on C is wrong.
        dependencies = [dep for dep in st.dependencies if st.priorities.get(dep) != PRI_INDIRECT]
        if not manager.use_fine_grained_cache():
            # TODO: Ideally we could skip here modules that appeared in st.suppressed
            # because they are not in build with `follow-imports=skip`.
            # This way we could avoid overhead of cloning options in `State.__init__()`
            # below to get the option value. This is quite minor performance loss however.
            added = [dep for dep in st.suppressed if find_module_simple(dep, manager)]
        else:
            # During initial loading we don't care about newly added modules,
            # they will be taken care of during fine grained update. See also
            # comment about this in `State.__init__()`.
            added = []
        for dep in st.ancestors + dependencies + st.suppressed:
            ignored = dep in st.suppressed_set and dep not in entry_points
            if ignored and dep not in added:
                manager.missing_modules.add(dep)
            elif dep not in graph:
                try:
                    if dep in st.ancestors:
                        # TODO: Why not 'if dep not in st.dependencies' ?
                        # Ancestors don't have import context.
                        newst = State(id=dep, path=None, source=None, manager=manager,
                                      ancestor_for=st)
                    else:
                        newst = State(id=dep, path=None, source=None, manager=manager,
                                      caller_state=st, caller_line=st.dep_line_map.get(dep, 1))
                except ModuleNotFound:
                    if dep in st.dependencies_set:
                        st.suppress_dependency(dep)
                else:
                    if newst.path:
                        newst_path = os.path.abspath(newst.path)

                        if newst_path in seen_files:
                            manager.errors.report(
                                -1, 0,
                                'Source file found twice under different module names: '
                                '"{}" and "{}"'.format(seen_files[newst_path].id, newst.id),
                                blocker=True,
                            )
                            manager.errors.report(
                                -1, 0,
                                "See https://mypy.readthedocs.io/en/stable/running_mypy.html#mapping-file-paths-to-modules "  # noqa: E501
                                "for more info",
                                severity='note',
                            )
                            manager.errors.report(
                                -1, 0,
                                "Common resolutions include: a) adding `__init__.py` somewhere, "
                                "b) using `--explicit-package-bases` or adjusting MYPYPATH",
                                severity='note',
                            )
                            manager.errors.raise_error()

                        seen_files[newst_path] = newst

                    assert newst.id not in graph, newst.id
                    graph[newst.id] = newst
                    new.append(newst)
            if dep in graph and dep in st.suppressed_set:
                # Previously suppressed file is now visible
                st.add_dependency(dep)
    manager.plugin.set_modules(manager.modules)
    return graph


</t>
<t tx="ekr.20220525082933.328">def process_graph(graph: Graph, manager: BuildManager) -&gt; None:
    """Process everything in dependency order."""
    sccs = sorted_components(graph)
    manager.log("Found %d SCCs; largest has %d nodes" %
                (len(sccs), max(len(scc) for scc in sccs)))

    fresh_scc_queue: List[List[str]] = []

    # We're processing SCCs from leaves (those without further
    # dependencies) to roots (those from which everything else can be
    # reached).
    for ascc in sccs:
        # Order the SCC's nodes using a heuristic.
        # Note that ascc is a set, and scc is a list.
        scc = order_ascc(graph, ascc)
        # Make the order of the SCC that includes 'builtins' and 'typing',
        # among other things, predictable. Various things may  break if
        # the order changes.
        if 'builtins' in ascc:
            scc = sorted(scc, reverse=True)
            # If builtins is in the list, move it last.  (This is a bit of
            # a hack, but it's necessary because the builtins module is
            # part of a small cycle involving at least {builtins, abc,
            # typing}.  Of these, builtins must be processed last or else
            # some builtin objects will be incompletely processed.)
            scc.remove('builtins')
            scc.append('builtins')
        if manager.options.verbosity &gt;= 2:
            for id in scc:
                manager.trace(f"Priorities for {id}:",
                              " ".join("%s:%d" % (x, graph[id].priorities[x])
                                       for x in graph[id].dependencies
                                       if x in ascc and x in graph[id].priorities))
        # Because the SCCs are presented in topological sort order, we
        # don't need to look at dependencies recursively for staleness
        # -- the immediate dependencies are sufficient.
        stale_scc = {id for id in scc if not graph[id].is_fresh()}
        fresh = not stale_scc
        deps = set()
        for id in scc:
            deps.update(graph[id].dependencies)
        deps -= ascc
        stale_deps = {id for id in deps if id in graph and not graph[id].is_interface_fresh()}
        fresh = fresh and not stale_deps
        undeps = set()
        if fresh:
            # Check if any dependencies that were suppressed according
            # to the cache have been added back in this run.
            # NOTE: Newly suppressed dependencies are handled by is_fresh().
            for id in scc:
                undeps.update(graph[id].suppressed)
            undeps &amp;= graph.keys()
            if undeps:
                fresh = False
        if fresh:
            # All cache files are fresh.  Check that no dependency's
            # cache file is newer than any scc node's cache file.
            oldest_in_scc = min(graph[id].xmeta.data_mtime for id in scc)
            viable = {id for id in stale_deps if graph[id].meta is not None}
            newest_in_deps = 0 if not viable else max(graph[dep].xmeta.data_mtime
                                                      for dep in viable)
            if manager.options.verbosity &gt;= 3:  # Dump all mtimes for extreme debugging.
                all_ids = sorted(ascc | viable, key=lambda id: graph[id].xmeta.data_mtime)
                for id in all_ids:
                    if id in scc:
                        if graph[id].xmeta.data_mtime &lt; newest_in_deps:
                            key = "*id:"
                        else:
                            key = "id:"
                    else:
                        if graph[id].xmeta.data_mtime &gt; oldest_in_scc:
                            key = "+dep:"
                        else:
                            key = "dep:"
                    manager.trace(" %5s %.0f %s" % (key, graph[id].xmeta.data_mtime, id))
            # If equal, give the benefit of the doubt, due to 1-sec time granularity
            # (on some platforms).
            if oldest_in_scc &lt; newest_in_deps:
                fresh = False
                fresh_msg = f"out of date by {newest_in_deps - oldest_in_scc:.0f} seconds"
            else:
                fresh_msg = "fresh"
        elif undeps:
            fresh_msg = f"stale due to changed suppression ({' '.join(sorted(undeps))})"
        elif stale_scc:
            fresh_msg = "inherently stale"
            if stale_scc != ascc:
                fresh_msg += f" ({' '.join(sorted(stale_scc))})"
            if stale_deps:
                fresh_msg += f" with stale deps ({' '.join(sorted(stale_deps))})"
        else:
            fresh_msg = f"stale due to deps ({' '.join(sorted(stale_deps))})"

        # Initialize transitive_error for all SCC members from union
        # of transitive_error of dependencies.
        if any(graph[dep].transitive_error for dep in deps if dep in graph):
            for id in scc:
                graph[id].transitive_error = True

        scc_str = " ".join(scc)
        if fresh:
            manager.trace(f"Queuing {fresh_msg} SCC ({scc_str})")
            fresh_scc_queue.append(scc)
        else:
            if len(fresh_scc_queue) &gt; 0:
                manager.log(f"Processing {len(fresh_scc_queue)} queued fresh SCCs")
                # Defer processing fresh SCCs until we actually run into a stale SCC
                # and need the earlier modules to be loaded.
                #
                # Note that `process_graph` may end with us not having processed every
                # single fresh SCC. This is intentional -- we don't need those modules
                # loaded if there are no more stale SCCs to be rechecked.
                #
                # Also note we shouldn't have to worry about transitive_error here,
                # since modules with transitive errors aren't written to the cache,
                # and if any dependencies were changed, this SCC would be stale.
                # (Also, in quick_and_dirty mode we don't care about transitive errors.)
                #
                # TODO: see if it's possible to determine if we need to process only a
                # _subset_ of the past SCCs instead of having to process them all.
                for prev_scc in fresh_scc_queue:
                    process_fresh_modules(graph, prev_scc, manager)
                fresh_scc_queue = []
            size = len(scc)
            if size == 1:
                manager.log(f"Processing SCC singleton ({scc_str}) as {fresh_msg}")
            else:
                manager.log("Processing SCC of size %d (%s) as %s" % (size, scc_str, fresh_msg))
            process_stale_scc(graph, scc, manager)

    sccs_left = len(fresh_scc_queue)
    nodes_left = sum(len(scc) for scc in fresh_scc_queue)
    manager.add_stats(sccs_left=sccs_left, nodes_left=nodes_left)
    if sccs_left:
        manager.log("{} fresh SCCs ({} nodes) left in queue (and will remain unprocessed)"
                    .format(sccs_left, nodes_left))
        manager.trace(str(fresh_scc_queue))
    else:
        manager.log("No fresh SCCs left in queue")


</t>
<t tx="ekr.20220525082933.329">def order_ascc(graph: Graph, ascc: AbstractSet[str], pri_max: int = PRI_ALL) -&gt; List[str]:
    """Come up with the ideal processing order within an SCC.

    Using the priorities assigned by all_imported_modules_in_file(),
    try to reduce the cycle to a DAG, by omitting arcs representing
    dependencies of lower priority.

    In the simplest case, if we have A &lt;--&gt; B where A has a top-level
    "import B" (medium priority) but B only has the reverse "import A"
    inside a function (low priority), we turn the cycle into a DAG by
    dropping the B --&gt; A arc, which leaves only A --&gt; B.

    If all arcs have the same priority, we fall back to sorting by
    reverse global order (the order in which modules were first
    encountered).

    The algorithm is recursive, as follows: when as arcs of different
    priorities are present, drop all arcs of the lowest priority,
    identify SCCs in the resulting graph, and apply the algorithm to
    each SCC thus found.  The recursion is bounded because at each
    recursion the spread in priorities is (at least) one less.

    In practice there are only a few priority levels (less than a
    dozen) and in the worst case we just carry out the same algorithm
    for finding SCCs N times.  Thus the complexity is no worse than
    the complexity of the original SCC-finding algorithm -- see
    strongly_connected_components() below for a reference.
    """
    if len(ascc) == 1:
        return [s for s in ascc]
    pri_spread = set()
    for id in ascc:
        state = graph[id]
        for dep in state.dependencies:
            if dep in ascc:
                pri = state.priorities.get(dep, PRI_HIGH)
                if pri &lt; pri_max:
                    pri_spread.add(pri)
    if len(pri_spread) == 1:
        # Filtered dependencies are uniform -- order by global order.
        return sorted(ascc, key=lambda id: -graph[id].order)
    pri_max = max(pri_spread)
    sccs = sorted_components(graph, ascc, pri_max)
    # The recursion is bounded by the len(pri_spread) check above.
    return [s for ss in sccs for s in order_ascc(graph, ss, pri_max)]


</t>
<t tx="ekr.20220525082933.33">def report_most_common(chunks: List[JsonDict], amount: Optional[int] = None) -&gt; None:
    report_counter(Counter(str(chunk) for chunk in chunks), amount)


</t>
<t tx="ekr.20220525082933.330">def process_fresh_modules(graph: Graph, modules: List[str], manager: BuildManager) -&gt; None:
    """Process the modules in one group of modules from their cached data.

    This can be used to process an SCC of modules
    This involves loading the tree from JSON and then doing various cleanups.
    """
    t0 = time.time()
    for id in modules:
        graph[id].load_tree()
    t1 = time.time()
    for id in modules:
        graph[id].fix_cross_refs()
    t2 = time.time()
    manager.add_stats(process_fresh_time=t2 - t0, load_tree_time=t1 - t0)


</t>
<t tx="ekr.20220525082933.331">def process_stale_scc(graph: Graph, scc: List[str], manager: BuildManager) -&gt; None:
    """Process the modules in one SCC from source code.

    Exception: If quick_and_dirty is set, use the cache for fresh modules.
    """
    stale = scc
    for id in stale:
        # We may already have parsed the module, or not.
        # If the former, parse_file() is a no-op.
        graph[id].parse_file()
    if 'typing' in scc:
        # For historical reasons we need to manually add typing aliases
        # for built-in generic collections, see docstring of
        # SemanticAnalyzerPass2.add_builtin_aliases for details.
        typing_mod = graph['typing'].tree
        assert typing_mod, "The typing module was not parsed"
    mypy.semanal_main.semantic_analysis_for_scc(graph, scc, manager.errors)

    # Track what modules aren't yet done so we can finish them as soon
    # as possible, saving memory.
    unfinished_modules = set(stale)
    for id in stale:
        graph[id].type_check_first_pass()
        if not graph[id].type_checker().deferred_nodes:
            unfinished_modules.discard(id)
            graph[id].finish_passes()

    while unfinished_modules:
        for id in stale:
            if id not in unfinished_modules:
                continue
            if not graph[id].type_check_second_pass():
                unfinished_modules.discard(id)
                graph[id].finish_passes()
    for id in stale:
        graph[id].generate_unused_ignore_notes()
        graph[id].generate_ignore_without_code_notes()
    if any(manager.errors.is_errors_for_file(graph[id].xpath) for id in stale):
        for id in stale:
            graph[id].transitive_error = True
    for id in stale:
        manager.flush_errors(manager.errors.file_messages(graph[id].xpath), False)
        graph[id].write_cache()
        graph[id].mark_as_rechecked()


</t>
<t tx="ekr.20220525082933.332">def sorted_components(graph: Graph,
                      vertices: Optional[AbstractSet[str]] = None,
                      pri_max: int = PRI_ALL) -&gt; List[AbstractSet[str]]:
    """Return the graph's SCCs, topologically sorted by dependencies.

    The sort order is from leaves (nodes without dependencies) to
    roots (nodes on which no other nodes depend).

    This works for a subset of the full dependency graph too;
    dependencies that aren't present in graph.keys() are ignored.
    """
    # Compute SCCs.
    if vertices is None:
        vertices = set(graph)
    edges = {id: deps_filtered(graph, vertices, id, pri_max) for id in vertices}
    sccs = list(strongly_connected_components(vertices, edges))
    # Topsort.
    sccsmap = {id: frozenset(scc) for scc in sccs for id in scc}
    data: Dict[AbstractSet[str], Set[AbstractSet[str]]] = {}
    for scc in sccs:
        deps: Set[AbstractSet[str]] = set()
        for id in scc:
            deps.update(sccsmap[x] for x in deps_filtered(graph, vertices, id, pri_max))
        data[frozenset(scc)] = deps
    res = []
    for ready in topsort(data):
        # Sort the sets in ready by reversed smallest State.order.  Examples:
        #
        # - If ready is [{x}, {y}], x.order == 1, y.order == 2, we get
        #   [{y}, {x}].
        #
        # - If ready is [{a, b}, {c, d}], a.order == 1, b.order == 3,
        #   c.order == 2, d.order == 4, the sort keys become [1, 2]
        #   and the result is [{c, d}, {a, b}].
        res.extend(sorted(ready,
                          key=lambda scc: -min(graph[id].order for id in scc)))
    return res


</t>
<t tx="ekr.20220525082933.333">def deps_filtered(graph: Graph, vertices: AbstractSet[str], id: str, pri_max: int) -&gt; List[str]:
    """Filter dependencies for id with pri &lt; pri_max."""
    if id not in vertices:
        return []
    state = graph[id]
    return [dep
            for dep in state.dependencies
            if dep in vertices and state.priorities.get(dep, PRI_HIGH) &lt; pri_max]


</t>
<t tx="ekr.20220525082933.334">def strongly_connected_components(vertices: AbstractSet[str],
                                  edges: Dict[str, List[str]]) -&gt; Iterator[Set[str]]:
    """Compute Strongly Connected Components of a directed graph.

    Args:
      vertices: the labels for the vertices
      edges: for each vertex, gives the target vertices of its outgoing edges

    Returns:
      An iterator yielding strongly connected components, each
      represented as a set of vertices.  Each input vertex will occur
      exactly once; vertices not part of a SCC are returned as
      singleton sets.

    From http://code.activestate.com/recipes/578507/.
    """
    identified: Set[str] = set()
    stack: List[str] = []
    index: Dict[str, int] = {}
    boundaries: List[int] = []

    @others
    for v in vertices:
        if v not in index:
            yield from dfs(v)


</t>
<t tx="ekr.20220525082933.335">def dfs(v: str) -&gt; Iterator[Set[str]]:
    index[v] = len(stack)
    stack.append(v)
    boundaries.append(index[v])

    for w in edges[v]:
        if w not in index:
            yield from dfs(w)
        elif w not in identified:
            while index[w] &lt; boundaries[-1]:
                boundaries.pop()

    if boundaries[-1] == index[v]:
        boundaries.pop()
        scc = set(stack[index[v]:])
        del stack[index[v]:]
        identified.update(scc)
        yield scc

</t>
<t tx="ekr.20220525082933.336">T = TypeVar("T")


</t>
<t tx="ekr.20220525082933.337">def topsort(data: Dict[T, Set[T]]) -&gt; Iterable[Set[T]]:
    """Topological sort.

    Args:
      data: A map from vertices to all vertices that it has an edge
            connecting it to.  NOTE: This data structure
            is modified in place -- for normalization purposes,
            self-dependencies are removed and entries representing
            orphans are added.

    Returns:
      An iterator yielding sets of vertices that have an equivalent
      ordering.

    Example:
      Suppose the input has the following structure:

        {A: {B, C}, B: {D}, C: {D}}

      This is normalized to:

        {A: {B, C}, B: {D}, C: {D}, D: {}}

      The algorithm will yield the following values:

        {D}
        {B, C}
        {A}

    From http://code.activestate.com/recipes/577413/.
    """
    # TODO: Use a faster algorithm?
    for k, v in data.items():
        v.discard(k)  # Ignore self dependencies.
    for item in set.union(*data.values()) - set(data.keys()):
        data[item] = set()
    while True:
        ready = {item for item, dep in data.items() if not dep}
        if not ready:
            break
        yield ready
        data = {item: (dep - ready)
                for item, dep in data.items()
                if item not in ready}
    assert not data, f"A cyclic dependency exists amongst {data!r}"


</t>
<t tx="ekr.20220525082933.338">def missing_stubs_file(cache_dir: str) -&gt; str:
    return os.path.join(cache_dir, 'missing_stubs')


</t>
<t tx="ekr.20220525082933.339">def record_missing_stub_packages(cache_dir: str, missing_stub_packages: Set[str]) -&gt; None:
    """Write a file containing missing stub packages.

    This allows a subsequent "mypy --install-types" run (without other arguments)
    to install missing stub packages.
    """
    fnam = missing_stubs_file(cache_dir)
    if missing_stub_packages:
        with open(fnam, 'w') as f:
            for pkg in sorted(missing_stub_packages):
                f.write(f'{pkg}\n')
    else:
        if os.path.isfile(fnam):
            os.remove(fnam)
</t>
<t tx="ekr.20220525082933.34">def compress(chunk: JsonDict) -&gt; JsonDict:
    cache = {}  # type: Dict[int, JsonDict]
    counter = 0
    @others
    out = helper(chunk)
    return out

</t>
<t tx="ekr.20220525082933.340">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Mypy type checker."""

import itertools
import fnmatch
from collections import defaultdict
from contextlib import contextmanager

from typing import (
    Any, Dict, Set, List, cast, Tuple, TypeVar, Union, Optional, NamedTuple, Iterator,
    Iterable, Sequence, Mapping, Generic, AbstractSet, Callable, overload
)
from typing_extensions import Final, TypeAlias as _TypeAlias

from mypy.backports import nullcontext
from mypy.errors import Errors, report_internal_error
from mypy.nodes import (
    SymbolTable, Statement, MypyFile, Var, Expression, Lvalue, Node,
    OverloadedFuncDef, FuncDef, FuncItem, FuncBase, TypeInfo,
    ClassDef, Block, AssignmentStmt, NameExpr, MemberExpr, IndexExpr,
    TupleExpr, ListExpr, ExpressionStmt, ReturnStmt, IfStmt,
    WhileStmt, OperatorAssignmentStmt, WithStmt, AssertStmt,
    RaiseStmt, TryStmt, ForStmt, DelStmt, CallExpr, IntExpr, StrExpr,
    UnicodeExpr, OpExpr, UnaryExpr, LambdaExpr, TempNode, SymbolTableNode,
    Context, Decorator, PrintStmt, BreakStmt, PassStmt, ContinueStmt,
    ComparisonExpr, StarExpr, EllipsisExpr, RefExpr, PromoteExpr,
    Import, ImportFrom, ImportAll, ImportBase, TypeAlias,
    ARG_POS, ARG_STAR, ARG_NAMED, LITERAL_TYPE, LDEF, MDEF, GDEF,
    CONTRAVARIANT, COVARIANT, INVARIANT, TypeVarExpr, AssignmentExpr,
    is_final_node, MatchStmt)
from mypy import nodes
from mypy import operators
from mypy.literals import literal, literal_hash, Key
from mypy.typeanal import has_any_from_unimported_type, check_for_explicit_any, make_optional_type
from mypy.types import (
    Type, AnyType, CallableType, FunctionLike, Overloaded, TupleType, TypedDictType,
    Instance, NoneType, strip_type, TypeType, TypeOfAny,
    UnionType, TypeVarId, TypeVarType, PartialType, DeletedType, UninhabitedType,
    is_named_instance, union_items, TypeQuery, LiteralType,
    is_optional, remove_optional, TypeTranslator, StarType, get_proper_type, ProperType,
    get_proper_types, is_literal_type, TypeAliasType, TypeGuardedType, ParamSpecType,
    OVERLOAD_NAMES,
)
from mypy.sametypes import is_same_type
from mypy.messages import (
    MessageBuilder, make_inferred_type_note, append_invariance_notes, pretty_seq,
    format_type, format_type_bare, format_type_distinctly, SUGGESTED_TEST_FIXTURES
)
import mypy.checkexpr
from mypy.checkmember import (
    MemberContext, analyze_member_access, analyze_descriptor_access,
    type_object_type,
    analyze_decorator_or_funcbase_access,
)
from mypy.checkpattern import PatternChecker
from mypy.semanal_enum import ENUM_BASES, ENUM_SPECIAL_PROPS
from mypy.typeops import (
    map_type_from_supertype, bind_self, erase_to_bound, make_simplified_union,
    erase_def_to_union_or_bound, erase_to_union_or_bound, coerce_to_literal,
    try_getting_str_literals_from_type, try_getting_int_literals_from_type,
    tuple_fallback, is_singleton_type, try_expanding_sum_type_to_union,
    true_only, false_only, function_type, get_type_vars, custom_special_method,
    is_literal_type_like,
)
from mypy import message_registry
from mypy.message_registry import ErrorMessage
from mypy.subtypes import (
    is_subtype, is_equivalent, is_proper_subtype, is_more_precise,
    restrict_subtype_away, is_callable_compatible,
    unify_generic_callable, find_member
)
from mypy.constraints import SUPERTYPE_OF
from mypy.maptype import map_instance_to_supertype
from mypy.typevars import fill_typevars, has_no_typevars, fill_typevars_with_any
from mypy.semanal import set_callable_name, refers_to_fullname
from mypy.mro import calculate_mro, MroError
from mypy.erasetype import erase_typevars, remove_instance_last_known_values, erase_type
from mypy.expandtype import expand_type, expand_type_by_instance
from mypy.visitor import NodeVisitor
from mypy.join import join_types
from mypy.treetransform import TransformVisitor
from mypy.binder import ConditionalTypeBinder, get_declaration
from mypy.meet import is_overlapping_erased_types, is_overlapping_types
from mypy.options import Options
from mypy.plugin import Plugin, CheckerPluginInterface
from mypy.sharedparse import BINARY_MAGIC_METHODS
from mypy.scope import Scope
from mypy import errorcodes as codes
from mypy.state import state
from mypy.traverser import has_return_statement, all_return_statements
from mypy.errorcodes import ErrorCode, UNUSED_AWAITABLE, UNUSED_COROUTINE
from mypy.util import is_typeshed_file, is_dunder, is_sunder

T = TypeVar('T')

DEFAULT_LAST_PASS: Final = 1  # Pass numbers start at 0

DeferredNodeType: _TypeAlias = Union[FuncDef, LambdaExpr, OverloadedFuncDef, Decorator]
FineGrainedDeferredNodeType: _TypeAlias = Union[FuncDef, MypyFile, OverloadedFuncDef]


@others
</t>
<t tx="ekr.20220525082933.341"># A node which is postponed to be processed during the next pass.
# In normal mode one can defer functions and methods (also decorated and/or overloaded)
# and lambda expressions. Nested functions can't be deferred -- only top-level functions
# and methods of classes not defined within a function can be deferred.
class DeferredNode(NamedTuple):
    node: DeferredNodeType
    # And its TypeInfo (for semantic analysis self type handling
    active_typeinfo: Optional[TypeInfo]


</t>
<t tx="ekr.20220525082933.342"># Same as above, but for fine-grained mode targets. Only top-level functions/methods
# and module top levels are allowed as such.
class FineGrainedDeferredNode(NamedTuple):
    node: FineGrainedDeferredNodeType
    active_typeinfo: Optional[TypeInfo]


</t>
<t tx="ekr.20220525082933.343"># Data structure returned by find_isinstance_check representing
# information learned from the truth or falsehood of a condition.  The
# dict maps nodes representing expressions like 'a[0].x' to their
# refined types under the assumption that the condition has a
# particular truth value. A value of None means that the condition can
# never have that truth value.

# NB: The keys of this dict are nodes in the original source program,
# which are compared by reference equality--effectively, being *the
# same* expression of the program, not just two identical expressions
# (such as two references to the same variable). TODO: it would
# probably be better to have the dict keyed by the nodes' literal_hash
# field instead.
TypeMap: _TypeAlias = Optional[Dict[Expression, Type]]


</t>
<t tx="ekr.20220525082933.344"># An object that represents either a precise type or a type with an upper bound;
# it is important for correct type inference with isinstance.
class TypeRange(NamedTuple):
    item: Type
    is_upper_bound: bool  # False =&gt; precise type


</t>
<t tx="ekr.20220525082933.345"># Keeps track of partial types in a single scope. In fine-grained incremental
# mode partial types initially defined at the top level cannot be completed in
# a function, and we use the 'is_function' attribute to enforce this.
class PartialTypeScope(NamedTuple):
    map: Dict[Var, Context]
    is_function: bool
    is_local: bool


</t>
<t tx="ekr.20220525082933.346">class TypeChecker(NodeVisitor[None], CheckerPluginInterface):
    """Mypy type checker.

    Type check mypy source files that have been semantically analyzed.

    You must create a separate instance for each source file.
    """

    # Are we type checking a stub?
    is_stub = False
    # Error message reporter
    errors: Errors
    # Utility for generating messages
    msg: MessageBuilder
    # Types of type checked nodes. The first item is the "master" type
    # map that will store the final, exported types. Additional items
    # are temporary type maps used during type inference, and these
    # will be eventually popped and either discarded or merged into
    # the master type map.
    #
    # Avoid accessing this directly, but prefer the lookup_type(),
    # has_type() etc. helpers instead.
    _type_maps: List[Dict[Expression, Type]]

    # Helper for managing conditional types
    binder: ConditionalTypeBinder
    # Helper for type checking expressions
    expr_checker: mypy.checkexpr.ExpressionChecker

    pattern_checker: PatternChecker

    tscope: Scope
    scope: "CheckerScope"
    # Stack of function return types
    return_types: List[Type]
    # Flags; true for dynamically typed functions
    dynamic_funcs: List[bool]
    # Stack of collections of variables with partial types
    partial_types: List[PartialTypeScope]
    # Vars for which partial type errors are already reported
    # (to avoid logically duplicate errors with different error context).
    partial_reported: Set[Var]
    globals: SymbolTable
    modules: Dict[str, MypyFile]
    # Nodes that couldn't be checked because some types weren't available. We'll run
    # another pass and try these again.
    deferred_nodes: List[DeferredNode]
    # Type checking pass number (0 = first pass)
    pass_num = 0
    # Last pass number to take
    last_pass = DEFAULT_LAST_PASS
    # Have we deferred the current function? If yes, don't infer additional
    # types during this pass within the function.
    current_node_deferred = False
    # Is this file a typeshed stub?
    is_typeshed_stub = False
    # Should strict Optional-related errors be suppressed in this file?
    suppress_none_errors = False  # TODO: Get it from options instead
    options: Options
    # Used for collecting inferred attribute types so that they can be checked
    # for consistency.
    inferred_attribute_types: Optional[Dict[Var, Type]] = None
    # Don't infer partial None types if we are processing assignment from Union
    no_partial_types: bool = False

    # The set of all dependencies (suppressed or not) that this module accesses, either
    # directly or indirectly.
    module_refs: Set[str]

    # A map from variable nodes to a snapshot of the frame ids of the
    # frames that were active when the variable was declared. This can
    # be used to determine nearest common ancestor frame of a variable's
    # declaration and the current frame, which lets us determine if it
    # was declared in a different branch of the same `if` statement
    # (if that frame is a conditional_frame).
    var_decl_frames: Dict[Var, Set[int]]

    # Plugin that provides special type checking rules for specific library
    # functions such as open(), etc.
    plugin: Plugin

    @others
</t>
<t tx="ekr.20220525082933.347">def __init__(self, errors: Errors, modules: Dict[str, MypyFile], options: Options,
             tree: MypyFile, path: str, plugin: Plugin) -&gt; None:
    """Construct a type checker.

    Use errors to report type check errors.
    """
    self.errors = errors
    self.modules = modules
    self.options = options
    self.tree = tree
    self.path = path
    self.msg = MessageBuilder(errors, modules)
    self.plugin = plugin
    self.expr_checker = mypy.checkexpr.ExpressionChecker(self, self.msg, self.plugin)
    self.pattern_checker = PatternChecker(self, self.msg, self.plugin)
    self.tscope = Scope()
    self.scope = CheckerScope(tree)
    self.binder = ConditionalTypeBinder()
    self.globals = tree.names
    self.return_types = []
    self.dynamic_funcs = []
    self.partial_types = []
    self.partial_reported = set()
    self.var_decl_frames = {}
    self.deferred_nodes = []
    self._type_maps = [{}]
    self.module_refs = set()
    self.pass_num = 0
    self.current_node_deferred = False
    self.is_stub = tree.is_stub
    self.is_typeshed_stub = is_typeshed_file(path)
    self.inferred_attribute_types = None
    if options.strict_optional_whitelist is None:
        self.suppress_none_errors = not options.show_none_errors
    else:
        self.suppress_none_errors = not any(fnmatch.fnmatch(path, pattern)
                                            for pattern
                                            in options.strict_optional_whitelist)
    # If True, process function definitions. If False, don't. This is used
    # for processing module top levels in fine-grained incremental mode.
    self.recurse_into_functions = True
    # This internal flag is used to track whether we a currently type-checking
    # a final declaration (assignment), so that some errors should be suppressed.
    # Should not be set manually, use get_final_context/enter_final_context instead.
    # NOTE: we use the context manager to avoid "threading" an additional `is_final_def`
    # argument through various `checker` and `checkmember` functions.
    self._is_final_def = False

</t>
<t tx="ekr.20220525082933.348">@property
def type_context(self) -&gt; List[Optional[Type]]:
    return self.expr_checker.type_context

</t>
<t tx="ekr.20220525082933.349">def reset(self) -&gt; None:
    """Cleanup stale state that might be left over from a typechecking run.

    This allows us to reuse TypeChecker objects in fine-grained
    incremental mode.
    """
    # TODO: verify this is still actually worth it over creating new checkers
    self.partial_reported.clear()
    self.module_refs.clear()
    self.binder = ConditionalTypeBinder()
    self._type_maps[1:] = []
    self._type_maps[0].clear()
    self.temp_type_map = None
    self.expr_checker.reset()

    assert self.inferred_attribute_types is None
    assert self.partial_types == []
    assert self.deferred_nodes == []
    assert len(self.scope.stack) == 1
    assert self.partial_types == []

</t>
<t tx="ekr.20220525082933.35">def helper(chunk: Any) -&gt; Any:
    nonlocal counter
    if not isinstance(chunk, dict):
        return chunk

    if len(chunk) &lt;= 2:
        return chunk
    id = hash(str(chunk))

    if id in cache:
        return cache[id]
    else:
        cache[id] = {'.id': counter}
        chunk['.cache_id'] = counter
        counter += 1

    for name in sorted(chunk.keys()):
        value = chunk[name]
        if isinstance(value, list):
            chunk[name] = [helper(child) for child in value]
        elif isinstance(value, dict):
            chunk[name] = helper(value)

    return chunk
</t>
<t tx="ekr.20220525082933.350">def check_first_pass(self) -&gt; None:
    """Type check the entire file, but defer functions with unresolved references.

    Unresolved references are forward references to variables
    whose types haven't been inferred yet.  They may occur later
    in the same file or in a different file that's being processed
    later (usually due to an import cycle).

    Deferred functions will be processed by check_second_pass().
    """
    self.recurse_into_functions = True
    with state.strict_optional_set(self.options.strict_optional):
        self.errors.set_file(self.path, self.tree.fullname, scope=self.tscope)
        with self.tscope.module_scope(self.tree.fullname):
            with self.enter_partial_types(), self.binder.top_frame_context():
                for d in self.tree.defs:
                    if (self.binder.is_unreachable()
                            and self.should_report_unreachable_issues()
                            and not self.is_raising_or_empty(d)):
                        self.msg.unreachable_statement(d)
                        break
                    self.accept(d)

            assert not self.current_node_deferred

            all_ = self.globals.get('__all__')
            if all_ is not None and all_.type is not None:
                all_node = all_.node
                assert all_node is not None
                seq_str = self.named_generic_type('typing.Sequence',
                                                  [self.named_type('builtins.str')])
                if self.options.python_version[0] &lt; 3:
                    seq_str = self.named_generic_type('typing.Sequence',
                                                      [self.named_type('builtins.unicode')])
                if not is_subtype(all_.type, seq_str):
                    str_seq_s, all_s = format_type_distinctly(seq_str, all_.type)
                    self.fail(message_registry.ALL_MUST_BE_SEQ_STR.format(str_seq_s, all_s),
                              all_node)

</t>
<t tx="ekr.20220525082933.351">def check_second_pass(self,
                      todo: Optional[Sequence[Union[DeferredNode,
                                                    FineGrainedDeferredNode]]] = None
                      ) -&gt; bool:
    """Run second or following pass of type checking.

    This goes through deferred nodes, returning True if there were any.
    """
    self.recurse_into_functions = True
    with state.strict_optional_set(self.options.strict_optional):
        if not todo and not self.deferred_nodes:
            return False
        self.errors.set_file(self.path, self.tree.fullname, scope=self.tscope)
        with self.tscope.module_scope(self.tree.fullname):
            self.pass_num += 1
            if not todo:
                todo = self.deferred_nodes
            else:
                assert not self.deferred_nodes
            self.deferred_nodes = []
            done: Set[Union[DeferredNodeType, FineGrainedDeferredNodeType]] = set()
            for node, active_typeinfo in todo:
                if node in done:
                    continue
                # This is useful for debugging:
                # print("XXX in pass %d, class %s, function %s" %
                #       (self.pass_num, type_name, node.fullname or node.name))
                done.add(node)
                with self.tscope.class_scope(active_typeinfo) if active_typeinfo \
                        else nullcontext():
                    with self.scope.push_class(active_typeinfo) if active_typeinfo \
                            else nullcontext():
                        self.check_partial(node)
        return True

</t>
<t tx="ekr.20220525082933.352">def check_partial(self, node: Union[DeferredNodeType, FineGrainedDeferredNodeType]) -&gt; None:
    if isinstance(node, MypyFile):
        self.check_top_level(node)
    else:
        self.recurse_into_functions = True
        if isinstance(node, LambdaExpr):
            self.expr_checker.accept(node)
        else:
            self.accept(node)

</t>
<t tx="ekr.20220525082933.353">def check_top_level(self, node: MypyFile) -&gt; None:
    """Check only the top-level of a module, skipping function definitions."""
    self.recurse_into_functions = False
    with self.enter_partial_types():
        with self.binder.top_frame_context():
            for d in node.defs:
                d.accept(self)

    assert not self.current_node_deferred
    # TODO: Handle __all__

</t>
<t tx="ekr.20220525082933.354">def defer_node(self, node: DeferredNodeType, enclosing_class: Optional[TypeInfo]) -&gt; None:
    """Defer a node for processing during next type-checking pass.

    Args:
        node: function/method being deferred
        enclosing_class: for methods, the class where the method is defined
    NOTE: this can't handle nested functions/methods.
    """
    # We don't freeze the entire scope since only top-level functions and methods
    # can be deferred. Only module/class level scope information is needed.
    # Module-level scope information is preserved in the TypeChecker instance.
    self.deferred_nodes.append(DeferredNode(node, enclosing_class))

</t>
<t tx="ekr.20220525082933.355">def handle_cannot_determine_type(self, name: str, context: Context) -&gt; None:
    node = self.scope.top_non_lambda_function()
    if self.pass_num &lt; self.last_pass and isinstance(node, FuncDef):
        # Don't report an error yet. Just defer. Note that we don't defer
        # lambdas because they are coupled to the surrounding function
        # through the binder and the inferred type of the lambda, so it
        # would get messy.
        enclosing_class = self.scope.enclosing_class()
        self.defer_node(node, enclosing_class)
        # Set a marker so that we won't infer additional types in this
        # function. Any inferred types could be bogus, because there's at
        # least one type that we don't know.
        self.current_node_deferred = True
    else:
        self.msg.cannot_determine_type(name, context)

</t>
<t tx="ekr.20220525082933.356">def accept(self, stmt: Statement) -&gt; None:
    """Type check a node in the given type context."""
    try:
        stmt.accept(self)
    except Exception as err:
        report_internal_error(err, self.errors.file, stmt.line, self.errors, self.options)

</t>
<t tx="ekr.20220525082933.357">def accept_loop(self, body: Statement, else_body: Optional[Statement] = None, *,
                exit_condition: Optional[Expression] = None) -&gt; None:
    """Repeatedly type check a loop body until the frame doesn't change.
    If exit_condition is set, assume it must be False on exit from the loop.

    Then check the else_body.
    """
    # The outer frame accumulates the results of all iterations
    with self.binder.frame_context(can_skip=False, conditional_frame=True):
        while True:
            with self.binder.frame_context(can_skip=True,
                                           break_frame=2, continue_frame=1):
                self.accept(body)
            if not self.binder.last_pop_changed:
                break
        if exit_condition:
            _, else_map = self.find_isinstance_check(exit_condition)
            self.push_type_map(else_map)
        if else_body:
            self.accept(else_body)

</t>
<t tx="ekr.20220525082933.358">#
# Definitions
#

</t>
<t tx="ekr.20220525082933.359">def visit_overloaded_func_def(self, defn: OverloadedFuncDef) -&gt; None:
    if not self.recurse_into_functions:
        return
    with self.tscope.function_scope(defn):
        self._visit_overloaded_func_def(defn)

</t>
<t tx="ekr.20220525082933.36">def decompress(chunk: JsonDict) -&gt; JsonDict:
    cache = {}  # type: Dict[int, JsonDict]
    @others
    return helper(chunk)




</t>
<t tx="ekr.20220525082933.360">def _visit_overloaded_func_def(self, defn: OverloadedFuncDef) -&gt; None:
    num_abstract = 0
    if not defn.items:
        # In this case we have already complained about none of these being
        # valid overloads.
        return None
    if len(defn.items) == 1:
        self.fail(message_registry.MULTIPLE_OVERLOADS_REQUIRED, defn)

    if defn.is_property:
        # HACK: Infer the type of the property.
        self.visit_decorator(cast(Decorator, defn.items[0]))
    for fdef in defn.items:
        assert isinstance(fdef, Decorator)
        self.check_func_item(fdef.func, name=fdef.func.name)
        if fdef.func.is_abstract:
            num_abstract += 1
    if num_abstract not in (0, len(defn.items)):
        self.fail(message_registry.INCONSISTENT_ABSTRACT_OVERLOAD, defn)
    if defn.impl:
        defn.impl.accept(self)
    if defn.info:
        self.check_method_override(defn)
        self.check_inplace_operator_method(defn)
    if not defn.is_property:
        self.check_overlapping_overloads(defn)
    return None

</t>
<t tx="ekr.20220525082933.361">def check_overlapping_overloads(self, defn: OverloadedFuncDef) -&gt; None:
    # At this point we should have set the impl already, and all remaining
    # items are decorators

    if self.msg.errors.file in self.msg.errors.ignored_files:
        # This is a little hacky, however, the quadratic check here is really expensive, this
        # method has no side effects, so we should skip it if we aren't going to report
        # anything. In some other places we swallow errors in stubs, but this error is very
        # useful for stubs!
        return

    # Compute some info about the implementation (if it exists) for use below
    impl_type: Optional[CallableType] = None
    if defn.impl:
        if isinstance(defn.impl, FuncDef):
            inner_type: Optional[Type] = defn.impl.type
        elif isinstance(defn.impl, Decorator):
            inner_type = defn.impl.var.type
        else:
            assert False, "Impl isn't the right type"

        # This can happen if we've got an overload with a different
        # decorator or if the implementation is untyped -- we gave up on the types.
        inner_type = get_proper_type(inner_type)
        if inner_type is not None and not isinstance(inner_type, AnyType):
            if isinstance(inner_type, CallableType):
                impl_type = inner_type
            elif isinstance(inner_type, Instance):
                inner_call = get_proper_type(
                    analyze_member_access(
                        name='__call__',
                        typ=inner_type,
                        context=defn.impl,
                        is_lvalue=False,
                        is_super=False,
                        is_operator=True,
                        msg=self.msg,
                        original_type=inner_type,
                        chk=self,
                    ),
                )
                if isinstance(inner_call, CallableType):
                    impl_type = inner_call
            if impl_type is None:
                self.msg.not_callable(inner_type, defn.impl)

    is_descriptor_get = defn.info and defn.name == "__get__"
    for i, item in enumerate(defn.items):
        # TODO overloads involving decorators
        assert isinstance(item, Decorator)
        sig1 = self.function_type(item.func)
        assert isinstance(sig1, CallableType)

        for j, item2 in enumerate(defn.items[i + 1:]):
            assert isinstance(item2, Decorator)
            sig2 = self.function_type(item2.func)
            assert isinstance(sig2, CallableType)

            if not are_argument_counts_overlapping(sig1, sig2):
                continue

            if overload_can_never_match(sig1, sig2):
                self.msg.overloaded_signature_will_never_match(
                    i + 1, i + j + 2, item2.func)
            elif not is_descriptor_get:
                # Note: we force mypy to check overload signatures in strict-optional mode
                # so we don't incorrectly report errors when a user tries typing an overload
                # that happens to have a 'if the argument is None' fallback.
                #
                # For example, the following is fine in strict-optional mode but would throw
                # the unsafe overlap error when strict-optional is disabled:
                #
                #     @overload
                #     def foo(x: None) -&gt; int: ...
                #     @overload
                #     def foo(x: str) -&gt; str: ...
                #
                # See Python 2's map function for a concrete example of this kind of overload.
                with state.strict_optional_set(True):
                    if is_unsafe_overlapping_overload_signatures(sig1, sig2):
                        self.msg.overloaded_signatures_overlap(
                            i + 1, i + j + 2, item.func)

        if impl_type is not None:
            assert defn.impl is not None

            # We perform a unification step that's very similar to what
            # 'is_callable_compatible' would have done if we had set
            # 'unify_generics' to True -- the only difference is that
            # we check and see if the impl_type's return value is a
            # *supertype* of the overload alternative, not a *subtype*.
            #
            # This is to match the direction the implementation's return
            # needs to be compatible in.
            if impl_type.variables:
                impl = unify_generic_callable(impl_type, sig1,
                                              ignore_return=False,
                                              return_constraint_direction=SUPERTYPE_OF)
                if impl is None:
                    self.msg.overloaded_signatures_typevar_specific(i + 1, defn.impl)
                    continue
            else:
                impl = impl_type

            # Prevent extra noise from inconsistent use of @classmethod by copying
            # the first arg from the method being checked against.
            if sig1.arg_types and defn.info:
                impl = impl.copy_modified(arg_types=[sig1.arg_types[0]] + impl.arg_types[1:])

            # Is the overload alternative's arguments subtypes of the implementation's?
            if not is_callable_compatible(impl, sig1,
                                          is_compat=is_subtype_no_promote,
                                          ignore_return=True):
                self.msg.overloaded_signatures_arg_specific(i + 1, defn.impl)

            # Is the overload alternative's return type a subtype of the implementation's?
            if not (is_subtype_no_promote(sig1.ret_type, impl.ret_type) or
                    is_subtype_no_promote(impl.ret_type, sig1.ret_type)):
                self.msg.overloaded_signatures_ret_specific(i + 1, defn.impl)

</t>
<t tx="ekr.20220525082933.362"># Here's the scoop about generators and coroutines.
#
# There are two kinds of generators: classic generators (functions
# with `yield` or `yield from` in the body) and coroutines
# (functions declared with `async def`).  The latter are specified
# in PEP 492 and only available in Python &gt;= 3.5.
#
# Classic generators can be parameterized with three types:
# - ty is the Yield type (the type of y in `yield y`)
# - tc is the type reCeived by yield (the type of c in `c = yield`).
# - tr is the Return type (the type of r in `return r`)
#
# A classic generator must define a return type that's either
# `Generator[ty, tc, tr]`, Iterator[ty], or Iterable[ty] (or
# object or Any).  If tc/tr are not given, both are None.
#
# A coroutine must define a return type corresponding to tr; the
# other two are unconstrained.  The "external" return type (seen
# by the caller) is Awaitable[tr].
#
# In addition, there's the synthetic type AwaitableGenerator: it
# inherits from both Awaitable and Generator and can be used both
# in `yield from` and in `await`.  This type is set automatically
# for functions decorated with `@types.coroutine` or
# `@asyncio.coroutine`.  Its single parameter corresponds to tr.
#
# PEP 525 adds a new type, the asynchronous generator, which was
# first released in Python 3.6. Async generators are `async def`
# functions that can also `yield` values. They can be parameterized
# with two types, ty and tc, because they cannot return a value.
#
# There are several useful methods, each taking a type t and a
# flag c indicating whether it's for a generator or coroutine:
#
# - is_generator_return_type(t, c) returns whether t is a Generator,
#   Iterator, Iterable (if not c), or Awaitable (if c), or
#   AwaitableGenerator (regardless of c).
# - is_async_generator_return_type(t) returns whether t is an
#   AsyncGenerator.
# - get_generator_yield_type(t, c) returns ty.
# - get_generator_receive_type(t, c) returns tc.
# - get_generator_return_type(t, c) returns tr.

</t>
<t tx="ekr.20220525082933.363">def is_generator_return_type(self, typ: Type, is_coroutine: bool) -&gt; bool:
    """Is `typ` a valid type for a generator/coroutine?

    True if `typ` is a *supertype* of Generator or Awaitable.
    Also true it it's *exactly* AwaitableGenerator (modulo type parameters).
    """
    typ = get_proper_type(typ)
    if is_coroutine:
        # This means we're in Python 3.5 or later.
        at = self.named_generic_type('typing.Awaitable', [AnyType(TypeOfAny.special_form)])
        if is_subtype(at, typ):
            return True
    else:
        any_type = AnyType(TypeOfAny.special_form)
        gt = self.named_generic_type('typing.Generator', [any_type, any_type, any_type])
        if is_subtype(gt, typ):
            return True
    return isinstance(typ, Instance) and typ.type.fullname == 'typing.AwaitableGenerator'

</t>
<t tx="ekr.20220525082933.364">def is_async_generator_return_type(self, typ: Type) -&gt; bool:
    """Is `typ` a valid type for an async generator?

    True if `typ` is a supertype of AsyncGenerator.
    """
    try:
        any_type = AnyType(TypeOfAny.special_form)
        agt = self.named_generic_type('typing.AsyncGenerator', [any_type, any_type])
    except KeyError:
        # we're running on a version of typing that doesn't have AsyncGenerator yet
        return False
    return is_subtype(agt, typ)

</t>
<t tx="ekr.20220525082933.365">def get_generator_yield_type(self, return_type: Type, is_coroutine: bool) -&gt; Type:
    """Given the declared return type of a generator (t), return the type it yields (ty)."""
    return_type = get_proper_type(return_type)

    if isinstance(return_type, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=return_type)
    elif (not self.is_generator_return_type(return_type, is_coroutine)
            and not self.is_async_generator_return_type(return_type)):
        # If the function doesn't have a proper Generator (or
        # Awaitable) return type, anything is permissible.
        return AnyType(TypeOfAny.from_error)
    elif not isinstance(return_type, Instance):
        # Same as above, but written as a separate branch so the typechecker can understand.
        return AnyType(TypeOfAny.from_error)
    elif return_type.type.fullname == 'typing.Awaitable':
        # Awaitable: ty is Any.
        return AnyType(TypeOfAny.special_form)
    elif return_type.args:
        # AwaitableGenerator, Generator, AsyncGenerator, Iterator, or Iterable; ty is args[0].
        ret_type = return_type.args[0]
        # TODO not best fix, better have dedicated yield token
        return ret_type
    else:
        # If the function's declared supertype of Generator has no type
        # parameters (i.e. is `object`), then the yielded values can't
        # be accessed so any type is acceptable.  IOW, ty is Any.
        # (However, see https://github.com/python/mypy/issues/1933)
        return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082933.366">def get_generator_receive_type(self, return_type: Type, is_coroutine: bool) -&gt; Type:
    """Given a declared generator return type (t), return the type its yield receives (tc)."""
    return_type = get_proper_type(return_type)

    if isinstance(return_type, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=return_type)
    elif (not self.is_generator_return_type(return_type, is_coroutine)
            and not self.is_async_generator_return_type(return_type)):
        # If the function doesn't have a proper Generator (or
        # Awaitable) return type, anything is permissible.
        return AnyType(TypeOfAny.from_error)
    elif not isinstance(return_type, Instance):
        # Same as above, but written as a separate branch so the typechecker can understand.
        return AnyType(TypeOfAny.from_error)
    elif return_type.type.fullname == 'typing.Awaitable':
        # Awaitable, AwaitableGenerator: tc is Any.
        return AnyType(TypeOfAny.special_form)
    elif (return_type.type.fullname in ('typing.Generator', 'typing.AwaitableGenerator')
          and len(return_type.args) &gt;= 3):
        # Generator: tc is args[1].
        return return_type.args[1]
    elif return_type.type.fullname == 'typing.AsyncGenerator' and len(return_type.args) &gt;= 2:
        return return_type.args[1]
    else:
        # `return_type` is a supertype of Generator, so callers won't be able to send it
        # values.  IOW, tc is None.
        return NoneType()

</t>
<t tx="ekr.20220525082933.367">def get_coroutine_return_type(self, return_type: Type) -&gt; Type:
    return_type = get_proper_type(return_type)
    if isinstance(return_type, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=return_type)
    assert isinstance(return_type, Instance), "Should only be called on coroutine functions."
    # Note: return type is the 3rd type parameter of Coroutine.
    return return_type.args[2]

</t>
<t tx="ekr.20220525082933.368">def get_generator_return_type(self, return_type: Type, is_coroutine: bool) -&gt; Type:
    """Given the declared return type of a generator (t), return the type it returns (tr)."""
    return_type = get_proper_type(return_type)

    if isinstance(return_type, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=return_type)
    elif not self.is_generator_return_type(return_type, is_coroutine):
        # If the function doesn't have a proper Generator (or
        # Awaitable) return type, anything is permissible.
        return AnyType(TypeOfAny.from_error)
    elif not isinstance(return_type, Instance):
        # Same as above, but written as a separate branch so the typechecker can understand.
        return AnyType(TypeOfAny.from_error)
    elif return_type.type.fullname == 'typing.Awaitable' and len(return_type.args) == 1:
        # Awaitable: tr is args[0].
        return return_type.args[0]
    elif (return_type.type.fullname in ('typing.Generator', 'typing.AwaitableGenerator')
          and len(return_type.args) &gt;= 3):
        # AwaitableGenerator, Generator: tr is args[2].
        return return_type.args[2]
    else:
        # Supertype of Generator (Iterator, Iterable, object): tr is any.
        return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082933.369">def visit_func_def(self, defn: FuncDef) -&gt; None:
    if not self.recurse_into_functions:
        return
    with self.tscope.function_scope(defn):
        self._visit_func_def(defn)

</t>
<t tx="ekr.20220525082933.37">def helper(chunk: Any) -&gt; Any:
    if not isinstance(chunk, dict):
        return chunk
    if '.id' in chunk:
        return cache[chunk['.id']]

    counter = None
    if '.cache_id' in chunk:
        counter = chunk['.cache_id']
        del chunk['.cache_id']

    for name in sorted(chunk.keys()):
        value = chunk[name]
        if isinstance(value, list):
            chunk[name] = [helper(child) for child in value]
        elif isinstance(value, dict):
            chunk[name] = helper(value)

    if counter is not None:
        cache[counter] = chunk

    return chunk
</t>
<t tx="ekr.20220525082933.370">def _visit_func_def(self, defn: FuncDef) -&gt; None:
    """Type check a function definition."""
    self.check_func_item(defn, name=defn.name)
    if defn.info:
        if not defn.is_dynamic() and not defn.is_overload and not defn.is_decorated:
            # If the definition is the implementation for an
            # overload, the legality of the override has already
            # been typechecked, and decorated methods will be
            # checked when the decorator is.
            self.check_method_override(defn)
        self.check_inplace_operator_method(defn)
    if defn.original_def:
        # Override previous definition.
        new_type = self.function_type(defn)
        if isinstance(defn.original_def, FuncDef):
            # Function definition overrides function definition.
            if not is_same_type(new_type, self.function_type(defn.original_def)):
                self.msg.incompatible_conditional_function_def(defn)
        else:
            # Function definition overrides a variable initialized via assignment or a
            # decorated function.
            orig_type = defn.original_def.type
            if orig_type is None:
                # XXX This can be None, as happens in
                # test_testcheck_TypeCheckSuite.testRedefinedFunctionInTryWithElse
                self.msg.note("Internal mypy error checking function redefinition", defn)
                return
            if isinstance(orig_type, PartialType):
                if orig_type.type is None:
                    # Ah this is a partial type. Give it the type of the function.
                    orig_def = defn.original_def
                    if isinstance(orig_def, Decorator):
                        var = orig_def.var
                    else:
                        var = orig_def
                    partial_types = self.find_partial_types(var)
                    if partial_types is not None:
                        var.type = new_type
                        del partial_types[var]
                else:
                    # Trying to redefine something like partial empty list as function.
                    self.fail(message_registry.INCOMPATIBLE_REDEFINITION, defn)
            else:
                # TODO: Update conditional type binder.
                self.check_subtype(new_type, orig_type, defn,
                                   message_registry.INCOMPATIBLE_REDEFINITION,
                                   'redefinition with type',
                                   'original type')

</t>
<t tx="ekr.20220525082933.371">def check_func_item(self, defn: FuncItem,
                    type_override: Optional[CallableType] = None,
                    name: Optional[str] = None) -&gt; None:
    """Type check a function.

    If type_override is provided, use it as the function type.
    """
    self.dynamic_funcs.append(defn.is_dynamic() and not type_override)

    with self.enter_partial_types(is_function=True):
        typ = self.function_type(defn)
        if type_override:
            typ = type_override.copy_modified(line=typ.line, column=typ.column)
        if isinstance(typ, CallableType):
            with self.enter_attribute_inference_context():
                self.check_func_def(defn, typ, name)
        else:
            raise RuntimeError('Not supported')

    self.dynamic_funcs.pop()
    self.current_node_deferred = False

    if name == '__exit__':
        self.check__exit__return_type(defn)

</t>
<t tx="ekr.20220525082933.372">@contextmanager
def enter_attribute_inference_context(self) -&gt; Iterator[None]:
    old_types = self.inferred_attribute_types
    self.inferred_attribute_types = {}
    yield None
    self.inferred_attribute_types = old_types

</t>
<t tx="ekr.20220525082933.373">def check_func_def(self, defn: FuncItem, typ: CallableType, name: Optional[str]) -&gt; None:
    """Type check a function definition."""
    # Expand type variables with value restrictions to ordinary types.
    expanded = self.expand_typevars(defn, typ)
    for item, typ in expanded:
        old_binder = self.binder
        self.binder = ConditionalTypeBinder()
        with self.binder.top_frame_context():
            defn.expanded.append(item)

            # We may be checking a function definition or an anonymous
            # function. In the first case, set up another reference with the
            # precise type.
            if isinstance(item, FuncDef):
                fdef = item
                # Check if __init__ has an invalid, non-None return type.
                if (fdef.info and fdef.name in ('__init__', '__init_subclass__') and
                        not isinstance(get_proper_type(typ.ret_type), NoneType) and
                        not self.dynamic_funcs[-1]):
                    self.fail(message_registry.MUST_HAVE_NONE_RETURN_TYPE.format(fdef.name),
                              item)

                # Check validity of __new__ signature
                if fdef.info and fdef.name == '__new__':
                    self.check___new___signature(fdef, typ)

                self.check_for_missing_annotations(fdef)
                if self.options.disallow_any_unimported:
                    if fdef.type and isinstance(fdef.type, CallableType):
                        ret_type = fdef.type.ret_type
                        if has_any_from_unimported_type(ret_type):
                            self.msg.unimported_type_becomes_any("Return type", ret_type, fdef)
                        for idx, arg_type in enumerate(fdef.type.arg_types):
                            if has_any_from_unimported_type(arg_type):
                                prefix = f'Argument {idx + 1} to "{fdef.name}"'
                                self.msg.unimported_type_becomes_any(prefix, arg_type, fdef)
                check_for_explicit_any(fdef.type, self.options, self.is_typeshed_stub,
                                       self.msg, context=fdef)

            if name:  # Special method names
                if defn.info and self.is_reverse_op_method(name):
                    self.check_reverse_op_method(item, typ, name, defn)
                elif name in ('__getattr__', '__getattribute__'):
                    self.check_getattr_method(typ, defn, name)
                elif name == '__setattr__':
                    self.check_setattr_method(typ, defn)

            # Refuse contravariant return type variable
            if isinstance(typ.ret_type, TypeVarType):
                if typ.ret_type.variance == CONTRAVARIANT:
                    self.fail(message_registry.RETURN_TYPE_CANNOT_BE_CONTRAVARIANT,
                              typ.ret_type)

            # Check that Generator functions have the appropriate return type.
            if defn.is_generator:
                if defn.is_async_generator:
                    if not self.is_async_generator_return_type(typ.ret_type):
                        self.fail(message_registry.INVALID_RETURN_TYPE_FOR_ASYNC_GENERATOR,
                                  typ)
                else:
                    if not self.is_generator_return_type(typ.ret_type, defn.is_coroutine):
                        self.fail(message_registry.INVALID_RETURN_TYPE_FOR_GENERATOR, typ)

                # Python 2 generators aren't allowed to return values.
                orig_ret_type = get_proper_type(typ.ret_type)
                if (self.options.python_version[0] == 2 and
                        isinstance(orig_ret_type, Instance) and
                        orig_ret_type.type.fullname == 'typing.Generator'):
                    if not isinstance(get_proper_type(orig_ret_type.args[2]),
                                      (NoneType, AnyType)):
                        self.fail(message_registry.INVALID_GENERATOR_RETURN_ITEM_TYPE, typ)

            # Fix the type if decorated with `@types.coroutine` or `@asyncio.coroutine`.
            if defn.is_awaitable_coroutine:
                # Update the return type to AwaitableGenerator.
                # (This doesn't exist in typing.py, only in typing.pyi.)
                t = typ.ret_type
                c = defn.is_coroutine
                ty = self.get_generator_yield_type(t, c)
                tc = self.get_generator_receive_type(t, c)
                if c:
                    tr = self.get_coroutine_return_type(t)
                else:
                    tr = self.get_generator_return_type(t, c)
                ret_type = self.named_generic_type('typing.AwaitableGenerator',
                                                   [ty, tc, tr, t])
                typ = typ.copy_modified(ret_type=ret_type)
                defn.type = typ

            # Push return type.
            self.return_types.append(typ.ret_type)

            # Store argument types.
            for i in range(len(typ.arg_types)):
                arg_type = typ.arg_types[i]
                with self.scope.push_function(defn):
                    # We temporary push the definition to get the self type as
                    # visible from *inside* of this function/method.
                    ref_type: Optional[Type] = self.scope.active_self_type()
                if (isinstance(defn, FuncDef) and ref_type is not None and i == 0
                        and not defn.is_static
                        and typ.arg_kinds[0] not in [nodes.ARG_STAR, nodes.ARG_STAR2]):
                    isclass = defn.is_class or defn.name in ('__new__', '__init_subclass__')
                    if isclass:
                        ref_type = mypy.types.TypeType.make_normalized(ref_type)
                    erased = get_proper_type(erase_to_bound(arg_type))
                    if not is_subtype(ref_type, erased, ignore_type_params=True):
                        note = None
                        if (isinstance(erased, Instance) and erased.type.is_protocol or
                                isinstance(erased, TypeType) and
                                isinstance(erased.item, Instance) and
                                erased.item.type.is_protocol):
                            # We allow the explicit self-type to be not a supertype of
                            # the current class if it is a protocol. For such cases
                            # the consistency check will be performed at call sites.
                            msg = None
                        elif typ.arg_names[i] in {'self', 'cls'}:
                            if (self.options.python_version[0] &lt; 3
                                    and is_same_type(erased, arg_type) and not isclass):
                                msg = message_registry.INVALID_SELF_TYPE_OR_EXTRA_ARG
                                note = '(Hint: typically annotations omit the type for self)'
                            else:
                                msg = message_registry.ERASED_SELF_TYPE_NOT_SUPERTYPE.format(
                                    erased, ref_type)
                        else:
                            msg = message_registry.MISSING_OR_INVALID_SELF_TYPE
                        if msg:
                            self.fail(msg, defn)
                            if note:
                                self.note(note, defn)
                elif isinstance(arg_type, TypeVarType):
                    # Refuse covariant parameter type variables
                    # TODO: check recursively for inner type variables
                    if (
                        arg_type.variance == COVARIANT and
                        defn.name not in ('__init__', '__new__')
                    ):
                        ctx: Context = arg_type
                        if ctx.line &lt; 0:
                            ctx = typ
                        self.fail(message_registry.FUNCTION_PARAMETER_CANNOT_BE_COVARIANT, ctx)
                if typ.arg_kinds[i] == nodes.ARG_STAR:
                    if not isinstance(arg_type, ParamSpecType):
                        # builtins.tuple[T] is typing.Tuple[T, ...]
                        arg_type = self.named_generic_type('builtins.tuple',
                                                           [arg_type])
                elif typ.arg_kinds[i] == nodes.ARG_STAR2:
                    if not isinstance(arg_type, ParamSpecType):
                        arg_type = self.named_generic_type('builtins.dict',
                                                           [self.str_type(),
                                                            arg_type])
                item.arguments[i].variable.type = arg_type

            # Type check initialization expressions.
            body_is_trivial = self.is_trivial_body(defn.body)
            self.check_default_args(item, body_is_trivial)

        # Type check body in a new scope.
        with self.binder.top_frame_context():
            with self.scope.push_function(defn):
                # We suppress reachability warnings when we use TypeVars with value
                # restrictions: we only want to report a warning if a certain statement is
                # marked as being suppressed in *all* of the expansions, but we currently
                # have no good way of doing this.
                #
                # TODO: Find a way of working around this limitation
                if len(expanded) &gt;= 2:
                    self.binder.suppress_unreachable_warnings()
                self.accept(item.body)
            unreachable = self.binder.is_unreachable()

        if self.options.warn_no_return and not unreachable:
            if (defn.is_generator or
                    is_named_instance(self.return_types[-1], 'typing.AwaitableGenerator')):
                return_type = self.get_generator_return_type(self.return_types[-1],
                                                             defn.is_coroutine)
            elif defn.is_coroutine:
                return_type = self.get_coroutine_return_type(self.return_types[-1])
            else:
                return_type = self.return_types[-1]

            return_type = get_proper_type(return_type)
            if not isinstance(return_type, (NoneType, AnyType)) and not body_is_trivial:
                # Control flow fell off the end of a function that was
                # declared to return a non-None type and is not
                # entirely pass/Ellipsis/raise NotImplementedError.
                if isinstance(return_type, UninhabitedType):
                    # This is a NoReturn function
                    self.fail(message_registry.INVALID_IMPLICIT_RETURN, defn)
                else:
                    self.fail(message_registry.MISSING_RETURN_STATEMENT, defn)

        self.return_types.pop()

        self.binder = old_binder

</t>
<t tx="ekr.20220525082933.374">def check_default_args(self, item: FuncItem, body_is_trivial: bool) -&gt; None:
    for arg in item.arguments:
        if arg.initializer is None:
            continue
        if body_is_trivial and isinstance(arg.initializer, EllipsisExpr):
            continue
        name = arg.variable.name
        msg = 'Incompatible default for '
        if name.startswith('__tuple_arg_'):
            msg += f"tuple argument {name[12:]}"
        else:
            msg += f'argument "{name}"'
        self.check_simple_assignment(
            arg.variable.type,
            arg.initializer,
            context=arg.initializer,
            msg=msg,
            lvalue_name='argument',
            rvalue_name='default',
            code=codes.ASSIGNMENT)

</t>
<t tx="ekr.20220525082933.375">def is_forward_op_method(self, method_name: str) -&gt; bool:
    if self.options.python_version[0] == 2 and method_name == '__div__':
        return True
    else:
        return method_name in operators.reverse_op_methods

</t>
<t tx="ekr.20220525082933.376">def is_reverse_op_method(self, method_name: str) -&gt; bool:
    if self.options.python_version[0] == 2 and method_name == '__rdiv__':
        return True
    else:
        return method_name in operators.reverse_op_method_set

</t>
<t tx="ekr.20220525082933.377">def check_for_missing_annotations(self, fdef: FuncItem) -&gt; None:
    # Check for functions with unspecified/not fully specified types.
    def is_unannotated_any(t: Type) -&gt; bool:
        if not isinstance(t, ProperType):
            return False
        return isinstance(t, AnyType) and t.type_of_any == TypeOfAny.unannotated

    has_explicit_annotation = (isinstance(fdef.type, CallableType)
                               and any(not is_unannotated_any(t)
                                       for t in fdef.type.arg_types + [fdef.type.ret_type]))

    show_untyped = not self.is_typeshed_stub or self.options.warn_incomplete_stub
    check_incomplete_defs = self.options.disallow_incomplete_defs and has_explicit_annotation
    if show_untyped and (self.options.disallow_untyped_defs or check_incomplete_defs):
        if fdef.type is None and self.options.disallow_untyped_defs:
            if (not fdef.arguments or (len(fdef.arguments) == 1 and
                    (fdef.arg_names[0] == 'self' or fdef.arg_names[0] == 'cls'))):
                self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
                if not has_return_statement(fdef) and not fdef.is_generator:
                    self.note('Use "-&gt; None" if function does not return a value', fdef,
                              code=codes.NO_UNTYPED_DEF)
            else:
                self.fail(message_registry.FUNCTION_TYPE_EXPECTED, fdef)
        elif isinstance(fdef.type, CallableType):
            ret_type = get_proper_type(fdef.type.ret_type)
            if is_unannotated_any(ret_type):
                self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
            elif fdef.is_generator:
                if is_unannotated_any(self.get_generator_return_type(ret_type,
                                                                     fdef.is_coroutine)):
                    self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
            elif fdef.is_coroutine and isinstance(ret_type, Instance):
                if is_unannotated_any(self.get_coroutine_return_type(ret_type)):
                    self.fail(message_registry.RETURN_TYPE_EXPECTED, fdef)
            if any(is_unannotated_any(t) for t in fdef.type.arg_types):
                self.fail(message_registry.ARGUMENT_TYPE_EXPECTED, fdef)

</t>
<t tx="ekr.20220525082933.378">def check___new___signature(self, fdef: FuncDef, typ: CallableType) -&gt; None:
    self_type = fill_typevars_with_any(fdef.info)
    bound_type = bind_self(typ, self_type, is_classmethod=True)
    # Check that __new__ (after binding cls) returns an instance
    # type (or any).
    if isinstance(fdef.info, TypeInfo) and fdef.info.is_metaclass():
        # This is a metaclass, so it must return a new unrelated type.
        self.check_subtype(
            bound_type.ret_type,
            self.type_type(),
            fdef,
            message_registry.INVALID_NEW_TYPE,
            'returns',
            'but must return a subtype of'
        )
    elif not isinstance(get_proper_type(bound_type.ret_type),
                      (AnyType, Instance, TupleType)):
        self.fail(
            message_registry.NON_INSTANCE_NEW_TYPE.format(
                format_type(bound_type.ret_type)),
            fdef)
    else:
        # And that it returns a subtype of the class
        self.check_subtype(
            bound_type.ret_type,
            self_type,
            fdef,
            message_registry.INVALID_NEW_TYPE,
            'returns',
            'but must return a subtype of'
        )

</t>
<t tx="ekr.20220525082933.379">def is_trivial_body(self, block: Block) -&gt; bool:
    """Returns 'true' if the given body is "trivial" -- if it contains just a "pass",
    "..." (ellipsis), or "raise NotImplementedError()". A trivial body may also
    start with a statement containing just a string (e.g. a docstring).

    Note: functions that raise other kinds of exceptions do not count as
    "trivial". We use this function to help us determine when it's ok to
    relax certain checks on body, but functions that raise arbitrary exceptions
    are more likely to do non-trivial work. For example:

       def halt(self, reason: str = ...) -&gt; NoReturn:
           raise MyCustomError("Fatal error: " + reason, self.line, self.context)

    A function that raises just NotImplementedError is much less likely to be
    this complex.
    """
    body = block.body

    # Skip a docstring
    if (body and isinstance(body[0], ExpressionStmt) and
            isinstance(body[0].expr, (StrExpr, UnicodeExpr))):
        body = block.body[1:]

    if len(body) == 0:
        # There's only a docstring (or no body at all).
        return True
    elif len(body) &gt; 1:
        return False

    stmt = body[0]

    if isinstance(stmt, RaiseStmt):
        expr = stmt.expr
        if expr is None:
            return False
        if isinstance(expr, CallExpr):
            expr = expr.callee

        return (isinstance(expr, NameExpr)
                and expr.fullname == 'builtins.NotImplementedError')

    return (isinstance(stmt, PassStmt) or
            (isinstance(stmt, ExpressionStmt) and
             isinstance(stmt.expr, EllipsisExpr)))

</t>
<t tx="ekr.20220525082933.38">def main() -&gt; None:
    json_chunks = list(get_files(ROOT))
    class_chunks = list(extract_classes(json_chunks))

    total_size = sum(chunk.total_size for chunk in json_chunks)
    print(f"Total cache size: {total_size / (1024 * 1024):.3f} megabytes")
    print()

    class_name_counter = Counter(chunk[".class"] for chunk in class_chunks)
    print("Most commonly used classes:")
    report_counter(class_name_counter)

    print("Most common literal chunks:")
    report_most_common(class_chunks, 15)

    build = None
    for chunk in json_chunks:
        if 'build.*.json' in chunk.filename:
            build = chunk
            break
    original = json.dumps(build.data, sort_keys=True)
    print(f"Size of build.data.json, in kilobytes: {len(original) / 1024:.3f}")

    build.data = compress(build.data)
    compressed = json.dumps(build.data, sort_keys=True)
    print(f"Size of compressed build.data.json, in kilobytes: {len(compressed) / 1024:.3f}")

    build.data = decompress(build.data)
    decompressed = json.dumps(build.data, sort_keys=True)
    print(f"Size of decompressed build.data.json, in kilobytes: {len(decompressed) / 1024:.3f}")

    print("Lossless conversion back", original == decompressed)


    '''var_chunks = list(pluck("Var", class_chunks))
    report_most_common(var_chunks, 20)
    print()

    #for var in var_chunks:
    #    if var['fullname'] == 'self' and not (isinstance(var['type'], dict) and var['type']['.class'] == 'AnyType'):
    #        print(var)
    #argument_chunks = list(pluck("Argument", class_chunks))

    symbol_table_node_chunks = list(pluck("SymbolTableNode", class_chunks))
    report_most_common(symbol_table_node_chunks, 20)

    print()
    print("Most common")
    report_most_common(class_chunks, 20)
    print()'''


</t>
<t tx="ekr.20220525082933.380">def check_reverse_op_method(self, defn: FuncItem,
                            reverse_type: CallableType, reverse_name: str,
                            context: Context) -&gt; None:
    """Check a reverse operator method such as __radd__."""
    # Decides whether it's worth calling check_overlapping_op_methods().

    # This used to check for some very obscure scenario.  It now
    # just decides whether it's worth calling
    # check_overlapping_op_methods().

    assert defn.info

    # First check for a valid signature
    method_type = CallableType([AnyType(TypeOfAny.special_form),
                                AnyType(TypeOfAny.special_form)],
                               [nodes.ARG_POS, nodes.ARG_POS],
                               [None, None],
                               AnyType(TypeOfAny.special_form),
                               self.named_type('builtins.function'))
    if not is_subtype(reverse_type, method_type):
        self.msg.invalid_signature(reverse_type, context)
        return

    if reverse_name in ('__eq__', '__ne__'):
        # These are defined for all objects =&gt; can't cause trouble.
        return

    # With 'Any' or 'object' return type we are happy, since any possible
    # return value is valid.
    ret_type = get_proper_type(reverse_type.ret_type)
    if isinstance(ret_type, AnyType):
        return
    if isinstance(ret_type, Instance):
        if ret_type.type.fullname == 'builtins.object':
            return
    if reverse_type.arg_kinds[0] == ARG_STAR:
        reverse_type = reverse_type.copy_modified(arg_types=[reverse_type.arg_types[0]] * 2,
                                                  arg_kinds=[ARG_POS] * 2,
                                                  arg_names=[reverse_type.arg_names[0], "_"])
    assert len(reverse_type.arg_types) &gt;= 2

    if self.options.python_version[0] == 2 and reverse_name == '__rdiv__':
        forward_name = '__div__'
    else:
        forward_name = operators.normal_from_reverse_op[reverse_name]
    forward_inst = get_proper_type(reverse_type.arg_types[1])
    if isinstance(forward_inst, TypeVarType):
        forward_inst = get_proper_type(forward_inst.upper_bound)
    elif isinstance(forward_inst, TupleType):
        forward_inst = tuple_fallback(forward_inst)
    elif isinstance(forward_inst, (FunctionLike, TypedDictType, LiteralType)):
        forward_inst = forward_inst.fallback
    if isinstance(forward_inst, TypeType):
        item = forward_inst.item
        if isinstance(item, Instance):
            opt_meta = item.type.metaclass_type
            if opt_meta is not None:
                forward_inst = opt_meta
    if not (isinstance(forward_inst, (Instance, UnionType))
            and forward_inst.has_readable_member(forward_name)):
        return
    forward_base = reverse_type.arg_types[1]
    forward_type = self.expr_checker.analyze_external_member_access(forward_name, forward_base,
                                                                    context=defn)
    self.check_overlapping_op_methods(reverse_type, reverse_name, defn.info,
                                      forward_type, forward_name, forward_base,
                                      context=defn)

</t>
<t tx="ekr.20220525082933.381">def check_overlapping_op_methods(self,
                                 reverse_type: CallableType,
                                 reverse_name: str,
                                 reverse_class: TypeInfo,
                                 forward_type: Type,
                                 forward_name: str,
                                 forward_base: Type,
                                 context: Context) -&gt; None:
    """Check for overlapping method and reverse method signatures.

    This function assumes that:

    -   The reverse method has valid argument count and kinds.
    -   If the reverse operator method accepts some argument of type
        X, the forward operator method also belong to class X.

        For example, if we have the reverse operator `A.__radd__(B)`, then the
        corresponding forward operator must have the type `B.__add__(...)`.
    """

    # Note: Suppose we have two operator methods "A.__rOP__(B) -&gt; R1" and
    # "B.__OP__(C) -&gt; R2". We check if these two methods are unsafely overlapping
    # by using the following algorithm:
    #
    # 1. Rewrite "B.__OP__(C) -&gt; R1"  to "temp1(B, C) -&gt; R1"
    #
    # 2. Rewrite "A.__rOP__(B) -&gt; R2" to "temp2(B, A) -&gt; R2"
    #
    # 3. Treat temp1 and temp2 as if they were both variants in the same
    #    overloaded function. (This mirrors how the Python runtime calls
    #    operator methods: we first try __OP__, then __rOP__.)
    #
    #    If the first signature is unsafely overlapping with the second,
    #    report an error.
    #
    # 4. However, if temp1 shadows temp2 (e.g. the __rOP__ method can never
    #    be called), do NOT report an error.
    #
    #    This behavior deviates from how we handle overloads -- many of the
    #    modules in typeshed seem to define __OP__ methods that shadow the
    #    corresponding __rOP__ method.
    #
    # Note: we do not attempt to handle unsafe overlaps related to multiple
    # inheritance. (This is consistent with how we handle overloads: we also
    # do not try checking unsafe overlaps due to multiple inheritance there.)

    for forward_item in union_items(forward_type):
        if isinstance(forward_item, CallableType):
            if self.is_unsafe_overlapping_op(forward_item, forward_base, reverse_type):
                self.msg.operator_method_signatures_overlap(
                    reverse_class, reverse_name,
                    forward_base, forward_name, context)
        elif isinstance(forward_item, Overloaded):
            for item in forward_item.items:
                if self.is_unsafe_overlapping_op(item, forward_base, reverse_type):
                    self.msg.operator_method_signatures_overlap(
                        reverse_class, reverse_name,
                        forward_base, forward_name,
                        context)
        elif not isinstance(forward_item, AnyType):
            self.msg.forward_operator_not_callable(forward_name, context)

</t>
<t tx="ekr.20220525082933.382">def is_unsafe_overlapping_op(self,
                             forward_item: CallableType,
                             forward_base: Type,
                             reverse_type: CallableType) -&gt; bool:
    # TODO: check argument kinds?
    if len(forward_item.arg_types) &lt; 1:
        # Not a valid operator method -- can't succeed anyway.
        return False

    # Erase the type if necessary to make sure we don't have a single
    # TypeVar in forward_tweaked. (Having a function signature containing
    # just a single TypeVar can lead to unpredictable behavior.)
    forward_base_erased = forward_base
    if isinstance(forward_base, TypeVarType):
        forward_base_erased = erase_to_bound(forward_base)

    # Construct normalized function signatures corresponding to the
    # operator methods. The first argument is the left operand and the
    # second operand is the right argument -- we switch the order of
    # the arguments of the reverse method.

    forward_tweaked = forward_item.copy_modified(
        arg_types=[forward_base_erased, forward_item.arg_types[0]],
        arg_kinds=[nodes.ARG_POS] * 2,
        arg_names=[None] * 2,
    )
    reverse_tweaked = reverse_type.copy_modified(
        arg_types=[reverse_type.arg_types[1], reverse_type.arg_types[0]],
        arg_kinds=[nodes.ARG_POS] * 2,
        arg_names=[None] * 2,
    )

    reverse_base_erased = reverse_type.arg_types[0]
    if isinstance(reverse_base_erased, TypeVarType):
        reverse_base_erased = erase_to_bound(reverse_base_erased)

    if is_same_type(reverse_base_erased, forward_base_erased):
        return False
    elif is_subtype(reverse_base_erased, forward_base_erased):
        first = reverse_tweaked
        second = forward_tweaked
    else:
        first = forward_tweaked
        second = reverse_tweaked

    return is_unsafe_overlapping_overload_signatures(first, second)

</t>
<t tx="ekr.20220525082933.383">def check_inplace_operator_method(self, defn: FuncBase) -&gt; None:
    """Check an inplace operator method such as __iadd__.

    They cannot arbitrarily overlap with __add__.
    """
    method = defn.name
    if method not in operators.inplace_operator_methods:
        return
    typ = bind_self(self.function_type(defn))
    cls = defn.info
    other_method = '__' + method[3:]
    if cls.has_readable_member(other_method):
        instance = fill_typevars(cls)
        typ2 = get_proper_type(self.expr_checker.analyze_external_member_access(
            other_method, instance, defn))
        fail = False
        if isinstance(typ2, FunctionLike):
            if not is_more_general_arg_prefix(typ, typ2):
                fail = True
        else:
            # TODO overloads
            fail = True
        if fail:
            self.msg.signatures_incompatible(method, other_method, defn)

</t>
<t tx="ekr.20220525082933.384">def check_getattr_method(self, typ: Type, context: Context, name: str) -&gt; None:
    if len(self.scope.stack) == 1:
        # module scope
        if name == '__getattribute__':
            self.fail(message_registry.MODULE_LEVEL_GETATTRIBUTE, context)
            return
        # __getattr__ is fine at the module level as of Python 3.7 (PEP 562). We could
        # show an error for Python &lt; 3.7, but that would be annoying in code that supports
        # both 3.7 and older versions.
        method_type = CallableType([self.named_type('builtins.str')],
                                   [nodes.ARG_POS],
                                   [None],
                                   AnyType(TypeOfAny.special_form),
                                   self.named_type('builtins.function'))
    elif self.scope.active_class():
        method_type = CallableType([AnyType(TypeOfAny.special_form),
                                    self.named_type('builtins.str')],
                                   [nodes.ARG_POS, nodes.ARG_POS],
                                   [None, None],
                                   AnyType(TypeOfAny.special_form),
                                   self.named_type('builtins.function'))
    else:
        return
    if not is_subtype(typ, method_type):
        self.msg.invalid_signature_for_special_method(typ, context, name)

</t>
<t tx="ekr.20220525082933.385">def check_setattr_method(self, typ: Type, context: Context) -&gt; None:
    if not self.scope.active_class():
        return
    method_type = CallableType([AnyType(TypeOfAny.special_form),
                                self.named_type('builtins.str'),
                                AnyType(TypeOfAny.special_form)],
                               [nodes.ARG_POS, nodes.ARG_POS, nodes.ARG_POS],
                               [None, None, None],
                               NoneType(),
                               self.named_type('builtins.function'))
    if not is_subtype(typ, method_type):
        self.msg.invalid_signature_for_special_method(typ, context, '__setattr__')

</t>
<t tx="ekr.20220525082933.386">def check_slots_definition(self, typ: Type, context: Context) -&gt; None:
    """Check the type of __slots__."""
    str_type = self.named_type("builtins.str")
    expected_type = UnionType([str_type,
                               self.named_generic_type("typing.Iterable", [str_type])])
    self.check_subtype(typ, expected_type, context,
                       message_registry.INVALID_TYPE_FOR_SLOTS,
                       'actual type',
                       'expected type',
                       code=codes.ASSIGNMENT)

</t>
<t tx="ekr.20220525082933.387">def check_match_args(self, var: Var, typ: Type, context: Context) -&gt; None:
    """Check that __match_args__ contains literal strings"""
    typ = get_proper_type(typ)
    if not isinstance(typ, TupleType) or \
            not all([is_string_literal(item) for item in typ.items]):
        self.msg.note("__match_args__ must be a tuple containing string literals for checking "
                      "of match statements to work", context, code=codes.LITERAL_REQ)

</t>
<t tx="ekr.20220525082933.388">def expand_typevars(self, defn: FuncItem,
                    typ: CallableType) -&gt; List[Tuple[FuncItem, CallableType]]:
    # TODO use generator
    subst: List[List[Tuple[TypeVarId, Type]]] = []
    tvars = list(typ.variables) or []
    if defn.info:
        # Class type variables
        tvars += defn.info.defn.type_vars or []
    # TODO(PEP612): audit for paramspec
    for tvar in tvars:
        if isinstance(tvar, TypeVarType) and tvar.values:
            subst.append([(tvar.id, value) for value in tvar.values])
    # Make a copy of the function to check for each combination of
    # value restricted type variables. (Except when running mypyc,
    # where we need one canonical version of the function.)
    if subst and not self.options.mypyc:
        result: List[Tuple[FuncItem, CallableType]] = []
        for substitutions in itertools.product(*subst):
            mapping = dict(substitutions)
            expanded = cast(CallableType, expand_type(typ, mapping))
            result.append((expand_func(defn, mapping), expanded))
        return result
    else:
        return [(defn, typ)]

</t>
<t tx="ekr.20220525082933.389">def check_method_override(self, defn: Union[FuncDef, OverloadedFuncDef, Decorator]) -&gt; None:
    """Check if function definition is compatible with base classes.

    This may defer the method if a signature is not available in at least one base class.
    """
    # Check against definitions in base classes.
    for base in defn.info.mro[1:]:
        if self.check_method_or_accessor_override_for_base(defn, base):
            # Node was deferred, we will have another attempt later.
            return

</t>
<t tx="ekr.20220525082933.39">@path C:/Repos/mypy/misc/
@language python
@tabwidth -4
#!/usr/bin/env python3
"""Script for applying a cache diff.

With some infrastructure, this can allow for distributing small cache diffs to users in
many cases instead of full cache artifacts.
"""

import argparse
import json
import os
import sys

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from mypy.metastore import MetadataStore, FilesystemMetadataStore, SqliteMetadataStore


@others
if __name__ == '__main__':
    main()
</t>
<t tx="ekr.20220525082933.390">def check_method_or_accessor_override_for_base(self, defn: Union[FuncDef,
                                                                 OverloadedFuncDef,
                                                                 Decorator],
                                               base: TypeInfo) -&gt; bool:
    """Check if method definition is compatible with a base class.

    Return True if the node was deferred because one of the corresponding
    superclass nodes is not ready.
    """
    if base:
        name = defn.name
        base_attr = base.names.get(name)
        if base_attr:
            # First, check if we override a final (always an error, even with Any types).
            if is_final_node(base_attr.node):
                self.msg.cant_override_final(name, base.name, defn)
            # Second, final can't override anything writeable independently of types.
            if defn.is_final:
                self.check_if_final_var_override_writable(name, base_attr.node, defn)

        # Check the type of override.
        if name not in ('__init__', '__new__', '__init_subclass__'):
            # Check method override
            # (__init__, __new__, __init_subclass__ are special).
            if self.check_method_override_for_base_with_name(defn, name, base):
                return True
            if name in operators.inplace_operator_methods:
                # Figure out the name of the corresponding operator method.
                method = '__' + name[3:]
                # An inplace operator method such as __iadd__ might not be
                # always introduced safely if a base class defined __add__.
                # TODO can't come up with an example where this is
                #      necessary; now it's "just in case"
                return self.check_method_override_for_base_with_name(defn, method,
                                                                     base)
    return False

</t>
<t tx="ekr.20220525082933.391">def check_method_override_for_base_with_name(
        self, defn: Union[FuncDef, OverloadedFuncDef, Decorator],
        name: str, base: TypeInfo) -&gt; bool:
    """Check if overriding an attribute `name` of `base` with `defn` is valid.

    Return True if the supertype node was not analysed yet, and `defn` was deferred.
    """
    base_attr = base.names.get(name)
    if base_attr:
        # The name of the method is defined in the base class.

        # Point errors at the 'def' line (important for backward compatibility
        # of type ignores).
        if not isinstance(defn, Decorator):
            context = defn
        else:
            context = defn.func

        # Construct the type of the overriding method.
        if isinstance(defn, (FuncDef, OverloadedFuncDef)):
            typ: Type = self.function_type(defn)
            override_class_or_static = defn.is_class or defn.is_static
            override_class = defn.is_class
        else:
            assert defn.var.is_ready
            assert defn.var.type is not None
            typ = defn.var.type
            override_class_or_static = defn.func.is_class or defn.func.is_static
            override_class = defn.func.is_class
        typ = get_proper_type(typ)
        if isinstance(typ, FunctionLike) and not is_static(context):
            typ = bind_self(typ, self.scope.active_self_type(),
                            is_classmethod=override_class)
        # Map the overridden method type to subtype context so that
        # it can be checked for compatibility.
        original_type = get_proper_type(base_attr.type)
        original_node = base_attr.node
        if original_type is None:
            if self.pass_num &lt; self.last_pass:
                # If there are passes left, defer this node until next pass,
                # otherwise try reconstructing the method type from available information.
                self.defer_node(defn, defn.info)
                return True
            elif isinstance(original_node, (FuncDef, OverloadedFuncDef)):
                original_type = self.function_type(original_node)
            elif isinstance(original_node, Decorator):
                original_type = self.function_type(original_node.func)
            elif isinstance(original_node, Var):
                # Super type can define method as an attribute.
                # See https://github.com/python/mypy/issues/10134

                # We also check that sometimes `original_node.type` is None.
                # This is the case when we use something like `__hash__ = None`.
                if original_node.type is not None:
                    original_type = get_proper_type(original_node.type)
                else:
                    original_type = NoneType()
            else:
                assert False, str(base_attr.node)
        if isinstance(original_node, (FuncDef, OverloadedFuncDef)):
            original_class_or_static = original_node.is_class or original_node.is_static
        elif isinstance(original_node, Decorator):
            fdef = original_node.func
            original_class_or_static = fdef.is_class or fdef.is_static
        else:
            original_class_or_static = False  # a variable can't be class or static
        if isinstance(original_type, AnyType) or isinstance(typ, AnyType):
            pass
        elif isinstance(original_type, FunctionLike) and isinstance(typ, FunctionLike):
            original = self.bind_and_map_method(base_attr, original_type,
                                                defn.info, base)
            # Check that the types are compatible.
            # TODO overloaded signatures
            self.check_override(typ,
                                original,
                                defn.name,
                                name,
                                base.name,
                                original_class_or_static,
                                override_class_or_static,
                                context)
        elif is_equivalent(original_type, typ):
            # Assume invariance for a non-callable attribute here. Note
            # that this doesn't affect read-only properties which can have
            # covariant overrides.
            #
            pass
        elif (base_attr.node and not self.is_writable_attribute(base_attr.node)
              and is_subtype(typ, original_type)):
            # If the attribute is read-only, allow covariance
            pass
        else:
            self.msg.signature_incompatible_with_supertype(
                defn.name, name, base.name, context)
    return False

</t>
<t tx="ekr.20220525082933.392">def bind_and_map_method(self, sym: SymbolTableNode, typ: FunctionLike,
                        sub_info: TypeInfo, super_info: TypeInfo) -&gt; FunctionLike:
    """Bind self-type and map type variables for a method.

    Arguments:
        sym: a symbol that points to method definition
        typ: method type on the definition
        sub_info: class where the method is used
        super_info: class where the method was defined
    """
    if (isinstance(sym.node, (FuncDef, OverloadedFuncDef, Decorator))
            and not is_static(sym.node)):
        if isinstance(sym.node, Decorator):
            is_class_method = sym.node.func.is_class
        else:
            is_class_method = sym.node.is_class
        bound = bind_self(typ, self.scope.active_self_type(), is_class_method)
    else:
        bound = typ
    return cast(FunctionLike, map_type_from_supertype(bound, sub_info, super_info))

</t>
<t tx="ekr.20220525082933.393">def get_op_other_domain(self, tp: FunctionLike) -&gt; Optional[Type]:
    if isinstance(tp, CallableType):
        if tp.arg_kinds and tp.arg_kinds[0] == ARG_POS:
            return tp.arg_types[0]
        return None
    elif isinstance(tp, Overloaded):
        raw_items = [self.get_op_other_domain(it) for it in tp.items]
        items = [it for it in raw_items if it]
        if items:
            return make_simplified_union(items)
        return None
    else:
        assert False, "Need to check all FunctionLike subtypes here"

</t>
<t tx="ekr.20220525082933.394">def check_override(self, override: FunctionLike, original: FunctionLike,
                   name: str, name_in_super: str, supertype: str,
                   original_class_or_static: bool,
                   override_class_or_static: bool,
                   node: Context) -&gt; None:
    """Check a method override with given signatures.

    Arguments:
      override:  The signature of the overriding method.
      original:  The signature of the original supertype method.
      name:      The name of the subtype. This and the next argument are
                 only used for generating error messages.
      supertype: The name of the supertype.
    """
    # Use boolean variable to clarify code.
    fail = False
    op_method_wider_note = False
    if not is_subtype(override, original, ignore_pos_arg_names=True):
        fail = True
    elif isinstance(override, Overloaded) and self.is_forward_op_method(name):
        # Operator method overrides cannot extend the domain, as
        # this could be unsafe with reverse operator methods.
        original_domain = self.get_op_other_domain(original)
        override_domain = self.get_op_other_domain(override)
        if (original_domain and override_domain and
                not is_subtype(override_domain, original_domain)):
            fail = True
            op_method_wider_note = True
    if isinstance(original, FunctionLike) and isinstance(override, FunctionLike):
        if original_class_or_static and not override_class_or_static:
            fail = True
        elif isinstance(original, CallableType) and isinstance(override, CallableType):
            if original.type_guard is not None and override.type_guard is None:
                fail = True

    if is_private(name):
        fail = False

    if fail:
        emitted_msg = False
        if (isinstance(override, CallableType) and
                isinstance(original, CallableType) and
                len(override.arg_types) == len(original.arg_types) and
                override.min_args == original.min_args):
            # Give more detailed messages for the common case of both
            # signatures having the same number of arguments and no
            # overloads.

            # override might have its own generic function type
            # variables. If an argument or return type of override
            # does not have the correct subtyping relationship
            # with the original type even after these variables
            # are erased, then it is definitely an incompatibility.

            override_ids = override.type_var_ids()
            type_name = None
            if isinstance(override.definition, FuncDef):
                type_name = override.definition.info.name

            def erase_override(t: Type) -&gt; Type:
                return erase_typevars(t, ids_to_erase=override_ids)

            for i in range(len(override.arg_types)):
                if not is_subtype(original.arg_types[i],
                                  erase_override(override.arg_types[i])):
                    arg_type_in_super = original.arg_types[i]
                    self.msg.argument_incompatible_with_supertype(
                        i + 1,
                        name,
                        type_name,
                        name_in_super,
                        arg_type_in_super,
                        supertype,
                        node
                    )
                    emitted_msg = True

            if not is_subtype(erase_override(override.ret_type),
                              original.ret_type):
                self.msg.return_type_incompatible_with_supertype(
                    name, name_in_super, supertype, original.ret_type, override.ret_type, node)
                emitted_msg = True
        elif isinstance(override, Overloaded) and isinstance(original, Overloaded):
            # Give a more detailed message in the case where the user is trying to
            # override an overload, and the subclass's overload is plausible, except
            # that the order of the variants are wrong.
            #
            # For example, if the parent defines the overload f(int) -&gt; int and f(str) -&gt; str
            # (in that order), and if the child swaps the two and does f(str) -&gt; str and
            # f(int) -&gt; int
            order = []
            for child_variant in override.items:
                for i, parent_variant in enumerate(original.items):
                    if is_subtype(child_variant, parent_variant):
                        order.append(i)
                        break

            if len(order) == len(original.items) and order != sorted(order):
                self.msg.overload_signature_incompatible_with_supertype(
                    name, name_in_super, supertype, node)
                emitted_msg = True

        if not emitted_msg:
            # Fall back to generic incompatibility message.
            self.msg.signature_incompatible_with_supertype(
                name, name_in_super, supertype, node, original=original, override=override)
        if op_method_wider_note:
            self.note("Overloaded operator methods can't have wider argument types"
                      " in overrides", node, code=codes.OVERRIDE)

</t>
<t tx="ekr.20220525082933.395">def check__exit__return_type(self, defn: FuncItem) -&gt; None:
    """Generate error if the return type of __exit__ is problematic.

    If __exit__ always returns False but the return type is declared
    as bool, mypy thinks that a with statement may "swallow"
    exceptions even though this is not the case, resulting in
    invalid reachability inference.
    """
    if not defn.type or not isinstance(defn.type, CallableType):
        return

    ret_type = get_proper_type(defn.type.ret_type)
    if not has_bool_item(ret_type):
        return

    returns = all_return_statements(defn)
    if not returns:
        return

    if all(isinstance(ret.expr, NameExpr) and ret.expr.fullname == 'builtins.False'
           for ret in returns):
        self.msg.incorrect__exit__return(defn)

</t>
<t tx="ekr.20220525082933.396">def visit_class_def(self, defn: ClassDef) -&gt; None:
    """Type check a class definition."""
    typ = defn.info
    for base in typ.mro[1:]:
        if base.is_final:
            self.fail(message_registry.CANNOT_INHERIT_FROM_FINAL.format(base.name), defn)
    with self.tscope.class_scope(defn.info), self.enter_partial_types(is_class=True):
        old_binder = self.binder
        self.binder = ConditionalTypeBinder()
        with self.binder.top_frame_context():
            with self.scope.push_class(defn.info):
                self.accept(defn.defs)
        self.binder = old_binder
        if not (defn.info.typeddict_type or defn.info.tuple_type or defn.info.is_enum):
            # If it is not a normal class (not a special form) check class keywords.
            self.check_init_subclass(defn)
        if not defn.has_incompatible_baseclass:
            # Otherwise we've already found errors; more errors are not useful
            self.check_multiple_inheritance(typ)
        self.check_final_deletable(typ)

        if defn.decorators:
            sig: Type = type_object_type(defn.info, self.named_type)
            # Decorators are applied in reverse order.
            for decorator in reversed(defn.decorators):
                if (isinstance(decorator, CallExpr)
                        and isinstance(decorator.analyzed, PromoteExpr)):
                    # _promote is a special type checking related construct.
                    continue

                dec = self.expr_checker.accept(decorator)
                temp = self.temp_node(sig, context=decorator)
                fullname = None
                if isinstance(decorator, RefExpr):
                    fullname = decorator.fullname

                # TODO: Figure out how to have clearer error messages.
                # (e.g. "class decorator must be a function that accepts a type."
                sig, _ = self.expr_checker.check_call(dec, [temp],
                                                      [nodes.ARG_POS], defn,
                                                      callable_name=fullname)
            # TODO: Apply the sig to the actual TypeInfo so we can handle decorators
            # that completely swap out the type.  (e.g. Callable[[Type[A]], Type[B]])
    if typ.is_protocol and typ.defn.type_vars:
        self.check_protocol_variance(defn)
    if not defn.has_incompatible_baseclass and defn.info.is_enum:
        self.check_enum(defn)

</t>
<t tx="ekr.20220525082933.397">def check_final_deletable(self, typ: TypeInfo) -&gt; None:
    # These checks are only for mypyc. Only perform some checks that are easier
    # to implement here than in mypyc.
    for attr in typ.deletable_attributes:
        node = typ.names.get(attr)
        if node and isinstance(node.node, Var) and node.node.is_final:
            self.fail(message_registry.CANNOT_MAKE_DELETABLE_FINAL, node.node)

</t>
<t tx="ekr.20220525082933.398">def check_init_subclass(self, defn: ClassDef) -&gt; None:
    """Check that keywords in a class definition are valid arguments for __init_subclass__().

    In this example:
        1   class Base:
        2       def __init_subclass__(cls, thing: int):
        3           pass
        4   class Child(Base, thing=5):
        5       def __init_subclass__(cls):
        6           pass
        7   Child()

    Base.__init_subclass__(thing=5) is called at line 4. This is what we simulate here.
    Child.__init_subclass__ is never called.
    """
    if (defn.info.metaclass_type and
            defn.info.metaclass_type.type.fullname not in ('builtins.type', 'abc.ABCMeta')):
        # We can't safely check situations when both __init_subclass__ and a custom
        # metaclass are present.
        return
    # At runtime, only Base.__init_subclass__ will be called, so
    # we skip the current class itself.
    for base in defn.info.mro[1:]:
        if '__init_subclass__' not in base.names:
            continue
        name_expr = NameExpr(defn.name)
        name_expr.node = base
        callee = MemberExpr(name_expr, '__init_subclass__')
        args = list(defn.keywords.values())
        arg_names: List[Optional[str]] = list(defn.keywords.keys())
        # 'metaclass' keyword is consumed by the rest of the type machinery,
        # and is never passed to __init_subclass__ implementations
        if 'metaclass' in arg_names:
            idx = arg_names.index('metaclass')
            arg_names.pop(idx)
            args.pop(idx)
        arg_kinds = [ARG_NAMED] * len(args)
        call_expr = CallExpr(callee, args, arg_kinds, arg_names)
        call_expr.line = defn.line
        call_expr.column = defn.column
        call_expr.end_line = defn.end_line
        self.expr_checker.accept(call_expr,
                                 allow_none_return=True,
                                 always_allow_any=True)
        # We are only interested in the first Base having __init_subclass__,
        # all other bases have already been checked.
        break

</t>
<t tx="ekr.20220525082933.399">def check_enum(self, defn: ClassDef) -&gt; None:
    assert defn.info.is_enum
    if defn.info.fullname not in ENUM_BASES:
        for sym in defn.info.names.values():
            if (isinstance(sym.node, Var) and sym.node.has_explicit_value and
                    sym.node.name == '__members__'):
                # `__members__` will always be overwritten by `Enum` and is considered
                # read-only so we disallow assigning a value to it
                self.fail(
                    message_registry.ENUM_MEMBERS_ATTR_WILL_BE_OVERRIDEN, sym.node
                )
    for base in defn.info.mro[1:-1]:  # we don't need self and `object`
        if base.is_enum and base.fullname not in ENUM_BASES:
            self.check_final_enum(defn, base)

    self.check_enum_bases(defn)
    self.check_enum_new(defn)

</t>
<t tx="ekr.20220525082933.4">def run_cmd(name: str) -&gt; int:
    status = 0
    cmd = cmds[name]
    print(f'run {name}: {cmd}')
    proc = subprocess.run(cmd, stderr=subprocess.STDOUT)
    if proc.returncode:
        print('\nFAILED: %s' % name)
        status = proc.returncode
        if name in FAST_FAIL:
            exit(status)
    return status


</t>
<t tx="ekr.20220525082933.40">def make_cache(input_dir: str, sqlite: bool) -&gt; MetadataStore:
    if sqlite:
        return SqliteMetadataStore(input_dir)
    else:
        return FilesystemMetadataStore(input_dir)


</t>
<t tx="ekr.20220525082933.400">def check_final_enum(self, defn: ClassDef, base: TypeInfo) -&gt; None:
    for sym in base.names.values():
        if self.is_final_enum_value(sym):
            self.fail(
                f'Cannot extend enum with existing members: "{base.name}"',
                defn,
            )
            break

</t>
<t tx="ekr.20220525082933.401">def is_final_enum_value(self, sym: SymbolTableNode) -&gt; bool:
    if isinstance(sym.node, (FuncBase, Decorator)):
        return False  # A method is fine
    if not isinstance(sym.node, Var):
        return True  # Can be a class or anything else

    # Now, only `Var` is left, we need to check:
    # 1. Private name like in `__prop = 1`
    # 2. Dunder name like `__hash__ = some_hasher`
    # 3. Sunder name like `_order_ = 'a, b, c'`
    # 4. If it is a method / descriptor like in `method = classmethod(func)`
    if (
        is_private(sym.node.name)
        or is_dunder(sym.node.name)
        or is_sunder(sym.node.name)
        # TODO: make sure that `x = @class/staticmethod(func)`
        # and `x = property(prop)` both work correctly.
        # Now they are incorrectly counted as enum members.
        or isinstance(get_proper_type(sym.node.type), FunctionLike)
    ):
        return False

    if self.is_stub or sym.node.has_explicit_value:
        return True
    return False

</t>
<t tx="ekr.20220525082933.402">def check_enum_bases(self, defn: ClassDef) -&gt; None:
    enum_base: Optional[Instance] = None
    for base in defn.info.bases:
        if enum_base is None and base.type.is_enum:
            enum_base = base
            continue
        elif enum_base is not None:
            self.fail(
                f'No base classes are allowed after "{enum_base}"',
                defn,
            )
            break

</t>
<t tx="ekr.20220525082933.403">def check_enum_new(self, defn: ClassDef) -&gt; None:
    def has_new_method(info: TypeInfo) -&gt; bool:
        new_method = info.get('__new__')
        return bool(
            new_method
            and new_method.node
            and new_method.node.fullname != 'builtins.object.__new__'
        )

    has_new = False
    for base in defn.info.bases:
        candidate = False

        if base.type.is_enum:
            # If we have an `Enum`, then we need to check all its bases.
            candidate = any(
                not b.is_enum and has_new_method(b)
                for b in base.type.mro[1:-1]
            )
        else:
            candidate = has_new_method(base.type)

        if candidate and has_new:
            self.fail(
                'Only a single data type mixin is allowed for Enum subtypes, '
                'found extra "{}"'.format(base),
                defn,
            )
        elif candidate:
            has_new = True

</t>
<t tx="ekr.20220525082933.404">def check_protocol_variance(self, defn: ClassDef) -&gt; None:
    """Check that protocol definition is compatible with declared
    variances of type variables.

    Note that we also prohibit declaring protocol classes as invariant
    if they are actually covariant/contravariant, since this may break
    transitivity of subtyping, see PEP 544.
    """
    info = defn.info
    object_type = Instance(info.mro[-1], [])
    tvars = info.defn.type_vars
    for i, tvar in enumerate(tvars):
        up_args: List[Type] = [
            object_type if i == j else AnyType(TypeOfAny.special_form)
            for j, _ in enumerate(tvars)
        ]
        down_args: List[Type] = [
            UninhabitedType() if i == j else AnyType(TypeOfAny.special_form)
            for j, _ in enumerate(tvars)
        ]
        up, down = Instance(info, up_args), Instance(info, down_args)
        # TODO: add advanced variance checks for recursive protocols
        if is_subtype(down, up, ignore_declared_variance=True):
            expected = COVARIANT
        elif is_subtype(up, down, ignore_declared_variance=True):
            expected = CONTRAVARIANT
        else:
            expected = INVARIANT
        if isinstance(tvar, TypeVarType) and expected != tvar.variance:
            self.msg.bad_proto_variance(tvar.variance, tvar.name, expected, defn)

</t>
<t tx="ekr.20220525082933.405">def check_multiple_inheritance(self, typ: TypeInfo) -&gt; None:
    """Check for multiple inheritance related errors."""
    if len(typ.bases) &lt;= 1:
        # No multiple inheritance.
        return
    # Verify that inherited attributes are compatible.
    mro = typ.mro[1:]
    for i, base in enumerate(mro):
        # Attributes defined in both the type and base are skipped.
        # Normal checks for attribute compatibility should catch any problems elsewhere.
        non_overridden_attrs = base.names.keys() - typ.names.keys()
        for name in non_overridden_attrs:
            if is_private(name):
                continue
            for base2 in mro[i + 1:]:
                # We only need to check compatibility of attributes from classes not
                # in a subclass relationship. For subclasses, normal (single inheritance)
                # checks suffice (these are implemented elsewhere).
                if name in base2.names and base2 not in base.mro:
                    self.check_compatibility(name, base, base2, typ)

</t>
<t tx="ekr.20220525082933.406">def determine_type_of_class_member(self, sym: SymbolTableNode) -&gt; Optional[Type]:
    if sym.type is not None:
        return sym.type
    if isinstance(sym.node, FuncBase):
        return self.function_type(sym.node)
    if isinstance(sym.node, TypeInfo):
        # nested class
        return type_object_type(sym.node, self.named_type)
    if isinstance(sym.node, TypeVarExpr):
        # Use of TypeVars is rejected in an expression/runtime context, so
        # we don't need to check supertype compatibility for them.
        return AnyType(TypeOfAny.special_form)
    return None

</t>
<t tx="ekr.20220525082933.407">def check_compatibility(self, name: str, base1: TypeInfo,
                        base2: TypeInfo, ctx: TypeInfo) -&gt; None:
    """Check if attribute name in base1 is compatible with base2 in multiple inheritance.

    Assume base1 comes before base2 in the MRO, and that base1 and base2 don't have
    a direct subclass relationship (i.e., the compatibility requirement only derives from
    multiple inheritance).

    This check verifies that a definition taken from base1 (and mapped to the current
    class ctx), is type compatible with the definition taken from base2 (also mapped), so
    that unsafe subclassing like this can be detected:
        class A(Generic[T]):
            def foo(self, x: T) -&gt; None: ...

        class B:
            def foo(self, x: str) -&gt; None: ...

        class C(B, A[int]): ...  # this is unsafe because...

        x: A[int] = C()
        x.foo  # ...runtime type is (str) -&gt; None, while static type is (int) -&gt; None
    """
    if name in ('__init__', '__new__', '__init_subclass__'):
        # __init__ and friends can be incompatible -- it's a special case.
        return
    first = base1.names[name]
    second = base2.names[name]
    first_type = get_proper_type(self.determine_type_of_class_member(first))
    second_type = get_proper_type(self.determine_type_of_class_member(second))

    if (isinstance(first_type, FunctionLike) and
            isinstance(second_type, FunctionLike)):
        if first_type.is_type_obj() and second_type.is_type_obj():
            # For class objects only check the subtype relationship of the classes,
            # since we allow incompatible overrides of '__init__'/'__new__'
            ok = is_subtype(left=fill_typevars_with_any(first_type.type_object()),
                            right=fill_typevars_with_any(second_type.type_object()))
        else:
            # First bind/map method types when necessary.
            first_sig = self.bind_and_map_method(first, first_type, ctx, base1)
            second_sig = self.bind_and_map_method(second, second_type, ctx, base2)
            ok = is_subtype(first_sig, second_sig, ignore_pos_arg_names=True)
    elif first_type and second_type:
        ok = is_equivalent(first_type, second_type)
        if not ok:
            second_node = base2[name].node
            if isinstance(second_node, Decorator) and second_node.func.is_property:
                ok = is_subtype(first_type, cast(CallableType, second_type).ret_type)
    else:
        if first_type is None:
            self.msg.cannot_determine_type_in_base(name, base1.name, ctx)
        if second_type is None:
            self.msg.cannot_determine_type_in_base(name, base2.name, ctx)
        ok = True
    # Final attributes can never be overridden, but can override
    # non-final read-only attributes.
    if is_final_node(second.node):
        self.msg.cant_override_final(name, base2.name, ctx)
    if is_final_node(first.node):
        self.check_if_final_var_override_writable(name, second.node, ctx)
    # Some attributes like __slots__ and __deletable__ are special, and the type can
    # vary across class hierarchy.
    if isinstance(second.node, Var) and second.node.allow_incompatible_override:
        ok = True
    if not ok:
        self.msg.base_class_definitions_incompatible(name, base1, base2,
                                                     ctx)

</t>
<t tx="ekr.20220525082933.408">def visit_import_from(self, node: ImportFrom) -&gt; None:
    self.check_import(node)

</t>
<t tx="ekr.20220525082933.409">def visit_import_all(self, node: ImportAll) -&gt; None:
    self.check_import(node)

</t>
<t tx="ekr.20220525082933.41">def apply_diff(cache_dir: str, diff_file: str, sqlite: bool = False) -&gt; None:
    cache = make_cache(cache_dir, sqlite)
    with open(diff_file) as f:
        diff = json.load(f)

    old_deps = json.loads(cache.read("@deps.meta.json"))

    for file, data in diff.items():
        if data is None:
            cache.remove(file)
        else:
            cache.write(file, data)
            if file.endswith('.meta.json') and "@deps" not in file:
                meta = json.loads(data)
                old_deps["snapshot"][meta["id"]] = meta["hash"]

    cache.write("@deps.meta.json", json.dumps(old_deps))

    cache.commit()


</t>
<t tx="ekr.20220525082933.410">def visit_import(self, s: Import) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082933.411">def check_import(self, node: ImportBase) -&gt; None:
    for assign in node.assignments:
        lvalue = assign.lvalues[0]
        lvalue_type, _, __ = self.check_lvalue(lvalue)
        if lvalue_type is None:
            # TODO: This is broken.
            lvalue_type = AnyType(TypeOfAny.special_form)
        message = '{} "{}"'.format(message_registry.INCOMPATIBLE_IMPORT_OF,
                                   cast(NameExpr, assign.rvalue).name)
        self.check_simple_assignment(lvalue_type, assign.rvalue, node,
                                     msg=message, lvalue_name='local name',
                                     rvalue_name='imported name')

</t>
<t tx="ekr.20220525082933.412">#
# Statements
#

</t>
<t tx="ekr.20220525082933.413">def visit_block(self, b: Block) -&gt; None:
    if b.is_unreachable:
        # This block was marked as being unreachable during semantic analysis.
        # It turns out any blocks marked in this way are *intentionally* marked
        # as unreachable -- so we don't display an error.
        self.binder.unreachable()
        return
    for s in b.body:
        if self.binder.is_unreachable():
            if self.should_report_unreachable_issues() and not self.is_raising_or_empty(s):
                self.msg.unreachable_statement(s)
            break
        self.accept(s)

</t>
<t tx="ekr.20220525082933.414">def should_report_unreachable_issues(self) -&gt; bool:
    return (self.in_checked_function()
            and self.options.warn_unreachable
            and not self.binder.is_unreachable_warning_suppressed())

</t>
<t tx="ekr.20220525082933.415">def is_raising_or_empty(self, s: Statement) -&gt; bool:
    """Returns 'true' if the given statement either throws an error of some kind
    or is a no-op.

    We use this function mostly while handling the '--warn-unreachable' flag. When
    that flag is present, we normally report an error on any unreachable statement.
    But if that statement is just something like a 'pass' or a just-in-case 'assert False',
    reporting an error would be annoying.
    """
    if isinstance(s, AssertStmt) and is_false_literal(s.expr):
        return True
    elif isinstance(s, (RaiseStmt, PassStmt)):
        return True
    elif isinstance(s, ExpressionStmt):
        if isinstance(s.expr, EllipsisExpr):
            return True
        elif isinstance(s.expr, CallExpr):
            with self.expr_checker.msg.filter_errors():
                typ = get_proper_type(self.expr_checker.accept(
                    s.expr, allow_none_return=True, always_allow_any=True))

            if isinstance(typ, UninhabitedType):
                return True
    return False

</t>
<t tx="ekr.20220525082933.416">def visit_assignment_stmt(self, s: AssignmentStmt) -&gt; None:
    """Type check an assignment statement.

    Handle all kinds of assignment statements (simple, indexed, multiple).
    """
    # Avoid type checking type aliases in stubs to avoid false
    # positives about modern type syntax available in stubs such
    # as X | Y.
    if not (s.is_alias_def and self.is_stub):
        with self.enter_final_context(s.is_final_def):
            self.check_assignment(s.lvalues[-1], s.rvalue, s.type is None, s.new_syntax)

    if s.is_alias_def:
        self.check_type_alias_rvalue(s)

    if (s.type is not None and
            self.options.disallow_any_unimported and
            has_any_from_unimported_type(s.type)):
        if isinstance(s.lvalues[-1], TupleExpr):
            # This is a multiple assignment. Instead of figuring out which type is problematic,
            # give a generic error message.
            self.msg.unimported_type_becomes_any("A type on this line",
                                                 AnyType(TypeOfAny.special_form), s)
        else:
            self.msg.unimported_type_becomes_any("Type of variable", s.type, s)
    check_for_explicit_any(s.type, self.options, self.is_typeshed_stub, self.msg, context=s)

    if len(s.lvalues) &gt; 1:
        # Chained assignment (e.g. x = y = ...).
        # Make sure that rvalue type will not be reinferred.
        if not self.has_type(s.rvalue):
            self.expr_checker.accept(s.rvalue)
        rvalue = self.temp_node(self.lookup_type(s.rvalue), s)
        for lv in s.lvalues[:-1]:
            with self.enter_final_context(s.is_final_def):
                self.check_assignment(lv, rvalue, s.type is None)

    self.check_final(s)
    if (s.is_final_def and s.type and not has_no_typevars(s.type)
            and self.scope.active_class() is not None):
        self.fail(message_registry.DEPENDENT_FINAL_IN_CLASS_BODY, s)

</t>
<t tx="ekr.20220525082933.417">def check_type_alias_rvalue(self, s: AssignmentStmt) -&gt; None:
    if not (self.is_stub and isinstance(s.rvalue, OpExpr) and s.rvalue.op == '|'):
        # We do this mostly for compatibility with old semantic analyzer.
        # TODO: should we get rid of this?
        alias_type = self.expr_checker.accept(s.rvalue)
    else:
        # Avoid type checking 'X | Y' in stubs, since there can be errors
        # on older Python targets.
        alias_type = AnyType(TypeOfAny.special_form)

        @others
        accept_items(s.rvalue)
    self.store_type(s.lvalues[-1], alias_type)

</t>
<t tx="ekr.20220525082933.418">def accept_items(e: Expression) -&gt; None:
    if isinstance(e, OpExpr) and e.op == '|':
        accept_items(e.left)
        accept_items(e.right)
    else:
        # Nested union types have been converted to type context
        # in semantic analysis (such as in 'list[int | str]'),
        # so we don't need to deal with them here.
        self.expr_checker.accept(e)

</t>
<t tx="ekr.20220525082933.419">def check_assignment(self, lvalue: Lvalue, rvalue: Expression, infer_lvalue_type: bool = True,
                     new_syntax: bool = False) -&gt; None:
    """Type check a single assignment: lvalue = rvalue."""
    if isinstance(lvalue, TupleExpr) or isinstance(lvalue, ListExpr):
        self.check_assignment_to_multiple_lvalues(lvalue.items, rvalue, rvalue,
                                                  infer_lvalue_type)
    else:
        self.try_infer_partial_generic_type_from_assignment(lvalue, rvalue, '=')
        lvalue_type, index_lvalue, inferred = self.check_lvalue(lvalue)
        # If we're assigning to __getattr__ or similar methods, check that the signature is
        # valid.
        if isinstance(lvalue, NameExpr) and lvalue.node:
            name = lvalue.node.name
            if name in ('__setattr__', '__getattribute__', '__getattr__'):
                # If an explicit type is given, use that.
                if lvalue_type:
                    signature = lvalue_type
                else:
                    signature = self.expr_checker.accept(rvalue)
                if signature:
                    if name == '__setattr__':
                        self.check_setattr_method(signature, lvalue)
                    else:
                        self.check_getattr_method(signature, lvalue, name)

            if name == '__slots__':
                typ = lvalue_type or self.expr_checker.accept(rvalue)
                self.check_slots_definition(typ, lvalue)
            if name == '__match_args__' and inferred is not None:
                typ = self.expr_checker.accept(rvalue)
                self.check_match_args(inferred, typ, lvalue)

        # Defer PartialType's super type checking.
        if (isinstance(lvalue, RefExpr) and
                not (isinstance(lvalue_type, PartialType) and
                     lvalue_type.type is None) and
                not (isinstance(lvalue, NameExpr) and lvalue.name == '__match_args__')):
            if self.check_compatibility_all_supers(lvalue, lvalue_type, rvalue):
                # We hit an error on this line; don't check for any others
                return

        if isinstance(lvalue, MemberExpr) and lvalue.name == '__match_args__':
            self.fail(message_registry.CANNOT_MODIFY_MATCH_ARGS, lvalue)

        if lvalue_type:
            if isinstance(lvalue_type, PartialType) and lvalue_type.type is None:
                # Try to infer a proper type for a variable with a partial None type.
                rvalue_type = self.expr_checker.accept(rvalue)
                if isinstance(get_proper_type(rvalue_type), NoneType):
                    # This doesn't actually provide any additional information -- multiple
                    # None initializers preserve the partial None type.
                    return

                if is_valid_inferred_type(rvalue_type):
                    var = lvalue_type.var
                    partial_types = self.find_partial_types(var)
                    if partial_types is not None:
                        if not self.current_node_deferred:
                            # Partial type can't be final, so strip any literal values.
                            rvalue_type = remove_instance_last_known_values(rvalue_type)
                            inferred_type = make_simplified_union(
                                [rvalue_type, NoneType()])
                            self.set_inferred_type(var, lvalue, inferred_type)
                        else:
                            var.type = None
                        del partial_types[var]
                        lvalue_type = var.type
                else:
                    # Try to infer a partial type. No need to check the return value, as
                    # an error will be reported elsewhere.
                    self.infer_partial_type(lvalue_type.var, lvalue, rvalue_type)
                # Handle None PartialType's super type checking here, after it's resolved.
                if (isinstance(lvalue, RefExpr) and
                        self.check_compatibility_all_supers(lvalue, lvalue_type, rvalue)):
                    # We hit an error on this line; don't check for any others
                    return
            elif (is_literal_none(rvalue) and
                    isinstance(lvalue, NameExpr) and
                    isinstance(lvalue.node, Var) and
                    lvalue.node.is_initialized_in_class and
                    not new_syntax):
                # Allow None's to be assigned to class variables with non-Optional types.
                rvalue_type = lvalue_type
            elif (isinstance(lvalue, MemberExpr) and
                    lvalue.kind is None):  # Ignore member access to modules
                instance_type = self.expr_checker.accept(lvalue.expr)
                rvalue_type, lvalue_type, infer_lvalue_type = self.check_member_assignment(
                    instance_type, lvalue_type, rvalue, context=rvalue)
            else:
                # Hacky special case for assigning a literal None
                # to a variable defined in a previous if
                # branch. When we detect this, we'll go back and
                # make the type optional. This is somewhat
                # unpleasant, and a generalization of this would
                # be an improvement!
                if (is_literal_none(rvalue) and
                        isinstance(lvalue, NameExpr) and
                        lvalue.kind == LDEF and
                        isinstance(lvalue.node, Var) and
                        lvalue.node.type and
                        lvalue.node in self.var_decl_frames and
                        not isinstance(get_proper_type(lvalue_type), AnyType)):
                    decl_frame_map = self.var_decl_frames[lvalue.node]
                    # Check if the nearest common ancestor frame for the definition site
                    # and the current site is the enclosing frame of an if/elif/else block.
                    has_if_ancestor = False
                    for frame in reversed(self.binder.frames):
                        if frame.id in decl_frame_map:
                            has_if_ancestor = frame.conditional_frame
                            break
                    if has_if_ancestor:
                        lvalue_type = make_optional_type(lvalue_type)
                        self.set_inferred_type(lvalue.node, lvalue, lvalue_type)

                rvalue_type = self.check_simple_assignment(lvalue_type, rvalue, context=rvalue,
                                                           code=codes.ASSIGNMENT)

            # Special case: only non-abstract non-protocol classes can be assigned to
            # variables with explicit type Type[A], where A is protocol or abstract.
            rvalue_type = get_proper_type(rvalue_type)
            lvalue_type = get_proper_type(lvalue_type)
            if (isinstance(rvalue_type, CallableType) and rvalue_type.is_type_obj() and
                    (rvalue_type.type_object().is_abstract or
                     rvalue_type.type_object().is_protocol) and
                    isinstance(lvalue_type, TypeType) and
                    isinstance(lvalue_type.item, Instance) and
                    (lvalue_type.item.type.is_abstract or
                     lvalue_type.item.type.is_protocol)):
                self.msg.concrete_only_assign(lvalue_type, rvalue)
                return
            if rvalue_type and infer_lvalue_type and not isinstance(lvalue_type, PartialType):
                # Don't use type binder for definitions of special forms, like named tuples.
                if not (isinstance(lvalue, NameExpr) and lvalue.is_special_form):
                    self.binder.assign_type(lvalue, rvalue_type, lvalue_type, False)

        elif index_lvalue:
            self.check_indexed_assignment(index_lvalue, rvalue, lvalue)

        if inferred:
            rvalue_type = self.expr_checker.accept(rvalue)
            if not (inferred.is_final or (isinstance(lvalue, NameExpr) and
                                          lvalue.name == '__match_args__')):
                rvalue_type = remove_instance_last_known_values(rvalue_type)
            self.infer_variable_type(inferred, lvalue, rvalue_type, rvalue)
        self.check_assignment_to_slots(lvalue)

</t>
<t tx="ekr.20220525082933.42">def main() -&gt; None:
    parser = argparse.ArgumentParser()
    parser.add_argument('--sqlite', action='store_true', default=False,
                        help='Use a sqlite cache')
    parser.add_argument('cache_dir',
                        help="Directory for the cache")
    parser.add_argument('diff',
                        help="Cache diff file")
    args = parser.parse_args()

    apply_diff(args.cache_dir, args.diff, args.sqlite)


</t>
<t tx="ekr.20220525082933.420"># (type, operator) tuples for augmented assignments supported with partial types
partial_type_augmented_ops: Final = {
    ('builtins.list', '+'),
    ('builtins.set', '|'),
}

</t>
<t tx="ekr.20220525082933.421">def try_infer_partial_generic_type_from_assignment(self,
                                                   lvalue: Lvalue,
                                                   rvalue: Expression,
                                                   op: str) -&gt; None:
    """Try to infer a precise type for partial generic type from assignment.

    'op' is '=' for normal assignment and a binary operator ('+', ...) for
    augmented assignment.

    Example where this happens:

        x = []
        if foo():
            x = [1]  # Infer List[int] as type of 'x'
    """
    var = None
    if (isinstance(lvalue, NameExpr)
            and isinstance(lvalue.node, Var)
            and isinstance(lvalue.node.type, PartialType)):
        var = lvalue.node
    elif isinstance(lvalue, MemberExpr):
        var = self.expr_checker.get_partial_self_var(lvalue)
    if var is not None:
        typ = var.type
        assert isinstance(typ, PartialType)
        if typ.type is None:
            return
        # Return if this is an unsupported augmented assignment.
        if op != '=' and (typ.type.fullname, op) not in self.partial_type_augmented_ops:
            return
        # TODO: some logic here duplicates the None partial type counterpart
        #       inlined in check_assignment(), see #8043.
        partial_types = self.find_partial_types(var)
        if partial_types is None:
            return
        rvalue_type = self.expr_checker.accept(rvalue)
        rvalue_type = get_proper_type(rvalue_type)
        if isinstance(rvalue_type, Instance):
            if rvalue_type.type == typ.type and is_valid_inferred_type(rvalue_type):
                var.type = rvalue_type
                del partial_types[var]
        elif isinstance(rvalue_type, AnyType):
            var.type = fill_typevars_with_any(typ.type)
            del partial_types[var]

</t>
<t tx="ekr.20220525082933.422">def check_compatibility_all_supers(self, lvalue: RefExpr, lvalue_type: Optional[Type],
                                   rvalue: Expression) -&gt; bool:
    lvalue_node = lvalue.node
    # Check if we are a class variable with at least one base class
    if (isinstance(lvalue_node, Var) and
            lvalue.kind in (MDEF, None) and  # None for Vars defined via self
            len(lvalue_node.info.bases) &gt; 0):

        for base in lvalue_node.info.mro[1:]:
            tnode = base.names.get(lvalue_node.name)
            if tnode is not None:
                if not self.check_compatibility_classvar_super(lvalue_node,
                                                               base,
                                                               tnode.node):
                    # Show only one error per variable
                    break

                if not self.check_compatibility_final_super(lvalue_node,
                                                            base,
                                                            tnode.node):
                    # Show only one error per variable
                    break

        direct_bases = lvalue_node.info.direct_base_classes()
        last_immediate_base = direct_bases[-1] if direct_bases else None

        for base in lvalue_node.info.mro[1:]:
            # The type of "__slots__" and some other attributes usually doesn't need to
            # be compatible with a base class. We'll still check the type of "__slots__"
            # against "object" as an exception.
            if (isinstance(lvalue_node, Var) and lvalue_node.allow_incompatible_override and
                    not (lvalue_node.name == "__slots__" and
                         base.fullname == "builtins.object")):
                continue

            if is_private(lvalue_node.name):
                continue

            base_type, base_node = self.lvalue_type_from_base(lvalue_node, base)

            if base_type:
                assert base_node is not None
                if not self.check_compatibility_super(lvalue,
                                                      lvalue_type,
                                                      rvalue,
                                                      base,
                                                      base_type,
                                                      base_node):
                    # Only show one error per variable; even if other
                    # base classes are also incompatible
                    return True
                if base is last_immediate_base:
                    # At this point, the attribute was found to be compatible with all
                    # immediate parents.
                    break
    return False

</t>
<t tx="ekr.20220525082933.423">def check_compatibility_super(self, lvalue: RefExpr, lvalue_type: Optional[Type],
                              rvalue: Expression, base: TypeInfo, base_type: Type,
                              base_node: Node) -&gt; bool:
    lvalue_node = lvalue.node
    assert isinstance(lvalue_node, Var)

    # Do not check whether the rvalue is compatible if the
    # lvalue had a type defined; this is handled by other
    # parts, and all we have to worry about in that case is
    # that lvalue is compatible with the base class.
    compare_node = None
    if lvalue_type:
        compare_type = lvalue_type
        compare_node = lvalue.node
    else:
        compare_type = self.expr_checker.accept(rvalue, base_type)
        if isinstance(rvalue, NameExpr):
            compare_node = rvalue.node
            if isinstance(compare_node, Decorator):
                compare_node = compare_node.func

    base_type = get_proper_type(base_type)
    compare_type = get_proper_type(compare_type)
    if compare_type:
        if (isinstance(base_type, CallableType) and
                isinstance(compare_type, CallableType)):
            base_static = is_node_static(base_node)
            compare_static = is_node_static(compare_node)

            # In case compare_static is unknown, also check
            # if 'definition' is set. The most common case for
            # this is with TempNode(), where we lose all
            # information about the real rvalue node (but only get
            # the rvalue type)
            if compare_static is None and compare_type.definition:
                compare_static = is_node_static(compare_type.definition)

            # Compare against False, as is_node_static can return None
            if base_static is False and compare_static is False:
                # Class-level function objects and classmethods become bound
                # methods: the former to the instance, the latter to the
                # class
                base_type = bind_self(base_type, self.scope.active_self_type())
                compare_type = bind_self(compare_type, self.scope.active_self_type())

            # If we are a static method, ensure to also tell the
            # lvalue it now contains a static method
            if base_static and compare_static:
                lvalue_node.is_staticmethod = True

        return self.check_subtype(compare_type, base_type, rvalue,
                                  message_registry.INCOMPATIBLE_TYPES_IN_ASSIGNMENT,
                                  'expression has type',
                                  f'base class "{base.name}" defined the type as',
                                  code=codes.ASSIGNMENT)
    return True

</t>
<t tx="ekr.20220525082933.424">def lvalue_type_from_base(self, expr_node: Var,
                          base: TypeInfo) -&gt; Tuple[Optional[Type], Optional[Node]]:
    """For a NameExpr that is part of a class, walk all base classes and try
    to find the first class that defines a Type for the same name."""
    expr_name = expr_node.name
    base_var = base.names.get(expr_name)

    if base_var:
        base_node = base_var.node
        base_type = base_var.type
        if isinstance(base_node, Decorator):
            base_node = base_node.func
            base_type = base_node.type

        if base_type:
            if not has_no_typevars(base_type):
                self_type = self.scope.active_self_type()
                assert self_type is not None, "Internal error: base lookup outside class"
                if isinstance(self_type, TupleType):
                    instance = tuple_fallback(self_type)
                else:
                    instance = self_type
                itype = map_instance_to_supertype(instance, base)
                base_type = expand_type_by_instance(base_type, itype)

            base_type = get_proper_type(base_type)
            if isinstance(base_type, CallableType) and isinstance(base_node, FuncDef):
                # If we are a property, return the Type of the return
                # value, not the Callable
                if base_node.is_property:
                    base_type = get_proper_type(base_type.ret_type)
            if isinstance(base_type, FunctionLike) and isinstance(base_node,
                                                                  OverloadedFuncDef):
                # Same for properties with setter
                if base_node.is_property:
                    base_type = base_type.items[0].ret_type

            return base_type, base_node

    return None, None

</t>
<t tx="ekr.20220525082933.425">def check_compatibility_classvar_super(self, node: Var,
                                       base: TypeInfo, base_node: Optional[Node]) -&gt; bool:
    if not isinstance(base_node, Var):
        return True
    if node.is_classvar and not base_node.is_classvar:
        self.fail(message_registry.CANNOT_OVERRIDE_INSTANCE_VAR.format(base.name), node)
        return False
    elif not node.is_classvar and base_node.is_classvar:
        self.fail(message_registry.CANNOT_OVERRIDE_CLASS_VAR.format(base.name), node)
        return False
    return True

</t>
<t tx="ekr.20220525082933.426">def check_compatibility_final_super(self, node: Var,
                                    base: TypeInfo, base_node: Optional[Node]) -&gt; bool:
    """Check if an assignment overrides a final attribute in a base class.

    This only checks situations where either a node in base class is not a variable
    but a final method, or where override is explicitly declared as final.
    In these cases we give a more detailed error message. In addition, we check that
    a final variable doesn't override writeable attribute, which is not safe.

    Other situations are checked in `check_final()`.
    """
    if not isinstance(base_node, (Var, FuncBase, Decorator)):
        return True
    if base_node.is_final and (node.is_final or not isinstance(base_node, Var)):
        # Give this error only for explicit override attempt with `Final`, or
        # if we are overriding a final method with variable.
        # Other override attempts will be flagged as assignment to constant
        # in `check_final()`.
        self.msg.cant_override_final(node.name, base.name, node)
        return False
    if node.is_final:
        if base.fullname in ENUM_BASES or node.name in ENUM_SPECIAL_PROPS:
            return True
        self.check_if_final_var_override_writable(node.name, base_node, node)
    return True

</t>
<t tx="ekr.20220525082933.427">def check_if_final_var_override_writable(self,
                                         name: str,
                                         base_node: Optional[Node],
                                         ctx: Context) -&gt; None:
    """Check that a final variable doesn't override writeable attribute.

    This is done to prevent situations like this:
        class C:
            attr = 1
        class D(C):
            attr: Final = 2

        x: C = D()
        x.attr = 3  # Oops!
    """
    writable = True
    if base_node:
        writable = self.is_writable_attribute(base_node)
    if writable:
        self.msg.final_cant_override_writable(name, ctx)

</t>
<t tx="ekr.20220525082933.428">def get_final_context(self) -&gt; bool:
    """Check whether we a currently checking a final declaration."""
    return self._is_final_def

</t>
<t tx="ekr.20220525082933.429">@contextmanager
def enter_final_context(self, is_final_def: bool) -&gt; Iterator[None]:
    """Store whether the current checked assignment is a final declaration."""
    old_ctx = self._is_final_def
    self._is_final_def = is_final_def
    try:
        yield
    finally:
        self._is_final_def = old_ctx

</t>
<t tx="ekr.20220525082933.43">@path C:/Repos/mypy/misc/
@language python
@tabwidth -4
#!/usr/bin/env python3
"""Test various combinations of generators/coroutines.

This was used to cross-check the errors in the test case
testFullCoroutineMatrix in test-data/unit/check-async-await.test.
"""

import sys
from types import coroutine
from typing import Any, Awaitable, Generator, Iterator

# The various things you might try to use in `await` or `yield from`.

@others
# Run main().

if __name__ == '__main__':
    main()
</t>
<t tx="ekr.20220525082933.430">def check_final(self,
                s: Union[AssignmentStmt, OperatorAssignmentStmt, AssignmentExpr]) -&gt; None:
    """Check if this assignment does not assign to a final attribute.

    This function performs the check only for name assignments at module
    and class scope. The assignments to `obj.attr` and `Cls.attr` are checked
    in checkmember.py.
    """
    if isinstance(s, AssignmentStmt):
        lvs = self.flatten_lvalues(s.lvalues)
    elif isinstance(s, AssignmentExpr):
        lvs = [s.target]
    else:
        lvs = [s.lvalue]
    is_final_decl = s.is_final_def if isinstance(s, AssignmentStmt) else False
    if is_final_decl and self.scope.active_class():
        lv = lvs[0]
        assert isinstance(lv, RefExpr)
        assert isinstance(lv.node, Var)
        if (lv.node.final_unset_in_class and not lv.node.final_set_in_init and
                not self.is_stub and  # It is OK to skip initializer in stub files.
                # Avoid extra error messages, if there is no type in Final[...],
                # then we already reported the error about missing r.h.s.
                isinstance(s, AssignmentStmt) and s.type is not None):
            self.msg.final_without_value(s)
    for lv in lvs:
        if isinstance(lv, RefExpr) and isinstance(lv.node, Var):
            name = lv.node.name
            cls = self.scope.active_class()
            if cls is not None:
                # These additional checks exist to give more error messages
                # even if the final attribute was overridden with a new symbol
                # (which is itself an error)...
                for base in cls.mro[1:]:
                    sym = base.names.get(name)
                    # We only give this error if base node is variable,
                    # overriding final method will be caught in
                    # `check_compatibility_final_super()`.
                    if sym and isinstance(sym.node, Var):
                        if sym.node.is_final and not is_final_decl:
                            self.msg.cant_assign_to_final(name, sym.node.info is None, s)
                            # ...but only once
                            break
            if lv.node.is_final and not is_final_decl:
                self.msg.cant_assign_to_final(name, lv.node.info is None, s)

</t>
<t tx="ekr.20220525082933.431">def check_assignment_to_slots(self, lvalue: Lvalue) -&gt; None:
    if not isinstance(lvalue, MemberExpr):
        return

    inst = get_proper_type(self.expr_checker.accept(lvalue.expr))
    if not isinstance(inst, Instance):
        return
    if inst.type.slots is None:
        return  # Slots do not exist, we can allow any assignment
    if lvalue.name in inst.type.slots:
        return  # We are assigning to an existing slot
    for base_info in inst.type.mro[:-1]:
        if base_info.names.get('__setattr__') is not None:
            # When type has `__setattr__` defined,
            # we can assign any dynamic value.
            # We exclude object, because it always has `__setattr__`.
            return

    definition = inst.type.get(lvalue.name)
    if definition is None:
        # We don't want to duplicate
        # `"SomeType" has no attribute "some_attr"`
        # error twice.
        return
    if self.is_assignable_slot(lvalue, definition.type):
        return

    self.fail(
        message_registry.NAME_NOT_IN_SLOTS.format(
            lvalue.name, inst.type.fullname,
        ),
        lvalue,
    )

</t>
<t tx="ekr.20220525082933.432">def is_assignable_slot(self, lvalue: Lvalue, typ: Optional[Type]) -&gt; bool:
    if getattr(lvalue, 'node', None):
        return False  # This is a definition

    typ = get_proper_type(typ)
    if typ is None or isinstance(typ, AnyType):
        return True  # Any can be literally anything, like `@propery`
    if isinstance(typ, Instance):
        # When working with instances, we need to know if they contain
        # `__set__` special method. Like `@property` does.
        # This makes assigning to properties possible,
        # even without extra slot spec.
        return typ.type.get('__set__') is not None
    if isinstance(typ, FunctionLike):
        return True  # Can be a property, or some other magic
    if isinstance(typ, UnionType):
        return all(self.is_assignable_slot(lvalue, u) for u in typ.items)
    return False

</t>
<t tx="ekr.20220525082933.433">def check_assignment_to_multiple_lvalues(self, lvalues: List[Lvalue], rvalue: Expression,
                                         context: Context,
                                         infer_lvalue_type: bool = True) -&gt; None:
    if isinstance(rvalue, TupleExpr) or isinstance(rvalue, ListExpr):
        # Recursively go into Tuple or List expression rhs instead of
        # using the type of rhs, because this allowed more fine grained
        # control in cases like: a, b = [int, str] where rhs would get
        # type List[object]
        rvalues: List[Expression] = []
        iterable_type: Optional[Type] = None
        last_idx: Optional[int] = None
        for idx_rval, rval in enumerate(rvalue.items):
            if isinstance(rval, StarExpr):
                typs = get_proper_type(self.expr_checker.visit_star_expr(rval).type)
                if isinstance(typs, TupleType):
                    rvalues.extend([TempNode(typ) for typ in typs.items])
                elif self.type_is_iterable(typs) and isinstance(typs, Instance):
                    if (iterable_type is not None
                            and iterable_type != self.iterable_item_type(typs)):
                        self.fail(message_registry.CONTIGUOUS_ITERABLE_EXPECTED, context)
                    else:
                        if last_idx is None or last_idx + 1 == idx_rval:
                            rvalues.append(rval)
                            last_idx = idx_rval
                            iterable_type = self.iterable_item_type(typs)
                        else:
                            self.fail(message_registry.CONTIGUOUS_ITERABLE_EXPECTED, context)
                else:
                    self.fail(message_registry.ITERABLE_TYPE_EXPECTED.format(typs),
                         context)
            else:
                rvalues.append(rval)
        iterable_start: Optional[int] = None
        iterable_end: Optional[int] = None
        for i, rval in enumerate(rvalues):
            if isinstance(rval, StarExpr):
                typs = get_proper_type(self.expr_checker.visit_star_expr(rval).type)
                if self.type_is_iterable(typs) and isinstance(typs, Instance):
                    if iterable_start is None:
                        iterable_start = i
                    iterable_end = i
        if (iterable_start is not None
                and iterable_end is not None
                and iterable_type is not None):
            iterable_num = iterable_end - iterable_start + 1
            rvalue_needed = len(lvalues) - (len(rvalues) - iterable_num)
            if rvalue_needed &gt; 0:
                rvalues = rvalues[0: iterable_start] + [TempNode(iterable_type)
                    for i in range(rvalue_needed)] + rvalues[iterable_end + 1:]

        if self.check_rvalue_count_in_assignment(lvalues, len(rvalues), context):
            star_index = next((i for i, lv in enumerate(lvalues) if
                               isinstance(lv, StarExpr)), len(lvalues))

            left_lvs = lvalues[:star_index]
            star_lv = cast(StarExpr,
                           lvalues[star_index]) if star_index != len(lvalues) else None
            right_lvs = lvalues[star_index + 1:]

            left_rvs, star_rvs, right_rvs = self.split_around_star(
                rvalues, star_index, len(lvalues))

            lr_pairs = list(zip(left_lvs, left_rvs))
            if star_lv:
                rv_list = ListExpr(star_rvs)
                rv_list.set_line(rvalue.get_line())
                lr_pairs.append((star_lv.expr, rv_list))
            lr_pairs.extend(zip(right_lvs, right_rvs))

            for lv, rv in lr_pairs:
                self.check_assignment(lv, rv, infer_lvalue_type)
    else:
        self.check_multi_assignment(lvalues, rvalue, context, infer_lvalue_type)

</t>
<t tx="ekr.20220525082933.434">def check_rvalue_count_in_assignment(self, lvalues: List[Lvalue], rvalue_count: int,
                                     context: Context) -&gt; bool:
    if any(isinstance(lvalue, StarExpr) for lvalue in lvalues):
        if len(lvalues) - 1 &gt; rvalue_count:
            self.msg.wrong_number_values_to_unpack(rvalue_count,
                                                   len(lvalues) - 1, context)
            return False
    elif rvalue_count != len(lvalues):
        self.msg.wrong_number_values_to_unpack(rvalue_count, len(lvalues), context)
        return False
    return True

</t>
<t tx="ekr.20220525082933.435">def check_multi_assignment(self, lvalues: List[Lvalue],
                           rvalue: Expression,
                           context: Context,
                           infer_lvalue_type: bool = True,
                           rv_type: Optional[Type] = None,
                           undefined_rvalue: bool = False) -&gt; None:
    """Check the assignment of one rvalue to a number of lvalues."""

    # Infer the type of an ordinary rvalue expression.
    # TODO: maybe elsewhere; redundant.
    rvalue_type = get_proper_type(rv_type or self.expr_checker.accept(rvalue))

    if isinstance(rvalue_type, UnionType):
        # If this is an Optional type in non-strict Optional code, unwrap it.
        relevant_items = rvalue_type.relevant_items()
        if len(relevant_items) == 1:
            rvalue_type = get_proper_type(relevant_items[0])

    if isinstance(rvalue_type, AnyType):
        for lv in lvalues:
            if isinstance(lv, StarExpr):
                lv = lv.expr
            temp_node = self.temp_node(AnyType(TypeOfAny.from_another_any,
                                               source_any=rvalue_type), context)
            self.check_assignment(lv, temp_node, infer_lvalue_type)
    elif isinstance(rvalue_type, TupleType):
        self.check_multi_assignment_from_tuple(lvalues, rvalue, rvalue_type,
                                               context, undefined_rvalue, infer_lvalue_type)
    elif isinstance(rvalue_type, UnionType):
        self.check_multi_assignment_from_union(lvalues, rvalue, rvalue_type, context,
                                               infer_lvalue_type)
    elif isinstance(rvalue_type, Instance) and rvalue_type.type.fullname == 'builtins.str':
        self.msg.unpacking_strings_disallowed(context)
    else:
        self.check_multi_assignment_from_iterable(lvalues, rvalue_type,
                                                  context, infer_lvalue_type)

</t>
<t tx="ekr.20220525082933.436">def check_multi_assignment_from_union(self, lvalues: List[Expression], rvalue: Expression,
                                      rvalue_type: UnionType, context: Context,
                                      infer_lvalue_type: bool) -&gt; None:
    """Check assignment to multiple lvalue targets when rvalue type is a Union[...].
    For example:

        t: Union[Tuple[int, int], Tuple[str, str]]
        x, y = t
        reveal_type(x)  # Union[int, str]

    The idea in this case is to process the assignment for every item of the union.
    Important note: the types are collected in two places, 'union_types' contains
    inferred types for first assignments, 'assignments' contains the narrowed types
    for binder.
    """
    self.no_partial_types = True
    transposed: Tuple[List[Type], ...] = tuple([] for _ in self.flatten_lvalues(lvalues))
    # Notify binder that we want to defer bindings and instead collect types.
    with self.binder.accumulate_type_assignments() as assignments:
        for item in rvalue_type.items:
            # Type check the assignment separately for each union item and collect
            # the inferred lvalue types for each union item.
            self.check_multi_assignment(lvalues, rvalue, context,
                                        infer_lvalue_type=infer_lvalue_type,
                                        rv_type=item, undefined_rvalue=True)
            for t, lv in zip(transposed, self.flatten_lvalues(lvalues)):
                # We can access _type_maps directly since temporary type maps are
                # only created within expressions.
                t.append(self._type_maps[0].pop(lv, AnyType(TypeOfAny.special_form)))
    union_types = tuple(make_simplified_union(col) for col in transposed)
    for expr, items in assignments.items():
        # Bind a union of types collected in 'assignments' to every expression.
        if isinstance(expr, StarExpr):
            expr = expr.expr

        # TODO: See todo in binder.py, ConditionalTypeBinder.assign_type
        # It's unclear why the 'declared_type' param is sometimes 'None'
        clean_items: List[Tuple[Type, Type]] = []
        for type, declared_type in items:
            assert declared_type is not None
            clean_items.append((type, declared_type))

        # TODO: fix signature of zip() in typeshed.
        types, declared_types = cast(Any, zip)(*clean_items)
        self.binder.assign_type(expr,
                                make_simplified_union(list(types)),
                                make_simplified_union(list(declared_types)),
                                False)
    for union, lv in zip(union_types, self.flatten_lvalues(lvalues)):
        # Properly store the inferred types.
        _1, _2, inferred = self.check_lvalue(lv)
        if inferred:
            self.set_inferred_type(inferred, lv, union)
        else:
            self.store_type(lv, union)
    self.no_partial_types = False

</t>
<t tx="ekr.20220525082933.437">def flatten_lvalues(self, lvalues: List[Expression]) -&gt; List[Expression]:
    res: List[Expression] = []
    for lv in lvalues:
        if isinstance(lv, (TupleExpr, ListExpr)):
            res.extend(self.flatten_lvalues(lv.items))
        if isinstance(lv, StarExpr):
            # Unwrap StarExpr, since it is unwrapped by other helpers.
            lv = lv.expr
        res.append(lv)
    return res

</t>
<t tx="ekr.20220525082933.438">def check_multi_assignment_from_tuple(self, lvalues: List[Lvalue], rvalue: Expression,
                                      rvalue_type: TupleType, context: Context,
                                      undefined_rvalue: bool,
                                      infer_lvalue_type: bool = True) -&gt; None:
    if self.check_rvalue_count_in_assignment(lvalues, len(rvalue_type.items), context):
        star_index = next((i for i, lv in enumerate(lvalues)
                           if isinstance(lv, StarExpr)), len(lvalues))

        left_lvs = lvalues[:star_index]
        star_lv = cast(StarExpr, lvalues[star_index]) if star_index != len(lvalues) else None
        right_lvs = lvalues[star_index + 1:]

        if not undefined_rvalue:
            # Infer rvalue again, now in the correct type context.
            lvalue_type = self.lvalue_type_for_inference(lvalues, rvalue_type)
            reinferred_rvalue_type = get_proper_type(self.expr_checker.accept(rvalue,
                                                                              lvalue_type))

            if isinstance(reinferred_rvalue_type, UnionType):
                # If this is an Optional type in non-strict Optional code, unwrap it.
                relevant_items = reinferred_rvalue_type.relevant_items()
                if len(relevant_items) == 1:
                    reinferred_rvalue_type = get_proper_type(relevant_items[0])
            if isinstance(reinferred_rvalue_type, UnionType):
                self.check_multi_assignment_from_union(lvalues, rvalue,
                                                       reinferred_rvalue_type, context,
                                                       infer_lvalue_type)
                return
            if isinstance(reinferred_rvalue_type, AnyType):
                # We can get Any if the current node is
                # deferred. Doing more inference in deferred nodes
                # is hard, so give up for now.  We can also get
                # here if reinferring types above changes the
                # inferred return type for an overloaded function
                # to be ambiguous.
                return
            assert isinstance(reinferred_rvalue_type, TupleType)
            rvalue_type = reinferred_rvalue_type

        left_rv_types, star_rv_types, right_rv_types = self.split_around_star(
            rvalue_type.items, star_index, len(lvalues))

        for lv, rv_type in zip(left_lvs, left_rv_types):
            self.check_assignment(lv, self.temp_node(rv_type, context), infer_lvalue_type)
        if star_lv:
            list_expr = ListExpr([self.temp_node(rv_type, context)
                                  for rv_type in star_rv_types])
            list_expr.set_line(context.get_line())
            self.check_assignment(star_lv.expr, list_expr, infer_lvalue_type)
        for lv, rv_type in zip(right_lvs, right_rv_types):
            self.check_assignment(lv, self.temp_node(rv_type, context), infer_lvalue_type)

</t>
<t tx="ekr.20220525082933.439">def lvalue_type_for_inference(self, lvalues: List[Lvalue], rvalue_type: TupleType) -&gt; Type:
    star_index = next((i for i, lv in enumerate(lvalues)
                       if isinstance(lv, StarExpr)), len(lvalues))
    left_lvs = lvalues[:star_index]
    star_lv = cast(StarExpr, lvalues[star_index]) if star_index != len(lvalues) else None
    right_lvs = lvalues[star_index + 1:]
    left_rv_types, star_rv_types, right_rv_types = self.split_around_star(
        rvalue_type.items, star_index, len(lvalues))

    type_parameters: List[Type] = []

    def append_types_for_inference(lvs: List[Expression], rv_types: List[Type]) -&gt; None:
        for lv, rv_type in zip(lvs, rv_types):
            sub_lvalue_type, index_expr, inferred = self.check_lvalue(lv)
            if sub_lvalue_type and not isinstance(sub_lvalue_type, PartialType):
                type_parameters.append(sub_lvalue_type)
            else:  # index lvalue
                # TODO Figure out more precise type context, probably
                #      based on the type signature of the _set method.
                type_parameters.append(rv_type)

    append_types_for_inference(left_lvs, left_rv_types)

    if star_lv:
        sub_lvalue_type, index_expr, inferred = self.check_lvalue(star_lv.expr)
        if sub_lvalue_type and not isinstance(sub_lvalue_type, PartialType):
            type_parameters.extend([sub_lvalue_type] * len(star_rv_types))
        else:  # index lvalue
            # TODO Figure out more precise type context, probably
            #      based on the type signature of the _set method.
            type_parameters.extend(star_rv_types)

    append_types_for_inference(right_lvs, right_rv_types)

    return TupleType(type_parameters, self.named_type('builtins.tuple'))

</t>
<t tx="ekr.20220525082933.44">def plain_generator() -&gt; Generator[str, None, int]:
    yield 'a'
    return 1

</t>
<t tx="ekr.20220525082933.440">def split_around_star(self, items: List[T], star_index: int,
                      length: int) -&gt; Tuple[List[T], List[T], List[T]]:
    """Splits a list of items in three to match another list of length 'length'
    that contains a starred expression at 'star_index' in the following way:

    star_index = 2, length = 5 (i.e., [a,b,*,c,d]), items = [1,2,3,4,5,6,7]
    returns in: ([1,2], [3,4,5], [6,7])
    """
    nr_right_of_star = length - star_index - 1
    right_index = -nr_right_of_star if nr_right_of_star != 0 else len(items)
    left = items[:star_index]
    star = items[star_index:right_index]
    right = items[right_index:]
    return left, star, right

</t>
<t tx="ekr.20220525082933.441">def type_is_iterable(self, type: Type) -&gt; bool:
    type = get_proper_type(type)
    if isinstance(type, CallableType) and type.is_type_obj():
        type = type.fallback
    return is_subtype(type, self.named_generic_type('typing.Iterable',
                                                    [AnyType(TypeOfAny.special_form)]))

</t>
<t tx="ekr.20220525082933.442">def check_multi_assignment_from_iterable(self, lvalues: List[Lvalue], rvalue_type: Type,
                                         context: Context,
                                         infer_lvalue_type: bool = True) -&gt; None:
    rvalue_type = get_proper_type(rvalue_type)
    if self.type_is_iterable(rvalue_type) and isinstance(rvalue_type, Instance):
        item_type = self.iterable_item_type(rvalue_type)
        for lv in lvalues:
            if isinstance(lv, StarExpr):
                items_type = self.named_generic_type('builtins.list', [item_type])
                self.check_assignment(lv.expr, self.temp_node(items_type, context),
                                      infer_lvalue_type)
            else:
                self.check_assignment(lv, self.temp_node(item_type, context),
                                      infer_lvalue_type)
    else:
        self.msg.type_not_iterable(rvalue_type, context)

</t>
<t tx="ekr.20220525082933.443">def check_lvalue(self, lvalue: Lvalue) -&gt; Tuple[Optional[Type],
                                                Optional[IndexExpr],
                                                Optional[Var]]:
    lvalue_type = None
    index_lvalue = None
    inferred = None

    if self.is_definition(lvalue) and (
        not isinstance(lvalue, NameExpr) or isinstance(lvalue.node, Var)
    ):
        if isinstance(lvalue, NameExpr):
            inferred = cast(Var, lvalue.node)
            assert isinstance(inferred, Var)
        else:
            assert isinstance(lvalue, MemberExpr)
            self.expr_checker.accept(lvalue.expr)
            inferred = lvalue.def_var
    elif isinstance(lvalue, IndexExpr):
        index_lvalue = lvalue
    elif isinstance(lvalue, MemberExpr):
        lvalue_type = self.expr_checker.analyze_ordinary_member_access(lvalue, True)
        self.store_type(lvalue, lvalue_type)
    elif isinstance(lvalue, NameExpr):
        lvalue_type = self.expr_checker.analyze_ref_expr(lvalue, lvalue=True)
        self.store_type(lvalue, lvalue_type)
    elif isinstance(lvalue, TupleExpr) or isinstance(lvalue, ListExpr):
        types = [self.check_lvalue(sub_expr)[0] or
                 # This type will be used as a context for further inference of rvalue,
                 # we put Uninhabited if there is no information available from lvalue.
                 UninhabitedType() for sub_expr in lvalue.items]
        lvalue_type = TupleType(types, self.named_type('builtins.tuple'))
    elif isinstance(lvalue, StarExpr):
        typ, _, _ = self.check_lvalue(lvalue.expr)
        lvalue_type = StarType(typ) if typ else None
    else:
        lvalue_type = self.expr_checker.accept(lvalue)

    return lvalue_type, index_lvalue, inferred

</t>
<t tx="ekr.20220525082933.444">def is_definition(self, s: Lvalue) -&gt; bool:
    if isinstance(s, NameExpr):
        if s.is_inferred_def:
            return True
        # If the node type is not defined, this must the first assignment
        # that we process =&gt; this is a definition, even though the semantic
        # analyzer did not recognize this as such. This can arise in code
        # that uses isinstance checks, if type checking of the primary
        # definition is skipped due to an always False type check.
        node = s.node
        if isinstance(node, Var):
            return node.type is None
    elif isinstance(s, MemberExpr):
        return s.is_inferred_def
    return False

</t>
<t tx="ekr.20220525082933.445">def infer_variable_type(self, name: Var, lvalue: Lvalue,
                        init_type: Type, context: Context) -&gt; None:
    """Infer the type of initialized variables from initializer type."""
    init_type = get_proper_type(init_type)
    if isinstance(init_type, DeletedType):
        self.msg.deleted_as_rvalue(init_type, context)
    elif not is_valid_inferred_type(init_type) and not self.no_partial_types:
        # We cannot use the type of the initialization expression for full type
        # inference (it's not specific enough), but we might be able to give
        # partial type which will be made more specific later. A partial type
        # gets generated in assignment like 'x = []' where item type is not known.
        if not self.infer_partial_type(name, lvalue, init_type):
            self.msg.need_annotation_for_var(name, context, self.options.python_version)
            self.set_inference_error_fallback_type(name, lvalue, init_type)
    elif (isinstance(lvalue, MemberExpr) and self.inferred_attribute_types is not None
          and lvalue.def_var and lvalue.def_var in self.inferred_attribute_types
          and not is_same_type(self.inferred_attribute_types[lvalue.def_var], init_type)):
        # Multiple, inconsistent types inferred for an attribute.
        self.msg.need_annotation_for_var(name, context, self.options.python_version)
        name.type = AnyType(TypeOfAny.from_error)
    else:
        # Infer type of the target.

        # Make the type more general (strip away function names etc.).
        init_type = strip_type(init_type)

        self.set_inferred_type(name, lvalue, init_type)

</t>
<t tx="ekr.20220525082933.446">def infer_partial_type(self, name: Var, lvalue: Lvalue, init_type: Type) -&gt; bool:
    init_type = get_proper_type(init_type)
    if isinstance(init_type, NoneType):
        partial_type = PartialType(None, name)
    elif isinstance(init_type, Instance):
        fullname = init_type.type.fullname
        is_ref = isinstance(lvalue, RefExpr)
        if (is_ref and
                (fullname == 'builtins.list' or
                 fullname == 'builtins.set' or
                 fullname == 'builtins.dict' or
                 fullname == 'collections.OrderedDict') and
                all(isinstance(t, (NoneType, UninhabitedType))
                    for t in get_proper_types(init_type.args))):
            partial_type = PartialType(init_type.type, name)
        elif is_ref and fullname == 'collections.defaultdict':
            arg0 = get_proper_type(init_type.args[0])
            arg1 = get_proper_type(init_type.args[1])
            if (isinstance(arg0, (NoneType, UninhabitedType)) and
                    self.is_valid_defaultdict_partial_value_type(arg1)):
                arg1 = erase_type(arg1)
                assert isinstance(arg1, Instance)
                partial_type = PartialType(init_type.type, name, arg1)
            else:
                return False
        else:
            return False
    else:
        return False
    self.set_inferred_type(name, lvalue, partial_type)
    self.partial_types[-1].map[name] = lvalue
    return True

</t>
<t tx="ekr.20220525082933.447">def is_valid_defaultdict_partial_value_type(self, t: ProperType) -&gt; bool:
    """Check if t can be used as the basis for a partial defaultdict value type.

    Examples:

      * t is 'int' --&gt; True
      * t is 'list[&lt;nothing&gt;]' --&gt; True
      * t is 'dict[...]' --&gt; False (only generic types with a single type
        argument supported)
    """
    if not isinstance(t, Instance):
        return False
    if len(t.args) == 0:
        return True
    if len(t.args) == 1:
        arg = get_proper_type(t.args[0])
        # TODO: This is too permissive -- we only allow TypeVarType since
        #       they leak in cases like defaultdict(list) due to a bug.
        #       This can result in incorrect types being inferred, but only
        #       in rare cases.
        if isinstance(arg, (TypeVarType, UninhabitedType, NoneType)):
            return True
    return False

</t>
<t tx="ekr.20220525082933.448">def set_inferred_type(self, var: Var, lvalue: Lvalue, type: Type) -&gt; None:
    """Store inferred variable type.

    Store the type to both the variable node and the expression node that
    refers to the variable (lvalue). If var is None, do nothing.
    """
    if var and not self.current_node_deferred:
        var.type = type
        var.is_inferred = True
        if var not in self.var_decl_frames:
            # Used for the hack to improve optional type inference in conditionals
            self.var_decl_frames[var] = {frame.id for frame in self.binder.frames}
        if isinstance(lvalue, MemberExpr) and self.inferred_attribute_types is not None:
            # Store inferred attribute type so that we can check consistency afterwards.
            if lvalue.def_var is not None:
                self.inferred_attribute_types[lvalue.def_var] = type
        self.store_type(lvalue, type)

</t>
<t tx="ekr.20220525082933.449">def set_inference_error_fallback_type(self, var: Var, lvalue: Lvalue, type: Type) -&gt; None:
    """Store best known type for variable if type inference failed.

    If a program ignores error on type inference error, the variable should get some
    inferred type so that if can used later on in the program. Example:

      x = []  # type: ignore
      x.append(1)   # Should be ok!

    We implement this here by giving x a valid type (replacing inferred &lt;nothing&gt; with Any).
    """
    fallback = self.inference_error_fallback_type(type)
    self.set_inferred_type(var, lvalue, fallback)

</t>
<t tx="ekr.20220525082933.45">async def plain_coroutine() -&gt; int:
    return 1

</t>
<t tx="ekr.20220525082933.450">def inference_error_fallback_type(self, type: Type) -&gt; Type:
    fallback = type.accept(SetNothingToAny())
    # Type variables may leak from inference, see https://github.com/python/mypy/issues/5738,
    # we therefore need to erase them.
    return erase_typevars(fallback)

</t>
<t tx="ekr.20220525082933.451">def check_simple_assignment(self, lvalue_type: Optional[Type], rvalue: Expression,
                            context: Context,
                            msg: str = message_registry.INCOMPATIBLE_TYPES_IN_ASSIGNMENT,
                            lvalue_name: str = 'variable',
                            rvalue_name: str = 'expression', *,
                            code: Optional[ErrorCode] = None) -&gt; Type:
    if self.is_stub and isinstance(rvalue, EllipsisExpr):
        # '...' is always a valid initializer in a stub.
        return AnyType(TypeOfAny.special_form)
    else:
        lvalue_type = get_proper_type(lvalue_type)
        always_allow_any = lvalue_type is not None and not isinstance(lvalue_type, AnyType)
        rvalue_type = self.expr_checker.accept(rvalue, lvalue_type,
                                               always_allow_any=always_allow_any)
        rvalue_type = get_proper_type(rvalue_type)
        if isinstance(rvalue_type, DeletedType):
            self.msg.deleted_as_rvalue(rvalue_type, context)
        if isinstance(lvalue_type, DeletedType):
            self.msg.deleted_as_lvalue(lvalue_type, context)
        elif lvalue_type:
            self.check_subtype(rvalue_type, lvalue_type, context, msg,
                               f'{rvalue_name} has type',
                               f'{lvalue_name} has type', code=code)
        return rvalue_type

</t>
<t tx="ekr.20220525082933.452">def check_member_assignment(self, instance_type: Type, attribute_type: Type,
                            rvalue: Expression, context: Context) -&gt; Tuple[Type, Type, bool]:
    """Type member assignment.

    This defers to check_simple_assignment, unless the member expression
    is a descriptor, in which case this checks descriptor semantics as well.

    Return the inferred rvalue_type, inferred lvalue_type, and whether to use the binder
    for this assignment.

    Note: this method exists here and not in checkmember.py, because we need to take
    care about interaction between binder and __set__().
    """
    instance_type = get_proper_type(instance_type)
    attribute_type = get_proper_type(attribute_type)
    # Descriptors don't participate in class-attribute access
    if ((isinstance(instance_type, FunctionLike) and instance_type.is_type_obj()) or
            isinstance(instance_type, TypeType)):
        rvalue_type = self.check_simple_assignment(attribute_type, rvalue, context,
                                                   code=codes.ASSIGNMENT)
        return rvalue_type, attribute_type, True

    if not isinstance(attribute_type, Instance):
        # TODO: support __set__() for union types.
        rvalue_type = self.check_simple_assignment(attribute_type, rvalue, context,
                                                   code=codes.ASSIGNMENT)
        return rvalue_type, attribute_type, True

    mx = MemberContext(
        is_lvalue=False, is_super=False, is_operator=False,
        original_type=instance_type, context=context, self_type=None,
        msg=self.msg, chk=self,
    )
    get_type = analyze_descriptor_access(attribute_type, mx)
    if not attribute_type.type.has_readable_member('__set__'):
        # If there is no __set__, we type-check that the assigned value matches
        # the return type of __get__. This doesn't match the python semantics,
        # (which allow you to override the descriptor with any value), but preserves
        # the type of accessing the attribute (even after the override).
        rvalue_type = self.check_simple_assignment(get_type, rvalue, context,
                                                   code=codes.ASSIGNMENT)
        return rvalue_type, get_type, True

    dunder_set = attribute_type.type.get_method('__set__')
    if dunder_set is None:
        self.fail(message_registry.DESCRIPTOR_SET_NOT_CALLABLE.format(attribute_type), context)
        return AnyType(TypeOfAny.from_error), get_type, False

    bound_method = analyze_decorator_or_funcbase_access(
        defn=dunder_set, itype=attribute_type, info=attribute_type.type,
        self_type=attribute_type, name='__set__', mx=mx)
    typ = map_instance_to_supertype(attribute_type, dunder_set.info)
    dunder_set_type = expand_type_by_instance(bound_method, typ)

    callable_name = self.expr_checker.method_fullname(attribute_type, "__set__")
    dunder_set_type = self.expr_checker.transform_callee_type(
        callable_name, dunder_set_type,
        [TempNode(instance_type, context=context), rvalue],
        [nodes.ARG_POS, nodes.ARG_POS],
        context, object_type=attribute_type,
    )

    # For non-overloaded setters, the result should be type-checked like a regular assignment.
    # Hence, we first only try to infer the type by using the rvalue as type context.
    type_context = rvalue
    with self.msg.filter_errors():
        _, inferred_dunder_set_type = self.expr_checker.check_call(
            dunder_set_type,
            [TempNode(instance_type, context=context), type_context],
            [nodes.ARG_POS, nodes.ARG_POS],
            context, object_type=attribute_type,
            callable_name=callable_name)

    # And now we in fact type check the call, to show errors related to wrong arguments
    # count, etc., replacing the type context for non-overloaded setters only.
    inferred_dunder_set_type = get_proper_type(inferred_dunder_set_type)
    if isinstance(inferred_dunder_set_type, CallableType):
        type_context = TempNode(AnyType(TypeOfAny.special_form), context=context)
    self.expr_checker.check_call(
        dunder_set_type,
        [TempNode(instance_type, context=context), type_context],
        [nodes.ARG_POS, nodes.ARG_POS],
        context, object_type=attribute_type,
        callable_name=callable_name)

    # In the following cases, a message already will have been recorded in check_call.
    if ((not isinstance(inferred_dunder_set_type, CallableType)) or
            (len(inferred_dunder_set_type.arg_types) &lt; 2)):
        return AnyType(TypeOfAny.from_error), get_type, False

    set_type = inferred_dunder_set_type.arg_types[1]
    # Special case: if the rvalue_type is a subtype of both '__get__' and '__set__' types,
    # and '__get__' type is narrower than '__set__', then we invoke the binder to narrow type
    # by this assignment. Technically, this is not safe, but in practice this is
    # what a user expects.
    rvalue_type = self.check_simple_assignment(set_type, rvalue, context,
                                               code=codes.ASSIGNMENT)
    infer = is_subtype(rvalue_type, get_type) and is_subtype(get_type, set_type)
    return rvalue_type if infer else set_type, get_type, infer

</t>
<t tx="ekr.20220525082933.453">def check_indexed_assignment(self, lvalue: IndexExpr,
                             rvalue: Expression, context: Context) -&gt; None:
    """Type check indexed assignment base[index] = rvalue.

    The lvalue argument is the base[index] expression.
    """
    self.try_infer_partial_type_from_indexed_assignment(lvalue, rvalue)
    basetype = get_proper_type(self.expr_checker.accept(lvalue.base))
    method_type = self.expr_checker.analyze_external_member_access(
        '__setitem__', basetype, lvalue)

    lvalue.method_type = method_type
    self.expr_checker.check_method_call(
        '__setitem__', basetype, method_type, [lvalue.index, rvalue],
        [nodes.ARG_POS, nodes.ARG_POS], context)

</t>
<t tx="ekr.20220525082933.454">def try_infer_partial_type_from_indexed_assignment(
        self, lvalue: IndexExpr, rvalue: Expression) -&gt; None:
    # TODO: Should we share some of this with try_infer_partial_type?
    var = None
    if isinstance(lvalue.base, RefExpr) and isinstance(lvalue.base.node, Var):
        var = lvalue.base.node
    elif isinstance(lvalue.base, MemberExpr):
        var = self.expr_checker.get_partial_self_var(lvalue.base)
    if isinstance(var, Var):
        if isinstance(var.type, PartialType):
            type_type = var.type.type
            if type_type is None:
                return  # The partial type is None.
            partial_types = self.find_partial_types(var)
            if partial_types is None:
                return
            typename = type_type.fullname
            if (typename == 'builtins.dict'
                    or typename == 'collections.OrderedDict'
                    or typename == 'collections.defaultdict'):
                # TODO: Don't infer things twice.
                key_type = self.expr_checker.accept(lvalue.index)
                value_type = self.expr_checker.accept(rvalue)
                if (is_valid_inferred_type(key_type) and
                        is_valid_inferred_type(value_type) and
                        not self.current_node_deferred and
                        not (typename == 'collections.defaultdict' and
                             var.type.value_type is not None and
                             not is_equivalent(value_type, var.type.value_type))):
                    var.type = self.named_generic_type(typename,
                                                       [key_type, value_type])
                    del partial_types[var]

</t>
<t tx="ekr.20220525082933.455">def type_requires_usage(self, typ: Type) -&gt; Optional[Tuple[str, ErrorCode]]:
    """Some types require usage in all cases. The classic example is
    an unused coroutine.

    In the case that it does require usage, returns a note to attach
    to the error message.
    """
    proper_type = get_proper_type(typ)
    if isinstance(proper_type, Instance):
        # We use different error codes for generic awaitable vs coroutine.
        # Coroutines are on by default, whereas generic awaitables are not.
        if proper_type.type.fullname == "typing.Coroutine":
            return ("Are you missing an await?", UNUSED_COROUTINE)
        if proper_type.type.get("__await__") is not None:
            return ("Are you missing an await?", UNUSED_AWAITABLE)
    return None

</t>
<t tx="ekr.20220525082933.456">def visit_expression_stmt(self, s: ExpressionStmt) -&gt; None:
    expr_type = self.expr_checker.accept(s.expr, allow_none_return=True, always_allow_any=True)
    error_note_and_code = self.type_requires_usage(expr_type)
    if error_note_and_code:
        error_note, code = error_note_and_code
        self.fail(
            message_registry.TYPE_MUST_BE_USED.format(format_type(expr_type)), s, code=code
        )
        self.note(error_note, s, code=code)

</t>
<t tx="ekr.20220525082933.457">def visit_return_stmt(self, s: ReturnStmt) -&gt; None:
    """Type check a return statement."""
    self.check_return_stmt(s)
    self.binder.unreachable()

</t>
<t tx="ekr.20220525082933.458">def check_return_stmt(self, s: ReturnStmt) -&gt; None:
    defn = self.scope.top_function()
    if defn is not None:
        if defn.is_generator:
            return_type = self.get_generator_return_type(self.return_types[-1],
                                                         defn.is_coroutine)
        elif defn.is_coroutine:
            return_type = self.get_coroutine_return_type(self.return_types[-1])
        else:
            return_type = self.return_types[-1]
        return_type = get_proper_type(return_type)

        if isinstance(return_type, UninhabitedType):
            self.fail(message_registry.NO_RETURN_EXPECTED, s)
            return

        if s.expr:
            is_lambda = isinstance(self.scope.top_function(), LambdaExpr)
            declared_none_return = isinstance(return_type, NoneType)
            declared_any_return = isinstance(return_type, AnyType)

            # This controls whether or not we allow a function call that
            # returns None as the expression of this return statement.
            # E.g. `return f()` for some `f` that returns None.  We allow
            # this only if we're in a lambda or in a function that returns
            # `None` or `Any`.
            allow_none_func_call = is_lambda or declared_none_return or declared_any_return

            # Return with a value.
            typ = get_proper_type(self.expr_checker.accept(
                s.expr, return_type, allow_none_return=allow_none_func_call))

            if defn.is_async_generator:
                self.fail(message_registry.RETURN_IN_ASYNC_GENERATOR, s)
                return
            # Returning a value of type Any is always fine.
            if isinstance(typ, AnyType):
                # (Unless you asked to be warned in that case, and the
                # function is not declared to return Any)
                if (self.options.warn_return_any
                    and not self.current_node_deferred
                    and not is_proper_subtype(AnyType(TypeOfAny.special_form), return_type)
                    and not (defn.name in BINARY_MAGIC_METHODS and
                             is_literal_not_implemented(s.expr))
                    and not (isinstance(return_type, Instance) and
                             return_type.type.fullname == 'builtins.object')):
                    self.msg.incorrectly_returning_any(return_type, s)
                return

            # Disallow return expressions in functions declared to return
            # None, subject to two exceptions below.
            if declared_none_return:
                # Lambdas are allowed to have None returns.
                # Functions returning a value of type None are allowed to have a None return.
                if is_lambda or isinstance(typ, NoneType):
                    return
                self.fail(message_registry.NO_RETURN_VALUE_EXPECTED, s)
            else:
                self.check_subtype(
                    subtype_label='got',
                    subtype=typ,
                    supertype_label='expected',
                    supertype=return_type,
                    context=s.expr,
                    outer_context=s,
                    msg=message_registry.INCOMPATIBLE_RETURN_VALUE_TYPE,
                    code=codes.RETURN_VALUE)
        else:
            # Empty returns are valid in Generators with Any typed returns, but not in
            # coroutines.
            if (defn.is_generator and not defn.is_coroutine and
                    isinstance(return_type, AnyType)):
                return

            if isinstance(return_type, (NoneType, AnyType)):
                return

            if self.in_checked_function():
                self.fail(message_registry.RETURN_VALUE_EXPECTED, s)

</t>
<t tx="ekr.20220525082933.459">def visit_if_stmt(self, s: IfStmt) -&gt; None:
    """Type check an if statement."""
    # This frame records the knowledge from previous if/elif clauses not being taken.
    # Fall-through to the original frame is handled explicitly in each block.
    with self.binder.frame_context(can_skip=False, conditional_frame=True, fall_through=0):
        for e, b in zip(s.expr, s.body):
            t = get_proper_type(self.expr_checker.accept(e))

            if isinstance(t, DeletedType):
                self.msg.deleted_as_rvalue(t, s)

            if_map, else_map = self.find_isinstance_check(e)

            # XXX Issue a warning if condition is always False?
            with self.binder.frame_context(can_skip=True, fall_through=2):
                self.push_type_map(if_map)
                self.accept(b)

            # XXX Issue a warning if condition is always True?
            self.push_type_map(else_map)

        with self.binder.frame_context(can_skip=False, fall_through=2):
            if s.else_body:
                self.accept(s.else_body)

</t>
<t tx="ekr.20220525082933.46">@coroutine
def decorated_generator() -&gt; Generator[str, None, int]:
    yield 'a'
    return 1

</t>
<t tx="ekr.20220525082933.460">def visit_while_stmt(self, s: WhileStmt) -&gt; None:
    """Type check a while statement."""
    if_stmt = IfStmt([s.expr], [s.body], None)
    if_stmt.set_line(s.get_line(), s.get_column())
    self.accept_loop(if_stmt, s.else_body,
                     exit_condition=s.expr)

</t>
<t tx="ekr.20220525082933.461">def visit_operator_assignment_stmt(self,
                                   s: OperatorAssignmentStmt) -&gt; None:
    """Type check an operator assignment statement, e.g. x += 1."""
    self.try_infer_partial_generic_type_from_assignment(s.lvalue, s.rvalue, s.op)
    if isinstance(s.lvalue, MemberExpr):
        # Special case, some additional errors may be given for
        # assignments to read-only or final attributes.
        lvalue_type = self.expr_checker.visit_member_expr(s.lvalue, True)
    else:
        lvalue_type = self.expr_checker.accept(s.lvalue)
    inplace, method = infer_operator_assignment_method(lvalue_type, s.op)
    if inplace:
        # There is __ifoo__, treat as x = x.__ifoo__(y)
        rvalue_type, method_type = self.expr_checker.check_op(
            method, lvalue_type, s.rvalue, s)
        if not is_subtype(rvalue_type, lvalue_type):
            self.msg.incompatible_operator_assignment(s.op, s)
    else:
        # There is no __ifoo__, treat as x = x &lt;foo&gt; y
        expr = OpExpr(s.op, s.lvalue, s.rvalue)
        expr.set_line(s)
        self.check_assignment(lvalue=s.lvalue, rvalue=expr,
                              infer_lvalue_type=True, new_syntax=False)
    self.check_final(s)

</t>
<t tx="ekr.20220525082933.462">def visit_assert_stmt(self, s: AssertStmt) -&gt; None:
    self.expr_checker.accept(s.expr)

    if isinstance(s.expr, TupleExpr) and len(s.expr.items) &gt; 0:
        self.fail(message_registry.MALFORMED_ASSERT, s)

    # If this is asserting some isinstance check, bind that type in the following code
    true_map, else_map = self.find_isinstance_check(s.expr)
    if s.msg is not None:
        self.expr_checker.analyze_cond_branch(else_map, s.msg, None)
    self.push_type_map(true_map)

</t>
<t tx="ekr.20220525082933.463">def visit_raise_stmt(self, s: RaiseStmt) -&gt; None:
    """Type check a raise statement."""
    if s.expr:
        self.type_check_raise(s.expr, s)
    if s.from_expr:
        self.type_check_raise(s.from_expr, s, optional=True)
    self.binder.unreachable()

</t>
<t tx="ekr.20220525082933.464">def type_check_raise(self, e: Expression, s: RaiseStmt,
                     optional: bool = False) -&gt; None:
    typ = get_proper_type(self.expr_checker.accept(e))
    if isinstance(typ, DeletedType):
        self.msg.deleted_as_rvalue(typ, e)
        return

    if self.options.python_version[0] == 2:
        # Since `raise` has very different rule on python2, we use a different helper.
        # https://github.com/python/mypy/pull/11289
        self._type_check_raise_python2(e, s, typ)
        return

    # Python3 case:
    exc_type = self.named_type('builtins.BaseException')
    expected_type_items = [exc_type, TypeType(exc_type)]
    if optional:
        # This is used for `x` part in a case like `raise e from x`,
        # where we allow `raise e from None`.
        expected_type_items.append(NoneType())

    self.check_subtype(
        typ, UnionType.make_union(expected_type_items), s,
        message_registry.INVALID_EXCEPTION,
    )

    if isinstance(typ, FunctionLike):
        # https://github.com/python/mypy/issues/11089
        self.expr_checker.check_call(typ, [], [], e)

</t>
<t tx="ekr.20220525082933.465">def _type_check_raise_python2(self, e: Expression, s: RaiseStmt, typ: ProperType) -&gt; None:
    # Python2 has two possible major cases:
    # 1. `raise expr`, where `expr` is some expression, it can be:
    #    - Exception typ
    #    - Exception instance
    #    - Old style class (not supported)
    #    - Tuple, where 0th item is exception type or instance
    # 2. `raise exc, msg, traceback`, where:
    #    - `exc` is exception type (not instance!)
    #    - `traceback` is `types.TracebackType | None`
    # Important note: `raise exc, msg` is not the same as `raise (exc, msg)`
    # We call `raise exc, msg, traceback` - legacy mode.
    exc_type = self.named_type('builtins.BaseException')
    exc_inst_or_type = UnionType([exc_type, TypeType(exc_type)])

    if (not s.legacy_mode and (isinstance(typ, TupleType) and typ.items
            or (isinstance(typ, Instance) and typ.args
                and typ.type.fullname == 'builtins.tuple'))):
        # `raise (exc, ...)` case:
        item = typ.items[0] if isinstance(typ, TupleType) else typ.args[0]
        self.check_subtype(
            item, exc_inst_or_type, s,
            'When raising a tuple, first element must by derived from BaseException',
        )
        return
    elif s.legacy_mode:
        # `raise Exception, msg` case
        # `raise Exception, msg, traceback` case
        # https://docs.python.org/2/reference/simple_stmts.html#the-raise-statement
        assert isinstance(typ, TupleType)  # Is set in fastparse2.py
        if (len(typ.items) &gt;= 2
                and isinstance(get_proper_type(typ.items[1]), NoneType)):
            expected_type: Type = exc_inst_or_type
        else:
            expected_type = TypeType(exc_type)
        self.check_subtype(
            typ.items[0], expected_type, s,
            f'Argument 1 must be "{expected_type}" subtype',
        )

        # Typecheck `traceback` part:
        if len(typ.items) == 3:
            # Now, we typecheck `traceback` argument if it is present.
            # We do this after the main check for better error message
            # and better ordering: first about `BaseException` subtype,
            # then about `traceback` type.
            traceback_type = UnionType.make_union([
                self.named_type('types.TracebackType'),
                NoneType(),
            ])
            self.check_subtype(
                typ.items[2], traceback_type, s,
                f'Argument 3 must be "{traceback_type}" subtype',
            )
    else:
        expected_type_items = [
            # `raise Exception` and `raise Exception()` cases:
            exc_type, TypeType(exc_type),
        ]
        self.check_subtype(
            typ, UnionType.make_union(expected_type_items),
            s, message_registry.INVALID_EXCEPTION,
        )

</t>
<t tx="ekr.20220525082933.466">def visit_try_stmt(self, s: TryStmt) -&gt; None:
    """Type check a try statement."""
    # Our enclosing frame will get the result if the try/except falls through.
    # This one gets all possible states after the try block exited abnormally
    # (by exception, return, break, etc.)
    with self.binder.frame_context(can_skip=False, fall_through=0):
        # Not only might the body of the try statement exit
        # abnormally, but so might an exception handler or else
        # clause. The finally clause runs in *all* cases, so we
        # need an outer try frame to catch all intermediate states
        # in case an exception is raised during an except or else
        # clause. As an optimization, only create the outer try
        # frame when there actually is a finally clause.
        self.visit_try_without_finally(s, try_frame=bool(s.finally_body))
        if s.finally_body:
            # First we check finally_body is type safe on all abnormal exit paths
            self.accept(s.finally_body)

    if s.finally_body:
        # Then we try again for the more restricted set of options
        # that can fall through. (Why do we need to check the
        # finally clause twice? Depending on whether the finally
        # clause was reached by the try clause falling off the end
        # or exiting abnormally, after completing the finally clause
        # either flow will continue to after the entire try statement
        # or the exception/return/etc. will be processed and control
        # flow will escape. We need to check that the finally clause
        # type checks in both contexts, but only the resulting types
        # from the latter context affect the type state in the code
        # that follows the try statement.)
        if not self.binder.is_unreachable():
            self.accept(s.finally_body)

</t>
<t tx="ekr.20220525082933.467">def visit_try_without_finally(self, s: TryStmt, try_frame: bool) -&gt; None:
    """Type check a try statement, ignoring the finally block.

    On entry, the top frame should receive all flow that exits the
    try block abnormally (i.e., such that the else block does not
    execute), and its parent should receive all flow that exits
    the try block normally.
    """
    # This frame will run the else block if the try fell through.
    # In that case, control flow continues to the parent of what
    # was the top frame on entry.
    with self.binder.frame_context(can_skip=False, fall_through=2, try_frame=try_frame):
        # This frame receives exit via exception, and runs exception handlers
        with self.binder.frame_context(can_skip=False, conditional_frame=True, fall_through=2):
            # Finally, the body of the try statement
            with self.binder.frame_context(can_skip=False, fall_through=2, try_frame=True):
                self.accept(s.body)
            for i in range(len(s.handlers)):
                with self.binder.frame_context(can_skip=True, fall_through=4):
                    typ = s.types[i]
                    if typ:
                        t = self.check_except_handler_test(typ)
                        var = s.vars[i]
                        if var:
                            # To support local variables, we make this a definition line,
                            # causing assignment to set the variable's type.
                            var.is_inferred_def = True
                            # We also temporarily set current_node_deferred to False to
                            # make sure the inference happens.
                            # TODO: Use a better solution, e.g. a
                            # separate Var for each except block.
                            am_deferring = self.current_node_deferred
                            self.current_node_deferred = False
                            self.check_assignment(var, self.temp_node(t, var))
                            self.current_node_deferred = am_deferring
                    self.accept(s.handlers[i])
                    var = s.vars[i]
                    if var:
                        # Exception variables are deleted in python 3 but not python 2.
                        # But, since it's bad form in python 2 and the type checking
                        # wouldn't work very well, we delete it anyway.

                        # Unfortunately, this doesn't let us detect usage before the
                        # try/except block.
                        if self.options.python_version[0] &gt;= 3:
                            source = var.name
                        else:
                            source = ('(exception variable "{}", which we do not '
                                      'accept outside except: blocks even in '
                                      'python 2)'.format(var.name))
                        if isinstance(var.node, Var):
                            var.node.type = DeletedType(source=source)
                        self.binder.cleanse(var)
        if s.else_body:
            self.accept(s.else_body)

</t>
<t tx="ekr.20220525082933.468">def check_except_handler_test(self, n: Expression) -&gt; Type:
    """Type check an exception handler test clause."""
    typ = self.expr_checker.accept(n)

    all_types: List[Type] = []
    test_types = self.get_types_from_except_handler(typ, n)

    for ttype in get_proper_types(test_types):
        if isinstance(ttype, AnyType):
            all_types.append(ttype)
            continue

        if isinstance(ttype, FunctionLike):
            item = ttype.items[0]
            if not item.is_type_obj():
                self.fail(message_registry.INVALID_EXCEPTION_TYPE, n)
                return AnyType(TypeOfAny.from_error)
            exc_type = item.ret_type
        elif isinstance(ttype, TypeType):
            exc_type = ttype.item
        else:
            self.fail(message_registry.INVALID_EXCEPTION_TYPE, n)
            return AnyType(TypeOfAny.from_error)

        if not is_subtype(exc_type, self.named_type('builtins.BaseException')):
            self.fail(message_registry.INVALID_EXCEPTION_TYPE, n)
            return AnyType(TypeOfAny.from_error)

        all_types.append(exc_type)

    return make_simplified_union(all_types)

</t>
<t tx="ekr.20220525082933.469">def get_types_from_except_handler(self, typ: Type, n: Expression) -&gt; List[Type]:
    """Helper for check_except_handler_test to retrieve handler types."""
    typ = get_proper_type(typ)
    if isinstance(typ, TupleType):
        return typ.items
    elif isinstance(typ, UnionType):
        return [
            union_typ
            for item in typ.relevant_items()
            for union_typ in self.get_types_from_except_handler(item, n)
        ]
    elif isinstance(typ, Instance) and is_named_instance(typ, 'builtins.tuple'):
        # variadic tuple
        return [typ.args[0]]
    else:
        return [typ]

</t>
<t tx="ekr.20220525082933.47">@coroutine
async def decorated_coroutine() -&gt; int:
    return 1

</t>
<t tx="ekr.20220525082933.470">def visit_for_stmt(self, s: ForStmt) -&gt; None:
    """Type check a for statement."""
    if s.is_async:
        iterator_type, item_type = self.analyze_async_iterable_item_type(s.expr)
    else:
        iterator_type, item_type = self.analyze_iterable_item_type(s.expr)
    s.inferred_item_type = item_type
    s.inferred_iterator_type = iterator_type
    self.analyze_index_variables(s.index, item_type, s.index_type is None, s)
    self.accept_loop(s.body, s.else_body)

</t>
<t tx="ekr.20220525082933.471">def analyze_async_iterable_item_type(self, expr: Expression) -&gt; Tuple[Type, Type]:
    """Analyse async iterable expression and return iterator and iterator item types."""
    echk = self.expr_checker
    iterable = echk.accept(expr)
    iterator = echk.check_method_call_by_name('__aiter__', iterable, [], [], expr)[0]
    awaitable = echk.check_method_call_by_name('__anext__', iterator, [], [], expr)[0]
    item_type = echk.check_awaitable_expr(awaitable, expr,
                                          message_registry.INCOMPATIBLE_TYPES_IN_ASYNC_FOR)
    return iterator, item_type

</t>
<t tx="ekr.20220525082933.472">def analyze_iterable_item_type(self, expr: Expression) -&gt; Tuple[Type, Type]:
    """Analyse iterable expression and return iterator and iterator item types."""
    echk = self.expr_checker
    iterable = get_proper_type(echk.accept(expr))
    iterator = echk.check_method_call_by_name('__iter__', iterable, [], [], expr)[0]

    if isinstance(iterable, TupleType):
        joined: Type = UninhabitedType()
        for item in iterable.items:
            joined = join_types(joined, item)
        return iterator, joined
    else:
        # Non-tuple iterable.
        if self.options.python_version[0] &gt;= 3:
            nextmethod = '__next__'
        else:
            nextmethod = 'next'
        return iterator, echk.check_method_call_by_name(nextmethod, iterator, [], [], expr)[0]

</t>
<t tx="ekr.20220525082933.473">def analyze_container_item_type(self, typ: Type) -&gt; Optional[Type]:
    """Check if a type is a nominal container of a union of such.

    Return the corresponding container item type.
    """
    typ = get_proper_type(typ)
    if isinstance(typ, UnionType):
        types: List[Type] = []
        for item in typ.items:
            c_type = self.analyze_container_item_type(item)
            if c_type:
                types.append(c_type)
        return UnionType.make_union(types)
    if isinstance(typ, Instance) and typ.type.has_base('typing.Container'):
        supertype = self.named_type('typing.Container').type
        super_instance = map_instance_to_supertype(typ, supertype)
        assert len(super_instance.args) == 1
        return super_instance.args[0]
    if isinstance(typ, TupleType):
        return self.analyze_container_item_type(tuple_fallback(typ))
    return None

</t>
<t tx="ekr.20220525082933.474">def analyze_index_variables(self, index: Expression, item_type: Type,
                            infer_lvalue_type: bool, context: Context) -&gt; None:
    """Type check or infer for loop or list comprehension index vars."""
    self.check_assignment(index, self.temp_node(item_type, context), infer_lvalue_type)

</t>
<t tx="ekr.20220525082933.475">def visit_del_stmt(self, s: DelStmt) -&gt; None:
    if isinstance(s.expr, IndexExpr):
        e = s.expr
        m = MemberExpr(e.base, '__delitem__')
        m.line = s.line
        m.column = s.column
        c = CallExpr(m, [e.index], [nodes.ARG_POS], [None])
        c.line = s.line
        c.column = s.column
        self.expr_checker.accept(c, allow_none_return=True)
    else:
        s.expr.accept(self.expr_checker)
        for elt in flatten(s.expr):
            if isinstance(elt, NameExpr):
                self.binder.assign_type(elt, DeletedType(source=elt.name),
                                        get_declaration(elt), False)

</t>
<t tx="ekr.20220525082933.476">def visit_decorator(self, e: Decorator) -&gt; None:
    for d in e.decorators:
        if isinstance(d, RefExpr):
            if d.fullname == 'typing.no_type_check':
                e.var.type = AnyType(TypeOfAny.special_form)
                e.var.is_ready = True
                return

    if self.recurse_into_functions:
        with self.tscope.function_scope(e.func):
            self.check_func_item(e.func, name=e.func.name)

    # Process decorators from the inside out to determine decorated signature, which
    # may be different from the declared signature.
    sig: Type = self.function_type(e.func)
    for d in reversed(e.decorators):
        if refers_to_fullname(d, OVERLOAD_NAMES):
            self.fail(message_registry.MULTIPLE_OVERLOADS_REQUIRED, e)
            continue
        dec = self.expr_checker.accept(d)
        temp = self.temp_node(sig, context=e)
        fullname = None
        if isinstance(d, RefExpr):
            fullname = d.fullname
        # if this is a expression like @b.a where b is an object, get the type of b
        # so we can pass it the method hook in the plugins
        object_type: Optional[Type] = None
        if fullname is None and isinstance(d, MemberExpr) and self.has_type(d.expr):
            object_type = self.lookup_type(d.expr)
            fullname = self.expr_checker.method_fullname(object_type, d.name)
        self.check_for_untyped_decorator(e.func, dec, d)
        sig, t2 = self.expr_checker.check_call(dec, [temp],
                                               [nodes.ARG_POS], e,
                                               callable_name=fullname,
                                               object_type=object_type)
    self.check_untyped_after_decorator(sig, e.func)
    sig = set_callable_name(sig, e.func)
    e.var.type = sig
    e.var.is_ready = True
    if e.func.is_property:
        self.check_incompatible_property_override(e)
    if e.func.info and not e.func.is_dynamic():
        self.check_method_override(e)

    if e.func.info and e.func.name in ('__init__', '__new__'):
        if e.type and not isinstance(get_proper_type(e.type), (FunctionLike, AnyType)):
            self.fail(message_registry.BAD_CONSTRUCTOR_TYPE, e)

</t>
<t tx="ekr.20220525082933.477">def check_for_untyped_decorator(self,
                                func: FuncDef,
                                dec_type: Type,
                                dec_expr: Expression) -&gt; None:
    if (self.options.disallow_untyped_decorators and
            is_typed_callable(func.type) and
            is_untyped_decorator(dec_type)):
        self.msg.typed_function_untyped_decorator(func.name, dec_expr)

</t>
<t tx="ekr.20220525082933.478">def check_incompatible_property_override(self, e: Decorator) -&gt; None:
    if not e.var.is_settable_property and e.func.info:
        name = e.func.name
        for base in e.func.info.mro[1:]:
            base_attr = base.names.get(name)
            if not base_attr:
                continue
            if (isinstance(base_attr.node, OverloadedFuncDef) and
                    base_attr.node.is_property and
                    cast(Decorator,
                         base_attr.node.items[0]).var.is_settable_property):
                self.fail(message_registry.READ_ONLY_PROPERTY_OVERRIDES_READ_WRITE, e)

</t>
<t tx="ekr.20220525082933.479">def visit_with_stmt(self, s: WithStmt) -&gt; None:
    exceptions_maybe_suppressed = False
    for expr, target in zip(s.expr, s.target):
        if s.is_async:
            exit_ret_type = self.check_async_with_item(expr, target, s.unanalyzed_type is None)
        else:
            exit_ret_type = self.check_with_item(expr, target, s.unanalyzed_type is None)

        # Based on the return type, determine if this context manager 'swallows'
        # exceptions or not. We determine this using a heuristic based on the
        # return type of the __exit__ method -- see the discussion in
        # https://github.com/python/mypy/issues/7214 and the section about context managers
        # in https://github.com/python/typeshed/blob/master/CONTRIBUTING.md#conventions
        # for more details.

        exit_ret_type = get_proper_type(exit_ret_type)
        if is_literal_type(exit_ret_type, "builtins.bool", False):
            continue

        if (is_literal_type(exit_ret_type, "builtins.bool", True)
                or (isinstance(exit_ret_type, Instance)
                    and exit_ret_type.type.fullname == 'builtins.bool'
                    and state.strict_optional)):
            # Note: if strict-optional is disabled, this bool instance
            # could actually be an Optional[bool].
            exceptions_maybe_suppressed = True

    if exceptions_maybe_suppressed:
        # Treat this 'with' block in the same way we'd treat a 'try: BODY; except: pass'
        # block. This means control flow can continue after the 'with' even if the 'with'
        # block immediately returns.
        with self.binder.frame_context(can_skip=True, try_frame=True):
            self.accept(s.body)
    else:
        self.accept(s.body)

</t>
<t tx="ekr.20220525082933.48">class It(Iterator[str]):
    stop = False
    @others
</t>
<t tx="ekr.20220525082933.480">def check_untyped_after_decorator(self, typ: Type, func: FuncDef) -&gt; None:
    if not self.options.disallow_any_decorated or self.is_stub:
        return

    if mypy.checkexpr.has_any_type(typ):
        self.msg.untyped_decorated_function(typ, func)

</t>
<t tx="ekr.20220525082933.481">def check_async_with_item(self, expr: Expression, target: Optional[Expression],
                          infer_lvalue_type: bool) -&gt; Type:
    echk = self.expr_checker
    ctx = echk.accept(expr)
    obj = echk.check_method_call_by_name('__aenter__', ctx, [], [], expr)[0]
    obj = echk.check_awaitable_expr(
        obj, expr, message_registry.INCOMPATIBLE_TYPES_IN_ASYNC_WITH_AENTER)
    if target:
        self.check_assignment(target, self.temp_node(obj, expr), infer_lvalue_type)
    arg = self.temp_node(AnyType(TypeOfAny.special_form), expr)
    res, _ = echk.check_method_call_by_name(
        '__aexit__', ctx, [arg] * 3, [nodes.ARG_POS] * 3, expr)
    return echk.check_awaitable_expr(
        res, expr, message_registry.INCOMPATIBLE_TYPES_IN_ASYNC_WITH_AEXIT)

</t>
<t tx="ekr.20220525082933.482">def check_with_item(self, expr: Expression, target: Optional[Expression],
                    infer_lvalue_type: bool) -&gt; Type:
    echk = self.expr_checker
    ctx = echk.accept(expr)
    obj = echk.check_method_call_by_name('__enter__', ctx, [], [], expr)[0]
    if target:
        self.check_assignment(target, self.temp_node(obj, expr), infer_lvalue_type)
    arg = self.temp_node(AnyType(TypeOfAny.special_form), expr)
    res, _ = echk.check_method_call_by_name(
        '__exit__', ctx, [arg] * 3, [nodes.ARG_POS] * 3, expr)
    return res

</t>
<t tx="ekr.20220525082933.483">def visit_print_stmt(self, s: PrintStmt) -&gt; None:
    for arg in s.args:
        self.expr_checker.accept(arg)
    if s.target:
        target_type = get_proper_type(self.expr_checker.accept(s.target))
        if not isinstance(target_type, NoneType):
            write_type = self.expr_checker.analyze_external_member_access(
                'write', target_type, s.target)
            required_type = CallableType(
                arg_types=[self.named_type('builtins.str')],
                arg_kinds=[ARG_POS],
                arg_names=[None],
                ret_type=AnyType(TypeOfAny.implementation_artifact),
                fallback=self.named_type('builtins.function'),
            )
            # This has to be hard-coded, since it is a syntax pattern, not a function call.
            if not is_subtype(write_type, required_type):
                self.fail(message_registry.PYTHON2_PRINT_FILE_TYPE.format(
                    write_type,
                    required_type,
                ), s.target)

</t>
<t tx="ekr.20220525082933.484">def visit_break_stmt(self, s: BreakStmt) -&gt; None:
    self.binder.handle_break()

</t>
<t tx="ekr.20220525082933.485">def visit_continue_stmt(self, s: ContinueStmt) -&gt; None:
    self.binder.handle_continue()
    return None

</t>
<t tx="ekr.20220525082933.486">def visit_match_stmt(self, s: MatchStmt) -&gt; None:
    with self.binder.frame_context(can_skip=False, fall_through=0):
        subject_type = get_proper_type(self.expr_checker.accept(s.subject))

        if isinstance(subject_type, DeletedType):
            self.msg.deleted_as_rvalue(subject_type, s)

        # We infer types of patterns twice. The first pass is used
        # to infer the types of capture variables. The type of a
        # capture variable may depend on multiple patterns (it
        # will be a union of all capture types). This pass ignores
        # guard expressions.
        pattern_types = [self.pattern_checker.accept(p, subject_type) for p in s.patterns]
        type_maps: List[TypeMap] = [t.captures for t in pattern_types]
        inferred_types = self.infer_variable_types_from_type_maps(type_maps)

        # The second pass narrows down the types and type checks bodies.
        for p, g, b in zip(s.patterns, s.guards, s.bodies):
            current_subject_type = self.expr_checker.narrow_type_from_binder(s.subject,
                                                                             subject_type)
            pattern_type = self.pattern_checker.accept(p, current_subject_type)
            with self.binder.frame_context(can_skip=True, fall_through=2):
                if b.is_unreachable or isinstance(get_proper_type(pattern_type.type),
                                                  UninhabitedType):
                    self.push_type_map(None)
                    else_map: TypeMap = {}
                else:
                    pattern_map, else_map = conditional_types_to_typemaps(
                        s.subject,
                        pattern_type.type,
                        pattern_type.rest_type
                    )
                    self.remove_capture_conflicts(pattern_type.captures,
                                                  inferred_types)
                    self.push_type_map(pattern_map)
                    self.push_type_map(pattern_type.captures)
                if g is not None:
                    with self.binder.frame_context(can_skip=True, fall_through=3):
                        gt = get_proper_type(self.expr_checker.accept(g))

                        if isinstance(gt, DeletedType):
                            self.msg.deleted_as_rvalue(gt, s)

                        guard_map, guard_else_map = self.find_isinstance_check(g)
                        else_map = or_conditional_maps(else_map, guard_else_map)

                        self.push_type_map(guard_map)
                        self.accept(b)
                else:
                    self.accept(b)
            self.push_type_map(else_map)

        # This is needed due to a quirk in frame_context. Without it types will stay narrowed
        # after the match.
        with self.binder.frame_context(can_skip=False, fall_through=2):
            pass

</t>
<t tx="ekr.20220525082933.487">def infer_variable_types_from_type_maps(self, type_maps: List[TypeMap]) -&gt; Dict[Var, Type]:
    all_captures: Dict[Var, List[Tuple[NameExpr, Type]]] = defaultdict(list)
    for tm in type_maps:
        if tm is not None:
            for expr, typ in tm.items():
                if isinstance(expr, NameExpr):
                    node = expr.node
                    assert isinstance(node, Var)
                    all_captures[node].append((expr, typ))

    inferred_types: Dict[Var, Type] = {}
    for var, captures in all_captures.items():
        already_exists = False
        types: List[Type] = []
        for expr, typ in captures:
            types.append(typ)

            previous_type, _, _ = self.check_lvalue(expr)
            if previous_type is not None:
                already_exists = True
                if self.check_subtype(typ, previous_type, expr,
                                      msg=message_registry.INCOMPATIBLE_TYPES_IN_CAPTURE,
                                      subtype_label="pattern captures type",
                                      supertype_label="variable has type"):
                    inferred_types[var] = previous_type

        if not already_exists:
            new_type = UnionType.make_union(types)
            # Infer the union type at the first occurrence
            first_occurrence, _ = captures[0]
            inferred_types[var] = new_type
            self.infer_variable_type(var, first_occurrence, new_type, first_occurrence)
    return inferred_types

</t>
<t tx="ekr.20220525082933.488">def remove_capture_conflicts(self, type_map: TypeMap, inferred_types: Dict[Var, Type]) -&gt; None:
    if type_map:
        for expr, typ in list(type_map.items()):
            if isinstance(expr, NameExpr):
                node = expr.node
                assert isinstance(node, Var)
                if node not in inferred_types or not is_subtype(typ, inferred_types[node]):
                    del type_map[expr]

</t>
<t tx="ekr.20220525082933.489">def make_fake_typeinfo(self,
                       curr_module_fullname: str,
                       class_gen_name: str,
                       class_short_name: str,
                       bases: List[Instance],
                       ) -&gt; Tuple[ClassDef, TypeInfo]:
    # Build the fake ClassDef and TypeInfo together.
    # The ClassDef is full of lies and doesn't actually contain a body.
    # Use format_bare to generate a nice name for error messages.
    # We skip fully filling out a handful of TypeInfo fields because they
    # should be irrelevant for a generated type like this:
    # is_protocol, protocol_members, is_abstract
    cdef = ClassDef(class_short_name, Block([]))
    cdef.fullname = curr_module_fullname + '.' + class_gen_name
    info = TypeInfo(SymbolTable(), cdef, curr_module_fullname)
    cdef.info = info
    info.bases = bases
    calculate_mro(info)
    info.calculate_metaclass_type()
    return cdef, info

</t>
<t tx="ekr.20220525082933.49">def __iter__(self) -&gt; 'It':
    return self
</t>
<t tx="ekr.20220525082933.490">def intersect_instances(self,
                        instances: Tuple[Instance, Instance],
                        ctx: Context,
                        ) -&gt; Optional[Instance]:
    """Try creating an ad-hoc intersection of the given instances.

    Note that this function does *not* try and create a full-fledged
    intersection type. Instead, it returns an instance of a new ad-hoc
    subclass of the given instances.

    This is mainly useful when you need a way of representing some
    theoretical subclass of the instances the user may be trying to use
    the generated intersection can serve as a placeholder.

    This function will create a fresh subclass every time you call it,
    even if you pass in the exact same arguments. So this means calling
    `self.intersect_intersection([inst_1, inst_2], ctx)` twice will result
    in instances of two distinct subclasses of inst_1 and inst_2.

    This is by design: we want each ad-hoc intersection to be unique since
    they're supposed represent some other unknown subclass.

    Returns None if creating the subclass is impossible (e.g. due to
    MRO errors or incompatible signatures). If we do successfully create
    a subclass, its TypeInfo will automatically be added to the global scope.
    """
    curr_module = self.scope.stack[0]
    assert isinstance(curr_module, MypyFile)

    def _get_base_classes(instances_: Tuple[Instance, Instance]) -&gt; List[Instance]:
        base_classes_ = []
        for inst in instances_:
            if inst.type.is_intersection:
                expanded = inst.type.bases
            else:
                expanded = [inst]

            for expanded_inst in expanded:
                base_classes_.append(expanded_inst)
        return base_classes_

    def _make_fake_typeinfo_and_full_name(
            base_classes_: List[Instance],
            curr_module_: MypyFile,
    ) -&gt; Tuple[TypeInfo, str]:
        names_list = pretty_seq([x.type.name for x in base_classes_], "and")
        short_name = f'&lt;subclass of {names_list}&gt;'
        full_name_ = gen_unique_name(short_name, curr_module_.names)
        cdef, info_ = self.make_fake_typeinfo(
            curr_module_.fullname,
            full_name_,
            short_name,
            base_classes_,
        )
        return info_, full_name_

    base_classes = _get_base_classes(instances)
    # We use the pretty_names_list for error messages but can't
    # use it for the real name that goes into the symbol table
    # because it can have dots in it.
    pretty_names_list = pretty_seq(format_type_distinctly(*base_classes, bare=True), "and")
    try:
        info, full_name = _make_fake_typeinfo_and_full_name(base_classes, curr_module)
        with self.msg.filter_errors() as local_errors:
            self.check_multiple_inheritance(info)
        if local_errors.has_new_errors():
            # "class A(B, C)" unsafe, now check "class A(C, B)":
            base_classes = _get_base_classes(instances[::-1])
            info, full_name = _make_fake_typeinfo_and_full_name(base_classes, curr_module)
            with self.msg.filter_errors() as local_errors:
                self.check_multiple_inheritance(info)
        info.is_intersection = True
    except MroError:
        if self.should_report_unreachable_issues():
            self.msg.impossible_intersection(
                pretty_names_list, "inconsistent method resolution order", ctx)
        return None

    if local_errors.has_new_errors():
        if self.should_report_unreachable_issues():
            self.msg.impossible_intersection(
                pretty_names_list, "incompatible method signatures", ctx)
        return None

    curr_module.names[full_name] = SymbolTableNode(GDEF, info)
    return Instance(info, [])

</t>
<t tx="ekr.20220525082933.491">def intersect_instance_callable(self, typ: Instance, callable_type: CallableType) -&gt; Instance:
    """Creates a fake type that represents the intersection of an Instance and a CallableType.

    It operates by creating a bare-minimum dummy TypeInfo that
    subclasses type and adds a __call__ method matching callable_type.
    """

    # In order for this to work in incremental mode, the type we generate needs to
    # have a valid fullname and a corresponding entry in a symbol table. We generate
    # a unique name inside the symbol table of the current module.
    cur_module = cast(MypyFile, self.scope.stack[0])
    gen_name = gen_unique_name(f"&lt;callable subtype of {typ.type.name}&gt;",
                               cur_module.names)

    # Synthesize a fake TypeInfo
    short_name = format_type_bare(typ)
    cdef, info = self.make_fake_typeinfo(cur_module.fullname, gen_name, short_name, [typ])

    # Build up a fake FuncDef so we can populate the symbol table.
    func_def = FuncDef('__call__', [], Block([]), callable_type)
    func_def._fullname = cdef.fullname + '.__call__'
    func_def.info = info
    info.names['__call__'] = SymbolTableNode(MDEF, func_def)

    cur_module.names[gen_name] = SymbolTableNode(GDEF, info)

    return Instance(info, [])

</t>
<t tx="ekr.20220525082933.492">def make_fake_callable(self, typ: Instance) -&gt; Instance:
    """Produce a new type that makes type Callable with a generic callable type."""

    fallback = self.named_type('builtins.function')
    callable_type = CallableType([AnyType(TypeOfAny.explicit),
                                  AnyType(TypeOfAny.explicit)],
                                 [nodes.ARG_STAR, nodes.ARG_STAR2],
                                 [None, None],
                                 ret_type=AnyType(TypeOfAny.explicit),
                                 fallback=fallback,
                                 is_ellipsis_args=True)

    return self.intersect_instance_callable(typ, callable_type)

</t>
<t tx="ekr.20220525082933.493">def partition_by_callable(self, typ: Type,
                          unsound_partition: bool) -&gt; Tuple[List[Type], List[Type]]:
    """Partitions a type into callable subtypes and uncallable subtypes.

    Thus, given:
    `callables, uncallables = partition_by_callable(type)`

    If we assert `callable(type)` then `type` has type Union[*callables], and
    If we assert `not callable(type)` then `type` has type Union[*uncallables]

    If unsound_partition is set, assume that anything that is not
    clearly callable is in fact not callable. Otherwise we generate a
    new subtype that *is* callable.

    Guaranteed to not return [], [].
    """
    typ = get_proper_type(typ)

    if isinstance(typ, FunctionLike) or isinstance(typ, TypeType):
        return [typ], []

    if isinstance(typ, AnyType):
        return [typ], [typ]

    if isinstance(typ, NoneType):
        return [], [typ]

    if isinstance(typ, UnionType):
        callables = []
        uncallables = []
        for subtype in typ.items:
            # Use unsound_partition when handling unions in order to
            # allow the expected type discrimination.
            subcallables, subuncallables = self.partition_by_callable(subtype,
                                                                      unsound_partition=True)
            callables.extend(subcallables)
            uncallables.extend(subuncallables)
        return callables, uncallables

    if isinstance(typ, TypeVarType):
        # We could do better probably?
        # Refine the the type variable's bound as our type in the case that
        # callable() is true. This unfortunately loses the information that
        # the type is a type variable in that branch.
        # This matches what is done for isinstance, but it may be possible to
        # do better.
        # If it is possible for the false branch to execute, return the original
        # type to avoid losing type information.
        callables, uncallables = self.partition_by_callable(erase_to_union_or_bound(typ),
                                                            unsound_partition)
        uncallables = [typ] if len(uncallables) else []
        return callables, uncallables

    # A TupleType is callable if its fallback is, but needs special handling
    # when we dummy up a new type.
    ityp = typ
    if isinstance(typ, TupleType):
        ityp = tuple_fallback(typ)

    if isinstance(ityp, Instance):
        method = ityp.type.get_method('__call__')
        if method and method.type:
            callables, uncallables = self.partition_by_callable(method.type,
                                                                unsound_partition=False)
            if len(callables) and not len(uncallables):
                # Only consider the type callable if its __call__ method is
                # definitely callable.
                return [typ], []

        if not unsound_partition:
            fake = self.make_fake_callable(ityp)
            if isinstance(typ, TupleType):
                fake.type.tuple_type = TupleType(typ.items, fake)
                return [fake.type.tuple_type], [typ]
            return [fake], [typ]

    if unsound_partition:
        return [], [typ]
    else:
        # We don't know how properly make the type callable.
        return [typ], [typ]

</t>
<t tx="ekr.20220525082933.494">def conditional_callable_type_map(self, expr: Expression,
                                  current_type: Optional[Type],
                                  ) -&gt; Tuple[TypeMap, TypeMap]:
    """Takes in an expression and the current type of the expression.

    Returns a 2-tuple: The first element is a map from the expression to
    the restricted type if it were callable. The second element is a
    map from the expression to the type it would hold if it weren't
    callable.
    """
    if not current_type:
        return {}, {}

    if isinstance(get_proper_type(current_type), AnyType):
        return {}, {}

    callables, uncallables = self.partition_by_callable(current_type,
                                                        unsound_partition=False)

    if len(callables) and len(uncallables):
        callable_map = {expr: UnionType.make_union(callables)} if len(callables) else None
        uncallable_map = {
            expr: UnionType.make_union(uncallables)} if len(uncallables) else None
        return callable_map, uncallable_map

    elif len(callables):
        return {}, None

    return None, {}

</t>
<t tx="ekr.20220525082933.495">def _is_truthy_type(self, t: ProperType) -&gt; bool:
    return (
        (
            isinstance(t, Instance) and
            bool(t.type) and
            not t.type.has_readable_member('__bool__') and
            not t.type.has_readable_member('__len__')
        )
        or isinstance(t, FunctionLike)
        or (
            isinstance(t, UnionType) and
            all(self._is_truthy_type(t) for t in get_proper_types(t.items))
        )
    )

</t>
<t tx="ekr.20220525082933.496">def _check_for_truthy_type(self, t: Type, expr: Expression) -&gt; None:
    if not state.strict_optional:
        return  # if everything can be None, all bets are off

    t = get_proper_type(t)
    if not self._is_truthy_type(t):
        return

    def format_expr_type() -&gt; str:
        typ = format_type(t)
        if isinstance(expr, MemberExpr):
            return f'Member "{expr.name}" has type {typ}'
        elif isinstance(expr, RefExpr) and expr.fullname:
            return f'"{expr.fullname}" has type {typ}'
        elif isinstance(expr, CallExpr):
            if isinstance(expr.callee, MemberExpr):
                return f'"{expr.callee.name}" returns {typ}'
            elif isinstance(expr.callee, RefExpr) and expr.callee.fullname:
                return f'"{expr.callee.fullname}" returns {typ}'
            return f'Call returns {typ}'
        else:
            return f'Expression has type {typ}'

    if isinstance(t, FunctionLike):
        self.fail(message_registry.FUNCTION_ALWAYS_TRUE.format(format_type(t)), expr)
    elif isinstance(t, UnionType):
        self.fail(message_registry.TYPE_ALWAYS_TRUE_UNIONTYPE.format(format_expr_type()),
                  expr)
    else:
        self.fail(message_registry.TYPE_ALWAYS_TRUE.format(format_expr_type()), expr)

</t>
<t tx="ekr.20220525082933.497">def find_type_equals_check(self, node: ComparisonExpr, expr_indices: List[int]
                           ) -&gt; Tuple[TypeMap, TypeMap]:
    """Narrow types based on any checks of the type ``type(x) == T``

    Args:
        node: The node that might contain the comparison
        expr_indices: The list of indices of expressions in ``node`` that are being
            compared
    """
    def is_type_call(expr: CallExpr) -&gt; bool:
        """Is expr a call to type with one argument?"""
        return (refers_to_fullname(expr.callee, 'builtins.type')
                and len(expr.args) == 1)

    # exprs that are being passed into type
    exprs_in_type_calls: List[Expression] = []
    # type that is being compared to type(expr)
    type_being_compared: Optional[List[TypeRange]] = None
    # whether the type being compared to is final
    is_final = False

    for index in expr_indices:
        expr = node.operands[index]

        if isinstance(expr, CallExpr) and is_type_call(expr):
            exprs_in_type_calls.append(expr.args[0])
        else:
            current_type = self.get_isinstance_type(expr)
            if current_type is None:
                continue
            if type_being_compared is not None:
                # It doesn't really make sense to have several types being
                # compared to the output of type (like type(x) == int == str)
                # because whether that's true is solely dependent on what the
                # types being compared are, so we don't try to narrow types any
                # further because we can't really get any information about the
                # type of x from that check
                return {}, {}
            else:
                if isinstance(expr, RefExpr) and isinstance(expr.node, TypeInfo):
                    is_final = expr.node.is_final
                type_being_compared = current_type

    if not exprs_in_type_calls:
        return {}, {}

    if_maps: List[TypeMap] = []
    else_maps: List[TypeMap] = []
    for expr in exprs_in_type_calls:
        current_if_type, current_else_type = self.conditional_types_with_intersection(
            self.lookup_type(expr),
            type_being_compared,
            expr
        )
        current_if_map, current_else_map = conditional_types_to_typemaps(expr,
                                                                         current_if_type,
                                                                         current_else_type)
        if_maps.append(current_if_map)
        else_maps.append(current_else_map)

    def combine_maps(list_maps: List[TypeMap]) -&gt; TypeMap:
        """Combine all typemaps in list_maps into one typemap"""
        result_map = {}
        for d in list_maps:
            if d is not None:
                result_map.update(d)
        return result_map

    if_map = combine_maps(if_maps)
    # type(x) == T is only true when x has the same type as T, meaning
    # that it can be false if x is an instance of a subclass of T. That means
    # we can't do any narrowing in the else case unless T is final, in which
    # case T can't be subclassed
    if is_final:
        else_map = combine_maps(else_maps)
    else:
        else_map = {}
    return if_map, else_map

</t>
<t tx="ekr.20220525082933.498">def find_isinstance_check(self, node: Expression
                          ) -&gt; Tuple[TypeMap, TypeMap]:
    """Find any isinstance checks (within a chain of ands).  Includes
    implicit and explicit checks for None and calls to callable.
    Also includes TypeGuard functions.

    Return value is a map of variables to their types if the condition
    is true and a map of variables to their types if the condition is false.

    If either of the values in the tuple is None, then that particular
    branch can never occur.

    May return {}, {}.
    Can return None, None in situations involving NoReturn.
    """
    if_map, else_map = self.find_isinstance_check_helper(node)
    new_if_map = self.propagate_up_typemap_info(if_map)
    new_else_map = self.propagate_up_typemap_info(else_map)
    return new_if_map, new_else_map

</t>
<t tx="ekr.20220525082933.499">def find_isinstance_check_helper(self, node: Expression) -&gt; Tuple[TypeMap, TypeMap]:
    if is_true_literal(node):
        return {}, None
    if is_false_literal(node):
        return None, {}

    if isinstance(node, CallExpr) and len(node.args) != 0:
        expr = collapse_walrus(node.args[0])
        if refers_to_fullname(node.callee, 'builtins.isinstance'):
            if len(node.args) != 2:  # the error will be reported elsewhere
                return {}, {}
            if literal(expr) == LITERAL_TYPE:
                return conditional_types_to_typemaps(
                    expr,
                    *self.conditional_types_with_intersection(
                        self.lookup_type(expr),
                        self.get_isinstance_type(node.args[1]),
                        expr
                    )
                )
        elif refers_to_fullname(node.callee, 'builtins.issubclass'):
            if len(node.args) != 2:  # the error will be reported elsewhere
                return {}, {}
            if literal(expr) == LITERAL_TYPE:
                return self.infer_issubclass_maps(node, expr)
        elif refers_to_fullname(node.callee, 'builtins.callable'):
            if len(node.args) != 1:  # the error will be reported elsewhere
                return {}, {}
            if literal(expr) == LITERAL_TYPE:
                vartype = self.lookup_type(expr)
                return self.conditional_callable_type_map(expr, vartype)
        elif isinstance(node.callee, RefExpr):
            if node.callee.type_guard is not None:
                # TODO: Follow keyword args or *args, **kwargs
                if node.arg_kinds[0] != nodes.ARG_POS:
                    self.fail(message_registry.TYPE_GUARD_POS_ARG_REQUIRED, node)
                    return {}, {}
                if literal(expr) == LITERAL_TYPE:
                    # Note: we wrap the target type, so that we can special case later.
                    # Namely, for isinstance() we use a normal meet, while TypeGuard is
                    # considered "always right" (i.e. even if the types are not overlapping).
                    # Also note that a care must be taken to unwrap this back at read places
                    # where we use this to narrow down declared type.
                    return {expr: TypeGuardedType(node.callee.type_guard)}, {}
    elif isinstance(node, ComparisonExpr):
        # Step 1: Obtain the types of each operand and whether or not we can
        # narrow their types. (For example, we shouldn't try narrowing the
        # types of literal string or enum expressions).

        operands = [collapse_walrus(x) for x in node.operands]
        operand_types = []
        narrowable_operand_index_to_hash = {}
        for i, expr in enumerate(operands):
            if not self.has_type(expr):
                return {}, {}
            expr_type = self.lookup_type(expr)
            operand_types.append(expr_type)

            if (literal(expr) == LITERAL_TYPE
                    and not is_literal_none(expr)
                    and not self.is_literal_enum(expr)):
                h = literal_hash(expr)
                if h is not None:
                    narrowable_operand_index_to_hash[i] = h

        # Step 2: Group operands chained by either the 'is' or '==' operands
        # together. For all other operands, we keep them in groups of size 2.
        # So the expression:
        #
        #   x0 == x1 == x2 &lt; x3 &lt; x4 is x5 is x6 is not x7 is not x8
        #
        # ...is converted into the simplified operator list:
        #
        #  [("==", [0, 1, 2]), ("&lt;", [2, 3]), ("&lt;", [3, 4]),
        #   ("is", [4, 5, 6]), ("is not", [6, 7]), ("is not", [7, 8])]
        #
        # We group identity/equality expressions so we can propagate information
        # we discover about one operand across the entire chain. We don't bother
        # handling 'is not' and '!=' chains in a special way: those are very rare
        # in practice.

        simplified_operator_list = group_comparison_operands(
            node.pairwise(),
            narrowable_operand_index_to_hash,
            {'==', 'is'},
        )

        # Step 3: Analyze each group and infer more precise type maps for each
        # assignable operand, if possible. We combine these type maps together
        # in the final step.

        partial_type_maps = []
        for operator, expr_indices in simplified_operator_list:
            if operator in {'is', 'is not', '==', '!='}:
                # is_valid_target:
                #   Controls which types we're allowed to narrow exprs to. Note that
                #   we cannot use 'is_literal_type_like' in both cases since doing
                #   'x = 10000 + 1; x is 10001' is not always True in all Python
                #   implementations.
                #
                # coerce_only_in_literal_context:
                #   If true, coerce types into literal types only if one or more of
                #   the provided exprs contains an explicit Literal type. This could
                #   technically be set to any arbitrary value, but it seems being liberal
                #   with narrowing when using 'is' and conservative when using '==' seems
                #   to break the least amount of real-world code.
                #
                # should_narrow_by_identity:
                #   Set to 'false' only if the user defines custom __eq__ or __ne__ methods
                #   that could cause identity-based narrowing to produce invalid results.
                if operator in {'is', 'is not'}:
                    is_valid_target: Callable[[Type], bool] = is_singleton_type
                    coerce_only_in_literal_context = False
                    should_narrow_by_identity = True
                else:
                    def is_exactly_literal_type(t: Type) -&gt; bool:
                        return isinstance(get_proper_type(t), LiteralType)

                    def has_no_custom_eq_checks(t: Type) -&gt; bool:
                        return (not custom_special_method(t, '__eq__', check_all=False)
                                and not custom_special_method(t, '__ne__', check_all=False))

                    is_valid_target = is_exactly_literal_type
                    coerce_only_in_literal_context = True

                    expr_types = [operand_types[i] for i in expr_indices]
                    should_narrow_by_identity = all(map(has_no_custom_eq_checks, expr_types))

                if_map: TypeMap = {}
                else_map: TypeMap = {}
                if should_narrow_by_identity:
                    if_map, else_map = self.refine_identity_comparison_expression(
                        operands,
                        operand_types,
                        expr_indices,
                        narrowable_operand_index_to_hash.keys(),
                        is_valid_target,
                        coerce_only_in_literal_context,
                    )

                # Strictly speaking, we should also skip this check if the objects in the expr
                # chain have custom __eq__ or __ne__ methods. But we (maybe optimistically)
                # assume nobody would actually create a custom objects that considers itself
                # equal to None.
                if if_map == {} and else_map == {}:
                    if_map, else_map = self.refine_away_none_in_comparison(
                        operands,
                        operand_types,
                        expr_indices,
                        narrowable_operand_index_to_hash.keys(),
                    )

                # If we haven't been able to narrow types yet, we might be dealing with a
                # explicit type(x) == some_type check
                if if_map == {} and else_map == {}:
                    if_map, else_map = self.find_type_equals_check(node, expr_indices)
            elif operator in {'in', 'not in'}:
                assert len(expr_indices) == 2
                left_index, right_index = expr_indices
                if left_index not in narrowable_operand_index_to_hash:
                    continue

                item_type = operand_types[left_index]
                collection_type = operand_types[right_index]

                # We only try and narrow away 'None' for now
                if not is_optional(item_type):
                    continue

                collection_item_type = get_proper_type(builtin_item_type(collection_type))
                if collection_item_type is None or is_optional(collection_item_type):
                    continue
                if (isinstance(collection_item_type, Instance)
                        and collection_item_type.type.fullname == 'builtins.object'):
                    continue
                if is_overlapping_erased_types(item_type, collection_item_type):
                    if_map, else_map = {operands[left_index]: remove_optional(item_type)}, {}
                else:
                    continue
            else:
                if_map = {}
                else_map = {}

            if operator in {'is not', '!=', 'not in'}:
                if_map, else_map = else_map, if_map

            partial_type_maps.append((if_map, else_map))

        return reduce_conditional_maps(partial_type_maps)
    elif isinstance(node, AssignmentExpr):
        if_map = {}
        else_map = {}

        if_assignment_map, else_assignment_map = self.find_isinstance_check(node.target)

        if if_assignment_map is not None:
            if_map.update(if_assignment_map)
        if else_assignment_map is not None:
            else_map.update(else_assignment_map)

        if_condition_map, else_condition_map = self.find_isinstance_check(node.value)

        if if_condition_map is not None:
            if_map.update(if_condition_map)
        if else_condition_map is not None:
            else_map.update(else_condition_map)

        return (
            (None if if_assignment_map is None or if_condition_map is None else if_map),
            (None if else_assignment_map is None or else_condition_map is None else else_map),
        )
    elif isinstance(node, OpExpr) and node.op == 'and':
        left_if_vars, left_else_vars = self.find_isinstance_check(node.left)
        right_if_vars, right_else_vars = self.find_isinstance_check(node.right)

        # (e1 and e2) is true if both e1 and e2 are true,
        # and false if at least one of e1 and e2 is false.
        return (and_conditional_maps(left_if_vars, right_if_vars),
                or_conditional_maps(left_else_vars, right_else_vars))
    elif isinstance(node, OpExpr) and node.op == 'or':
        left_if_vars, left_else_vars = self.find_isinstance_check(node.left)
        right_if_vars, right_else_vars = self.find_isinstance_check(node.right)

        # (e1 or e2) is true if at least one of e1 or e2 is true,
        # and false if both e1 and e2 are false.
        return (or_conditional_maps(left_if_vars, right_if_vars),
                and_conditional_maps(left_else_vars, right_else_vars))
    elif isinstance(node, UnaryExpr) and node.op == 'not':
        left, right = self.find_isinstance_check(node.expr)
        return right, left

    # Restrict the type of the variable to True-ish/False-ish in the if and else branches
    # respectively
    original_vartype = self.lookup_type(node)
    self._check_for_truthy_type(original_vartype, node)
    vartype = try_expanding_sum_type_to_union(original_vartype, "builtins.bool")

    if_type = true_only(vartype)
    else_type = false_only(vartype)
    if_map = (
        {node: if_type}
        if not isinstance(if_type, UninhabitedType)
        else None
    )
    else_map = (
        {node: else_type}
        if not isinstance(else_type, UninhabitedType)
        else None
    )
    return if_map, else_map

</t>
<t tx="ekr.20220525082933.5">def start_background_cmd(name: str) -&gt; Popen:
    cmd = cmds[name]
    proc = subprocess.Popen(cmd,
                            stderr=subprocess.STDOUT,
                            stdout=subprocess.PIPE)
    return proc


</t>
<t tx="ekr.20220525082933.50">def __next__(self) -&gt; str:
    if self.stop:
        raise StopIteration('end')
    else:
        self.stop = True
        return 'a'

</t>
<t tx="ekr.20220525082933.500">def propagate_up_typemap_info(self,
                              new_types: TypeMap) -&gt; TypeMap:
    """Attempts refining parent expressions of any MemberExpr or IndexExprs in new_types.

    Specifically, this function accepts two mappings of expression to original types:
    the original mapping (existing_types), and a new mapping (new_types) intended to
    update the original.

    This function iterates through new_types and attempts to use the information to try
    refining any parent types that happen to be unions.

    For example, suppose there are two types "A = Tuple[int, int]" and "B = Tuple[str, str]".
    Next, suppose that 'new_types' specifies the expression 'foo[0]' has a refined type
    of 'int' and that 'foo' was previously deduced to be of type Union[A, B].

    Then, this function will observe that since A[0] is an int and B[0] is not, the type of
    'foo' can be further refined from Union[A, B] into just B.

    We perform this kind of "parent narrowing" for member lookup expressions and indexing
    expressions into tuples, namedtuples, and typeddicts. We repeat this narrowing
    recursively if the parent is also a "lookup expression". So for example, if we have
    the expression "foo['bar'].baz[0]", we'd potentially end up refining types for the
    expressions "foo", "foo['bar']", and "foo['bar'].baz".

    We return the newly refined map. This map is guaranteed to be a superset of 'new_types'.
    """
    if new_types is None:
        return None
    output_map = {}
    for expr, expr_type in new_types.items():
        # The original inferred type should always be present in the output map, of course
        output_map[expr] = expr_type

        # Next, try using this information to refine the parent types, if applicable.
        new_mapping = self.refine_parent_types(expr, expr_type)
        for parent_expr, proposed_parent_type in new_mapping.items():
            # We don't try inferring anything if we've already inferred something for
            # the parent expression.
            # TODO: Consider picking the narrower type instead of always discarding this?
            if parent_expr in new_types:
                continue
            output_map[parent_expr] = proposed_parent_type
    return output_map

</t>
<t tx="ekr.20220525082933.501">def refine_parent_types(self,
                        expr: Expression,
                        expr_type: Type) -&gt; Mapping[Expression, Type]:
    """Checks if the given expr is a 'lookup operation' into a union and iteratively refines
    the parent types based on the 'expr_type'.

    For example, if 'expr' is an expression like 'a.b.c.d', we'll potentially return refined
    types for expressions 'a', 'a.b', and 'a.b.c'.

    For more details about what a 'lookup operation' is and how we use the expr_type to refine
    the parent types of lookup_expr, see the docstring in 'propagate_up_typemap_info'.
    """
    output: Dict[Expression, Type] = {}

    # Note: parent_expr and parent_type are progressively refined as we crawl up the
    # parent lookup chain.
    while True:
        # First, check if this expression is one that's attempting to
        # "lookup" some key in the parent type. If so, save the parent type
        # and create function that will try replaying the same lookup
        # operation against arbitrary types.
        if isinstance(expr, MemberExpr):
            parent_expr = expr.expr
            parent_type = self.lookup_type_or_none(parent_expr)
            member_name = expr.name

            def replay_lookup(new_parent_type: ProperType) -&gt; Optional[Type]:
                with self.msg.filter_errors() as w:
                    member_type = analyze_member_access(
                        name=member_name,
                        typ=new_parent_type,
                        context=parent_expr,
                        is_lvalue=False,
                        is_super=False,
                        is_operator=False,
                        msg=self.msg,
                        original_type=new_parent_type,
                        chk=self,
                        in_literal_context=False,
                    )
                if w.has_new_errors():
                    return None
                else:
                    return member_type
        elif isinstance(expr, IndexExpr):
            parent_expr = expr.base
            parent_type = self.lookup_type_or_none(parent_expr)

            index_type = self.lookup_type_or_none(expr.index)
            if index_type is None:
                return output

            str_literals = try_getting_str_literals_from_type(index_type)
            if str_literals is not None:
                # Refactoring these two indexing replay functions is surprisingly
                # tricky -- see https://github.com/python/mypy/pull/7917, which
                # was blocked by https://github.com/mypyc/mypyc/issues/586
                def replay_lookup(new_parent_type: ProperType) -&gt; Optional[Type]:
                    if not isinstance(new_parent_type, TypedDictType):
                        return None
                    try:
                        assert str_literals is not None
                        member_types = [new_parent_type.items[key] for key in str_literals]
                    except KeyError:
                        return None
                    return make_simplified_union(member_types)
            else:
                int_literals = try_getting_int_literals_from_type(index_type)
                if int_literals is not None:
                    def replay_lookup(new_parent_type: ProperType) -&gt; Optional[Type]:
                        if not isinstance(new_parent_type, TupleType):
                            return None
                        try:
                            assert int_literals is not None
                            member_types = [new_parent_type.items[key] for key in int_literals]
                        except IndexError:
                            return None
                        return make_simplified_union(member_types)
                else:
                    return output
        else:
            return output

        # If we somehow didn't previously derive the parent type, abort completely
        # with what we have so far: something went wrong at an earlier stage.
        if parent_type is None:
            return output

        # We currently only try refining the parent type if it's a Union.
        # If not, there's no point in trying to refine any further parents
        # since we have no further information we can use to refine the lookup
        # chain, so we end early as an optimization.
        parent_type = get_proper_type(parent_type)
        if not isinstance(parent_type, UnionType):
            return output

        # Take each element in the parent union and replay the original lookup procedure
        # to figure out which parents are compatible.
        new_parent_types = []
        for item in union_items(parent_type):
            member_type = replay_lookup(item)
            if member_type is None:
                # We were unable to obtain the member type. So, we give up on refining this
                # parent type entirely and abort.
                return output

            if is_overlapping_types(member_type, expr_type):
                new_parent_types.append(item)

        # If none of the parent types overlap (if we derived an empty union), something
        # went wrong. We should never hit this case, but deriving the uninhabited type or
        # reporting an error both seem unhelpful. So we abort.
        if not new_parent_types:
            return output

        expr = parent_expr
        expr_type = output[parent_expr] = make_simplified_union(new_parent_types)

</t>
<t tx="ekr.20220525082933.502">def refine_identity_comparison_expression(self,
                                          operands: List[Expression],
                                          operand_types: List[Type],
                                          chain_indices: List[int],
                                          narrowable_operand_indices: AbstractSet[int],
                                          is_valid_target: Callable[[ProperType], bool],
                                          coerce_only_in_literal_context: bool,
                                          ) -&gt; Tuple[TypeMap, TypeMap]:
    """Produce conditional type maps refining expressions by an identity/equality comparison.

    The 'operands' and 'operand_types' lists should be the full list of operands used
    in the overall comparison expression. The 'chain_indices' list is the list of indices
    actually used within this identity comparison chain.

    So if we have the expression:

        a &lt;= b is c is d &lt;= e

    ...then 'operands' and 'operand_types' would be lists of length 5 and 'chain_indices'
    would be the list [1, 2, 3].

    The 'narrowable_operand_indices' parameter is the set of all indices we are allowed
    to refine the types of: that is, all operands that will potentially be a part of
    the output TypeMaps.

    Although this function could theoretically try setting the types of the operands
    in the chains to the meet, doing that causes too many issues in real-world code.
    Instead, we use 'is_valid_target' to identify which of the given chain types
    we could plausibly use as the refined type for the expressions in the chain.

    Similarly, 'coerce_only_in_literal_context' controls whether we should try coercing
    expressions in the chain to a Literal type. Performing this coercion is sometimes
    too aggressive of a narrowing, depending on context.
    """
    should_coerce = True
    if coerce_only_in_literal_context:
        should_coerce = any(is_literal_type_like(operand_types[i]) for i in chain_indices)

    target: Optional[Type] = None
    possible_target_indices = []
    for i in chain_indices:
        expr_type = operand_types[i]
        if should_coerce:
            expr_type = coerce_to_literal(expr_type)
        if not is_valid_target(get_proper_type(expr_type)):
            continue
        if target and not is_same_type(target, expr_type):
            # We have multiple disjoint target types. So the 'if' branch
            # must be unreachable.
            return None, {}
        target = expr_type
        possible_target_indices.append(i)

    # There's nothing we can currently infer if none of the operands are valid targets,
    # so we end early and infer nothing.
    if target is None:
        return {}, {}

    # If possible, use an unassignable expression as the target.
    # We skip refining the type of the target below, so ideally we'd
    # want to pick an expression we were going to skip anyways.
    singleton_index = -1
    for i in possible_target_indices:
        if i not in narrowable_operand_indices:
            singleton_index = i

    # But if none of the possible singletons are unassignable ones, we give up
    # and arbitrarily pick the last item, mostly because other parts of the
    # type narrowing logic bias towards picking the rightmost item and it'd be
    # nice to stay consistent.
    #
    # That said, it shouldn't matter which index we pick. For example, suppose we
    # have this if statement, where 'x' and 'y' both have singleton types:
    #
    #     if x is y:
    #         reveal_type(x)
    #         reveal_type(y)
    #     else:
    #         reveal_type(x)
    #         reveal_type(y)
    #
    # At this point, 'x' and 'y' *must* have the same singleton type: we would have
    # ended early in the first for-loop in this function if they weren't.
    #
    # So, we should always get the same result in the 'if' case no matter which
    # index we pick. And while we do end up getting different results in the 'else'
    # case depending on the index (e.g. if we pick 'y', then its type stays the same
    # while 'x' is narrowed to '&lt;uninhabited&gt;'), this distinction is also moot: mypy
    # currently will just mark the whole branch as unreachable if either operand is
    # narrowed to &lt;uninhabited&gt;.
    if singleton_index == -1:
        singleton_index = possible_target_indices[-1]

    sum_type_name = None
    target = get_proper_type(target)
    if (isinstance(target, LiteralType) and
            (target.is_enum_literal() or isinstance(target.value, bool))):
        sum_type_name = target.fallback.type.fullname

    target_type = [TypeRange(target, is_upper_bound=False)]

    partial_type_maps = []
    for i in chain_indices:
        # If we try refining a type against itself, conditional_type_map
        # will end up assuming that the 'else' branch is unreachable. This is
        # typically not what we want: generally the user will intend for the
        # target type to be some fixed 'sentinel' value and will want to refine
        # the other exprs against this one instead.
        if i == singleton_index:
            continue

        # Naturally, we can't refine operands which are not permitted to be refined.
        if i not in narrowable_operand_indices:
            continue

        expr = operands[i]
        expr_type = coerce_to_literal(operand_types[i])

        if sum_type_name is not None:
            expr_type = try_expanding_sum_type_to_union(expr_type, sum_type_name)

        # We intentionally use 'conditional_types' directly here instead of
        # 'self.conditional_types_with_intersection': we only compute ad-hoc
        # intersections when working with pure instances.
        types = conditional_types(expr_type, target_type)
        partial_type_maps.append(conditional_types_to_typemaps(expr, *types))

    return reduce_conditional_maps(partial_type_maps)

</t>
<t tx="ekr.20220525082933.503">def refine_away_none_in_comparison(self,
                                   operands: List[Expression],
                                   operand_types: List[Type],
                                   chain_indices: List[int],
                                   narrowable_operand_indices: AbstractSet[int],
                                   ) -&gt; Tuple[TypeMap, TypeMap]:
    """Produces conditional type maps refining away None in an identity/equality chain.

    For more details about what the different arguments mean, see the
    docstring of 'refine_identity_comparison_expression' up above.
    """
    non_optional_types = []
    for i in chain_indices:
        typ = operand_types[i]
        if not is_optional(typ):
            non_optional_types.append(typ)

    # Make sure we have a mixture of optional and non-optional types.
    if len(non_optional_types) == 0 or len(non_optional_types) == len(chain_indices):
        return {}, {}

    if_map = {}
    for i in narrowable_operand_indices:
        expr_type = operand_types[i]
        if not is_optional(expr_type):
            continue
        if any(is_overlapping_erased_types(expr_type, t) for t in non_optional_types):
            if_map[operands[i]] = remove_optional(expr_type)

    return if_map, {}

</t>
<t tx="ekr.20220525082933.504">#
# Helpers
#

</t>
<t tx="ekr.20220525082933.505">def check_subtype(self,
                  subtype: Type,
                  supertype: Type,
                  context: Context,
                  msg: Union[str, ErrorMessage] = message_registry.INCOMPATIBLE_TYPES,
                  subtype_label: Optional[str] = None,
                  supertype_label: Optional[str] = None,
                  *,
                  code: Optional[ErrorCode] = None,
                  outer_context: Optional[Context] = None) -&gt; bool:
    """Generate an error if the subtype is not compatible with supertype."""
    if is_subtype(subtype, supertype, options=self.options):
        return True

    if isinstance(msg, ErrorMessage):
        msg_text = msg.value
        code = msg.code
    else:
        msg_text = msg
    subtype = get_proper_type(subtype)
    supertype = get_proper_type(supertype)
    if self.msg.try_report_long_tuple_assignment_error(subtype, supertype, context, msg_text,
                                   subtype_label, supertype_label, code=code):
        return False
    if self.should_suppress_optional_error([subtype]):
        return False
    extra_info: List[str] = []
    note_msg = ''
    notes: List[str] = []
    if subtype_label is not None or supertype_label is not None:
        subtype_str, supertype_str = format_type_distinctly(subtype, supertype)
        if subtype_label is not None:
            extra_info.append(subtype_label + ' ' + subtype_str)
        if supertype_label is not None:
            extra_info.append(supertype_label + ' ' + supertype_str)
        note_msg = make_inferred_type_note(outer_context or context, subtype,
                                           supertype, supertype_str)
        if isinstance(subtype, Instance) and isinstance(supertype, Instance):
            notes = append_invariance_notes([], subtype, supertype)
    if extra_info:
        msg_text += ' (' + ', '.join(extra_info) + ')'

    self.fail(ErrorMessage(msg_text, code=code), context)
    for note in notes:
        self.msg.note(note, context, code=code)
    if note_msg:
        self.note(note_msg, context, code=code)
    self.msg.maybe_note_concatenate_pos_args(subtype, supertype, context, code=code)
    if (isinstance(supertype, Instance) and supertype.type.is_protocol and
            isinstance(subtype, (Instance, TupleType, TypedDictType))):
        self.msg.report_protocol_problems(subtype, supertype, context, code=code)
    if isinstance(supertype, CallableType) and isinstance(subtype, Instance):
        call = find_member('__call__', subtype, subtype, is_operator=True)
        if call:
            self.msg.note_call(subtype, call, context, code=code)
    if isinstance(subtype, (CallableType, Overloaded)) and isinstance(supertype, Instance):
        if supertype.type.is_protocol and supertype.type.protocol_members == ['__call__']:
            call = find_member('__call__', supertype, subtype, is_operator=True)
            assert call is not None
            self.msg.note_call(supertype, call, context, code=code)
    return False

</t>
<t tx="ekr.20220525082933.506">def contains_none(self, t: Type) -&gt; bool:
    t = get_proper_type(t)
    return (
        isinstance(t, NoneType) or
        (isinstance(t, UnionType) and any(self.contains_none(ut) for ut in t.items)) or
        (isinstance(t, TupleType) and any(self.contains_none(tt) for tt in t.items)) or
        (isinstance(t, Instance) and bool(t.args)
         and any(self.contains_none(it) for it in t.args))
    )

</t>
<t tx="ekr.20220525082933.507">def should_suppress_optional_error(self, related_types: List[Type]) -&gt; bool:
    return self.suppress_none_errors and any(self.contains_none(t) for t in related_types)

</t>
<t tx="ekr.20220525082933.508">def named_type(self, name: str) -&gt; Instance:
    """Return an instance type with given name and implicit Any type args.

    For example, named_type('builtins.object') produces the 'object' type.
    """
    # Assume that the name refers to a type.
    sym = self.lookup_qualified(name)
    node = sym.node
    if isinstance(node, TypeAlias):
        assert isinstance(node.target, Instance)  # type: ignore
        node = node.target.type
    assert isinstance(node, TypeInfo)
    any_type = AnyType(TypeOfAny.from_omitted_generics)
    return Instance(node, [any_type] * len(node.defn.type_vars))

</t>
<t tx="ekr.20220525082933.509">def named_generic_type(self, name: str, args: List[Type]) -&gt; Instance:
    """Return an instance with the given name and type arguments.

    Assume that the number of arguments is correct.  Assume that
    the name refers to a compatible generic type.
    """
    info = self.lookup_typeinfo(name)
    args = [remove_instance_last_known_values(arg) for arg in args]
    # TODO: assert len(args) == len(info.defn.type_vars)
    return Instance(info, args)

</t>
<t tx="ekr.20220525082933.51">def other_iterator() -&gt; It:
    return It()

</t>
<t tx="ekr.20220525082933.510">def lookup_typeinfo(self, fullname: str) -&gt; TypeInfo:
    # Assume that the name refers to a class.
    sym = self.lookup_qualified(fullname)
    node = sym.node
    assert isinstance(node, TypeInfo)
    return node

</t>
<t tx="ekr.20220525082933.511">def type_type(self) -&gt; Instance:
    """Return instance type 'type'."""
    return self.named_type('builtins.type')

</t>
<t tx="ekr.20220525082933.512">def str_type(self) -&gt; Instance:
    """Return instance type 'str'."""
    return self.named_type('builtins.str')

</t>
<t tx="ekr.20220525082933.513">def store_type(self, node: Expression, typ: Type) -&gt; None:
    """Store the type of a node in the type map."""
    self._type_maps[-1][node] = typ

</t>
<t tx="ekr.20220525082933.514">def has_type(self, node: Expression) -&gt; bool:
    for m in reversed(self._type_maps):
        if node in m:
            return True
    return False

</t>
<t tx="ekr.20220525082933.515">def lookup_type_or_none(self, node: Expression) -&gt; Optional[Type]:
    for m in reversed(self._type_maps):
        if node in m:
            return m[node]
    return None

</t>
<t tx="ekr.20220525082933.516">def lookup_type(self, node: Expression) -&gt; Type:
    for m in reversed(self._type_maps):
        t = m.get(node)
        if t is not None:
            return t
    raise KeyError(node)

</t>
<t tx="ekr.20220525082933.517">def store_types(self, d: Dict[Expression, Type]) -&gt; None:
    self._type_maps[-1].update(d)

</t>
<t tx="ekr.20220525082933.518">@contextmanager
def local_type_map(self) -&gt; Iterator[Dict[Expression, Type]]:
    """Store inferred types into a temporary type map (returned).

    This can be used to perform type checking "experiments" without
    affecting exported types (which are used by mypyc).
    """
    temp_type_map: Dict[Expression, Type] = {}
    self._type_maps.append(temp_type_map)
    yield temp_type_map
    self._type_maps.pop()

</t>
<t tx="ekr.20220525082933.519">def in_checked_function(self) -&gt; bool:
    """Should we type-check the current function?

    - Yes if --check-untyped-defs is set.
    - Yes outside functions.
    - Yes in annotated functions.
    - No otherwise.
    """
    return (self.options.check_untyped_defs
            or not self.dynamic_funcs
            or not self.dynamic_funcs[-1])

</t>
<t tx="ekr.20220525082933.52">class Aw(Awaitable[int]):
    def __await__(self) -&gt; Generator[str, Any, int]:
        yield 'a'
        return 1

</t>
<t tx="ekr.20220525082933.520">def lookup(self, name: str) -&gt; SymbolTableNode:
    """Look up a definition from the symbol table with the given name.
    """
    if name in self.globals:
        return self.globals[name]
    else:
        b = self.globals.get('__builtins__', None)
        if b:
            table = cast(MypyFile, b.node).names
            if name in table:
                return table[name]
        raise KeyError(f'Failed lookup: {name}')

</t>
<t tx="ekr.20220525082933.521">def lookup_qualified(self, name: str) -&gt; SymbolTableNode:
    if '.' not in name:
        return self.lookup(name)
    else:
        parts = name.split('.')
        n = self.modules[parts[0]]
        for i in range(1, len(parts) - 1):
            sym = n.names.get(parts[i])
            assert sym is not None, "Internal error: attempted lookup of unknown name"
            n = cast(MypyFile, sym.node)
        last = parts[-1]
        if last in n.names:
            return n.names[last]
        elif len(parts) == 2 and parts[0] == 'builtins':
            fullname = 'builtins.' + last
            if fullname in SUGGESTED_TEST_FIXTURES:
                suggestion = ", e.g. add '[builtins fixtures/{}]' to your test".format(
                    SUGGESTED_TEST_FIXTURES[fullname])
            else:
                suggestion = ''
            raise KeyError("Could not find builtin symbol '{}' (If you are running a "
                           "test case, use a fixture that "
                           "defines this symbol{})".format(last, suggestion))
        else:
            msg = "Failed qualified lookup: '{}' (fullname = '{}')."
            raise KeyError(msg.format(last, name))

</t>
<t tx="ekr.20220525082933.522">@contextmanager
def enter_partial_types(self, *, is_function: bool = False,
                        is_class: bool = False) -&gt; Iterator[None]:
    """Enter a new scope for collecting partial types.

    Also report errors for (some) variables which still have partial
    types, i.e. we couldn't infer a complete type.
    """
    is_local = (self.partial_types and self.partial_types[-1].is_local) or is_function
    self.partial_types.append(PartialTypeScope({}, is_function, is_local))
    yield

    # Don't complain about not being able to infer partials if it is
    # at the toplevel (with allow_untyped_globals) or if it is in an
    # untyped function being checked with check_untyped_defs.
    permissive = (self.options.allow_untyped_globals and not is_local) or (
        self.options.check_untyped_defs
        and self.dynamic_funcs
        and self.dynamic_funcs[-1]
    )

    partial_types, _, _ = self.partial_types.pop()
    if not self.current_node_deferred:
        for var, context in partial_types.items():
            # If we require local partial types, there are a few exceptions where
            # we fall back to inferring just "None" as the type from a None initializer:
            #
            # 1. If all happens within a single function this is acceptable, since only
            #    the topmost function is a separate target in fine-grained incremental mode.
            #    We primarily want to avoid "splitting" partial types across targets.
            #
            # 2. A None initializer in the class body if the attribute is defined in a base
            #    class is fine, since the attribute is already defined and it's currently okay
            #    to vary the type of an attribute covariantly. The None type will still be
            #    checked for compatibility with base classes elsewhere. Without this exception
            #    mypy could require an annotation for an attribute that already has been
            #    declared in a base class, which would be bad.
            allow_none = (not self.options.local_partial_types
                          or is_function
                          or (is_class and self.is_defined_in_base_class(var)))
            if (allow_none
                    and isinstance(var.type, PartialType)
                    and var.type.type is None
                    and not permissive):
                var.type = NoneType()
            else:
                if var not in self.partial_reported and not permissive:
                    self.msg.need_annotation_for_var(var, context, self.options.python_version)
                    self.partial_reported.add(var)
                if var.type:
                    var.type = self.fixup_partial_type(var.type)

</t>
<t tx="ekr.20220525082933.523">def handle_partial_var_type(
        self, typ: PartialType, is_lvalue: bool, node: Var, context: Context) -&gt; Type:
    """Handle a reference to a partial type through a var.

    (Used by checkexpr and checkmember.)
    """
    in_scope, is_local, partial_types = self.find_partial_types_in_all_scopes(node)
    if typ.type is None and in_scope:
        # 'None' partial type. It has a well-defined type. In an lvalue context
        # we want to preserve the knowledge of it being a partial type.
        if not is_lvalue:
            return NoneType()
        else:
            return typ
    else:
        if partial_types is not None and not self.current_node_deferred:
            if in_scope:
                context = partial_types[node]
                if is_local or not self.options.allow_untyped_globals:
                    self.msg.need_annotation_for_var(node, context,
                                                     self.options.python_version)
                    self.partial_reported.add(node)
            else:
                # Defer the node -- we might get a better type in the outer scope
                self.handle_cannot_determine_type(node.name, context)
        return self.fixup_partial_type(typ)

</t>
<t tx="ekr.20220525082933.524">def fixup_partial_type(self, typ: Type) -&gt; Type:
    """Convert a partial type that we couldn't resolve into something concrete.

    This means, for None we make it Optional[Any], and for anything else we
    fill in all of the type arguments with Any.
    """
    if not isinstance(typ, PartialType):
        return typ
    if typ.type is None:
        return UnionType.make_union([AnyType(TypeOfAny.unannotated), NoneType()])
    else:
        return Instance(
            typ.type,
            [AnyType(TypeOfAny.unannotated)] * len(typ.type.type_vars))

</t>
<t tx="ekr.20220525082933.525">def is_defined_in_base_class(self, var: Var) -&gt; bool:
    if var.info:
        for base in var.info.mro[1:]:
            if base.get(var.name) is not None:
                return True
        if var.info.fallback_to_any:
            return True
    return False

</t>
<t tx="ekr.20220525082933.526">def find_partial_types(self, var: Var) -&gt; Optional[Dict[Var, Context]]:
    """Look for an active partial type scope containing variable.

    A scope is active if assignments in the current context can refine a partial
    type originally defined in the scope. This is affected by the local_partial_types
    configuration option.
    """
    in_scope, _, partial_types = self.find_partial_types_in_all_scopes(var)
    if in_scope:
        return partial_types
    return None

</t>
<t tx="ekr.20220525082933.527">def find_partial_types_in_all_scopes(
        self, var: Var) -&gt; Tuple[bool, bool, Optional[Dict[Var, Context]]]:
    """Look for partial type scope containing variable.

    Return tuple (is the scope active, is the scope a local scope, scope).
    """
    for scope in reversed(self.partial_types):
        if var in scope.map:
            # All scopes within the outermost function are active. Scopes out of
            # the outermost function are inactive to allow local reasoning (important
            # for fine-grained incremental mode).
            disallow_other_scopes = self.options.local_partial_types

            if isinstance(var.type, PartialType) and var.type.type is not None and var.info:
                # This is an ugly hack to make partial generic self attributes behave
                # as if --local-partial-types is always on (because it used to be like this).
                disallow_other_scopes = True

            scope_active = (not disallow_other_scopes
                            or scope.is_local == self.partial_types[-1].is_local)
            return scope_active, scope.is_local, scope.map
    return False, False, None

</t>
<t tx="ekr.20220525082933.528">def temp_node(self, t: Type, context: Optional[Context] = None) -&gt; TempNode:
    """Create a temporary node with the given, fixed type."""
    return TempNode(t, context=context)

</t>
<t tx="ekr.20220525082933.529">def fail(self, msg: Union[str, ErrorMessage], context: Context, *,
         code: Optional[ErrorCode] = None) -&gt; None:
    """Produce an error message."""
    if isinstance(msg, ErrorMessage):
        self.msg.fail(msg.value, context, code=msg.code)
        return
    self.msg.fail(msg, context, code=code)

</t>
<t tx="ekr.20220525082933.53">def other_coroutine() -&gt; Aw:
    return Aw()

</t>
<t tx="ekr.20220525082933.530">def note(self,
         msg: str,
         context: Context,
         offset: int = 0,
         *,
         code: Optional[ErrorCode] = None) -&gt; None:
    """Produce a note."""
    self.msg.note(msg, context, offset=offset, code=code)

</t>
<t tx="ekr.20220525082933.531">def iterable_item_type(self, instance: Instance) -&gt; Type:
    iterable = map_instance_to_supertype(
        instance,
        self.lookup_typeinfo('typing.Iterable'))
    item_type = iterable.args[0]
    if not isinstance(get_proper_type(item_type), AnyType):
        # This relies on 'map_instance_to_supertype' returning 'Iterable[Any]'
        # in case there is no explicit base class.
        return item_type
    # Try also structural typing.
    iter_type = get_proper_type(find_member('__iter__', instance, instance, is_operator=True))
    if iter_type and isinstance(iter_type, CallableType):
        ret_type = get_proper_type(iter_type.ret_type)
        if isinstance(ret_type, Instance):
            iterator = map_instance_to_supertype(ret_type,
                                                 self.lookup_typeinfo('typing.Iterator'))
            item_type = iterator.args[0]
    return item_type

</t>
<t tx="ekr.20220525082933.532">def function_type(self, func: FuncBase) -&gt; FunctionLike:
    return function_type(func, self.named_type('builtins.function'))

</t>
<t tx="ekr.20220525082933.533">def push_type_map(self, type_map: 'TypeMap') -&gt; None:
    if type_map is None:
        self.binder.unreachable()
    else:
        for expr, type in type_map.items():
            self.binder.put(expr, type)

</t>
<t tx="ekr.20220525082933.534">def infer_issubclass_maps(self, node: CallExpr,
                          expr: Expression,
                          ) -&gt; Tuple[TypeMap, TypeMap]:
    """Infer type restrictions for an expression in issubclass call."""
    vartype = self.lookup_type(expr)
    type = self.get_isinstance_type(node.args[1])
    if isinstance(vartype, TypeVarType):
        vartype = vartype.upper_bound
    vartype = get_proper_type(vartype)
    if isinstance(vartype, UnionType):
        union_list = []
        for t in get_proper_types(vartype.items):
            if isinstance(t, TypeType):
                union_list.append(t.item)
            else:
                # This is an error that should be reported earlier
                # if we reach here, we refuse to do any type inference.
                return {}, {}
        vartype = UnionType(union_list)
    elif isinstance(vartype, TypeType):
        vartype = vartype.item
    elif (isinstance(vartype, Instance) and
            vartype.type.fullname == 'builtins.type'):
        vartype = self.named_type('builtins.object')
    else:
        # Any other object whose type we don't know precisely
        # for example, Any or a custom metaclass.
        return {}, {}  # unknown type
    yes_type, no_type = self.conditional_types_with_intersection(vartype, type, expr)
    yes_map, no_map = conditional_types_to_typemaps(expr, yes_type, no_type)
    yes_map, no_map = map(convert_to_typetype, (yes_map, no_map))
    return yes_map, no_map

</t>
<t tx="ekr.20220525082933.535">@overload
def conditional_types_with_intersection(self,
                                        expr_type: Type,
                                        type_ranges: Optional[List[TypeRange]],
                                        ctx: Context,
                                        default: None = None
                                        ) -&gt; Tuple[Optional[Type], Optional[Type]]: ...

</t>
<t tx="ekr.20220525082933.536">@overload
def conditional_types_with_intersection(self,
                                        expr_type: Type,
                                        type_ranges: Optional[List[TypeRange]],
                                        ctx: Context,
                                        default: Type
                                        ) -&gt; Tuple[Type, Type]: ...

</t>
<t tx="ekr.20220525082933.537">def conditional_types_with_intersection(self,
                                        expr_type: Type,
                                        type_ranges: Optional[List[TypeRange]],
                                        ctx: Context,
                                        default: Optional[Type] = None
                                        ) -&gt; Tuple[Optional[Type], Optional[Type]]:
    initial_types = conditional_types(expr_type, type_ranges, default)
    # For some reason, doing "yes_map, no_map = conditional_types_to_typemaps(...)"
    # doesn't work: mypyc will decide that 'yes_map' is of type None if we try.
    yes_type: Optional[Type] = initial_types[0]
    no_type: Optional[Type] = initial_types[1]

    if not isinstance(get_proper_type(yes_type), UninhabitedType) or type_ranges is None:
        return yes_type, no_type

    # If conditional_types was unable to successfully narrow the expr_type
    # using the type_ranges and concluded if-branch is unreachable, we try
    # computing it again using a different algorithm that tries to generate
    # an ad-hoc intersection between the expr_type and the type_ranges.
    proper_type = get_proper_type(expr_type)
    if isinstance(proper_type, UnionType):
        possible_expr_types = get_proper_types(proper_type.relevant_items())
    else:
        possible_expr_types = [proper_type]

    possible_target_types = []
    for tr in type_ranges:
        item = get_proper_type(tr.item)
        if not isinstance(item, Instance) or tr.is_upper_bound:
            return yes_type, no_type
        possible_target_types.append(item)

    out = []
    for v in possible_expr_types:
        if not isinstance(v, Instance):
            return yes_type, no_type
        for t in possible_target_types:
            intersection = self.intersect_instances((v, t), ctx)
            if intersection is None:
                continue
            out.append(intersection)
    if len(out) == 0:
        return UninhabitedType(), expr_type
    new_yes_type = make_simplified_union(out)
    return new_yes_type, expr_type

</t>
<t tx="ekr.20220525082933.538">def is_writable_attribute(self, node: Node) -&gt; bool:
    """Check if an attribute is writable"""
    if isinstance(node, Var):
        return True
    elif isinstance(node, OverloadedFuncDef) and node.is_property:
        first_item = cast(Decorator, node.items[0])
        return first_item.var.is_settable_property
    else:
        return False

</t>
<t tx="ekr.20220525082933.539">def get_isinstance_type(self, expr: Expression) -&gt; Optional[List[TypeRange]]:
    if isinstance(expr, OpExpr) and expr.op == '|':
        left = self.get_isinstance_type(expr.left)
        right = self.get_isinstance_type(expr.right)
        if left is None or right is None:
            return None
        return left + right
    all_types = get_proper_types(flatten_types(self.lookup_type(expr)))
    types: List[TypeRange] = []
    for typ in all_types:
        if isinstance(typ, FunctionLike) and typ.is_type_obj():
            # Type variables may be present -- erase them, which is the best
            # we can do (outside disallowing them here).
            erased_type = erase_typevars(typ.items[0].ret_type)
            types.append(TypeRange(erased_type, is_upper_bound=False))
        elif isinstance(typ, TypeType):
            # Type[A] means "any type that is a subtype of A" rather than "precisely type A"
            # we indicate this by setting is_upper_bound flag
            types.append(TypeRange(typ.item, is_upper_bound=True))
        elif isinstance(typ, Instance) and typ.type.fullname == 'builtins.type':
            object_type = Instance(typ.type.mro[-1], [])
            types.append(TypeRange(object_type, is_upper_bound=True))
        elif isinstance(typ, AnyType):
            types.append(TypeRange(typ, is_upper_bound=False))
        else:  # we didn't see an actual type, but rather a variable with unknown value
            return None
    if not types:
        # this can happen if someone has empty tuple as 2nd argument to isinstance
        # strictly speaking, we should return UninhabitedType but for simplicity we will simply
        # refuse to do any type inference for now
        return None
    return types

</t>
<t tx="ekr.20220525082933.54"># The various contexts in which `await` or `yield from` might occur.

</t>
<t tx="ekr.20220525082933.540">def is_literal_enum(self, n: Expression) -&gt; bool:
    """Returns true if this expression (with the given type context) is an Enum literal.

    For example, if we had an enum:

        class Foo(Enum):
            A = 1
            B = 2

    ...and if the expression 'Foo' referred to that enum within the current type context,
    then the expression 'Foo.A' would be a literal enum. However, if we did 'a = Foo.A',
    then the variable 'a' would *not* be a literal enum.

    We occasionally special-case expressions like 'Foo.A' and treat them as a single primitive
    unit for the same reasons we sometimes treat 'True', 'False', or 'None' as a single
    primitive unit.
    """
    if not isinstance(n, MemberExpr) or not isinstance(n.expr, NameExpr):
        return False

    parent_type = self.lookup_type_or_none(n.expr)
    member_type = self.lookup_type_or_none(n)
    if member_type is None or parent_type is None:
        return False

    parent_type = get_proper_type(parent_type)
    member_type = get_proper_type(coerce_to_literal(member_type))
    if not isinstance(parent_type, FunctionLike) or not isinstance(member_type, LiteralType):
        return False

    if not parent_type.is_type_obj():
        return False

    return (member_type.is_enum_literal()
            and member_type.fallback.type == parent_type.type_object())


</t>
<t tx="ekr.20220525082933.541">@overload
def conditional_types(current_type: Type,
                      proposed_type_ranges: Optional[List[TypeRange]],
                      default: None = None
                      ) -&gt; Tuple[Optional[Type], Optional[Type]]: ...


</t>
<t tx="ekr.20220525082933.542">@overload
def conditional_types(current_type: Type,
                      proposed_type_ranges: Optional[List[TypeRange]],
                      default: Type
                      ) -&gt; Tuple[Type, Type]: ...


</t>
<t tx="ekr.20220525082933.543">def conditional_types(current_type: Type,
                      proposed_type_ranges: Optional[List[TypeRange]],
                      default: Optional[Type] = None
                      ) -&gt; Tuple[Optional[Type], Optional[Type]]:
    """Takes in the current type and a proposed type of an expression.

    Returns a 2-tuple: The first element is the proposed type, if the expression
    can be the proposed type. The second element is the type it would hold
    if it was not the proposed type, if any. UninhabitedType means unreachable.
    None means no new information can be inferred. If default is set it is returned
    instead."""
    if proposed_type_ranges:
        if len(proposed_type_ranges) == 1:
            target = proposed_type_ranges[0].item
            target = get_proper_type(target)
            if isinstance(target, LiteralType) and (target.is_enum_literal()
                                                    or isinstance(target.value, bool)):
                enum_name = target.fallback.type.fullname
                current_type = try_expanding_sum_type_to_union(current_type,
                                                               enum_name)
        proposed_items = [type_range.item for type_range in proposed_type_ranges]
        proposed_type = make_simplified_union(proposed_items)
        if isinstance(proposed_type, AnyType):
            # We don't really know much about the proposed type, so we shouldn't
            # attempt to narrow anything. Instead, we broaden the expr to Any to
            # avoid false positives
            return proposed_type, default
        elif (not any(type_range.is_upper_bound for type_range in proposed_type_ranges)
           and is_proper_subtype(current_type, proposed_type)):
            # Expression is always of one of the types in proposed_type_ranges
            return default, UninhabitedType()
        elif not is_overlapping_types(current_type, proposed_type,
                                      prohibit_none_typevar_overlap=True):
            # Expression is never of any type in proposed_type_ranges
            return UninhabitedType(), default
        else:
            # we can only restrict when the type is precise, not bounded
            proposed_precise_type = UnionType.make_union([type_range.item
                                      for type_range in proposed_type_ranges
                                      if not type_range.is_upper_bound])
            remaining_type = restrict_subtype_away(current_type, proposed_precise_type)
            return proposed_type, remaining_type
    else:
        # An isinstance check, but we don't understand the type
        return current_type, default


</t>
<t tx="ekr.20220525082933.544">def conditional_types_to_typemaps(expr: Expression,
                                  yes_type: Optional[Type],
                                  no_type: Optional[Type]
                                  ) -&gt; Tuple[TypeMap, TypeMap]:
    maps: List[TypeMap] = []
    for typ in (yes_type, no_type):
        proper_type = get_proper_type(typ)
        if isinstance(proper_type, UninhabitedType):
            maps.append(None)
        elif proper_type is None:
            maps.append({})
        else:
            assert typ is not None
            maps.append({expr: typ})

    return cast(Tuple[TypeMap, TypeMap], tuple(maps))


</t>
<t tx="ekr.20220525082933.545">def gen_unique_name(base: str, table: SymbolTable) -&gt; str:
    """Generate a name that does not appear in table by appending numbers to base."""
    if base not in table:
        return base
    i = 1
    while base + str(i) in table:
        i += 1
    return base + str(i)


</t>
<t tx="ekr.20220525082933.546">def is_true_literal(n: Expression) -&gt; bool:
    """Returns true if this expression is the 'True' literal/keyword."""
    return (refers_to_fullname(n, 'builtins.True')
            or isinstance(n, IntExpr) and n.value != 0)


</t>
<t tx="ekr.20220525082933.547">def is_false_literal(n: Expression) -&gt; bool:
    """Returns true if this expression is the 'False' literal/keyword."""
    return (refers_to_fullname(n, 'builtins.False')
            or isinstance(n, IntExpr) and n.value == 0)


</t>
<t tx="ekr.20220525082933.548">def is_literal_none(n: Expression) -&gt; bool:
    """Returns true if this expression is the 'None' literal/keyword."""
    return isinstance(n, NameExpr) and n.fullname == 'builtins.None'


</t>
<t tx="ekr.20220525082933.549">def is_literal_not_implemented(n: Expression) -&gt; bool:
    return isinstance(n, NameExpr) and n.fullname == 'builtins.NotImplemented'


</t>
<t tx="ekr.20220525082933.55">def plain_host_generator(func) -&gt; Generator[str, None, None]:
    yield 'a'
    x = 0
    f = func()
    try:
        x = yield from f
    finally:
        try:
            f.close()
        except AttributeError:
            pass

</t>
<t tx="ekr.20220525082933.550">def builtin_item_type(tp: Type) -&gt; Optional[Type]:
    """Get the item type of a builtin container.

    If 'tp' is not one of the built containers (these includes NamedTuple and TypedDict)
    or if the container is not parameterized (like List or List[Any])
    return None. This function is used to narrow optional types in situations like this:

        x: Optional[int]
        if x in (1, 2, 3):
            x + 42  # OK

    Note: this is only OK for built-in containers, where we know the behavior
    of __contains__.
    """
    tp = get_proper_type(tp)

    if isinstance(tp, Instance):
        if tp.type.fullname in [
            'builtins.list', 'builtins.tuple', 'builtins.dict',
            'builtins.set', 'builtins.frozenset',
        ]:
            if not tp.args:
                # TODO: fix tuple in lib-stub/builtins.pyi (it should be generic).
                return None
            if not isinstance(get_proper_type(tp.args[0]), AnyType):
                return tp.args[0]
    elif isinstance(tp, TupleType) and all(not isinstance(it, AnyType)
                                           for it in get_proper_types(tp.items)):
        return make_simplified_union(tp.items)  # this type is not externally visible
    elif isinstance(tp, TypedDictType):
        # TypedDict always has non-optional string keys. Find the key type from the Mapping
        # base class.
        for base in tp.fallback.type.mro:
            if base.fullname == 'typing.Mapping':
                return map_instance_to_supertype(tp.fallback, base).args[0]
        assert False, 'No Mapping base class found for TypedDict fallback'
    return None


</t>
<t tx="ekr.20220525082933.551">def and_conditional_maps(m1: TypeMap, m2: TypeMap) -&gt; TypeMap:
    """Calculate what information we can learn from the truth of (e1 and e2)
    in terms of the information that we can learn from the truth of e1 and
    the truth of e2.
    """

    if m1 is None or m2 is None:
        # One of the conditions can never be true.
        return None
    # Both conditions can be true; combine the information. Anything
    # we learn from either conditions's truth is valid. If the same
    # expression's type is refined by both conditions, we somewhat
    # arbitrarily give precedence to m2. (In the future, we could use
    # an intersection type.)
    result = m2.copy()
    m2_keys = {literal_hash(n2) for n2 in m2}
    for n1 in m1:
        if literal_hash(n1) not in m2_keys:
            result[n1] = m1[n1]
    return result


</t>
<t tx="ekr.20220525082933.552">def or_conditional_maps(m1: TypeMap, m2: TypeMap) -&gt; TypeMap:
    """Calculate what information we can learn from the truth of (e1 or e2)
    in terms of the information that we can learn from the truth of e1 and
    the truth of e2.
    """

    if m1 is None:
        return m2
    if m2 is None:
        return m1
    # Both conditions can be true. Combine information about
    # expressions whose type is refined by both conditions. (We do not
    # learn anything about expressions whose type is refined by only
    # one condition.)
    result: Dict[Expression, Type] = {}
    for n1 in m1:
        for n2 in m2:
            if literal_hash(n1) == literal_hash(n2):
                result[n1] = make_simplified_union([m1[n1], m2[n2]])
    return result


</t>
<t tx="ekr.20220525082933.553">def reduce_conditional_maps(type_maps: List[Tuple[TypeMap, TypeMap]],
                            ) -&gt; Tuple[TypeMap, TypeMap]:
    """Reduces a list containing pairs of if/else TypeMaps into a single pair.

    We "and" together all of the if TypeMaps and "or" together the else TypeMaps. So
    for example, if we had the input:

        [
            ({x: TypeIfX, shared: TypeIfShared1}, {x: TypeElseX, shared: TypeElseShared1}),
            ({y: TypeIfY, shared: TypeIfShared2}, {y: TypeElseY, shared: TypeElseShared2}),
        ]

    ...we'd return the output:

        (
            {x: TypeIfX,   y: TypeIfY,   shared: PseudoIntersection[TypeIfShared1, TypeIfShared2]},
            {shared: Union[TypeElseShared1, TypeElseShared2]},
        )

    ...where "PseudoIntersection[X, Y] == Y" because mypy actually doesn't understand intersections
    yet, so we settle for just arbitrarily picking the right expr's type.

    We only retain the shared expression in the 'else' case because we don't actually know
    whether x was refined or y was refined -- only just that one of the two was refined.
    """
    if len(type_maps) == 0:
        return {}, {}
    elif len(type_maps) == 1:
        return type_maps[0]
    else:
        final_if_map, final_else_map = type_maps[0]
        for if_map, else_map in type_maps[1:]:
            final_if_map = and_conditional_maps(final_if_map, if_map)
            final_else_map = or_conditional_maps(final_else_map, else_map)

        return final_if_map, final_else_map


</t>
<t tx="ekr.20220525082933.554">def convert_to_typetype(type_map: TypeMap) -&gt; TypeMap:
    converted_type_map: Dict[Expression, Type] = {}
    if type_map is None:
        return None
    for expr, typ in type_map.items():
        t = typ
        if isinstance(t, TypeVarType):
            t = t.upper_bound
        # TODO: should we only allow unions of instances as per PEP 484?
        if not isinstance(get_proper_type(t), (UnionType, Instance)):
            # unknown type; error was likely reported earlier
            return {}
        converted_type_map[expr] = TypeType.make_normalized(typ)
    return converted_type_map


</t>
<t tx="ekr.20220525082933.555">def flatten(t: Expression) -&gt; List[Expression]:
    """Flatten a nested sequence of tuples/lists into one list of nodes."""
    if isinstance(t, TupleExpr) or isinstance(t, ListExpr):
        return [b for a in t.items for b in flatten(a)]
    elif isinstance(t, StarExpr):
        return flatten(t.expr)
    else:
        return [t]


</t>
<t tx="ekr.20220525082933.556">def flatten_types(t: Type) -&gt; List[Type]:
    """Flatten a nested sequence of tuples into one list of nodes."""
    t = get_proper_type(t)
    if isinstance(t, TupleType):
        return [b for a in t.items for b in flatten_types(a)]
    else:
        return [t]


</t>
<t tx="ekr.20220525082933.557">def expand_func(defn: FuncItem, map: Dict[TypeVarId, Type]) -&gt; FuncItem:
    visitor = TypeTransformVisitor(map)
    ret = defn.accept(visitor)
    assert isinstance(ret, FuncItem)
    return ret


</t>
<t tx="ekr.20220525082933.558">class TypeTransformVisitor(TransformVisitor):
    def __init__(self, map: Dict[TypeVarId, Type]) -&gt; None:
        super().__init__()
        self.map = map

    def type(self, type: Type) -&gt; Type:
        return expand_type(type, self.map)


</t>
<t tx="ekr.20220525082933.559">def are_argument_counts_overlapping(t: CallableType, s: CallableType) -&gt; bool:
    """Can a single call match both t and s, based just on positional argument counts?
    """
    min_args = max(t.min_args, s.min_args)
    max_args = min(t.max_possible_positional_args(), s.max_possible_positional_args())
    return min_args &lt;= max_args


</t>
<t tx="ekr.20220525082933.56">async def plain_host_coroutine(func) -&gt; None:
    x = 0
    x = await func()

</t>
<t tx="ekr.20220525082933.560">def is_unsafe_overlapping_overload_signatures(signature: CallableType,
                                              other: CallableType) -&gt; bool:
    """Check if two overloaded signatures are unsafely overlapping or partially overlapping.

    We consider two functions 's' and 't' to be unsafely overlapping if both
    of the following are true:

    1.  s's parameters are all more precise or partially overlapping with t's
    2.  s's return type is NOT a subtype of t's.

    Assumes that 'signature' appears earlier in the list of overload
    alternatives then 'other' and that their argument counts are overlapping.
    """
    # Try detaching callables from the containing class so that all TypeVars
    # are treated as being free.
    #
    # This lets us identify cases where the two signatures use completely
    # incompatible types -- e.g. see the testOverloadingInferUnionReturnWithMixedTypevars
    # test case.
    signature = detach_callable(signature)
    other = detach_callable(other)

    # Note: We repeat this check twice in both directions due to a slight
    # asymmetry in 'is_callable_compatible'. When checking for partial overlaps,
    # we attempt to unify 'signature' and 'other' both against each other.
    #
    # If 'signature' cannot be unified with 'other', we end early. However,
    # if 'other' cannot be modified with 'signature', the function continues
    # using the older version of 'other'.
    #
    # This discrepancy is unfortunately difficult to get rid of, so we repeat the
    # checks twice in both directions for now.
    return (is_callable_compatible(signature, other,
                                  is_compat=is_overlapping_types_no_promote,
                                  is_compat_return=lambda l, r: not is_subtype_no_promote(l, r),
                                  ignore_return=False,
                                  check_args_covariantly=True,
                                  allow_partial_overlap=True) or
            is_callable_compatible(other, signature,
                                   is_compat=is_overlapping_types_no_promote,
                                   is_compat_return=lambda l, r: not is_subtype_no_promote(r, l),
                                   ignore_return=False,
                                   check_args_covariantly=False,
                                   allow_partial_overlap=True))


</t>
<t tx="ekr.20220525082933.561">def detach_callable(typ: CallableType) -&gt; CallableType:
    """Ensures that the callable's type variables are 'detached' and independent of the context.

    A callable normally keeps track of the type variables it uses within its 'variables' field.
    However, if the callable is from a method and that method is using a class type variable,
    the callable will not keep track of that type variable since it belongs to the class.

    This function will traverse the callable and find all used type vars and add them to the
    variables field if it isn't already present.

    The caller can then unify on all type variables whether or not the callable is originally
    from a class or not."""
    type_list = typ.arg_types + [typ.ret_type]

    appear_map: Dict[str, List[int]] = {}
    for i, inner_type in enumerate(type_list):
        typevars_available = get_type_vars(inner_type)
        for var in typevars_available:
            if var.fullname not in appear_map:
                appear_map[var.fullname] = []
            appear_map[var.fullname].append(i)

    used_type_var_names = set()
    for var_name, appearances in appear_map.items():
        used_type_var_names.add(var_name)

    all_type_vars = get_type_vars(typ)
    new_variables = []
    for var in set(all_type_vars):
        if var.fullname not in used_type_var_names:
            continue
        new_variables.append(TypeVarType(
            name=var.name,
            fullname=var.fullname,
            id=var.id,
            values=var.values,
            upper_bound=var.upper_bound,
            variance=var.variance,
        ))
    out = typ.copy_modified(
        variables=new_variables,
        arg_types=type_list[:-1],
        ret_type=type_list[-1],
    )
    return out


</t>
<t tx="ekr.20220525082933.562">def overload_can_never_match(signature: CallableType, other: CallableType) -&gt; bool:
    """Check if the 'other' method can never be matched due to 'signature'.

    This can happen if signature's parameters are all strictly broader then
    other's parameters.

    Assumes that both signatures have overlapping argument counts.
    """
    # The extra erasure is needed to prevent spurious errors
    # in situations where an `Any` overload is used as a fallback
    # for an overload with type variables. The spurious error appears
    # because the type variables turn into `Any` during unification in
    # the below subtype check and (surprisingly?) `is_proper_subtype(Any, Any)`
    # returns `True`.
    # TODO: find a cleaner solution instead of this ad-hoc erasure.
    exp_signature = expand_type(signature, {tvar.id: erase_def_to_union_or_bound(tvar)
                                for tvar in signature.variables})
    assert isinstance(exp_signature, ProperType)
    assert isinstance(exp_signature, CallableType)
    return is_callable_compatible(exp_signature, other,
                                  is_compat=is_more_precise,
                                  ignore_return=True)


</t>
<t tx="ekr.20220525082933.563">def is_more_general_arg_prefix(t: FunctionLike, s: FunctionLike) -&gt; bool:
    """Does t have wider arguments than s?"""
    # TODO should an overload with additional items be allowed to be more
    #      general than one with fewer items (or just one item)?
    if isinstance(t, CallableType):
        if isinstance(s, CallableType):
            return is_callable_compatible(t, s,
                                          is_compat=is_proper_subtype,
                                          ignore_return=True)
    elif isinstance(t, FunctionLike):
        if isinstance(s, FunctionLike):
            if len(t.items) == len(s.items):
                return all(is_same_arg_prefix(items, itemt)
                           for items, itemt in zip(t.items, s.items))
    return False


</t>
<t tx="ekr.20220525082933.564">def is_same_arg_prefix(t: CallableType, s: CallableType) -&gt; bool:
    return is_callable_compatible(t, s,
                                  is_compat=is_same_type,
                                  ignore_return=True,
                                  check_args_covariantly=True,
                                  ignore_pos_arg_names=True)


</t>
<t tx="ekr.20220525082933.565">def infer_operator_assignment_method(typ: Type, operator: str) -&gt; Tuple[bool, str]:
    """Determine if operator assignment on given value type is in-place, and the method name.

    For example, if operator is '+', return (True, '__iadd__') or (False, '__add__')
    depending on which method is supported by the type.
    """
    typ = get_proper_type(typ)
    method = operators.op_methods[operator]
    if isinstance(typ, Instance):
        if operator in operators.ops_with_inplace_method:
            inplace_method = '__i' + method[2:]
            if typ.type.has_readable_member(inplace_method):
                return True, inplace_method
    return False, method


</t>
<t tx="ekr.20220525082933.566">def is_valid_inferred_type(typ: Type) -&gt; bool:
    """Is an inferred type valid?

    Examples of invalid types include the None type or List[&lt;uninhabited&gt;].

    When not doing strict Optional checking, all types containing None are
    invalid.  When doing strict Optional checking, only None and types that are
    incompletely defined (i.e. contain UninhabitedType) are invalid.
    """
    if isinstance(get_proper_type(typ), (NoneType, UninhabitedType)):
        # With strict Optional checking, we *may* eventually infer NoneType when
        # the initializer is None, but we only do that if we can't infer a
        # specific Optional type.  This resolution happens in
        # leave_partial_types when we pop a partial types scope.
        return False
    return not typ.accept(NothingSeeker())


</t>
<t tx="ekr.20220525082933.567">class NothingSeeker(TypeQuery[bool]):
    """Find any &lt;nothing&gt; types resulting from failed (ambiguous) type inference."""

    @others
</t>
<t tx="ekr.20220525082933.568">def __init__(self) -&gt; None:
    super().__init__(any)

</t>
<t tx="ekr.20220525082933.569">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; bool:
    return t.ambiguous


</t>
<t tx="ekr.20220525082933.57">@coroutine
def decorated_host_generator(func) -&gt; Generator[str, None, None]:
    yield 'a'
    x = 0
    f = func()
    try:
        x = yield from f
    finally:
        try:
            f.close()
        except AttributeError:
            pass

</t>
<t tx="ekr.20220525082933.570">class SetNothingToAny(TypeTranslator):
    """Replace all ambiguous &lt;nothing&gt; types with Any (to avoid spurious extra errors)."""

    @others
</t>
<t tx="ekr.20220525082933.571">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; Type:
    if t.ambiguous:
        return AnyType(TypeOfAny.from_error)
    return t

</t>
<t tx="ekr.20220525082933.572">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # Target of the alias cannot by an ambiguous &lt;nothing&gt;, so we just
    # replace the arguments.
    return t.copy_modified(args=[a.accept(self) for a in t.args])


</t>
<t tx="ekr.20220525082933.573">def is_node_static(node: Optional[Node]) -&gt; Optional[bool]:
    """Find out if a node describes a static function method."""

    if isinstance(node, FuncDef):
        return node.is_static

    if isinstance(node, Var):
        return node.is_staticmethod

    return None


</t>
<t tx="ekr.20220525082933.574">class CheckerScope:
    # We keep two stacks combined, to maintain the relative order
    stack: List[Union[TypeInfo, FuncItem, MypyFile]]

    @others
</t>
<t tx="ekr.20220525082933.575">def __init__(self, module: MypyFile) -&gt; None:
    self.stack = [module]

</t>
<t tx="ekr.20220525082933.576">def top_function(self) -&gt; Optional[FuncItem]:
    for e in reversed(self.stack):
        if isinstance(e, FuncItem):
            return e
    return None

</t>
<t tx="ekr.20220525082933.577">def top_non_lambda_function(self) -&gt; Optional[FuncItem]:
    for e in reversed(self.stack):
        if isinstance(e, FuncItem) and not isinstance(e, LambdaExpr):
            return e
    return None

</t>
<t tx="ekr.20220525082933.578">def active_class(self) -&gt; Optional[TypeInfo]:
    if isinstance(self.stack[-1], TypeInfo):
        return self.stack[-1]
    return None

</t>
<t tx="ekr.20220525082933.579">def enclosing_class(self) -&gt; Optional[TypeInfo]:
    """Is there a class *directly* enclosing this function?"""
    top = self.top_function()
    assert top, "This method must be called from inside a function"
    index = self.stack.index(top)
    assert index, "CheckerScope stack must always start with a module"
    enclosing = self.stack[index - 1]
    if isinstance(enclosing, TypeInfo):
        return enclosing
    return None

</t>
<t tx="ekr.20220525082933.58">@coroutine
async def decorated_host_coroutine(func) -&gt; None:
    x = 0
    x = await func()

</t>
<t tx="ekr.20220525082933.580">def active_self_type(self) -&gt; Optional[Union[Instance, TupleType]]:
    """An instance or tuple type representing the current class.

    This returns None unless we are in class body or in a method.
    In particular, inside a function nested in method this returns None.
    """
    info = self.active_class()
    if not info and self.top_function():
        info = self.enclosing_class()
    if info:
        return fill_typevars(info)
    return None

</t>
<t tx="ekr.20220525082933.581">@contextmanager
def push_function(self, item: FuncItem) -&gt; Iterator[None]:
    self.stack.append(item)
    yield
    self.stack.pop()

</t>
<t tx="ekr.20220525082933.582">@contextmanager
def push_class(self, info: TypeInfo) -&gt; Iterator[None]:
    self.stack.append(info)
    yield
    self.stack.pop()


</t>
<t tx="ekr.20220525082933.583">TKey = TypeVar('TKey')
TValue = TypeVar('TValue')


</t>
<t tx="ekr.20220525082933.584">class DisjointDict(Generic[TKey, TValue]):
    """An variation of the union-find algorithm/data structure where instead of keeping
    track of just disjoint sets, we keep track of disjoint dicts -- keep track of multiple
    Set[Key] -&gt; Set[Value] mappings, where each mapping's keys are guaranteed to be disjoint.

    This data structure is currently used exclusively by 'group_comparison_operands' below
    to merge chains of '==' and 'is' comparisons when two or more chains use the same expression
    in best-case O(n), where n is the number of operands.

    Specifically, the `add_mapping()` function and `items()` functions will take on average
    O(k + v) and O(n) respectively, where k and v are the number of keys and values we're adding
    for a given chain. Note that k &lt;= n and v &lt;= n.

    We hit these average/best-case scenarios for most user code: e.g. when the user has just
    a single chain like 'a == b == c == d == ...' or multiple disjoint chains like
    'a==b &lt; c==d &lt; e==f &lt; ...'. (Note that a naive iterative merging would be O(n^2) for
    the latter case).

    In comparison, this data structure will make 'group_comparison_operands' have a worst-case
    runtime of O(n*log(n)): 'add_mapping()' and 'items()' are worst-case O(k*log(n) + v) and
    O(k*log(n)) respectively. This happens only in the rare case where the user keeps repeatedly
    making disjoint mappings before merging them in a way that persistently dodges the path
    compression optimization in '_lookup_root_id', which would end up constructing a single
    tree of height log_2(n). This makes root lookups no longer amoritized constant time when we
    finally call 'items()'.
    """
    @others
</t>
<t tx="ekr.20220525082933.585">def __init__(self) -&gt; None:
    # Each key maps to a unique ID
    self._key_to_id: Dict[TKey, int] = {}

    # Each id points to the parent id, forming a forest of upwards-pointing trees. If the
    # current id already is the root, it points to itself. We gradually flatten these trees
    # as we perform root lookups: eventually all nodes point directly to its root.
    self._id_to_parent_id: Dict[int, int] = {}

    # Each root id in turn maps to the set of values.
    self._root_id_to_values: Dict[int, Set[TValue]] = {}

</t>
<t tx="ekr.20220525082933.586">def add_mapping(self, keys: Set[TKey], values: Set[TValue]) -&gt; None:
    """Adds a 'Set[TKey] -&gt; Set[TValue]' mapping. If there already exists a mapping
    containing one or more of the given keys, we merge the input mapping with the old one.

    Note that the given set of keys must be non-empty -- otherwise, nothing happens.
    """
    if len(keys) == 0:
        return

    subtree_roots = [self._lookup_or_make_root_id(key) for key in keys]
    new_root = subtree_roots[0]

    root_values = self._root_id_to_values[new_root]
    root_values.update(values)
    for subtree_root in subtree_roots[1:]:
        if subtree_root == new_root or subtree_root not in self._root_id_to_values:
            continue
        self._id_to_parent_id[subtree_root] = new_root
        root_values.update(self._root_id_to_values.pop(subtree_root))

</t>
<t tx="ekr.20220525082933.587">def items(self) -&gt; List[Tuple[Set[TKey], Set[TValue]]]:
    """Returns all disjoint mappings in key-value pairs."""
    root_id_to_keys: Dict[int, Set[TKey]] = {}
    for key in self._key_to_id:
        root_id = self._lookup_root_id(key)
        if root_id not in root_id_to_keys:
            root_id_to_keys[root_id] = set()
        root_id_to_keys[root_id].add(key)

    output = []
    for root_id, keys in root_id_to_keys.items():
        output.append((keys, self._root_id_to_values[root_id]))

    return output

</t>
<t tx="ekr.20220525082933.588">def _lookup_or_make_root_id(self, key: TKey) -&gt; int:
    if key in self._key_to_id:
        return self._lookup_root_id(key)
    else:
        new_id = len(self._key_to_id)
        self._key_to_id[key] = new_id
        self._id_to_parent_id[new_id] = new_id
        self._root_id_to_values[new_id] = set()
        return new_id

</t>
<t tx="ekr.20220525082933.589">def _lookup_root_id(self, key: TKey) -&gt; int:
    i = self._key_to_id[key]
    while i != self._id_to_parent_id[i]:
        # Optimization: make keys directly point to their grandparents to speed up
        # future traversals. This prevents degenerate trees of height n from forming.
        new_parent = self._id_to_parent_id[self._id_to_parent_id[i]]
        self._id_to_parent_id[i] = new_parent
        i = new_parent
    return i


</t>
<t tx="ekr.20220525082933.59"># Main driver.

</t>
<t tx="ekr.20220525082933.590">def group_comparison_operands(pairwise_comparisons: Iterable[Tuple[str, Expression, Expression]],
                              operand_to_literal_hash: Mapping[int, Key],
                              operators_to_group: Set[str],
                              ) -&gt; List[Tuple[str, List[int]]]:
    """Group a series of comparison operands together chained by any operand
    in the 'operators_to_group' set. All other pairwise operands are kept in
    groups of size 2.

    For example, suppose we have the input comparison expression:

        x0 == x1 == x2 &lt; x3 &lt; x4 is x5 is x6 is not x7 is not x8

    If we get these expressions in a pairwise way (e.g. by calling ComparisionExpr's
    'pairwise()' method), we get the following as input:

        [('==', x0, x1), ('==', x1, x2), ('&lt;', x2, x3), ('&lt;', x3, x4),
         ('is', x4, x5), ('is', x5, x6), ('is not', x6, x7), ('is not', x7, x8)]

    If `operators_to_group` is the set {'==', 'is'}, this function will produce
    the following "simplified operator list":

       [("==", [0, 1, 2]), ("&lt;", [2, 3]), ("&lt;", [3, 4]),
        ("is", [4, 5, 6]), ("is not", [6, 7]), ("is not", [7, 8])]

    Note that (a) we yield *indices* to the operands rather then the operand
    expressions themselves and that (b) operands used in a consecutive chain
    of '==' or 'is' are grouped together.

    If two of these chains happen to contain operands with the same underlying
    literal hash (e.g. are assignable and correspond to the same expression),
    we combine those chains together. For example, if we had:

        same == x &lt; y == same

    ...and if 'operand_to_literal_hash' contained the same values for the indices
    0 and 3, we'd produce the following output:

        [("==", [0, 1, 2, 3]), ("&lt;", [1, 2])]

    But if the 'operand_to_literal_hash' did *not* contain an entry, we'd instead
    default to returning:

        [("==", [0, 1]), ("&lt;", [1, 2]), ("==", [2, 3])]

    This function is currently only used to assist with type-narrowing refinements
    and is extracted out to a helper function so we can unit test it.
    """
    groups: Dict[str, DisjointDict[Key, int]] = {op: DisjointDict() for op in operators_to_group}

    simplified_operator_list: List[Tuple[str, List[int]]] = []
    last_operator: Optional[str] = None
    current_indices: Set[int] = set()
    current_hashes: Set[Key] = set()
    for i, (operator, left_expr, right_expr) in enumerate(pairwise_comparisons):
        if last_operator is None:
            last_operator = operator

        if current_indices and (operator != last_operator or operator not in operators_to_group):
            # If some of the operands in the chain are assignable, defer adding it: we might
            # end up needing to merge it with other chains that appear later.
            if len(current_hashes) == 0:
                simplified_operator_list.append((last_operator, sorted(current_indices)))
            else:
                groups[last_operator].add_mapping(current_hashes, current_indices)
            last_operator = operator
            current_indices = set()
            current_hashes = set()

        # Note: 'i' corresponds to the left operand index, so 'i + 1' is the
        # right operand.
        current_indices.add(i)
        current_indices.add(i + 1)

        # We only ever want to combine operands/combine chains for these operators
        if operator in operators_to_group:
            left_hash = operand_to_literal_hash.get(i)
            if left_hash is not None:
                current_hashes.add(left_hash)
            right_hash = operand_to_literal_hash.get(i + 1)
            if right_hash is not None:
                current_hashes.add(right_hash)

    if last_operator is not None:
        if len(current_hashes) == 0:
            simplified_operator_list.append((last_operator, sorted(current_indices)))
        else:
            groups[last_operator].add_mapping(current_hashes, current_indices)

    # Now that we know which chains happen to contain the same underlying expressions
    # and can be merged together, add in this info back to the output.
    for operator, disjoint_dict in groups.items():
        for keys, indices in disjoint_dict.items():
            simplified_operator_list.append((operator, sorted(indices)))

    # For stability, reorder list by the first operand index to appear
    simplified_operator_list.sort(key=lambda item: item[1][0])
    return simplified_operator_list


</t>
<t tx="ekr.20220525082933.591">def is_typed_callable(c: Optional[Type]) -&gt; bool:
    c = get_proper_type(c)
    if not c or not isinstance(c, CallableType):
        return False
    return not all(isinstance(t, AnyType) and t.type_of_any == TypeOfAny.unannotated
                   for t in get_proper_types(c.arg_types + [c.ret_type]))


</t>
<t tx="ekr.20220525082933.592">def is_untyped_decorator(typ: Optional[Type]) -&gt; bool:
    typ = get_proper_type(typ)
    if not typ:
        return True
    elif isinstance(typ, CallableType):
        return not is_typed_callable(typ)
    elif isinstance(typ, Instance):
        method = typ.type.get_method('__call__')
        if method:
            if isinstance(method, Decorator):
                return (
                    is_untyped_decorator(method.func.type)
                    or is_untyped_decorator(method.var.type)
                )

            if isinstance(method.type, Overloaded):
                return any(is_untyped_decorator(item) for item in method.type.items)
            else:
                return not is_typed_callable(method.type)
        else:
            return False
    elif isinstance(typ, Overloaded):
        return any(is_untyped_decorator(item) for item in typ.items)
    return True


</t>
<t tx="ekr.20220525082933.593">def is_static(func: Union[FuncBase, Decorator]) -&gt; bool:
    if isinstance(func, Decorator):
        return is_static(func.func)
    elif isinstance(func, FuncBase):
        return func.is_static
    assert False, f"Unexpected func type: {type(func)}"


</t>
<t tx="ekr.20220525082933.594">def is_subtype_no_promote(left: Type, right: Type) -&gt; bool:
    return is_subtype(left, right, ignore_promotions=True)


</t>
<t tx="ekr.20220525082933.595">def is_overlapping_types_no_promote(left: Type, right: Type) -&gt; bool:
    return is_overlapping_types(left, right, ignore_promotions=True)


</t>
<t tx="ekr.20220525082933.596">def is_private(node_name: str) -&gt; bool:
    """Check if node is private to class definition."""
    return node_name.startswith('__') and not node_name.endswith('__')


</t>
<t tx="ekr.20220525082933.597">def is_string_literal(typ: Type) -&gt; bool:
    strs = try_getting_str_literals_from_type(typ)
    return strs is not None and len(strs) == 1


</t>
<t tx="ekr.20220525082933.598">def has_bool_item(typ: ProperType) -&gt; bool:
    """Return True if type is 'bool' or a union with a 'bool' item."""
    if is_named_instance(typ, 'builtins.bool'):
        return True
    if isinstance(typ, UnionType):
        return any(is_named_instance(item, 'builtins.bool')
                   for item in typ.items)
    return False


</t>
<t tx="ekr.20220525082933.599">def collapse_walrus(e: Expression) -&gt; Expression:
    """If an expression is an AssignmentExpr, pull out the assignment target.

    We don't make any attempt to pull out all the targets in code like `x := (y := z)`.
    We could support narrowing those if that sort of code turns out to be common.
    """
    if isinstance(e, AssignmentExpr):
        return e.target
    return e
</t>
<t tx="ekr.20220525082933.6">def wait_background_cmd(name: str, proc: Popen) -&gt; int:
    output = proc.communicate()[0]
    status = proc.returncode
    print(f'run {name}: {cmds[name]}')
    if status:
        print(output.decode().rstrip())
        print('\nFAILED: %s' % name)
        if name in FAST_FAIL:
            exit(status)
    return status


</t>
<t tx="ekr.20220525082933.60">def main():
    verbose = ('-v' in sys.argv)
    for host in [plain_host_generator, plain_host_coroutine,
                 decorated_host_generator, decorated_host_coroutine]:
        print()
        print("==== Host:", host.__name__)
        for func in [plain_generator, plain_coroutine,
                     decorated_generator, decorated_coroutine,
                     other_iterator, other_coroutine]:
            print("  ---- Func:", func.__name__)
            try:
                f = host(func)
                for i in range(10):
                    try:
                        x = f.send(None)
                        if verbose:
                            print("    yield:", x)
                    except StopIteration as e:
                        if verbose:
                            print("    stop:", e.value)
                        break
                else:
                    if verbose:
                        print("    ???? still going")
            except Exception as e:
                print("    error:", repr(e))

</t>
<t tx="ekr.20220525082933.600">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Expression type checker. This file is conceptually part of TypeChecker."""

from mypy.backports import OrderedDict
from contextlib import contextmanager
import itertools
from typing import (
    cast, Dict, Set, List, Tuple, Callable, Union, Optional, Sequence, Iterator
)
from typing_extensions import ClassVar, Final, overload, TypeAlias as _TypeAlias

from mypy.errors import report_internal_error, ErrorWatcher
from mypy.typeanal import (
    has_any_from_unimported_type, check_for_explicit_any, set_any_tvars, expand_type_alias,
    make_optional_type,
)
from mypy.semanal_enum import ENUM_BASES
from mypy.types import (
    Type, AnyType, CallableType, Overloaded, NoneType, TypeVarType,
    TupleType, TypedDictType, Instance, ErasedType, UnionType,
    PartialType, DeletedType, UninhabitedType, TypeType, TypeOfAny, LiteralType, LiteralValue,
    is_named_instance, FunctionLike, ParamSpecType, ParamSpecFlavor,
    StarType, is_optional, remove_optional, is_generic_instance, get_proper_type, ProperType,
    get_proper_types, flatten_nested_unions, LITERAL_TYPE_NAMES,
)
from mypy.nodes import (
    AssertTypeExpr, NameExpr, RefExpr, Var, FuncDef, OverloadedFuncDef, TypeInfo, CallExpr,
    MemberExpr, IntExpr, StrExpr, BytesExpr, UnicodeExpr, FloatExpr,
    OpExpr, UnaryExpr, IndexExpr, CastExpr, RevealExpr, TypeApplication, ListExpr,
    TupleExpr, DictExpr, LambdaExpr, SuperExpr, SliceExpr, Context, Expression,
    ListComprehension, GeneratorExpr, SetExpr, MypyFile, Decorator,
    ConditionalExpr, ComparisonExpr, TempNode, SetComprehension, AssignmentExpr,
    DictionaryComprehension, ComplexExpr, EllipsisExpr, StarExpr, AwaitExpr, YieldExpr,
    YieldFromExpr, TypedDictExpr, PromoteExpr, NewTypeExpr, NamedTupleExpr, TypeVarExpr,
    TypeAliasExpr, BackquoteExpr, EnumCallExpr, TypeAlias, SymbolNode, PlaceholderNode,
    ParamSpecExpr, TypeVarTupleExpr,
    ArgKind, ARG_POS, ARG_NAMED, ARG_STAR, ARG_STAR2, LITERAL_TYPE, REVEAL_TYPE,
)
from mypy.literals import literal
from mypy import nodes
from mypy import operators
import mypy.checker
from mypy import types
from mypy.sametypes import is_same_type
from mypy.erasetype import replace_meta_vars, erase_type, remove_instance_last_known_values
from mypy.maptype import map_instance_to_supertype
from mypy.messages import MessageBuilder
from mypy import message_registry
from mypy.infer import (
    ArgumentInferContext, infer_type_arguments, infer_function_type_arguments,
)
from mypy import join
from mypy.meet import narrow_declared_type, is_overlapping_types
from mypy.subtypes import is_subtype, is_proper_subtype, is_equivalent, non_method_protocol_members
from mypy import applytype
from mypy import erasetype
from mypy.checkmember import analyze_member_access, type_object_type
from mypy.argmap import ArgTypeExpander, map_actuals_to_formals, map_formals_to_actuals
from mypy.checkstrformat import StringFormatterChecker
from mypy.expandtype import expand_type, expand_type_by_instance, freshen_function_type_vars
from mypy.util import split_module_names
from mypy.typevars import fill_typevars
from mypy.visitor import ExpressionVisitor
from mypy.plugin import (
    Plugin,
    MethodContext, MethodSigContext,
    FunctionContext, FunctionSigContext,
)
from mypy.typeops import (
    try_expanding_sum_type_to_union, tuple_fallback, make_simplified_union,
    true_only, false_only, erase_to_union_or_bound, function_type,
    callable_type, try_getting_str_literals, custom_special_method,
    is_literal_type_like, simple_literal_type,
)
from mypy.message_registry import ErrorMessage
import mypy.errorcodes as codes

# Type of callback user for checking individual function arguments. See
# check_args() below for details.
ArgChecker: _TypeAlias = Callable[[
        Type,
        Type,
        ArgKind,
        Type,
        int,
        int,
        CallableType,
        Optional[Type],
        Context,
        Context,
    ],
    None,
]

# Maximum nesting level for math union in overloads, setting this to large values
# may cause performance issues. The reason is that although union math algorithm we use
# nicely captures most corner cases, its worst case complexity is exponential,
# see https://github.com/python/mypy/pull/5255#discussion_r196896335 for discussion.
MAX_UNIONS: Final = 5


# Types considered safe for comparisons with --strict-equality due to known behaviour of __eq__.
# NOTE: All these types are subtypes of AbstractSet.
OVERLAPPING_TYPES_ALLOWLIST: Final = [
    "builtins.set",
    "builtins.frozenset",
    "typing.KeysView",
    "typing.ItemsView",
    "builtins._dict_keys",
    "builtins._dict_items",
    "_collections_abc.dict_keys",
    "_collections_abc.dict_items",
]


@others
</t>
<t tx="ekr.20220525082933.601">class TooManyUnions(Exception):
    """Indicates that we need to stop splitting unions in an attempt
    to match an overload in order to save performance.
    """


</t>
<t tx="ekr.20220525082933.602">def allow_fast_container_literal(t: ProperType) -&gt; bool:
    return (
        isinstance(t, Instance)
        or (
                isinstance(t, TupleType)
                and all(allow_fast_container_literal(get_proper_type(it)) for it in t.items)
        )
    )


</t>
<t tx="ekr.20220525082933.603">def extract_refexpr_names(expr: RefExpr) -&gt; Set[str]:
    """Recursively extracts all module references from a reference expression.

    Note that currently, the only two subclasses of RefExpr are NameExpr and
    MemberExpr."""
    output: Set[str] = set()
    while isinstance(expr.node, MypyFile) or expr.fullname is not None:
        if isinstance(expr.node, MypyFile) and expr.fullname is not None:
            # If it's None, something's wrong (perhaps due to an
            # import cycle or a suppressed error).  For now we just
            # skip it.
            output.add(expr.fullname)

        if isinstance(expr, NameExpr):
            is_suppressed_import = isinstance(expr.node, Var) and expr.node.is_suppressed_import
            if isinstance(expr.node, TypeInfo):
                # Reference to a class or a nested class
                output.update(split_module_names(expr.node.module_name))
            elif expr.fullname is not None and '.' in expr.fullname and not is_suppressed_import:
                # Everything else (that is not a silenced import within a class)
                output.add(expr.fullname.rsplit('.', 1)[0])
            break
        elif isinstance(expr, MemberExpr):
            if isinstance(expr.expr, RefExpr):
                expr = expr.expr
            else:
                break
        else:
            raise AssertionError(f"Unknown RefExpr subclass: {type(expr)}")
    return output


</t>
<t tx="ekr.20220525082933.604">class Finished(Exception):
    """Raised if we can terminate overload argument check early (no match)."""


</t>
<t tx="ekr.20220525082933.605">class ExpressionChecker(ExpressionVisitor[Type]):
    """Expression type checker.

    This class works closely together with checker.TypeChecker.
    """

    # Some services are provided by a TypeChecker instance.
    chk: "mypy.checker.TypeChecker"
    # This is shared with TypeChecker, but stored also here for convenience.
    msg: MessageBuilder
    # Type context for type inference
    type_context: List[Optional[Type]]

    # cache resolved types in some cases
    resolved_type: Dict[Expression, ProperType]

    strfrm_checker: StringFormatterChecker
    plugin: Plugin

    @others
</t>
<t tx="ekr.20220525082933.606">def __init__(self,
             chk: 'mypy.checker.TypeChecker',
             msg: MessageBuilder,
             plugin: Plugin) -&gt; None:
    """Construct an expression type checker."""
    self.chk = chk
    self.msg = msg
    self.plugin = plugin
    self.type_context = [None]

    # Temporary overrides for expression types. This is currently
    # used by the union math in overloads.
    # TODO: refactor this to use a pattern similar to one in
    # multiassign_from_union, or maybe even combine the two?
    self.type_overrides: Dict[Expression, Type] = {}
    self.strfrm_checker = StringFormatterChecker(self, self.chk, self.msg)

    self.resolved_type = {}

</t>
<t tx="ekr.20220525082933.607">def reset(self) -&gt; None:
    self.resolved_type = {}

</t>
<t tx="ekr.20220525082933.608">def visit_name_expr(self, e: NameExpr) -&gt; Type:
    """Type check a name expression.

    It can be of any kind: local, member or global.
    """
    self.chk.module_refs.update(extract_refexpr_names(e))
    result = self.analyze_ref_expr(e)
    return self.narrow_type_from_binder(e, result)

</t>
<t tx="ekr.20220525082933.609">def analyze_ref_expr(self, e: RefExpr, lvalue: bool = False) -&gt; Type:
    result: Optional[Type] = None
    node = e.node

    if isinstance(e, NameExpr) and e.is_special_form:
        # A special form definition, nothing to check here.
        return AnyType(TypeOfAny.special_form)

    if isinstance(node, Var):
        # Variable reference.
        result = self.analyze_var_ref(node, e)
        if isinstance(result, PartialType):
            result = self.chk.handle_partial_var_type(result, lvalue, node, e)
    elif isinstance(node, FuncDef):
        # Reference to a global function.
        result = function_type(node, self.named_type('builtins.function'))
    elif isinstance(node, OverloadedFuncDef) and node.type is not None:
        # node.type is None when there are multiple definitions of a function
        # and it's decorated by something that is not typing.overload
        # TODO: use a dummy Overloaded instead of AnyType in this case
        # like we do in mypy.types.function_type()?
        result = node.type
    elif isinstance(node, TypeInfo):
        # Reference to a type object.
        result = type_object_type(node, self.named_type)
        if (isinstance(result, CallableType) and
                isinstance(result.ret_type, Instance)):  # type: ignore
            # We need to set correct line and column
            # TODO: always do this in type_object_type by passing the original context
            result.ret_type.line = e.line
            result.ret_type.column = e.column
        if isinstance(get_proper_type(self.type_context[-1]), TypeType):
            # This is the type in a Type[] expression, so substitute type
            # variables with Any.
            result = erasetype.erase_typevars(result)
    elif isinstance(node, MypyFile):
        # Reference to a module object.
        try:
            result = self.named_type('types.ModuleType')
        except KeyError:
            # In test cases might 'types' may not be available.
            # Fall back to a dummy 'object' type instead to
            # avoid a crash.
            result = self.named_type('builtins.object')
    elif isinstance(node, Decorator):
        result = self.analyze_var_ref(node.var, e)
    elif isinstance(node, TypeAlias):
        # Something that refers to a type alias appears in runtime context.
        # Note that we suppress bogus errors for alias redefinitions,
        # they are already reported in semanal.py.
        result = self.alias_type_in_runtime_context(node, node.no_args, e,
                                                    alias_definition=e.is_alias_rvalue
                                                    or lvalue)
    elif isinstance(node, (TypeVarExpr, ParamSpecExpr)):
        result = self.object_type()
    else:
        if isinstance(node, PlaceholderNode):
            assert False, f'PlaceholderNode {node.fullname!r} leaked to checker'
        # Unknown reference; use any type implicitly to avoid
        # generating extra type errors.
        result = AnyType(TypeOfAny.from_error)
    assert result is not None
    return result

</t>
<t tx="ekr.20220525082933.61">@path C:/Repos/mypy/misc/
@language python
@tabwidth -4
"""Script to build compiled binary wheels that can be uploaded to PyPI.

The main GitHub workflow where this script is used:
https://github.com/mypyc/mypy_mypyc-wheels/blob/master/.github/workflows/build.yml

This uses cibuildwheel (https://github.com/pypa/cibuildwheel) to build the wheels.

Usage:

  build_wheel.py --python-version &lt;python-version&gt; --output-dir &lt;dir&gt;

Wheels for the given Python version will be created in the given directory.
Python version is in form "39".

This works on macOS, Windows and Linux.

You can test locally by using --extra-opts. macOS example:

  mypy/misc/build_wheel.py --python-version 39 --output-dir out --extra-opts="--platform macos"
"""

import argparse
import os
import subprocess
from typing import Dict

# Clang package we use on Linux
LLVM_URL = 'https://github.com/mypyc/mypy_mypyc-wheels/releases/download/llvm/llvm-centos-5.tar.gz'

# Mypy repository root
ROOT_DIR = os.path.dirname(os.path.dirname(__file__))


@others
if __name__ == '__main__':
    main()
</t>
<t tx="ekr.20220525082933.610">def analyze_var_ref(self, var: Var, context: Context) -&gt; Type:
    if var.type:
        var_type = get_proper_type(var.type)
        if isinstance(var_type, Instance):
            if self.is_literal_context() and var_type.last_known_value is not None:
                return var_type.last_known_value
            if var.name in {'True', 'False'}:
                return self.infer_literal_expr_type(var.name == 'True', 'builtins.bool')
        return var.type
    else:
        if not var.is_ready and self.chk.in_checked_function():
            self.chk.handle_cannot_determine_type(var.name, context)
        # Implicit 'Any' type.
        return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082933.611">def visit_call_expr(self, e: CallExpr, allow_none_return: bool = False) -&gt; Type:
    """Type check a call expression."""
    if e.analyzed:
        if isinstance(e.analyzed, NamedTupleExpr) and not e.analyzed.is_typed:
            # Type check the arguments, but ignore the results. This relies
            # on the typeshed stubs to type check the arguments.
            self.visit_call_expr_inner(e)
        # It's really a special form that only looks like a call.
        return self.accept(e.analyzed, self.type_context[-1])
    return self.visit_call_expr_inner(e, allow_none_return=allow_none_return)

</t>
<t tx="ekr.20220525082933.612">def visit_call_expr_inner(self, e: CallExpr, allow_none_return: bool = False) -&gt; Type:
    if isinstance(e.callee, RefExpr) and isinstance(e.callee.node, TypeInfo) and \
            e.callee.node.typeddict_type is not None:
        # Use named fallback for better error messages.
        typeddict_type = e.callee.node.typeddict_type.copy_modified(
            fallback=Instance(e.callee.node, []))
        return self.check_typeddict_call(typeddict_type, e.arg_kinds, e.arg_names, e.args, e)
    if (isinstance(e.callee, NameExpr) and e.callee.name in ('isinstance', 'issubclass')
            and len(e.args) == 2):
        for typ in mypy.checker.flatten(e.args[1]):
            node = None
            if isinstance(typ, NameExpr):
                try:
                    node = self.chk.lookup_qualified(typ.name)
                except KeyError:
                    # Undefined names should already be reported in semantic analysis.
                    pass
            if is_expr_literal_type(typ):
                self.msg.cannot_use_function_with_type(e.callee.name, "Literal", e)
                continue
            if (node and isinstance(node.node, TypeAlias)
                    and isinstance(get_proper_type(node.node.target), AnyType)):
                self.msg.cannot_use_function_with_type(e.callee.name, "Any", e)
                continue
            if ((isinstance(typ, IndexExpr)
                    and isinstance(typ.analyzed, (TypeApplication, TypeAliasExpr)))
                    or (isinstance(typ, NameExpr) and node and
                        isinstance(node.node, TypeAlias) and not node.node.no_args)):
                self.msg.type_arguments_not_allowed(e)
            if isinstance(typ, RefExpr) and isinstance(typ.node, TypeInfo):
                if typ.node.typeddict_type:
                    self.msg.cannot_use_function_with_type(e.callee.name, "TypedDict", e)
                elif typ.node.is_newtype:
                    self.msg.cannot_use_function_with_type(e.callee.name, "NewType", e)
    self.try_infer_partial_type(e)
    type_context = None
    if isinstance(e.callee, LambdaExpr):
        formal_to_actual = map_actuals_to_formals(
            e.arg_kinds, e.arg_names,
            e.callee.arg_kinds, e.callee.arg_names,
            lambda i: self.accept(e.args[i]))

        arg_types = [join.join_type_list([self.accept(e.args[j]) for j in formal_to_actual[i]])
                     for i in range(len(e.callee.arg_kinds))]
        type_context = CallableType(arg_types, e.callee.arg_kinds, e.callee.arg_names,
                                    ret_type=self.object_type(),
                                    fallback=self.named_type('builtins.function'))
    callee_type = get_proper_type(self.accept(e.callee, type_context, always_allow_any=True))
    if (self.chk.options.disallow_untyped_calls and
            self.chk.in_checked_function() and
            isinstance(callee_type, CallableType)
            and callee_type.implicit):
        self.msg.untyped_function_call(callee_type, e)

    # Figure out the full name of the callee for plugin lookup.
    object_type = None
    member = None
    fullname = None
    if isinstance(e.callee, RefExpr):
        # There are two special cases where plugins might act:
        # * A "static" reference/alias to a class or function;
        #   get_function_hook() will be invoked for these.
        fullname = e.callee.fullname
        if isinstance(e.callee.node, TypeAlias):
            target = get_proper_type(e.callee.node.target)
            if isinstance(target, Instance):
                fullname = target.type.fullname
        # * Call to a method on object that has a full name (see
        #   method_fullname() for details on supported objects);
        #   get_method_hook() and get_method_signature_hook() will
        #   be invoked for these.
        if (fullname is None
                and isinstance(e.callee, MemberExpr)
                and self.chk.has_type(e.callee.expr)):
            member = e.callee.name
            object_type = self.chk.lookup_type(e.callee.expr)
    ret_type = self.check_call_expr_with_callee_type(callee_type, e, fullname,
                                                     object_type, member)
    if isinstance(e.callee, RefExpr) and len(e.args) == 2:
        if e.callee.fullname in ('builtins.isinstance', 'builtins.issubclass'):
            self.check_runtime_protocol_test(e)
        if e.callee.fullname == 'builtins.issubclass':
            self.check_protocol_issubclass(e)
    if isinstance(e.callee, MemberExpr) and e.callee.name == 'format':
        self.check_str_format_call(e)
    ret_type = get_proper_type(ret_type)
    if isinstance(ret_type, UnionType):
        ret_type = make_simplified_union(ret_type.items)
    if isinstance(ret_type, UninhabitedType) and not ret_type.ambiguous:
        self.chk.binder.unreachable()
    # Warn on calls to functions that always return None. The check
    # of ret_type is both a common-case optimization and prevents reporting
    # the error in dynamic functions (where it will be Any).
    if (not allow_none_return and isinstance(ret_type, NoneType)
            and self.always_returns_none(e.callee)):
        self.chk.msg.does_not_return_value(callee_type, e)
        return AnyType(TypeOfAny.from_error)
    return ret_type

</t>
<t tx="ekr.20220525082933.613">def check_str_format_call(self, e: CallExpr) -&gt; None:
    """More precise type checking for str.format() calls on literals."""
    assert isinstance(e.callee, MemberExpr)
    format_value = None
    if isinstance(e.callee.expr, (StrExpr, UnicodeExpr)):
        format_value = e.callee.expr.value
    elif self.chk.has_type(e.callee.expr):
        base_typ = try_getting_literal(self.chk.lookup_type(e.callee.expr))
        if isinstance(base_typ, LiteralType) and isinstance(base_typ.value, str):
            format_value = base_typ.value
    if format_value is not None:
        self.strfrm_checker.check_str_format_call(e, format_value)

</t>
<t tx="ekr.20220525082933.614">def method_fullname(self, object_type: Type, method_name: str) -&gt; Optional[str]:
    """Convert a method name to a fully qualified name, based on the type of the object that
    it is invoked on. Return `None` if the name of `object_type` cannot be determined.
    """
    object_type = get_proper_type(object_type)

    if isinstance(object_type, CallableType) and object_type.is_type_obj():
        # For class method calls, object_type is a callable representing the class object.
        # We "unwrap" it to a regular type, as the class/instance method difference doesn't
        # affect the fully qualified name.
        object_type = get_proper_type(object_type.ret_type)
    elif isinstance(object_type, TypeType):
        object_type = object_type.item

    type_name = None
    if isinstance(object_type, Instance):
        type_name = object_type.type.fullname
    elif isinstance(object_type, (TypedDictType, LiteralType)):
        info = object_type.fallback.type.get_containing_type_info(method_name)
        type_name = info.fullname if info is not None else None
    elif isinstance(object_type, TupleType):
        type_name = tuple_fallback(object_type).type.fullname

    if type_name is not None:
        return f'{type_name}.{method_name}'
    else:
        return None

</t>
<t tx="ekr.20220525082933.615">def always_returns_none(self, node: Expression) -&gt; bool:
    """Check if `node` refers to something explicitly annotated as only returning None."""
    if isinstance(node, RefExpr):
        if self.defn_returns_none(node.node):
            return True
    if isinstance(node, MemberExpr) and node.node is None:  # instance or class attribute
        typ = get_proper_type(self.chk.lookup_type(node.expr))
        if isinstance(typ, Instance):
            info = typ.type
        elif isinstance(typ, CallableType) and typ.is_type_obj():
            ret_type = get_proper_type(typ.ret_type)
            if isinstance(ret_type, Instance):
                info = ret_type.type
            else:
                return False
        else:
            return False
        sym = info.get(node.name)
        if sym and self.defn_returns_none(sym.node):
            return True
    return False

</t>
<t tx="ekr.20220525082933.616">def defn_returns_none(self, defn: Optional[SymbolNode]) -&gt; bool:
    """Check if `defn` can _only_ return None."""
    if isinstance(defn, FuncDef):
        return (isinstance(defn.type, CallableType) and
                isinstance(get_proper_type(defn.type.ret_type), NoneType))
    if isinstance(defn, OverloadedFuncDef):
        return all(self.defn_returns_none(item) for item in defn.items)
    if isinstance(defn, Var):
        typ = get_proper_type(defn.type)
        if (not defn.is_inferred and isinstance(typ, CallableType) and
                isinstance(get_proper_type(typ.ret_type), NoneType)):
            return True
        if isinstance(typ, Instance):
            sym = typ.type.get('__call__')
            if sym and self.defn_returns_none(sym.node):
                return True
    return False

</t>
<t tx="ekr.20220525082933.617">def check_runtime_protocol_test(self, e: CallExpr) -&gt; None:
    for expr in mypy.checker.flatten(e.args[1]):
        tp = get_proper_type(self.chk.lookup_type(expr))
        if (isinstance(tp, CallableType) and tp.is_type_obj() and
                tp.type_object().is_protocol and
                not tp.type_object().runtime_protocol):
            self.chk.fail(message_registry.RUNTIME_PROTOCOL_EXPECTED, e)

</t>
<t tx="ekr.20220525082933.618">def check_protocol_issubclass(self, e: CallExpr) -&gt; None:
    for expr in mypy.checker.flatten(e.args[1]):
        tp = get_proper_type(self.chk.lookup_type(expr))
        if (isinstance(tp, CallableType) and tp.is_type_obj() and
                tp.type_object().is_protocol):
            attr_members = non_method_protocol_members(tp.type_object())
            if attr_members:
                self.chk.msg.report_non_method_protocol(tp.type_object(),
                                                        attr_members, e)

</t>
<t tx="ekr.20220525082933.619">def check_typeddict_call(self, callee: TypedDictType,
                         arg_kinds: List[ArgKind],
                         arg_names: Sequence[Optional[str]],
                         args: List[Expression],
                         context: Context) -&gt; Type:
    if len(args) &gt;= 1 and all([ak == ARG_NAMED for ak in arg_kinds]):
        # ex: Point(x=42, y=1337)
        assert all(arg_name is not None for arg_name in arg_names)
        item_names = cast(List[str], arg_names)
        item_args = args
        return self.check_typeddict_call_with_kwargs(
            callee, OrderedDict(zip(item_names, item_args)), context)

    if len(args) == 1 and arg_kinds[0] == ARG_POS:
        unique_arg = args[0]
        if isinstance(unique_arg, DictExpr):
            # ex: Point({'x': 42, 'y': 1337})
            return self.check_typeddict_call_with_dict(callee, unique_arg, context)
        if isinstance(unique_arg, CallExpr) and isinstance(unique_arg.analyzed, DictExpr):
            # ex: Point(dict(x=42, y=1337))
            return self.check_typeddict_call_with_dict(callee, unique_arg.analyzed, context)

    if len(args) == 0:
        # ex: EmptyDict()
        return self.check_typeddict_call_with_kwargs(
            callee, OrderedDict(), context)

    self.chk.fail(message_registry.INVALID_TYPEDDICT_ARGS, context)
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20220525082933.62">def create_environ(python_version: str) -&gt; Dict[str, str]:
    """Set up environment variables for cibuildwheel."""
    env = os.environ.copy()

    env['CIBW_BUILD'] = f"cp{python_version}-*"

    # Don't build 32-bit wheels
    env['CIBW_SKIP'] = "*-manylinux_i686 *-win32"

    # Apple Silicon support
    # When cross-compiling on Intel, it is not possible to test arm64 and
    # the arm64 part of a universal2 wheel. Warnings will be silenced with
    # following CIBW_TEST_SKIP
    env['CIBW_ARCHS_MACOS'] = "x86_64 arm64 universal2"
    env['CIBW_TEST_SKIP'] = "*-macosx_arm64 *_universal2:arm64"

    env['CIBW_BUILD_VERBOSITY'] = '1'

    # mypy's isolated builds don't specify the requirements mypyc needs, so install
    # requirements and don't use isolated builds. we need to use build-requirements.txt
    # with recent mypy commits to get stub packages needed for compilation.
    env['CIBW_BEFORE_BUILD'] = """
      pip install -r {package}/build-requirements.txt
    """.replace('\n', ' ')

    # download a copy of clang to use to compile on linux. this was probably built in 2018,
    # speeds up compilation 2x
    env['CIBW_BEFORE_BUILD_LINUX'] = """
      (cd / &amp;&amp; curl -L %s | tar xzf -) &amp;&amp;
      pip install -r {package}/build-requirements.txt
    """.replace('\n', ' ') % LLVM_URL

    # the double negative is counterintuitive, https://github.com/pypa/pip/issues/5735
    env['CIBW_ENVIRONMENT'] = 'MYPY_USE_MYPYC=1 MYPYC_OPT_LEVEL=3 PIP_NO_BUILD_ISOLATION=no'
    env['CIBW_ENVIRONMENT_LINUX'] = (
        'MYPY_USE_MYPYC=1 MYPYC_OPT_LEVEL=3 PIP_NO_BUILD_ISOLATION=no ' +
        'CC=/opt/llvm/bin/clang'
    )
    env['CIBW_ENVIRONMENT_WINDOWS'] = (
        'MYPY_USE_MYPYC=1 MYPYC_OPT_LEVEL=2 PIP_NO_BUILD_ISOLATION=no'
    )

    # lxml doesn't have a wheel for Python 3.10 on the manylinux image we use.
    # lxml has historically been slow to support new Pythons as well.
    env['CIBW_BEFORE_TEST'] = """
      (
      grep -v lxml {project}/mypy/test-requirements.txt &gt; /tmp/test-requirements.txt
      &amp;&amp; cp {project}/mypy/mypy-requirements.txt /tmp/mypy-requirements.txt
      &amp;&amp; cp {project}/mypy/build-requirements.txt /tmp/build-requirements.txt
      &amp;&amp; pip install -r /tmp/test-requirements.txt
      )
    """.replace('\n', ' ')
    # lxml currently has wheels on Windows and doesn't have grep, so special case
    env['CIBW_BEFORE_TEST_WINDOWS'] = "pip install -r {project}/mypy/test-requirements.txt"

    # pytest looks for configuration files in the parent directories of where the tests live.
    # since we are trying to run the tests from their installed location, we copy those into
    # the venv. Ew ew ew.
    # We don't run tests that need lxml since we don't install lxml
    # We don't run external mypyc tests since there's some issue with compilation on the
    # manylinux image we use.
    env['CIBW_TEST_COMMAND'] = """
      (
      DIR=$(python -c 'import mypy, os; dn = os.path.dirname; print(dn(dn(mypy.__path__[0])))')
      &amp;&amp; cp '{project}/mypy/pytest.ini' '{project}/mypy/conftest.py' $DIR

      &amp;&amp; MYPY_TEST_DIR=$(python -c 'import mypy.test; print(mypy.test.__path__[0])')
      &amp;&amp; MYPY_TEST_PREFIX='{project}/mypy' pytest $MYPY_TEST_DIR -k 'not (reports.test or testreports)'

      &amp;&amp; MYPYC_TEST_DIR=$(python -c 'import mypyc.test; print(mypyc.test.__path__[0])')
      &amp;&amp; MYPY_TEST_PREFIX='{project}/mypy' pytest $MYPYC_TEST_DIR -k 'not test_external'
      )
    """.replace('\n', ' ')

    # i ran into some flaky tests on windows, so only run testcheck. it looks like we
    # previously didn't run any tests on windows wheels, so this is a net win.
    env['CIBW_TEST_COMMAND_WINDOWS'] = """
      bash -c "
      (
      DIR=$(python -c 'import mypy, os; dn = os.path.dirname; print(dn(dn(mypy.__path__[0])))')
      &amp;&amp; TEST_DIR=$(python -c 'import mypy.test; print(mypy.test.__path__[0])')
      &amp;&amp; cp '{project}/mypy/pytest.ini' '{project}/mypy/conftest.py' $DIR
      &amp;&amp; MYPY_TEST_PREFIX='{project}/mypy' pytest $TEST_DIR/testcheck.py
      )
      "
    """.replace('\n', ' ')
    return env


</t>
<t tx="ekr.20220525082933.620">def validate_typeddict_kwargs(
        self, kwargs: DictExpr) -&gt; 'Optional[OrderedDict[str, Expression]]':
    item_args = [item[1] for item in kwargs.items]

    item_names = []  # List[str]
    for item_name_expr, item_arg in kwargs.items:
        literal_value = None
        if item_name_expr:
            key_type = self.accept(item_name_expr)
            values = try_getting_str_literals(item_name_expr, key_type)
            if values and len(values) == 1:
                literal_value = values[0]
        if literal_value is None:
            key_context = item_name_expr or item_arg
            self.chk.fail(message_registry.TYPEDDICT_KEY_MUST_BE_STRING_LITERAL,
                          key_context)
            return None
        else:
            item_names.append(literal_value)
    return OrderedDict(zip(item_names, item_args))

</t>
<t tx="ekr.20220525082933.621">def match_typeddict_call_with_dict(self, callee: TypedDictType,
                                   kwargs: DictExpr,
                                   context: Context) -&gt; bool:
    validated_kwargs = self.validate_typeddict_kwargs(kwargs=kwargs)
    if validated_kwargs is not None:
        return (callee.required_keys &lt;= set(validated_kwargs.keys())
            &lt;= set(callee.items.keys()))
    else:
        return False

</t>
<t tx="ekr.20220525082933.622">def check_typeddict_call_with_dict(self, callee: TypedDictType,
                                   kwargs: DictExpr,
                                   context: Context) -&gt; Type:
    validated_kwargs = self.validate_typeddict_kwargs(kwargs=kwargs)
    if validated_kwargs is not None:
        return self.check_typeddict_call_with_kwargs(
            callee,
            kwargs=validated_kwargs,
            context=context)
    else:
        return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20220525082933.623">def check_typeddict_call_with_kwargs(self, callee: TypedDictType,
                                     kwargs: 'OrderedDict[str, Expression]',
                                     context: Context) -&gt; Type:
    if not (callee.required_keys &lt;= set(kwargs.keys()) &lt;= set(callee.items.keys())):
        expected_keys = [key for key in callee.items.keys()
                         if key in callee.required_keys or key in kwargs.keys()]
        actual_keys = kwargs.keys()
        self.msg.unexpected_typeddict_keys(
            callee,
            expected_keys=expected_keys,
            actual_keys=list(actual_keys),
            context=context)
        return AnyType(TypeOfAny.from_error)

    for (item_name, item_expected_type) in callee.items.items():
        if item_name in kwargs:
            item_value = kwargs[item_name]
            self.chk.check_simple_assignment(
                lvalue_type=item_expected_type, rvalue=item_value, context=item_value,
                msg=message_registry.INCOMPATIBLE_TYPES,
                lvalue_name=f'TypedDict item "{item_name}"',
                rvalue_name='expression',
                code=codes.TYPEDDICT_ITEM)

    return callee

</t>
<t tx="ekr.20220525082933.624">def get_partial_self_var(self, expr: MemberExpr) -&gt; Optional[Var]:
    """Get variable node for a partial self attribute.

    If the expression is not a self attribute, or attribute is not variable,
    or variable is not partial, return None.
    """
    if not (isinstance(expr.expr, NameExpr) and
            isinstance(expr.expr.node, Var) and expr.expr.node.is_self):
        # Not a self.attr expression.
        return None
    info = self.chk.scope.enclosing_class()
    if not info or expr.name not in info.names:
        # Don't mess with partial types in superclasses.
        return None
    sym = info.names[expr.name]
    if isinstance(sym.node, Var) and isinstance(sym.node.type, PartialType):
        return sym.node
    return None

</t>
<t tx="ekr.20220525082933.625"># Types and methods that can be used to infer partial types.
item_args: ClassVar[Dict[str, List[str]]] = {
    "builtins.list": ["append"],
    "builtins.set": ["add", "discard"],
}
container_args: ClassVar[Dict[str, Dict[str, List[str]]]] = {
    "builtins.list": {"extend": ["builtins.list"]},
    "builtins.dict": {"update": ["builtins.dict"]},
    "collections.OrderedDict": {"update": ["builtins.dict"]},
    "builtins.set": {"update": ["builtins.set", "builtins.list"]},
}

</t>
<t tx="ekr.20220525082933.626">def try_infer_partial_type(self, e: CallExpr) -&gt; None:
    """Try to make partial type precise from a call."""
    if not isinstance(e.callee, MemberExpr):
        return
    callee = e.callee
    if isinstance(callee.expr, RefExpr):
        # Call a method with a RefExpr callee, such as 'x.method(...)'.
        ret = self.get_partial_var(callee.expr)
        if ret is None:
            return
        var, partial_types = ret
        typ = self.try_infer_partial_value_type_from_call(e, callee.name, var)
        if typ is not None:
            var.type = typ
            del partial_types[var]
    elif isinstance(callee.expr, IndexExpr) and isinstance(callee.expr.base, RefExpr):
        # Call 'x[y].method(...)'; may infer type of 'x' if it's a partial defaultdict.
        if callee.expr.analyzed is not None:
            return  # A special form
        base = callee.expr.base
        index = callee.expr.index
        ret = self.get_partial_var(base)
        if ret is None:
            return
        var, partial_types = ret
        partial_type = get_partial_instance_type(var.type)
        if partial_type is None or partial_type.value_type is None:
            return
        value_type = self.try_infer_partial_value_type_from_call(e, callee.name, var)
        if value_type is not None:
            # Infer key type.
            key_type = self.accept(index)
            if mypy.checker.is_valid_inferred_type(key_type):
                # Store inferred partial type.
                assert partial_type.type is not None
                typename = partial_type.type.fullname
                var.type = self.chk.named_generic_type(typename,
                                                       [key_type, value_type])
                del partial_types[var]

</t>
<t tx="ekr.20220525082933.627">def get_partial_var(self, ref: RefExpr) -&gt; Optional[Tuple[Var, Dict[Var, Context]]]:
    var = ref.node
    if var is None and isinstance(ref, MemberExpr):
        var = self.get_partial_self_var(ref)
    if not isinstance(var, Var):
        return None
    partial_types = self.chk.find_partial_types(var)
    if partial_types is None:
        return None
    return var, partial_types

</t>
<t tx="ekr.20220525082933.628">def try_infer_partial_value_type_from_call(
        self,
        e: CallExpr,
        methodname: str,
        var: Var) -&gt; Optional[Instance]:
    """Try to make partial type precise from a call such as 'x.append(y)'."""
    if self.chk.current_node_deferred:
        return None
    partial_type = get_partial_instance_type(var.type)
    if partial_type is None:
        return None
    if partial_type.value_type:
        typename = partial_type.value_type.type.fullname
    else:
        assert partial_type.type is not None
        typename = partial_type.type.fullname
    # Sometimes we can infer a full type for a partial List, Dict or Set type.
    # TODO: Don't infer argument expression twice.
    if (typename in self.item_args and methodname in self.item_args[typename]
            and e.arg_kinds == [ARG_POS]):
        item_type = self.accept(e.args[0])
        if mypy.checker.is_valid_inferred_type(item_type):
            return self.chk.named_generic_type(typename, [item_type])
    elif (typename in self.container_args
          and methodname in self.container_args[typename]
          and e.arg_kinds == [ARG_POS]):
        arg_type = get_proper_type(self.accept(e.args[0]))
        if isinstance(arg_type, Instance):
            arg_typename = arg_type.type.fullname
            if arg_typename in self.container_args[typename][methodname]:
                if all(mypy.checker.is_valid_inferred_type(item_type)
                       for item_type in arg_type.args):
                    return self.chk.named_generic_type(typename,
                                                       list(arg_type.args))
        elif isinstance(arg_type, AnyType):
            return self.chk.named_type(typename)

    return None

</t>
<t tx="ekr.20220525082933.629">def apply_function_plugin(self,
                          callee: CallableType,
                          arg_kinds: List[ArgKind],
                          arg_types: List[Type],
                          arg_names: Optional[Sequence[Optional[str]]],
                          formal_to_actual: List[List[int]],
                          args: List[Expression],
                          fullname: str,
                          object_type: Optional[Type],
                          context: Context) -&gt; Type:
    """Use special case logic to infer the return type of a specific named function/method.

    Caller must ensure that a plugin hook exists. There are two different cases:

    - If object_type is None, the caller must ensure that a function hook exists
      for fullname.
    - If object_type is not None, the caller must ensure that a method hook exists
      for fullname.

    Return the inferred return type.
    """
    num_formals = len(callee.arg_types)
    formal_arg_types: List[List[Type]] = [[] for _ in range(num_formals)]
    formal_arg_exprs: List[List[Expression]] = [[] for _ in range(num_formals)]
    formal_arg_names: List[List[Optional[str]]] = [[] for _ in range(num_formals)]
    formal_arg_kinds: List[List[ArgKind]] = [[] for _ in range(num_formals)]
    for formal, actuals in enumerate(formal_to_actual):
        for actual in actuals:
            formal_arg_types[formal].append(arg_types[actual])
            formal_arg_exprs[formal].append(args[actual])
            if arg_names:
                formal_arg_names[formal].append(arg_names[actual])
            formal_arg_kinds[formal].append(arg_kinds[actual])

    if object_type is None:
        # Apply function plugin
        callback = self.plugin.get_function_hook(fullname)
        assert callback is not None  # Assume that caller ensures this
        return callback(
            FunctionContext(formal_arg_types, formal_arg_kinds,
                            callee.arg_names, formal_arg_names,
                            callee.ret_type, formal_arg_exprs, context, self.chk))
    else:
        # Apply method plugin
        method_callback = self.plugin.get_method_hook(fullname)
        assert method_callback is not None  # Assume that caller ensures this
        object_type = get_proper_type(object_type)
        return method_callback(
            MethodContext(object_type, formal_arg_types, formal_arg_kinds,
                          callee.arg_names, formal_arg_names,
                          callee.ret_type, formal_arg_exprs, context, self.chk))

</t>
<t tx="ekr.20220525082933.63">def main() -&gt; None:
    parser = argparse.ArgumentParser()
    parser.add_argument('--python-version', required=True, metavar='XY',
                        help='Python version (e.g. 38 or 39)')
    parser.add_argument('--output-dir', required=True, metavar='DIR',
                        help='Output directory for created wheels')
    parser.add_argument('--extra-opts', default='', metavar='OPTIONS',
                        help='Extra options passed to cibuildwheel verbatim')
    args = parser.parse_args()
    python_version = args.python_version
    output_dir = args.output_dir
    extra_opts = args.extra_opts
    environ = create_environ(python_version)
    script = f'python -m cibuildwheel {extra_opts} --output-dir {output_dir} {ROOT_DIR}'
    subprocess.check_call(script, shell=True, env=environ)


</t>
<t tx="ekr.20220525082933.630">def apply_signature_hook(
        self, callee: FunctionLike, args: List[Expression],
        arg_kinds: List[ArgKind],
        arg_names: Optional[Sequence[Optional[str]]],
        hook: Callable[
            [List[List[Expression]], CallableType],
            FunctionLike,
        ]) -&gt; FunctionLike:
    """Helper to apply a signature hook for either a function or method"""
    if isinstance(callee, CallableType):
        num_formals = len(callee.arg_kinds)
        formal_to_actual = map_actuals_to_formals(
            arg_kinds, arg_names,
            callee.arg_kinds, callee.arg_names,
            lambda i: self.accept(args[i]))
        formal_arg_exprs: List[List[Expression]] = [[] for _ in range(num_formals)]
        for formal, actuals in enumerate(formal_to_actual):
            for actual in actuals:
                formal_arg_exprs[formal].append(args[actual])
        return hook(formal_arg_exprs, callee)
    else:
        assert isinstance(callee, Overloaded)
        items = []
        for item in callee.items:
            adjusted = self.apply_signature_hook(
                item, args, arg_kinds, arg_names, hook)
            assert isinstance(adjusted, CallableType)
            items.append(adjusted)
        return Overloaded(items)

</t>
<t tx="ekr.20220525082933.631">def apply_function_signature_hook(
        self, callee: FunctionLike, args: List[Expression],
        arg_kinds: List[ArgKind], context: Context,
        arg_names: Optional[Sequence[Optional[str]]],
        signature_hook: Callable[[FunctionSigContext], FunctionLike]) -&gt; FunctionLike:
    """Apply a plugin hook that may infer a more precise signature for a function."""
    return self.apply_signature_hook(
        callee, args, arg_kinds, arg_names,
        (lambda args, sig:
         signature_hook(FunctionSigContext(args, sig, context, self.chk))))

</t>
<t tx="ekr.20220525082933.632">def apply_method_signature_hook(
        self, callee: FunctionLike, args: List[Expression],
        arg_kinds: List[ArgKind], context: Context,
        arg_names: Optional[Sequence[Optional[str]]], object_type: Type,
        signature_hook: Callable[[MethodSigContext], FunctionLike]) -&gt; FunctionLike:
    """Apply a plugin hook that may infer a more precise signature for a method."""
    pobject_type = get_proper_type(object_type)
    return self.apply_signature_hook(
        callee, args, arg_kinds, arg_names,
        (lambda args, sig:
         signature_hook(MethodSigContext(pobject_type, args, sig, context, self.chk))))

</t>
<t tx="ekr.20220525082933.633">def transform_callee_type(
        self, callable_name: Optional[str], callee: Type, args: List[Expression],
        arg_kinds: List[ArgKind], context: Context,
        arg_names: Optional[Sequence[Optional[str]]] = None,
        object_type: Optional[Type] = None) -&gt; Type:
    """Attempt to determine a more accurate signature for a method call.

    This is done by looking up and applying a method signature hook (if one exists for the
    given method name).

    If no matching method signature hook is found, callee is returned unmodified. The same
    happens if the arguments refer to a non-method callable (this is allowed so that the code
    calling transform_callee_type needs to perform fewer boilerplate checks).

    Note: this method is *not* called automatically as part of check_call, because in some
    cases check_call is called multiple times while checking a single call (for example when
    dealing with overloads). Instead, this method needs to be called explicitly
    (if appropriate) before the signature is passed to check_call.
    """
    callee = get_proper_type(callee)
    if callable_name is not None and isinstance(callee, FunctionLike):
        if object_type is not None:
            method_sig_hook = self.plugin.get_method_signature_hook(callable_name)
            if method_sig_hook:
                return self.apply_method_signature_hook(
                    callee, args, arg_kinds, context, arg_names, object_type, method_sig_hook)
        else:
            function_sig_hook = self.plugin.get_function_signature_hook(callable_name)
            if function_sig_hook:
                return self.apply_function_signature_hook(
                    callee, args, arg_kinds, context, arg_names, function_sig_hook)

    return callee

</t>
<t tx="ekr.20220525082933.634">def check_call_expr_with_callee_type(self,
                                     callee_type: Type,
                                     e: CallExpr,
                                     callable_name: Optional[str],
                                     object_type: Optional[Type],
                                     member: Optional[str] = None) -&gt; Type:
    """Type check call expression.

    The callee_type should be used as the type of callee expression. In particular,
    in case of a union type this can be a particular item of the union, so that we can
    apply plugin hooks to each item.

    The 'member', 'callable_name' and 'object_type' are only used to call plugin hooks.
    If 'callable_name' is None but 'member' is not None (member call), try constructing
    'callable_name' using 'object_type' (the base type on which the method is called),
    for example 'typing.Mapping.get'.
    """
    if callable_name is None and member is not None:
        assert object_type is not None
        callable_name = self.method_fullname(object_type, member)
    object_type = get_proper_type(object_type)
    if callable_name:
        # Try to refine the call signature using plugin hooks before checking the call.
        callee_type = self.transform_callee_type(
            callable_name, callee_type, e.args, e.arg_kinds, e, e.arg_names, object_type)
    # Unions are special-cased to allow plugins to act on each item in the union.
    elif member is not None and isinstance(object_type, UnionType):
        return self.check_union_call_expr(e, object_type, member)
    ret_type, callee_type = self.check_call(
        callee_type, e.args, e.arg_kinds, e,
        e.arg_names, callable_node=e.callee,
        callable_name=callable_name,
        object_type=object_type,
    )
    proper_callee = get_proper_type(callee_type)
    if (isinstance(e.callee, RefExpr)
            and isinstance(proper_callee, CallableType)
            and proper_callee.type_guard is not None):
        # Cache it for find_isinstance_check()
        e.callee.type_guard = proper_callee.type_guard
    return ret_type

</t>
<t tx="ekr.20220525082933.635">def check_union_call_expr(self, e: CallExpr, object_type: UnionType, member: str) -&gt; Type:
    """"Type check calling a member expression where the base type is a union."""
    res: List[Type] = []
    for typ in object_type.relevant_items():
        # Member access errors are already reported when visiting the member expression.
        with self.msg.filter_errors():
            item = analyze_member_access(member, typ, e, False, False, False,
                                         self.msg, original_type=object_type, chk=self.chk,
                                         in_literal_context=self.is_literal_context(),
                                         self_type=typ)
        narrowed = self.narrow_type_from_binder(e.callee, item, skip_non_overlapping=True)
        if narrowed is None:
            continue
        callable_name = self.method_fullname(typ, member)
        item_object_type = typ if callable_name else None
        res.append(self.check_call_expr_with_callee_type(narrowed, e, callable_name,
                                                         item_object_type))
    return make_simplified_union(res)

</t>
<t tx="ekr.20220525082933.636">def check_call(self,
               callee: Type,
               args: List[Expression],
               arg_kinds: List[ArgKind],
               context: Context,
               arg_names: Optional[Sequence[Optional[str]]] = None,
               callable_node: Optional[Expression] = None,
               callable_name: Optional[str] = None,
               object_type: Optional[Type] = None) -&gt; Tuple[Type, Type]:
    """Type check a call.

    Also infer type arguments if the callee is a generic function.

    Return (result type, inferred callee type).

    Arguments:
        callee: type of the called value
        args: actual argument expressions
        arg_kinds: contains nodes.ARG_* constant for each argument in args
             describing whether the argument is positional, *arg, etc.
        context: current expression context, used for inference.
        arg_names: names of arguments (optional)
        callable_node: associate the inferred callable type to this node,
            if specified
        callable_name: Fully-qualified name of the function/method to call,
            or None if unavailable (examples: 'builtins.open', 'typing.Mapping.get')
        object_type: If callable_name refers to a method, the type of the object
            on which the method is being called
    """
    callee = get_proper_type(callee)

    if isinstance(callee, CallableType):
        return self.check_callable_call(callee, args, arg_kinds, context, arg_names,
                                        callable_node, callable_name, object_type)
    elif isinstance(callee, Overloaded):
        return self.check_overload_call(callee, args, arg_kinds, arg_names, callable_name,
                                        object_type, context)
    elif isinstance(callee, AnyType) or not self.chk.in_checked_function():
        return self.check_any_type_call(args, callee)
    elif isinstance(callee, UnionType):
        return self.check_union_call(callee, args, arg_kinds, arg_names, context)
    elif isinstance(callee, Instance):
        call_function = analyze_member_access('__call__', callee, context, is_lvalue=False,
                                              is_super=False, is_operator=True, msg=self.msg,
                                              original_type=callee, chk=self.chk,
                                              in_literal_context=self.is_literal_context())
        callable_name = callee.type.fullname + ".__call__"
        # Apply method signature hook, if one exists
        call_function = self.transform_callee_type(
            callable_name, call_function, args, arg_kinds, context, arg_names, callee)
        result = self.check_call(call_function, args, arg_kinds, context, arg_names,
                                 callable_node, callable_name, callee)
        if callable_node:
            # check_call() stored "call_function" as the type, which is incorrect.
            # Override the type.
            self.chk.store_type(callable_node, callee)
        return result
    elif isinstance(callee, TypeVarType):
        return self.check_call(callee.upper_bound, args, arg_kinds, context, arg_names,
                               callable_node)
    elif isinstance(callee, TypeType):
        item = self.analyze_type_type_callee(callee.item, context)
        return self.check_call(item, args, arg_kinds, context, arg_names,
                               callable_node)
    elif isinstance(callee, TupleType):
        return self.check_call(tuple_fallback(callee), args, arg_kinds, context,
                               arg_names, callable_node, callable_name,
                               object_type)
    else:
        return self.msg.not_callable(callee, context), AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20220525082933.637">def check_callable_call(self,
                        callee: CallableType,
                        args: List[Expression],
                        arg_kinds: List[ArgKind],
                        context: Context,
                        arg_names: Optional[Sequence[Optional[str]]],
                        callable_node: Optional[Expression],
                        callable_name: Optional[str],
                        object_type: Optional[Type]) -&gt; Tuple[Type, Type]:
    """Type check a call that targets a callable value.

    See the docstring of check_call for more information.
    """
    if callable_name is None and callee.name:
        callable_name = callee.name
    ret_type = get_proper_type(callee.ret_type)
    if callee.is_type_obj() and isinstance(ret_type, Instance):
        callable_name = ret_type.type.fullname
    if isinstance(callable_node, RefExpr) and callable_node.fullname in ENUM_BASES:
        # An Enum() call that failed SemanticAnalyzerPass2.check_enum_call().
        return callee.ret_type, callee

    if (callee.is_type_obj() and callee.type_object().is_abstract
            # Exception for Type[...]
            and not callee.from_type_type
            and not callee.type_object().fallback_to_any):
        type = callee.type_object()
        self.msg.cannot_instantiate_abstract_class(
            callee.type_object().name, type.abstract_attributes,
            context)
    elif (callee.is_type_obj() and callee.type_object().is_protocol
          # Exception for Type[...]
          and not callee.from_type_type):
        self.chk.fail(message_registry.CANNOT_INSTANTIATE_PROTOCOL
                      .format(callee.type_object().name), context)

    formal_to_actual = map_actuals_to_formals(
        arg_kinds, arg_names,
        callee.arg_kinds, callee.arg_names,
        lambda i: self.accept(args[i]))

    if callee.is_generic():
        need_refresh = any(isinstance(v, ParamSpecType) for v in callee.variables)
        callee = freshen_function_type_vars(callee)
        callee = self.infer_function_type_arguments_using_context(
            callee, context)
        callee = self.infer_function_type_arguments(
            callee, args, arg_kinds, formal_to_actual, context)
        if need_refresh:
            # Argument kinds etc. may have changed due to
            # ParamSpec variables being replaced with an arbitrary
            # number of arguments; recalculate actual-to-formal map
            formal_to_actual = map_actuals_to_formals(
                arg_kinds, arg_names,
                callee.arg_kinds, callee.arg_names,
                lambda i: self.accept(args[i]))

    param_spec = callee.param_spec()
    if param_spec is not None and arg_kinds == [ARG_STAR, ARG_STAR2]:
        arg1 = self.accept(args[0])
        arg2 = self.accept(args[1])
        if (isinstance(arg1, ParamSpecType)
                and isinstance(arg2, ParamSpecType)
                and arg1.flavor == ParamSpecFlavor.ARGS
                and arg2.flavor == ParamSpecFlavor.KWARGS
                and arg1.id == arg2.id == param_spec.id):
            return callee.ret_type, callee

    arg_types = self.infer_arg_types_in_context(
        callee, args, arg_kinds, formal_to_actual)

    self.check_argument_count(callee, arg_types, arg_kinds,
                              arg_names, formal_to_actual, context)

    self.check_argument_types(arg_types, arg_kinds, args, callee, formal_to_actual, context,
                              object_type=object_type)

    if (callee.is_type_obj() and (len(arg_types) == 1)
            and is_equivalent(callee.ret_type, self.named_type('builtins.type'))):
        callee = callee.copy_modified(ret_type=TypeType.make_normalized(arg_types[0]))

    if callable_node:
        # Store the inferred callable type.
        self.chk.store_type(callable_node, callee)

    if (callable_name
            and ((object_type is None and self.plugin.get_function_hook(callable_name))
                 or (object_type is not None
                     and self.plugin.get_method_hook(callable_name)))):
        new_ret_type = self.apply_function_plugin(
            callee, arg_kinds, arg_types, arg_names, formal_to_actual, args,
            callable_name, object_type, context)
        callee = callee.copy_modified(ret_type=new_ret_type)
    return callee.ret_type, callee

</t>
<t tx="ekr.20220525082933.638">def analyze_type_type_callee(self, item: ProperType, context: Context) -&gt; Type:
    """Analyze the callee X in X(...) where X is Type[item].

    Return a Y that we can pass to check_call(Y, ...).
    """
    if isinstance(item, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=item)
    if isinstance(item, Instance):
        res = type_object_type(item.type, self.named_type)
        if isinstance(res, CallableType):
            res = res.copy_modified(from_type_type=True)
        expanded = get_proper_type(expand_type_by_instance(res, item))
        if isinstance(expanded, CallableType):
            # Callee of the form Type[...] should never be generic, only
            # proper class objects can be.
            expanded = expanded.copy_modified(variables=[])
        return expanded
    if isinstance(item, UnionType):
        return UnionType([self.analyze_type_type_callee(get_proper_type(tp), context)
                          for tp in item.relevant_items()], item.line)
    if isinstance(item, TypeVarType):
        # Pretend we're calling the typevar's upper bound,
        # i.e. its constructor (a poor approximation for reality,
        # but better than AnyType...), but replace the return type
        # with typevar.
        callee = self.analyze_type_type_callee(get_proper_type(item.upper_bound), context)
        callee = get_proper_type(callee)
        if isinstance(callee, CallableType):
            callee = callee.copy_modified(ret_type=item)
        elif isinstance(callee, Overloaded):
            callee = Overloaded([c.copy_modified(ret_type=item)
                                 for c in callee.items])
        return callee
    # We support Type of namedtuples but not of tuples in general
    if (isinstance(item, TupleType)
            and tuple_fallback(item).type.fullname != 'builtins.tuple'):
        return self.analyze_type_type_callee(tuple_fallback(item), context)

    self.msg.unsupported_type_type(item, context)
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20220525082933.639">def infer_arg_types_in_empty_context(self, args: List[Expression]) -&gt; List[Type]:
    """Infer argument expression types in an empty context.

    In short, we basically recurse on each argument without considering
    in what context the argument was called.
    """
    res: List[Type] = []

    for arg in args:
        arg_type = self.accept(arg)
        if has_erased_component(arg_type):
            res.append(NoneType())
        else:
            res.append(arg_type)
    return res

</t>
<t tx="ekr.20220525082933.64">@path C:/Repos/mypy/misc/
@language python
@tabwidth -4
"""Cherry-pick a commit from typeshed.

Usage:

  python3 misc/cherry-pick-typeshed.py --typeshed-dir dir hash
"""

import argparse
import os.path
import re
import subprocess
import sys
import tempfile


@others
if __name__ == '__main__':
    main()
</t>
<t tx="ekr.20220525082933.640">def infer_arg_types_in_context(
        self, callee: CallableType, args: List[Expression], arg_kinds: List[ArgKind],
        formal_to_actual: List[List[int]]) -&gt; List[Type]:
    """Infer argument expression types using a callable type as context.

    For example, if callee argument 2 has type List[int], infer the
    argument expression with List[int] type context.

    Returns the inferred types of *actual arguments*.
    """
    res: List[Optional[Type]] = [None] * len(args)

    for i, actuals in enumerate(formal_to_actual):
        for ai in actuals:
            if not arg_kinds[ai].is_star():
                res[ai] = self.accept(args[ai], callee.arg_types[i])

    # Fill in the rest of the argument types.
    for i, t in enumerate(res):
        if not t:
            res[i] = self.accept(args[i])
    assert all(tp is not None for tp in res)
    return cast(List[Type], res)

</t>
<t tx="ekr.20220525082933.641">def infer_function_type_arguments_using_context(
        self, callable: CallableType, error_context: Context) -&gt; CallableType:
    """Unify callable return type to type context to infer type vars.

    For example, if the return type is set[t] where 't' is a type variable
    of callable, and if the context is set[int], return callable modified
    by substituting 't' with 'int'.
    """
    ctx = self.type_context[-1]
    if not ctx:
        return callable
    # The return type may have references to type metavariables that
    # we are inferring right now. We must consider them as indeterminate
    # and they are not potential results; thus we replace them with the
    # special ErasedType type. On the other hand, class type variables are
    # valid results.
    erased_ctx = replace_meta_vars(ctx, ErasedType())
    ret_type = callable.ret_type
    if is_optional(ret_type) and is_optional(ctx):
        # If both the context and the return type are optional, unwrap the optional,
        # since in 99% cases this is what a user expects. In other words, we replace
        #     Optional[T] &lt;: Optional[int]
        # with
        #     T &lt;: int
        # while the former would infer T &lt;: Optional[int].
        ret_type = remove_optional(ret_type)
        erased_ctx = remove_optional(erased_ctx)
        #
        # TODO: Instead of this hack and the one below, we need to use outer and
        # inner contexts at the same time. This is however not easy because of two
        # reasons:
        #   * We need to support constraints like [1 &lt;: 2, 2 &lt;: X], i.e. with variables
        #     on both sides. (This is not too hard.)
        #   * We need to update all the inference "infrastructure", so that all
        #     variables in an expression are inferred at the same time.
        #     (And this is hard, also we need to be careful with lambdas that require
        #     two passes.)
    if isinstance(ret_type, TypeVarType):
        # Another special case: the return type is a type variable. If it's unrestricted,
        # we could infer a too general type for the type variable if we use context,
        # and this could result in confusing and spurious type errors elsewhere.
        #
        # So we give up and just use function arguments for type inference, with just two
        # exceptions:
        #
        # 1. If the context is a generic instance type, actually use it as context, as
        #    this *seems* to usually be the reasonable thing to do.
        #
        #    See also github issues #462 and #360.
        #
        # 2. If the context is some literal type, we want to "propagate" that information
        #    down so that we infer a more precise type for literal expressions. For example,
        #    the expression `3` normally has an inferred type of `builtins.int`: but if it's
        #    in a literal context like below, we want it to infer `Literal[3]` instead.
        #
        #        def expects_literal(x: Literal[3]) -&gt; None: pass
        #        def identity(x: T) -&gt; T: return x
        #
        #        expects_literal(identity(3))  # Should type-check
        if not is_generic_instance(ctx) and not is_literal_type_like(ctx):
            return callable.copy_modified()
    args = infer_type_arguments(callable.type_var_ids(), ret_type, erased_ctx)
    # Only substitute non-Uninhabited and non-erased types.
    new_args: List[Optional[Type]] = []
    for arg in args:
        if has_uninhabited_component(arg) or has_erased_component(arg):
            new_args.append(None)
        else:
            new_args.append(arg)
    # Don't show errors after we have only used the outer context for inference.
    # We will use argument context to infer more variables.
    return self.apply_generic_arguments(callable, new_args, error_context,
                                        skip_unsatisfied=True)

</t>
<t tx="ekr.20220525082933.642">def infer_function_type_arguments(self, callee_type: CallableType,
                                  args: List[Expression],
                                  arg_kinds: List[ArgKind],
                                  formal_to_actual: List[List[int]],
                                  context: Context) -&gt; CallableType:
    """Infer the type arguments for a generic callee type.

    Infer based on the types of arguments.

    Return a derived callable type that has the arguments applied.
    """
    if self.chk.in_checked_function():
        # Disable type errors during type inference. There may be errors
        # due to partial available context information at this time, but
        # these errors can be safely ignored as the arguments will be
        # inferred again later.
        with self.msg.filter_errors():
            arg_types = self.infer_arg_types_in_context(
                callee_type, args, arg_kinds, formal_to_actual)

        arg_pass_nums = self.get_arg_infer_passes(
            callee_type.arg_types, formal_to_actual, len(args))

        pass1_args: List[Optional[Type]] = []
        for i, arg in enumerate(arg_types):
            if arg_pass_nums[i] &gt; 1:
                pass1_args.append(None)
            else:
                pass1_args.append(arg)

        inferred_args = infer_function_type_arguments(
            callee_type, pass1_args, arg_kinds, formal_to_actual,
            context=self.argument_infer_context(),
            strict=self.chk.in_checked_function())

        if 2 in arg_pass_nums:
            # Second pass of type inference.
            (callee_type,
             inferred_args) = self.infer_function_type_arguments_pass2(
                callee_type, args, arg_kinds, formal_to_actual,
                inferred_args, context)

        if callee_type.special_sig == 'dict' and len(inferred_args) == 2 and (
                ARG_NAMED in arg_kinds or ARG_STAR2 in arg_kinds):
            # HACK: Infer str key type for dict(...) with keyword args. The type system
            #       can't represent this so we special case it, as this is a pretty common
            #       thing. This doesn't quite work with all possible subclasses of dict
            #       if they shuffle type variables around, as we assume that there is a 1-1
            #       correspondence with dict type variables. This is a marginal issue and
            #       a little tricky to fix so it's left unfixed for now.
            first_arg = get_proper_type(inferred_args[0])
            if isinstance(first_arg, (NoneType, UninhabitedType)):
                inferred_args[0] = self.named_type('builtins.str')
            elif not first_arg or not is_subtype(self.named_type('builtins.str'), first_arg):
                self.chk.fail(message_registry.KEYWORD_ARGUMENT_REQUIRES_STR_KEY_TYPE,
                              context)
    else:
        # In dynamically typed functions use implicit 'Any' types for
        # type variables.
        inferred_args = [AnyType(TypeOfAny.unannotated)] * len(callee_type.variables)
    return self.apply_inferred_arguments(callee_type, inferred_args,
                                         context)

</t>
<t tx="ekr.20220525082933.643">def infer_function_type_arguments_pass2(
        self, callee_type: CallableType,
        args: List[Expression],
        arg_kinds: List[ArgKind],
        formal_to_actual: List[List[int]],
        old_inferred_args: Sequence[Optional[Type]],
        context: Context) -&gt; Tuple[CallableType, List[Optional[Type]]]:
    """Perform second pass of generic function type argument inference.

    The second pass is needed for arguments with types such as Callable[[T], S],
    where both T and S are type variables, when the actual argument is a
    lambda with inferred types.  The idea is to infer the type variable T
    in the first pass (based on the types of other arguments).  This lets
    us infer the argument and return type of the lambda expression and
    thus also the type variable S in this second pass.

    Return (the callee with type vars applied, inferred actual arg types).
    """
    # None or erased types in inferred types mean that there was not enough
    # information to infer the argument. Replace them with None values so
    # that they are not applied yet below.
    inferred_args = list(old_inferred_args)
    for i, arg in enumerate(get_proper_types(inferred_args)):
        if isinstance(arg, (NoneType, UninhabitedType)) or has_erased_component(arg):
            inferred_args[i] = None
    callee_type = self.apply_generic_arguments(callee_type, inferred_args, context)

    arg_types = self.infer_arg_types_in_context(
        callee_type, args, arg_kinds, formal_to_actual)

    inferred_args = infer_function_type_arguments(
        callee_type, arg_types, arg_kinds, formal_to_actual,
        context=self.argument_infer_context(),
    )

    return callee_type, inferred_args

</t>
<t tx="ekr.20220525082933.644">def argument_infer_context(self) -&gt; ArgumentInferContext:
    return ArgumentInferContext(
        self.chk.named_type('typing.Mapping'),
        self.chk.named_type('typing.Iterable'),
    )

</t>
<t tx="ekr.20220525082933.645">def get_arg_infer_passes(self, arg_types: List[Type],
                         formal_to_actual: List[List[int]],
                         num_actuals: int) -&gt; List[int]:
    """Return pass numbers for args for two-pass argument type inference.

    For each actual, the pass number is either 1 (first pass) or 2 (second
    pass).

    Two-pass argument type inference primarily lets us infer types of
    lambdas more effectively.
    """
    res = [1] * num_actuals
    for i, arg in enumerate(arg_types):
        if arg.accept(ArgInferSecondPassQuery()):
            for j in formal_to_actual[i]:
                res[j] = 2
    return res

</t>
<t tx="ekr.20220525082933.646">def apply_inferred_arguments(self, callee_type: CallableType,
                             inferred_args: Sequence[Optional[Type]],
                             context: Context) -&gt; CallableType:
    """Apply inferred values of type arguments to a generic function.

    Inferred_args contains the values of function type arguments.
    """
    # Report error if some of the variables could not be solved. In that
    # case assume that all variables have type Any to avoid extra
    # bogus error messages.
    for i, inferred_type in enumerate(inferred_args):
        if not inferred_type or has_erased_component(inferred_type):
            # Could not infer a non-trivial type for a type variable.
            self.msg.could_not_infer_type_arguments(
                callee_type, i + 1, context)
            inferred_args = [AnyType(TypeOfAny.from_error)] * len(inferred_args)
    # Apply the inferred types to the function type. In this case the
    # return type must be CallableType, since we give the right number of type
    # arguments.
    return self.apply_generic_arguments(callee_type, inferred_args, context)

</t>
<t tx="ekr.20220525082933.647">def check_argument_count(self,
                         callee: CallableType,
                         actual_types: List[Type],
                         actual_kinds: List[ArgKind],
                         actual_names: Optional[Sequence[Optional[str]]],
                         formal_to_actual: List[List[int]],
                         context: Optional[Context]) -&gt; bool:
    """Check that there is a value for all required arguments to a function.

    Also check that there are no duplicate values for arguments. Report found errors
    using 'messages' if it's not None. If 'messages' is given, 'context' must also be given.

    Return False if there were any errors. Otherwise return True
    """
    if context is None:
        # Avoid "is None" checks
        context = TempNode(AnyType(TypeOfAny.special_form))

    # TODO(jukka): We could return as soon as we find an error if messages is None.

    # Collect dict of all actual arguments matched to formal arguments, with occurrence count
    all_actuals: Dict[int, int] = {}
    for actuals in formal_to_actual:
        for a in actuals:
            all_actuals[a] = all_actuals.get(a, 0) + 1

    ok, is_unexpected_arg_error = self.check_for_extra_actual_arguments(
        callee, actual_types, actual_kinds, actual_names, all_actuals, context)

    # Check for too many or few values for formals.
    for i, kind in enumerate(callee.arg_kinds):
        if kind.is_required() and not formal_to_actual[i] and not is_unexpected_arg_error:
            # No actual for a mandatory formal
            if kind.is_positional():
                self.msg.too_few_arguments(callee, context, actual_names)
            else:
                argname = callee.arg_names[i] or "?"
                self.msg.missing_named_argument(callee, context, argname)
            ok = False
        elif not kind.is_star() and is_duplicate_mapping(
                formal_to_actual[i], actual_types, actual_kinds):
            if (self.chk.in_checked_function() or
                    isinstance(get_proper_type(actual_types[formal_to_actual[i][0]]),
                               TupleType)):
                self.msg.duplicate_argument_value(callee, i, context)
                ok = False
        elif (kind.is_named() and formal_to_actual[i] and
              actual_kinds[formal_to_actual[i][0]] not in [nodes.ARG_NAMED, nodes.ARG_STAR2]):
            # Positional argument when expecting a keyword argument.
            self.msg.too_many_positional_arguments(callee, context)
            ok = False
    return ok

</t>
<t tx="ekr.20220525082933.648">def check_for_extra_actual_arguments(self,
                                     callee: CallableType,
                                     actual_types: List[Type],
                                     actual_kinds: List[ArgKind],
                                     actual_names: Optional[Sequence[Optional[str]]],
                                     all_actuals: Dict[int, int],
                                     context: Context) -&gt; Tuple[bool, bool]:
    """Check for extra actual arguments.

    Return tuple (was everything ok,
                  was there an extra keyword argument error [used to avoid duplicate errors]).
    """

    is_unexpected_arg_error = False  # Keep track of errors to avoid duplicate errors
    ok = True  # False if we've found any error

    for i, kind in enumerate(actual_kinds):
        if (i not in all_actuals and
                # We accept the other iterables than tuple (including Any)
                # as star arguments because they could be empty, resulting no arguments.
                (kind != nodes.ARG_STAR or is_non_empty_tuple(actual_types[i])) and
                # Accept all types for double-starred arguments, because they could be empty
                # dictionaries and we can't tell it from their types
                kind != nodes.ARG_STAR2):
            # Extra actual: not matched by a formal argument.
            ok = False
            if kind != nodes.ARG_NAMED:
                self.msg.too_many_arguments(callee, context)
            else:
                assert actual_names, "Internal error: named kinds without names given"
                act_name = actual_names[i]
                assert act_name is not None
                act_type = actual_types[i]
                self.msg.unexpected_keyword_argument(callee, act_name, act_type, context)
                is_unexpected_arg_error = True
        elif ((kind == nodes.ARG_STAR and nodes.ARG_STAR not in callee.arg_kinds)
              or kind == nodes.ARG_STAR2):
            actual_type = get_proper_type(actual_types[i])
            if isinstance(actual_type, (TupleType, TypedDictType)):
                if all_actuals.get(i, 0) &lt; len(actual_type.items):
                    # Too many tuple/dict items as some did not match.
                    if (kind != nodes.ARG_STAR2
                            or not isinstance(actual_type, TypedDictType)):
                        self.msg.too_many_arguments(callee, context)
                    else:
                        self.msg.too_many_arguments_from_typed_dict(callee, actual_type,
                                                                    context)
                        is_unexpected_arg_error = True
                    ok = False
            # *args/**kwargs can be applied even if the function takes a fixed
            # number of positional arguments. This may succeed at runtime.

    return ok, is_unexpected_arg_error

</t>
<t tx="ekr.20220525082933.649">def check_argument_types(self,
                         arg_types: List[Type],
                         arg_kinds: List[ArgKind],
                         args: List[Expression],
                         callee: CallableType,
                         formal_to_actual: List[List[int]],
                         context: Context,
                         check_arg: Optional[ArgChecker] = None,
                         object_type: Optional[Type] = None) -&gt; None:
    """Check argument types against a callable type.

    Report errors if the argument types are not compatible.

    The check_call docstring describes some of the arguments.
    """
    check_arg = check_arg or self.check_arg
    # Keep track of consumed tuple *arg items.
    mapper = ArgTypeExpander(self.argument_infer_context())
    for i, actuals in enumerate(formal_to_actual):
        for actual in actuals:
            actual_type = arg_types[actual]
            if actual_type is None:
                continue  # Some kind of error was already reported.
            actual_kind = arg_kinds[actual]
            # Check that a *arg is valid as varargs.
            if (actual_kind == nodes.ARG_STAR and
                    not self.is_valid_var_arg(actual_type)):
                self.msg.invalid_var_arg(actual_type, context)
            if (actual_kind == nodes.ARG_STAR2 and
                    not self.is_valid_keyword_var_arg(actual_type)):
                is_mapping = is_subtype(actual_type, self.chk.named_type('typing.Mapping'))
                self.msg.invalid_keyword_var_arg(actual_type, is_mapping, context)
            expanded_actual = mapper.expand_actual_type(
                actual_type, actual_kind,
                callee.arg_names[i], callee.arg_kinds[i])
            check_arg(expanded_actual, actual_type, arg_kinds[actual],
                      callee.arg_types[i],
                      actual + 1, i + 1, callee, object_type, args[actual], context)

</t>
<t tx="ekr.20220525082933.65">def parse_commit_title(diff: str) -&gt; str:
    m = re.search("\n    ([^ ].*)", diff)
    assert m is not None, "Could not parse diff"
    return m.group(1)


</t>
<t tx="ekr.20220525082933.650">def check_arg(self,
              caller_type: Type,
              original_caller_type: Type,
              caller_kind: ArgKind,
              callee_type: Type,
              n: int,
              m: int,
              callee: CallableType,
              object_type: Optional[Type],
              context: Context,
              outer_context: Context) -&gt; None:
    """Check the type of a single argument in a call."""
    caller_type = get_proper_type(caller_type)
    original_caller_type = get_proper_type(original_caller_type)
    callee_type = get_proper_type(callee_type)

    if isinstance(caller_type, DeletedType):
        self.msg.deleted_as_rvalue(caller_type, context)
    # Only non-abstract non-protocol class can be given where Type[...] is expected...
    elif (isinstance(caller_type, CallableType) and isinstance(callee_type, TypeType) and
          caller_type.is_type_obj() and
          (caller_type.type_object().is_abstract or caller_type.type_object().is_protocol) and
          isinstance(callee_type.item, Instance) and
          (callee_type.item.type.is_abstract or callee_type.item.type.is_protocol)):
        self.msg.concrete_only_call(callee_type, context)
    elif not is_subtype(caller_type, callee_type, options=self.chk.options):
        if self.chk.should_suppress_optional_error([caller_type, callee_type]):
            return
        code = self.msg.incompatible_argument(n,
                                              m,
                                              callee,
                                              original_caller_type,
                                              caller_kind,
                                              object_type=object_type,
                                              context=context,
                                              outer_context=outer_context)
        self.msg.incompatible_argument_note(original_caller_type, callee_type, context,
                                            code=code)

</t>
<t tx="ekr.20220525082933.651">def check_overload_call(self,
                        callee: Overloaded,
                        args: List[Expression],
                        arg_kinds: List[ArgKind],
                        arg_names: Optional[Sequence[Optional[str]]],
                        callable_name: Optional[str],
                        object_type: Optional[Type],
                        context: Context) -&gt; Tuple[Type, Type]:
    """Checks a call to an overloaded function."""
    arg_types = self.infer_arg_types_in_empty_context(args)
    # Step 1: Filter call targets to remove ones where the argument counts don't match
    plausible_targets = self.plausible_overload_call_targets(arg_types, arg_kinds,
                                                             arg_names, callee)

    # Step 2: If the arguments contain a union, we try performing union math first,
    #         instead of picking the first matching overload.
    #         This is because picking the first overload often ends up being too greedy:
    #         for example, when we have a fallback alternative that accepts an unrestricted
    #         typevar. See https://github.com/python/mypy/issues/4063 for related discussion.
    erased_targets: Optional[List[CallableType]] = None
    unioned_result: Optional[Tuple[Type, Type]] = None
    union_interrupted = False  # did we try all union combinations?
    if any(self.real_union(arg) for arg in arg_types):
        try:
            with self.msg.filter_errors():
                unioned_return = self.union_overload_result(plausible_targets, args,
                                                            arg_types, arg_kinds, arg_names,
                                                            callable_name, object_type,
                                                            context)
        except TooManyUnions:
            union_interrupted = True
        else:
            # Record if we succeeded. Next we need to see if maybe normal procedure
            # gives a narrower type.
            if unioned_return:
                returns, inferred_types = zip(*unioned_return)
                # Note that we use `combine_function_signatures` instead of just returning
                # a union of inferred callables because for example a call
                # Union[int -&gt; int, str -&gt; str](Union[int, str]) is invalid and
                # we don't want to introduce internal inconsistencies.
                unioned_result = (make_simplified_union(list(returns),
                                                        context.line,
                                                        context.column),
                                  self.combine_function_signatures(inferred_types))

    # Step 3: We try checking each branch one-by-one.
    inferred_result = self.infer_overload_return_type(plausible_targets, args, arg_types,
                                                      arg_kinds, arg_names, callable_name,
                                                      object_type, context)
    # If any of checks succeed, stop early.
    if inferred_result is not None and unioned_result is not None:
        # Both unioned and direct checks succeeded, choose the more precise type.
        if (is_subtype(inferred_result[0], unioned_result[0]) and
                not isinstance(get_proper_type(inferred_result[0]), AnyType)):
            return inferred_result
        return unioned_result
    elif unioned_result is not None:
        return unioned_result
    elif inferred_result is not None:
        return inferred_result

    # Step 4: Failure. At this point, we know there is no match. We fall back to trying
    #         to find a somewhat plausible overload target using the erased types
    #         so we can produce a nice error message.
    #
    #         For example, suppose the user passes a value of type 'List[str]' into an
    #         overload with signatures f(x: int) -&gt; int and f(x: List[int]) -&gt; List[int].
    #
    #         Neither alternative matches, but we can guess the user probably wants the
    #         second one.
    erased_targets = self.overload_erased_call_targets(plausible_targets, arg_types,
                                                       arg_kinds, arg_names, args, context)

    # Step 5: We try and infer a second-best alternative if possible. If not, fall back
    #         to using 'Any'.
    if len(erased_targets) &gt; 0:
        # Pick the first plausible erased target as the fallback
        # TODO: Adjust the error message here to make it clear there was no match.
        #       In order to do this, we need to find a clean way of associating
        #       a note with whatever error message 'self.check_call' will generate.
        #       In particular, the note's line and column numbers need to be the same
        #       as the error's.
        target: Type = erased_targets[0]
    else:
        # There was no plausible match: give up
        target = AnyType(TypeOfAny.from_error)

        if not self.chk.should_suppress_optional_error(arg_types):
            if not is_operator_method(callable_name):
                code = None
            else:
                code = codes.OPERATOR
            self.msg.no_variant_matches_arguments(
                callee, arg_types, context, code=code)

    result = self.check_call(target, args, arg_kinds, context, arg_names,
                             callable_name=callable_name,
                             object_type=object_type)
    if union_interrupted:
        self.chk.fail(message_registry.TOO_MANY_UNION_COMBINATIONS, context)
    return result

</t>
<t tx="ekr.20220525082933.652">def plausible_overload_call_targets(self,
                                    arg_types: List[Type],
                                    arg_kinds: List[ArgKind],
                                    arg_names: Optional[Sequence[Optional[str]]],
                                    overload: Overloaded) -&gt; List[CallableType]:
    """Returns all overload call targets that having matching argument counts.

    If the given args contains a star-arg (*arg or **kwarg argument), this method
    will ensure all star-arg overloads appear at the start of the list, instead
    of their usual location.

    The only exception is if the starred argument is something like a Tuple or a
    NamedTuple, which has a definitive "shape". If so, we don't move the corresponding
    alternative to the front since we can infer a more precise match using the original
    order."""

    def has_shape(typ: Type) -&gt; bool:
        typ = get_proper_type(typ)
        return (isinstance(typ, TupleType) or isinstance(typ, TypedDictType)
                or (isinstance(typ, Instance) and typ.type.is_named_tuple))

    matches: List[CallableType] = []
    star_matches: List[CallableType] = []

    args_have_var_arg = False
    args_have_kw_arg = False
    for kind, typ in zip(arg_kinds, arg_types):
        if kind == ARG_STAR and not has_shape(typ):
            args_have_var_arg = True
        if kind == ARG_STAR2 and not has_shape(typ):
            args_have_kw_arg = True

    for typ in overload.items:
        formal_to_actual = map_actuals_to_formals(arg_kinds, arg_names,
                                                  typ.arg_kinds, typ.arg_names,
                                                  lambda i: arg_types[i])

        with self.msg.filter_errors():
            if self.check_argument_count(typ, arg_types, arg_kinds, arg_names,
                                         formal_to_actual, None):
                if args_have_var_arg and typ.is_var_arg:
                    star_matches.append(typ)
                elif args_have_kw_arg and typ.is_kw_arg:
                    star_matches.append(typ)
                else:
                    matches.append(typ)

    return star_matches + matches

</t>
<t tx="ekr.20220525082933.653">def infer_overload_return_type(self,
                               plausible_targets: List[CallableType],
                               args: List[Expression],
                               arg_types: List[Type],
                               arg_kinds: List[ArgKind],
                               arg_names: Optional[Sequence[Optional[str]]],
                               callable_name: Optional[str],
                               object_type: Optional[Type],
                               context: Context,
                               ) -&gt; Optional[Tuple[Type, Type]]:
    """Attempts to find the first matching callable from the given list.

    If a match is found, returns a tuple containing the result type and the inferred
    callee type. (This tuple is meant to be eventually returned by check_call.)
    If multiple targets match due to ambiguous Any parameters, returns (AnyType, AnyType).
    If no targets match, returns None.

    Assumes all of the given targets have argument counts compatible with the caller.
    """

    matches: List[CallableType] = []
    return_types: List[Type] = []
    inferred_types: List[Type] = []
    args_contain_any = any(map(has_any_type, arg_types))
    type_maps: List[Dict[Expression, Type]] = []

    for typ in plausible_targets:
        assert self.msg is self.chk.msg
        with self.msg.filter_errors() as w:
            with self.chk.local_type_map() as m:
                ret_type, infer_type = self.check_call(
                    callee=typ,
                    args=args,
                    arg_kinds=arg_kinds,
                    arg_names=arg_names,
                    context=context,
                    callable_name=callable_name,
                    object_type=object_type)
        is_match = not w.has_new_errors()
        if is_match:
            # Return early if possible; otherwise record info so we can
            # check for ambiguity due to 'Any' below.
            if not args_contain_any:
                return ret_type, infer_type
            matches.append(typ)
            return_types.append(ret_type)
            inferred_types.append(infer_type)
            type_maps.append(m)

    if len(matches) == 0:
        # No match was found
        return None
    elif any_causes_overload_ambiguity(matches, return_types, arg_types, arg_kinds, arg_names):
        # An argument of type or containing the type 'Any' caused ambiguity.
        # We try returning a precise type if we can. If not, we give up and just return 'Any'.
        if all_same_types(return_types):
            self.chk.store_types(type_maps[0])
            return return_types[0], inferred_types[0]
        elif all_same_types([erase_type(typ) for typ in return_types]):
            self.chk.store_types(type_maps[0])
            return erase_type(return_types[0]), erase_type(inferred_types[0])
        else:
            return self.check_call(callee=AnyType(TypeOfAny.special_form),
                                   args=args,
                                   arg_kinds=arg_kinds,
                                   arg_names=arg_names,
                                   context=context,
                                   callable_name=callable_name,
                                   object_type=object_type)
    else:
        # Success! No ambiguity; return the first match.
        self.chk.store_types(type_maps[0])
        return return_types[0], inferred_types[0]

</t>
<t tx="ekr.20220525082933.654">def overload_erased_call_targets(self,
                                 plausible_targets: List[CallableType],
                                 arg_types: List[Type],
                                 arg_kinds: List[ArgKind],
                                 arg_names: Optional[Sequence[Optional[str]]],
                                 args: List[Expression],
                                 context: Context) -&gt; List[CallableType]:
    """Returns a list of all targets that match the caller after erasing types.

    Assumes all of the given targets have argument counts compatible with the caller.
    """
    matches: List[CallableType] = []
    for typ in plausible_targets:
        if self.erased_signature_similarity(arg_types, arg_kinds, arg_names, args, typ,
                                            context):
            matches.append(typ)
    return matches

</t>
<t tx="ekr.20220525082933.655">def union_overload_result(self,
                          plausible_targets: List[CallableType],
                          args: List[Expression],
                          arg_types: List[Type],
                          arg_kinds: List[ArgKind],
                          arg_names: Optional[Sequence[Optional[str]]],
                          callable_name: Optional[str],
                          object_type: Optional[Type],
                          context: Context,
                          level: int = 0
                          ) -&gt; Optional[List[Tuple[Type, Type]]]:
    """Accepts a list of overload signatures and attempts to match calls by destructuring
    the first union.

    Return a list of (&lt;return type&gt;, &lt;inferred variant type&gt;) if call succeeds for every
    item of the desctructured union. Returns None if there is no match.
    """
    # Step 1: If we are already too deep, then stop immediately. Otherwise mypy might
    # hang for long time because of a weird overload call. The caller will get
    # the exception and generate an appropriate note message, if needed.
    if level &gt;= MAX_UNIONS:
        raise TooManyUnions

    # Step 2: Find position of the first union in arguments. Return the normal inferred
    # type if no more unions left.
    for idx, typ in enumerate(arg_types):
        if self.real_union(typ):
            break
    else:
        # No unions in args, just fall back to normal inference
        with self.type_overrides_set(args, arg_types):
            res = self.infer_overload_return_type(plausible_targets, args, arg_types,
                                                  arg_kinds, arg_names, callable_name,
                                                  object_type, context)
        if res is not None:
            return [res]
        return None

    # Step 3: Try a direct match before splitting to avoid unnecessary union splits
    # and save performance.
    with self.type_overrides_set(args, arg_types):
        direct = self.infer_overload_return_type(plausible_targets, args, arg_types,
                                                 arg_kinds, arg_names, callable_name,
                                                 object_type, context)
    if direct is not None and not isinstance(get_proper_type(direct[0]),
                                             (UnionType, AnyType)):
        # We only return non-unions soon, to avoid greedy match.
        return [direct]

    # Step 4: Split the first remaining union type in arguments into items and
    # try to match each item individually (recursive).
    first_union = get_proper_type(arg_types[idx])
    assert isinstance(first_union, UnionType)
    res_items = []
    for item in first_union.relevant_items():
        new_arg_types = arg_types.copy()
        new_arg_types[idx] = item
        sub_result = self.union_overload_result(plausible_targets, args, new_arg_types,
                                                arg_kinds, arg_names, callable_name,
                                                object_type, context,
                                                level + 1)
        if sub_result is not None:
            res_items.extend(sub_result)
        else:
            # Some item doesn't match, return soon.
            return None

    # Step 5: If splitting succeeded, then filter out duplicate items before returning.
    seen: Set[Tuple[Type, Type]] = set()
    result = []
    for pair in res_items:
        if pair not in seen:
            seen.add(pair)
            result.append(pair)
    return result

</t>
<t tx="ekr.20220525082933.656">def real_union(self, typ: Type) -&gt; bool:
    typ = get_proper_type(typ)
    return isinstance(typ, UnionType) and len(typ.relevant_items()) &gt; 1

</t>
<t tx="ekr.20220525082933.657">@contextmanager
def type_overrides_set(self, exprs: Sequence[Expression],
                       overrides: Sequence[Type]) -&gt; Iterator[None]:
    """Set _temporary_ type overrides for given expressions."""
    assert len(exprs) == len(overrides)
    for expr, typ in zip(exprs, overrides):
        self.type_overrides[expr] = typ
    try:
        yield
    finally:
        for expr in exprs:
            del self.type_overrides[expr]

</t>
<t tx="ekr.20220525082933.658">def combine_function_signatures(self, types: Sequence[Type]) -&gt; Union[AnyType, CallableType]:
    """Accepts a list of function signatures and attempts to combine them together into a
    new CallableType consisting of the union of all of the given arguments and return types.

    If there is at least one non-callable type, return Any (this can happen if there is
    an ambiguity because of Any in arguments).
    """
    assert types, "Trying to merge no callables"
    types = get_proper_types(types)
    if not all(isinstance(c, CallableType) for c in types):
        return AnyType(TypeOfAny.special_form)
    callables = cast(Sequence[CallableType], types)
    if len(callables) == 1:
        return callables[0]

    # Note: we are assuming here that if a user uses some TypeVar 'T' in
    # two different functions, they meant for that TypeVar to mean the
    # same thing.
    #
    # This function will make sure that all instances of that TypeVar 'T'
    # refer to the same underlying TypeVarType objects to simplify the union-ing
    # logic below.
    #
    # (If the user did *not* mean for 'T' to be consistently bound to the
    # same type in their overloads, well, their code is probably too
    # confusing and ought to be re-written anyways.)
    callables, variables = merge_typevars_in_callables_by_name(callables)

    new_args: List[List[Type]] = [[] for _ in range(len(callables[0].arg_types))]
    new_kinds = list(callables[0].arg_kinds)
    new_returns: List[Type] = []

    too_complex = False
    for target in callables:
        # We fall back to Callable[..., Union[&lt;returns&gt;]] if the functions do not have
        # the exact same signature. The only exception is if one arg is optional and
        # the other is positional: in that case, we continue unioning (and expect a
        # positional arg).
        # TODO: Enhance the merging logic to handle a wider variety of signatures.
        if len(new_kinds) != len(target.arg_kinds):
            too_complex = True
            break
        for i, (new_kind, target_kind) in enumerate(zip(new_kinds, target.arg_kinds)):
            if new_kind == target_kind:
                continue
            elif new_kind.is_positional() and target_kind.is_positional():
                new_kinds[i] = ARG_POS
            else:
                too_complex = True
                break

        if too_complex:
            break  # outer loop

        for i, arg in enumerate(target.arg_types):
            new_args[i].append(arg)
        new_returns.append(target.ret_type)

    union_return = make_simplified_union(new_returns)
    if too_complex:
        any = AnyType(TypeOfAny.special_form)
        return callables[0].copy_modified(
            arg_types=[any, any],
            arg_kinds=[ARG_STAR, ARG_STAR2],
            arg_names=[None, None],
            ret_type=union_return,
            variables=variables,
            implicit=True)

    final_args = []
    for args_list in new_args:
        new_type = make_simplified_union(args_list)
        final_args.append(new_type)

    return callables[0].copy_modified(
        arg_types=final_args,
        arg_kinds=new_kinds,
        ret_type=union_return,
        variables=variables,
        implicit=True)

</t>
<t tx="ekr.20220525082933.659">def erased_signature_similarity(self,
                                arg_types: List[Type],
                                arg_kinds: List[ArgKind],
                                arg_names: Optional[Sequence[Optional[str]]],
                                args: List[Expression],
                                callee: CallableType,
                                context: Context) -&gt; bool:
    """Determine whether arguments could match the signature at runtime, after
    erasing types."""
    formal_to_actual = map_actuals_to_formals(arg_kinds,
                                              arg_names,
                                              callee.arg_kinds,
                                              callee.arg_names,
                                              lambda i: arg_types[i])

    with self.msg.filter_errors():
        if not self.check_argument_count(callee, arg_types, arg_kinds, arg_names,
                                         formal_to_actual, None):
            # Too few or many arguments -&gt; no match.
            return False

    def check_arg(caller_type: Type,
                  original_ccaller_type: Type,
                  caller_kind: ArgKind,
                  callee_type: Type,
                  n: int,
                  m: int,
                  callee: CallableType,
                  object_type: Optional[Type],
                  context: Context,
                  outer_context: Context) -&gt; None:
        if not arg_approximate_similarity(caller_type, callee_type):
            # No match -- exit early since none of the remaining work can change
            # the result.
            raise Finished

    try:
        self.check_argument_types(arg_types, arg_kinds, args, callee,
                                  formal_to_actual, context=context, check_arg=check_arg)
        return True
    except Finished:
        return False

</t>
<t tx="ekr.20220525082933.66">def main() -&gt; None:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--typeshed-dir", help="location of typeshed", metavar="dir", required=True
    )
    parser.add_argument(
        "commit", help="typeshed commit hash to cherry-pick"
    )
    args = parser.parse_args()
    typeshed_dir = args.typeshed_dir
    commit = args.commit

    if not os.path.isdir(typeshed_dir):
        sys.exit(f"error: {typeshed_dir} does not exist")
    if not re.match("[0-9a-fA-F]+$", commit):
        sys.exit(f"error: Invalid commit {commit!r}")

    if not os.path.exists("mypy") or not os.path.exists("mypyc"):
        sys.exit(f"error: This script must be run at the mypy repository root directory")

    with tempfile.TemporaryDirectory() as d:
        diff_file = os.path.join(d, "diff")
        out = subprocess.run(["git", "show", commit],
                             capture_output=True,
                             text=True,
                             check=True,
                             cwd=typeshed_dir)
        with open(diff_file, "w") as f:
            f.write(out.stdout)
        subprocess.run(["git",
                        "apply",
                        "--index",
                        "--directory=mypy/typeshed",
                        "--exclude=**/tests/**",
                        diff_file],
                       check=True)

        title = parse_commit_title(out.stdout)
        subprocess.run(["git", "commit", "-m", f"Typeshed cherry-pick: {title}"], check=True)

    print()
    print(f"Cherry-picked commit {commit} from {typeshed_dir}")


</t>
<t tx="ekr.20220525082933.660">def apply_generic_arguments(self, callable: CallableType, types: Sequence[Optional[Type]],
                            context: Context, skip_unsatisfied: bool = False) -&gt; CallableType:
    """Simple wrapper around mypy.applytype.apply_generic_arguments."""
    return applytype.apply_generic_arguments(callable, types,
                                             self.msg.incompatible_typevar_value, context,
                                             skip_unsatisfied=skip_unsatisfied)

</t>
<t tx="ekr.20220525082933.661">def check_any_type_call(self, args: List[Expression], callee: Type) -&gt; Tuple[Type, Type]:
    self.infer_arg_types_in_empty_context(args)
    callee = get_proper_type(callee)
    if isinstance(callee, AnyType):
        return (AnyType(TypeOfAny.from_another_any, source_any=callee),
                AnyType(TypeOfAny.from_another_any, source_any=callee))
    else:
        return AnyType(TypeOfAny.special_form), AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082933.662">def check_union_call(self,
                     callee: UnionType,
                     args: List[Expression],
                     arg_kinds: List[ArgKind],
                     arg_names: Optional[Sequence[Optional[str]]],
                     context: Context) -&gt; Tuple[Type, Type]:
    with self.msg.disable_type_names():
        results = [
            self.check_call(
                subtype,
                args,
                arg_kinds,
                context,
                arg_names,
            )
            for subtype in callee.relevant_items()
        ]

    return (make_simplified_union([res[0] for res in results]),
            callee)

</t>
<t tx="ekr.20220525082933.663">def visit_member_expr(self, e: MemberExpr, is_lvalue: bool = False) -&gt; Type:
    """Visit member expression (of form e.id)."""
    self.chk.module_refs.update(extract_refexpr_names(e))
    result = self.analyze_ordinary_member_access(e, is_lvalue)
    return self.narrow_type_from_binder(e, result)

</t>
<t tx="ekr.20220525082933.664">def analyze_ordinary_member_access(self, e: MemberExpr,
                                   is_lvalue: bool) -&gt; Type:
    """Analyse member expression or member lvalue."""
    if e.kind is not None:
        # This is a reference to a module attribute.
        return self.analyze_ref_expr(e)
    else:
        # This is a reference to a non-module attribute.
        original_type = self.accept(e.expr)
        base = e.expr
        module_symbol_table = None

        if isinstance(base, RefExpr) and isinstance(base.node, MypyFile):
            module_symbol_table = base.node.names

        member_type = analyze_member_access(
            e.name, original_type, e, is_lvalue, False, False,
            self.msg, original_type=original_type, chk=self.chk,
            in_literal_context=self.is_literal_context(),
            module_symbol_table=module_symbol_table)

        return member_type

</t>
<t tx="ekr.20220525082933.665">def analyze_external_member_access(self, member: str, base_type: Type,
                                   context: Context) -&gt; Type:
    """Analyse member access that is external, i.e. it cannot
    refer to private definitions. Return the result type.
    """
    # TODO remove; no private definitions in mypy
    return analyze_member_access(member, base_type, context, False, False, False,
                                 self.msg, original_type=base_type, chk=self.chk,
                                 in_literal_context=self.is_literal_context())

</t>
<t tx="ekr.20220525082933.666">def is_literal_context(self) -&gt; bool:
    return is_literal_type_like(self.type_context[-1])

</t>
<t tx="ekr.20220525082933.667">def infer_literal_expr_type(self, value: LiteralValue, fallback_name: str) -&gt; Type:
    """Analyzes the given literal expression and determines if we should be
    inferring an Instance type, a Literal[...] type, or an Instance that
    remembers the original literal. We...

    1. ...Infer a normal Instance in most circumstances.

    2. ...Infer a Literal[...] if we're in a literal context. For example, if we
       were analyzing the "3" in "foo(3)" where "foo" has a signature of
       "def foo(Literal[3]) -&gt; None", we'd want to infer that the "3" has a
       type of Literal[3] instead of Instance.

    3. ...Infer an Instance that remembers the original Literal if we're declaring
       a Final variable with an inferred type -- for example, "bar" in "bar: Final = 3"
       would be assigned an Instance that remembers it originated from a '3'. See
       the comments in Instance's constructor for more details.
    """
    typ = self.named_type(fallback_name)
    if self.is_literal_context():
        return LiteralType(value=value, fallback=typ)
    else:
        return typ.copy_modified(last_known_value=LiteralType(
            value=value,
            fallback=typ,
            line=typ.line,
            column=typ.column,
        ))

</t>
<t tx="ekr.20220525082933.668">def concat_tuples(self, left: TupleType, right: TupleType) -&gt; TupleType:
    """Concatenate two fixed length tuples."""
    return TupleType(items=left.items + right.items,
                     fallback=self.named_type('builtins.tuple'))

</t>
<t tx="ekr.20220525082933.669">def visit_int_expr(self, e: IntExpr) -&gt; Type:
    """Type check an integer literal (trivial)."""
    return self.infer_literal_expr_type(e.value, 'builtins.int')

</t>
<t tx="ekr.20220525082933.67">@path C:/Repos/mypy/misc/
@language python
@tabwidth -4
#!/usr/bin/env python3
"""Script for converting between cache formats.

We support a filesystem tree based cache and a sqlite based cache.
See mypy/metastore.py for details.
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import argparse
from mypy.metastore import FilesystemMetadataStore, SqliteMetadataStore


@others
if __name__ == '__main__':
    main()
</t>
<t tx="ekr.20220525082933.670">def visit_str_expr(self, e: StrExpr) -&gt; Type:
    """Type check a string literal (trivial)."""
    return self.infer_literal_expr_type(e.value, 'builtins.str')

</t>
<t tx="ekr.20220525082933.671">def visit_bytes_expr(self, e: BytesExpr) -&gt; Type:
    """Type check a bytes literal (trivial)."""
    return self.infer_literal_expr_type(e.value, 'builtins.bytes')

</t>
<t tx="ekr.20220525082933.672">def visit_unicode_expr(self, e: UnicodeExpr) -&gt; Type:
    """Type check a unicode literal (trivial)."""
    return self.infer_literal_expr_type(e.value, 'builtins.unicode')

</t>
<t tx="ekr.20220525082933.673">def visit_float_expr(self, e: FloatExpr) -&gt; Type:
    """Type check a float literal (trivial)."""
    return self.named_type('builtins.float')

</t>
<t tx="ekr.20220525082933.674">def visit_complex_expr(self, e: ComplexExpr) -&gt; Type:
    """Type check a complex literal."""
    return self.named_type('builtins.complex')

</t>
<t tx="ekr.20220525082933.675">def visit_ellipsis(self, e: EllipsisExpr) -&gt; Type:
    """Type check '...'."""
    if self.chk.options.python_version[0] &gt;= 3:
        return self.named_type('builtins.ellipsis')
    else:
        # '...' is not valid in normal Python 2 code, but it can
        # be used in stubs.  The parser makes sure that we only
        # get this far if we are in a stub, and we can safely
        # return 'object' as ellipsis is special cased elsewhere.
        # The builtins.ellipsis type does not exist in Python 2.
        return self.named_type('builtins.object')

</t>
<t tx="ekr.20220525082933.676">def visit_op_expr(self, e: OpExpr) -&gt; Type:
    """Type check a binary operator expression."""
    if e.op == 'and' or e.op == 'or':
        return self.check_boolean_op(e, e)
    if e.op == '*' and isinstance(e.left, ListExpr):
        # Expressions of form [...] * e get special type inference.
        return self.check_list_multiply(e)
    if e.op == '%':
        pyversion = self.chk.options.python_version
        if pyversion[0] == 3:
            if isinstance(e.left, BytesExpr) and pyversion[1] &gt;= 5:
                return self.strfrm_checker.check_str_interpolation(e.left, e.right)
            if isinstance(e.left, StrExpr):
                return self.strfrm_checker.check_str_interpolation(e.left, e.right)
        elif pyversion[0] == 2:
            if isinstance(e.left, (StrExpr, BytesExpr, UnicodeExpr)):
                return self.strfrm_checker.check_str_interpolation(e.left, e.right)
    left_type = self.accept(e.left)

    proper_left_type = get_proper_type(left_type)
    if isinstance(proper_left_type, TupleType) and e.op == '+':
        left_add_method = proper_left_type.partial_fallback.type.get('__add__')
        if left_add_method and left_add_method.fullname == 'builtins.tuple.__add__':
            proper_right_type = get_proper_type(self.accept(e.right))
            if isinstance(proper_right_type, TupleType):
                right_radd_method = proper_right_type.partial_fallback.type.get('__radd__')
                if right_radd_method is None:
                    return self.concat_tuples(proper_left_type, proper_right_type)

    if e.op in operators.op_methods:
        method = self.get_operator_method(e.op)
        result, method_type = self.check_op(method, left_type, e.right, e,
                                            allow_reverse=True)
        e.method_type = method_type
        return result
    else:
        raise RuntimeError(f'Unknown operator {e.op}')

</t>
<t tx="ekr.20220525082933.677">def visit_comparison_expr(self, e: ComparisonExpr) -&gt; Type:
    """Type check a comparison expression.

    Comparison expressions are type checked consecutive-pair-wise
    That is, 'a &lt; b &gt; c == d' is check as 'a &lt; b and b &gt; c and c == d'
    """
    result: Optional[Type] = None
    sub_result: Optional[Type] = None

    # Check each consecutive operand pair and their operator
    for left, right, operator in zip(e.operands, e.operands[1:], e.operators):
        left_type = self.accept(left)

        method_type: Optional[mypy.types.Type] = None

        if operator == 'in' or operator == 'not in':
            # If the right operand has partial type, look it up without triggering
            # a "Need type annotation ..." message, as it would be noise.
            right_type = self.find_partial_type_ref_fast_path(right)
            if right_type is None:
                right_type = self.accept(right)  # Validate the right operand

            # Keep track of whether we get type check errors (these won't be reported, they
            # are just to verify whether something is valid typing wise).
            with self.msg.filter_errors(save_filtered_errors=True) as local_errors:
                _, method_type = self.check_method_call_by_name(
                    method='__contains__',
                    base_type=right_type,
                    args=[left],
                    arg_kinds=[ARG_POS],
                    context=e,
                )

            sub_result = self.bool_type()
            # Container item type for strict type overlap checks. Note: we need to only
            # check for nominal type, because a usual "Unsupported operands for in"
            # will be reported for types incompatible with __contains__().
            # See testCustomContainsCheckStrictEquality for an example.
            cont_type = self.chk.analyze_container_item_type(right_type)
            if isinstance(right_type, PartialType):
                # We don't really know if this is an error or not, so just shut up.
                pass
            elif (local_errors.has_new_errors() and
                # is_valid_var_arg is True for any Iterable
                    self.is_valid_var_arg(right_type)):
                _, itertype = self.chk.analyze_iterable_item_type(right)
                method_type = CallableType(
                    [left_type],
                    [nodes.ARG_POS],
                    [None],
                    self.bool_type(),
                    self.named_type('builtins.function'))
                if not is_subtype(left_type, itertype):
                    self.msg.unsupported_operand_types('in', left_type, right_type, e)
            # Only show dangerous overlap if there are no other errors.
            elif (not local_errors.has_new_errors() and cont_type and
                    self.dangerous_comparison(left_type, cont_type,
                                              original_container=right_type)):
                self.msg.dangerous_comparison(left_type, cont_type, 'container', e)
            else:
                self.msg.add_errors(local_errors.filtered_errors())
        elif operator in operators.op_methods:
            method = self.get_operator_method(operator)

            with ErrorWatcher(self.msg.errors) as w:
                sub_result, method_type = self.check_op(method, left_type, right, e,
                                                        allow_reverse=True)

            # Only show dangerous overlap if there are no other errors. See
            # testCustomEqCheckStrictEquality for an example.
            if not w.has_new_errors() and operator in ('==', '!='):
                right_type = self.accept(right)
                # We suppress the error if there is a custom __eq__() method on either
                # side. User defined (or even standard library) classes can define this
                # to return True for comparisons between non-overlapping types.
                if (not custom_special_method(left_type, '__eq__') and
                        not custom_special_method(right_type, '__eq__')):
                    # Also flag non-overlapping literals in situations like:
                    #    x: Literal['a', 'b']
                    #    if x == 'c':
                    #        ...
                    left_type = try_getting_literal(left_type)
                    right_type = try_getting_literal(right_type)
                    if self.dangerous_comparison(left_type, right_type):
                        self.msg.dangerous_comparison(left_type, right_type, 'equality', e)

        elif operator == 'is' or operator == 'is not':
            right_type = self.accept(right)  # validate the right operand
            sub_result = self.bool_type()
            left_type = try_getting_literal(left_type)
            right_type = try_getting_literal(right_type)
            if self.dangerous_comparison(left_type, right_type):
                self.msg.dangerous_comparison(left_type, right_type, 'identity', e)
            method_type = None
        else:
            raise RuntimeError(f'Unknown comparison operator {operator}')

        e.method_types.append(method_type)

        #  Determine type of boolean-and of result and sub_result
        if result is None:
            result = sub_result
        else:
            result = join.join_types(result, sub_result)

    assert result is not None
    return result

</t>
<t tx="ekr.20220525082933.678">def find_partial_type_ref_fast_path(self, expr: Expression) -&gt; Optional[Type]:
    """If expression has a partial generic type, return it without additional checks.

    In particular, this does not generate an error about a missing annotation.

    Otherwise, return None.
    """
    if not isinstance(expr, RefExpr):
        return None
    if isinstance(expr.node, Var):
        result = self.analyze_var_ref(expr.node, expr)
        if isinstance(result, PartialType) and result.type is not None:
            self.chk.store_type(expr, self.chk.fixup_partial_type(result))
            return result
    return None

</t>
<t tx="ekr.20220525082933.679">def dangerous_comparison(self, left: Type, right: Type,
                         original_container: Optional[Type] = None) -&gt; bool:
    """Check for dangerous non-overlapping comparisons like 42 == 'no'.

    The original_container is the original container type for 'in' checks
    (and None for equality checks).

    Rules:
        * X and None are overlapping even in strict-optional mode. This is to allow
        'assert x is not None' for x defined as 'x = None  # type: str' in class body
        (otherwise mypy itself would have couple dozen errors because of this).
        * Optional[X] and Optional[Y] are non-overlapping if X and Y are
        non-overlapping, although technically None is overlap, it is most
        likely an error.
        * Any overlaps with everything, i.e. always safe.
        * Special case: b'abc' in b'cde' is safe.
    """
    if not self.chk.options.strict_equality:
        return False

    left, right = get_proper_types((left, right))

    if self.chk.binder.is_unreachable_warning_suppressed():
        # We are inside a function that contains type variables with value restrictions in
        # its signature. In this case we just suppress all strict-equality checks to avoid
        # false positives for code like:
        #
        #     T = TypeVar('T', str, int)
        #     def f(x: T) -&gt; T:
        #         if x == 0:
        #             ...
        #         return x
        #
        # TODO: find a way of disabling the check only for types resulted from the expansion.
        return False
    if isinstance(left, NoneType) or isinstance(right, NoneType):
        return False
    if isinstance(left, UnionType) and isinstance(right, UnionType):
        left = remove_optional(left)
        right = remove_optional(right)
        left, right = get_proper_types((left, right))
    py2 = self.chk.options.python_version &lt; (3, 0)
    if (original_container and has_bytes_component(original_container, py2) and
            has_bytes_component(left, py2)):
        # We need to special case bytes and bytearray, because 97 in b'abc', b'a' in b'abc',
        # b'a' in bytearray(b'abc') etc. all return True (and we want to show the error only
        # if the check can _never_ be True).
        return False
    if isinstance(left, Instance) and isinstance(right, Instance):
        # Special case some builtin implementations of AbstractSet.
        if (left.type.fullname in OVERLAPPING_TYPES_ALLOWLIST and
                right.type.fullname in OVERLAPPING_TYPES_ALLOWLIST):
            abstract_set = self.chk.lookup_typeinfo('typing.AbstractSet')
            left = map_instance_to_supertype(left, abstract_set)
            right = map_instance_to_supertype(right, abstract_set)
            return not is_overlapping_types(left.args[0], right.args[0])
    if isinstance(left, LiteralType) and isinstance(right, LiteralType):
        if isinstance(left.value, bool) and isinstance(right.value, bool):
            # Comparing different booleans is not dangerous.
            return False
    return not is_overlapping_types(left, right, ignore_promotions=False)

</t>
<t tx="ekr.20220525082933.68">def main() -&gt; None:
    parser = argparse.ArgumentParser()
    parser.add_argument('--to-sqlite', action='store_true', default=False,
                        help='Convert to a sqlite cache (default: convert from)')
    parser.add_argument('--output_dir', action='store', default=None,
                        help="Output cache location (default: same as input)")
    parser.add_argument('input_dir',
                        help="Input directory for the cache")
    args = parser.parse_args()

    input_dir = args.input_dir
    output_dir = args.output_dir or input_dir
    if args.to_sqlite:
        input, output = FilesystemMetadataStore(input_dir), SqliteMetadataStore(output_dir)
    else:
        input, output = SqliteMetadataStore(input_dir), FilesystemMetadataStore(output_dir)

    for s in input.list_all():
        if s.endswith('.json'):
            assert output.write(s, input.read(s), input.getmtime(s)), "Failed to write cache file!"
    output.commit()


</t>
<t tx="ekr.20220525082933.680">def get_operator_method(self, op: str) -&gt; str:
    if op == '/' and self.chk.options.python_version[0] == 2:
        return (
            '__truediv__'
            if self.chk.tree.is_future_flag_set('division')
            else '__div__'
        )
    else:
        return operators.op_methods[op]

</t>
<t tx="ekr.20220525082933.681">def check_method_call_by_name(self,
                              method: str,
                              base_type: Type,
                              args: List[Expression],
                              arg_kinds: List[ArgKind],
                              context: Context,
                              original_type: Optional[Type] = None
                              ) -&gt; Tuple[Type, Type]:
    """Type check a call to a named method on an object.

    Return tuple (result type, inferred method type). The 'original_type'
    is used for error messages.
    """
    original_type = original_type or base_type
    # Unions are special-cased to allow plugins to act on each element of the union.
    base_type = get_proper_type(base_type)
    if isinstance(base_type, UnionType):
        return self.check_union_method_call_by_name(method, base_type,
                                                    args, arg_kinds,
                                                    context, original_type)

    method_type = analyze_member_access(method, base_type, context, False, False, True,
                                        self.msg, original_type=original_type,
                                        chk=self.chk,
                                        in_literal_context=self.is_literal_context())
    return self.check_method_call(
        method, base_type, method_type, args, arg_kinds, context)

</t>
<t tx="ekr.20220525082933.682">def check_union_method_call_by_name(self,
                                    method: str,
                                    base_type: UnionType,
                                    args: List[Expression],
                                    arg_kinds: List[ArgKind],
                                    context: Context,
                                    original_type: Optional[Type] = None
                                    ) -&gt; Tuple[Type, Type]:
    """Type check a call to a named method on an object with union type.

    This essentially checks the call using check_method_call_by_name() for each
    union item and unions the result. We do this to allow plugins to act on
    individual union items.
    """
    res: List[Type] = []
    meth_res: List[Type] = []
    for typ in base_type.relevant_items():
        # Format error messages consistently with
        # mypy.checkmember.analyze_union_member_access().
        with self.msg.disable_type_names():
            item, meth_item = self.check_method_call_by_name(
                method, typ, args, arg_kinds,
                context, original_type,
            )
        res.append(item)
        meth_res.append(meth_item)
    return make_simplified_union(res), make_simplified_union(meth_res)

</t>
<t tx="ekr.20220525082933.683">def check_method_call(self,
                      method_name: str,
                      base_type: Type,
                      method_type: Type,
                      args: List[Expression],
                      arg_kinds: List[ArgKind],
                      context: Context) -&gt; Tuple[Type, Type]:
    """Type check a call to a method with the given name and type on an object.

    Return tuple (result type, inferred method type).
    """
    callable_name = self.method_fullname(base_type, method_name)
    object_type = base_type if callable_name is not None else None

    # Try to refine the method signature using plugin hooks before checking the call.
    method_type = self.transform_callee_type(
        callable_name, method_type, args, arg_kinds, context, object_type=object_type)

    return self.check_call(method_type, args, arg_kinds,
                           context, callable_name=callable_name, object_type=base_type)

</t>
<t tx="ekr.20220525082933.684">def check_op_reversible(self,
                        op_name: str,
                        left_type: Type,
                        left_expr: Expression,
                        right_type: Type,
                        right_expr: Expression,
                        context: Context) -&gt; Tuple[Type, Type]:
    def lookup_operator(op_name: str, base_type: Type) -&gt; Optional[Type]:
        """Looks up the given operator and returns the corresponding type,
        if it exists."""

        # This check is an important performance optimization,
        # even though it is mostly a subset of
        # analyze_member_access.
        # TODO: Find a way to remove this call without performance implications.
        if not self.has_member(base_type, op_name):
            return None

        with self.msg.filter_errors() as w:
            member = analyze_member_access(
                name=op_name,
                typ=base_type,
                is_lvalue=False,
                is_super=False,
                is_operator=True,
                original_type=base_type,
                context=context,
                msg=self.msg,
                chk=self.chk,
                in_literal_context=self.is_literal_context()
            )
            return None if w.has_new_errors() else member

    def lookup_definer(typ: Instance, attr_name: str) -&gt; Optional[str]:
        """Returns the name of the class that contains the actual definition of attr_name.

        So if class A defines foo and class B subclasses A, running
        'get_class_defined_in(B, "foo")` would return the full name of A.

        However, if B were to override and redefine foo, that method call would
        return the full name of B instead.

        If the attr name is not present in the given class or its MRO, returns None.
        """
        for cls in typ.type.mro:
            if cls.names.get(attr_name):
                return cls.fullname
        return None

    left_type = get_proper_type(left_type)
    right_type = get_proper_type(right_type)

    # If either the LHS or the RHS are Any, we can't really concluding anything
    # about the operation since the Any type may or may not define an
    # __op__ or __rop__ method. So, we punt and return Any instead.

    if isinstance(left_type, AnyType):
        any_type = AnyType(TypeOfAny.from_another_any, source_any=left_type)
        return any_type, any_type
    if isinstance(right_type, AnyType):
        any_type = AnyType(TypeOfAny.from_another_any, source_any=right_type)
        return any_type, any_type

    # STEP 1:
    # We start by getting the __op__ and __rop__ methods, if they exist.

    rev_op_name = self.get_reverse_op_method(op_name)

    left_op = lookup_operator(op_name, left_type)
    right_op = lookup_operator(rev_op_name, right_type)

    # STEP 2a:
    # We figure out in which order Python will call the operator methods. As it
    # turns out, it's not as simple as just trying to call __op__ first and
    # __rop__ second.
    #
    # We store the determined order inside the 'variants_raw' variable,
    # which records tuples containing the method, base type, and the argument.

    bias_right = is_proper_subtype(right_type, left_type)
    if op_name in operators.op_methods_that_shortcut and is_same_type(left_type, right_type):
        # When we do "A() + A()", for example, Python will only call the __add__ method,
        # never the __radd__ method.
        #
        # This is the case even if the __add__ method is completely missing and the __radd__
        # method is defined.

        variants_raw = [
            (left_op, left_type, right_expr)
        ]
    elif (is_subtype(right_type, left_type)
            and isinstance(left_type, Instance)
            and isinstance(right_type, Instance)
            and lookup_definer(left_type, op_name) != lookup_definer(right_type, rev_op_name)):
        # When we do "A() + B()" where B is a subclass of B, we'll actually try calling
        # B's __radd__ method first, but ONLY if B explicitly defines or overrides the
        # __radd__ method.
        #
        # This mechanism lets subclasses "refine" the expected outcome of the operation, even
        # if they're located on the RHS.

        variants_raw = [
            (right_op, right_type, left_expr),
            (left_op, left_type, right_expr),
        ]
    else:
        # In all other cases, we do the usual thing and call __add__ first and
        # __radd__ second when doing "A() + B()".

        variants_raw = [
            (left_op, left_type, right_expr),
            (right_op, right_type, left_expr),
        ]

    # STEP 2b:
    # When running Python 2, we might also try calling the __cmp__ method.

    is_python_2 = self.chk.options.python_version[0] == 2
    if is_python_2 and op_name in operators.ops_falling_back_to_cmp:
        cmp_method = operators.comparison_fallback_method
        left_cmp_op = lookup_operator(cmp_method, left_type)
        right_cmp_op = lookup_operator(cmp_method, right_type)

        if bias_right:
            variants_raw.append((right_cmp_op, right_type, left_expr))
            variants_raw.append((left_cmp_op, left_type, right_expr))
        else:
            variants_raw.append((left_cmp_op, left_type, right_expr))
            variants_raw.append((right_cmp_op, right_type, left_expr))

    # STEP 3:
    # We now filter out all non-existent operators. The 'variants' list contains
    # all operator methods that are actually present, in the order that Python
    # attempts to invoke them.

    variants = [(op, obj, arg) for (op, obj, arg) in variants_raw if op is not None]

    # STEP 4:
    # We now try invoking each one. If an operation succeeds, end early and return
    # the corresponding result. Otherwise, return the result and errors associated
    # with the first entry.

    errors = []
    results = []
    for method, obj, arg in variants:
        with self.msg.filter_errors(save_filtered_errors=True) as local_errors:
            result = self.check_method_call(
                op_name, obj, method, [arg], [ARG_POS], context)
        if local_errors.has_new_errors():
            errors.append(local_errors.filtered_errors())
            results.append(result)
        else:
            return result

    # We finish invoking above operators and no early return happens. Therefore,
    # we check if either the LHS or the RHS is Instance and fallbacks to Any,
    # if so, we also return Any
    if ((isinstance(left_type, Instance) and left_type.type.fallback_to_any) or
            (isinstance(right_type, Instance) and right_type.type.fallback_to_any)):
        any_type = AnyType(TypeOfAny.special_form)
        return any_type, any_type

    # STEP 4b:
    # Sometimes, the variants list is empty. In that case, we fall-back to attempting to
    # call the __op__ method (even though it's missing).

    if not variants:
        with self.msg.filter_errors(save_filtered_errors=True) as local_errors:
            result = self.check_method_call_by_name(
                op_name, left_type, [right_expr], [ARG_POS], context)

        if local_errors.has_new_errors():
            errors.append(local_errors.filtered_errors())
            results.append(result)
        else:
            # In theory, we should never enter this case, but it seems
            # we sometimes do, when dealing with Type[...]? E.g. see
            # check-classes.testTypeTypeComparisonWorks.
            #
            # This is probably related to the TODO in lookup_operator(...)
            # up above.
            #
            # TODO: Remove this extra case
            return result

    self.msg.add_errors(errors[0])
    if len(results) == 1:
        return results[0]
    else:
        error_any = AnyType(TypeOfAny.from_error)
        result = error_any, error_any
        return result

</t>
<t tx="ekr.20220525082933.685">def check_op(self, method: str, base_type: Type,
             arg: Expression, context: Context,
             allow_reverse: bool = False) -&gt; Tuple[Type, Type]:
    """Type check a binary operation which maps to a method call.

    Return tuple (result type, inferred operator method type).
    """

    if allow_reverse:
        left_variants = [base_type]
        base_type = get_proper_type(base_type)
        if isinstance(base_type, UnionType):
            left_variants = [item for item in
                             flatten_nested_unions(base_type.relevant_items(),
                                                   handle_type_alias_type=True)]
        right_type = self.accept(arg)

        # Step 1: We first try leaving the right arguments alone and destructure
        # just the left ones. (Mypy can sometimes perform some more precise inference
        # if we leave the right operands a union -- see testOperatorWithEmptyListAndSum.)
        all_results = []
        all_inferred = []

        with self.msg.filter_errors() as local_errors:
            for left_possible_type in left_variants:
                result, inferred = self.check_op_reversible(
                    op_name=method,
                    left_type=left_possible_type,
                    left_expr=TempNode(left_possible_type, context=context),
                    right_type=right_type,
                    right_expr=arg,
                    context=context)
                all_results.append(result)
                all_inferred.append(inferred)

        if not local_errors.has_new_errors():
            results_final = make_simplified_union(all_results)
            inferred_final = make_simplified_union(all_inferred)
            return results_final, inferred_final

        # Step 2: If that fails, we try again but also destructure the right argument.
        # This is also necessary to make certain edge cases work -- see
        # testOperatorDoubleUnionInterwovenUnionAdd, for example.

        # Note: We want to pass in the original 'arg' for 'left_expr' and 'right_expr'
        # whenever possible so that plugins and similar things can introspect on the original
        # node if possible.
        #
        # We don't do the same for the base expression because it could lead to weird
        # type inference errors -- e.g. see 'testOperatorDoubleUnionSum'.
        # TODO: Can we use `type_overrides_set()` here?
        right_variants = [(right_type, arg)]
        right_type = get_proper_type(right_type)
        if isinstance(right_type, UnionType):
            right_variants = [
                (item, TempNode(item, context=context))
                for item in flatten_nested_unions(right_type.relevant_items(),
                                                  handle_type_alias_type=True)
            ]

        all_results = []
        all_inferred = []

        with self.msg.filter_errors(save_filtered_errors=True) as local_errors:
            for left_possible_type in left_variants:
                for right_possible_type, right_expr in right_variants:
                    result, inferred = self.check_op_reversible(
                        op_name=method,
                        left_type=left_possible_type,
                        left_expr=TempNode(left_possible_type, context=context),
                        right_type=right_possible_type,
                        right_expr=right_expr,
                        context=context)
                    all_results.append(result)
                    all_inferred.append(inferred)

        if local_errors.has_new_errors():
            self.msg.add_errors(local_errors.filtered_errors())
            # Point any notes to the same location as an existing message.
            err = local_errors.filtered_errors()[-1]
            recent_context = TempNode(NoneType())
            recent_context.line = err.line
            recent_context.column = err.column
            if len(left_variants) &gt;= 2 and len(right_variants) &gt;= 2:
                self.msg.warn_both_operands_are_from_unions(recent_context)
            elif len(left_variants) &gt;= 2:
                self.msg.warn_operand_was_from_union(
                    "Left", base_type, context=recent_context)
            elif len(right_variants) &gt;= 2:
                self.msg.warn_operand_was_from_union(
                    "Right", right_type, context=recent_context)

        # See the comment in 'check_overload_call' for more details on why
        # we call 'combine_function_signature' instead of just unioning the inferred
        # callable types.
        results_final = make_simplified_union(all_results)
        inferred_final = self.combine_function_signatures(all_inferred)
        return results_final, inferred_final
    else:
        return self.check_method_call_by_name(
            method=method,
            base_type=base_type,
            args=[arg],
            arg_kinds=[ARG_POS],
            context=context,
        )

</t>
<t tx="ekr.20220525082933.686">def get_reverse_op_method(self, method: str) -&gt; str:
    if method == '__div__' and self.chk.options.python_version[0] == 2:
        return '__rdiv__'
    else:
        return operators.reverse_op_methods[method]

</t>
<t tx="ekr.20220525082933.687">def check_boolean_op(self, e: OpExpr, context: Context) -&gt; Type:
    """Type check a boolean operation ('and' or 'or')."""

    # A boolean operation can evaluate to either of the operands.

    # We use the current type context to guide the type inference of of
    # the left operand. We also use the left operand type to guide the type
    # inference of the right operand so that expressions such as
    # '[1] or []' are inferred correctly.
    ctx = self.type_context[-1]
    left_type = self.accept(e.left, ctx)
    expanded_left_type = try_expanding_sum_type_to_union(
        self.accept(e.left, ctx), "builtins.bool"
    )

    assert e.op in ('and', 'or')  # Checked by visit_op_expr

    if e.right_always:
        left_map, right_map = None, {}  # type: mypy.checker.TypeMap, mypy.checker.TypeMap
    elif e.right_unreachable:
        left_map, right_map = {}, None
    elif e.op == 'and':
        right_map, left_map = self.chk.find_isinstance_check(e.left)
    elif e.op == 'or':
        left_map, right_map = self.chk.find_isinstance_check(e.left)

    # If left_map is None then we know mypy considers the left expression
    # to be redundant.
    if (
        codes.REDUNDANT_EXPR in self.chk.options.enabled_error_codes
        and left_map is None
        # don't report an error if it's intentional
        and not e.right_always
    ):
        self.msg.redundant_left_operand(e.op, e.left)

    if (
        self.chk.should_report_unreachable_issues()
        and right_map is None
        # don't report an error if it's intentional
        and not e.right_unreachable
    ):
        self.msg.unreachable_right_operand(e.op, e.right)

    # If right_map is None then we know mypy considers the right branch
    # to be unreachable and therefore any errors found in the right branch
    # should be suppressed.
    with self.msg.filter_errors(filter_errors=right_map is None):
        right_type = self.analyze_cond_branch(right_map, e.right, expanded_left_type)

    if left_map is None and right_map is None:
        return UninhabitedType()

    if right_map is None:
        # The boolean expression is statically known to be the left value
        assert left_map is not None
        return left_type
    if left_map is None:
        # The boolean expression is statically known to be the right value
        assert right_map is not None
        return right_type

    if e.op == 'and':
        restricted_left_type = false_only(expanded_left_type)
        result_is_left = not expanded_left_type.can_be_true
    elif e.op == 'or':
        restricted_left_type = true_only(expanded_left_type)
        result_is_left = not expanded_left_type.can_be_false

    if isinstance(restricted_left_type, UninhabitedType):
        # The left operand can never be the result
        return right_type
    elif result_is_left:
        # The left operand is always the result
        return left_type
    else:
        return make_simplified_union([restricted_left_type, right_type])

</t>
<t tx="ekr.20220525082933.688">def check_list_multiply(self, e: OpExpr) -&gt; Type:
    """Type check an expression of form '[...] * e'.

    Type inference is special-cased for this common construct.
    """
    right_type = self.accept(e.right)
    if is_subtype(right_type, self.named_type('builtins.int')):
        # Special case: [...] * &lt;int value&gt;. Use the type context of the
        # OpExpr, since the multiplication does not affect the type.
        left_type = self.accept(e.left, type_context=self.type_context[-1])
    else:
        left_type = self.accept(e.left)
    result, method_type = self.check_op('__mul__', left_type, e.right, e)
    e.method_type = method_type
    return result

</t>
<t tx="ekr.20220525082933.689">def visit_assignment_expr(self, e: AssignmentExpr) -&gt; Type:
    value = self.accept(e.value)
    self.chk.check_assignment(e.target, e.value)
    self.chk.check_final(e)
    self.chk.store_type(e.target, value)
    self.find_partial_type_ref_fast_path(e.target)
    return value

</t>
<t tx="ekr.20220525082933.69">@path C:/Repos/mypy/misc/
@language python
@tabwidth -4
#!/usr/bin/env python3
"""Produce a diff between mypy caches.

With some infrastructure, this can allow for distributing small cache diffs to users in
many cases instead of full cache artifacts.
"""

import argparse
import json
import os
import sys

from collections import defaultdict
from typing import Any, Dict, Optional, Set

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from mypy.metastore import FilesystemMetadataStore, MetadataStore, SqliteMetadataStore


@others
if __name__ == "__main__":
    main()
</t>
<t tx="ekr.20220525082933.690">def visit_unary_expr(self, e: UnaryExpr) -&gt; Type:
    """Type check an unary operation ('not', '-', '+' or '~')."""
    operand_type = self.accept(e.expr)
    op = e.op
    if op == 'not':
        result: Type = self.bool_type()
    else:
        method = operators.unary_op_methods[op]
        result, method_type = self.check_method_call_by_name(method, operand_type, [], [], e)
        e.method_type = method_type
    return result

</t>
<t tx="ekr.20220525082933.691">def visit_index_expr(self, e: IndexExpr) -&gt; Type:
    """Type check an index expression (base[index]).

    It may also represent type application.
    """
    result = self.visit_index_expr_helper(e)
    result = get_proper_type(self.narrow_type_from_binder(e, result))
    if (self.is_literal_context() and isinstance(result, Instance)
            and result.last_known_value is not None):
        result = result.last_known_value
    return result

</t>
<t tx="ekr.20220525082933.692">def visit_index_expr_helper(self, e: IndexExpr) -&gt; Type:
    if e.analyzed:
        # It's actually a type application.
        return self.accept(e.analyzed)
    left_type = self.accept(e.base)
    return self.visit_index_with_type(left_type, e)

</t>
<t tx="ekr.20220525082933.693">def visit_index_with_type(self, left_type: Type, e: IndexExpr,
                          original_type: Optional[ProperType] = None) -&gt; Type:
    """Analyze type of an index expression for a given type of base expression.

    The 'original_type' is used for error messages (currently used for union types).
    """
    index = e.index
    left_type = get_proper_type(left_type)

    # Visit the index, just to make sure we have a type for it available
    self.accept(index)

    if isinstance(left_type, UnionType):
        original_type = original_type or left_type
        # Don't combine literal types, since we may need them for type narrowing.
        return make_simplified_union([self.visit_index_with_type(typ, e,
                                                                 original_type)
                                      for typ in left_type.relevant_items()],
                                     contract_literals=False)
    elif isinstance(left_type, TupleType) and self.chk.in_checked_function():
        # Special case for tuples. They return a more specific type when
        # indexed by an integer literal.
        if isinstance(index, SliceExpr):
            return self.visit_tuple_slice_helper(left_type, index)

        ns = self.try_getting_int_literals(index)
        if ns is not None:
            out = []
            for n in ns:
                if n &lt; 0:
                    n += len(left_type.items)
                if 0 &lt;= n &lt; len(left_type.items):
                    out.append(left_type.items[n])
                else:
                    self.chk.fail(message_registry.TUPLE_INDEX_OUT_OF_RANGE, e)
                    return AnyType(TypeOfAny.from_error)
            return make_simplified_union(out)
        else:
            return self.nonliteral_tuple_index_helper(left_type, index)
    elif isinstance(left_type, TypedDictType):
        return self.visit_typeddict_index_expr(left_type, e.index)
    elif (isinstance(left_type, CallableType)
          and left_type.is_type_obj() and left_type.type_object().is_enum):
        return self.visit_enum_index_expr(left_type.type_object(), e.index, e)
    elif (isinstance(left_type, TypeVarType)
          and not self.has_member(left_type.upper_bound, "__getitem__")):
        return self.visit_index_with_type(left_type.upper_bound, e, original_type)
    else:
        result, method_type = self.check_method_call_by_name(
            '__getitem__', left_type, [e.index], [ARG_POS], e,
            original_type=original_type)
        e.method_type = method_type
        return result

</t>
<t tx="ekr.20220525082933.694">def visit_tuple_slice_helper(self, left_type: TupleType, slic: SliceExpr) -&gt; Type:
    begin: Sequence[Optional[int]] = [None]
    end: Sequence[Optional[int]] = [None]
    stride: Sequence[Optional[int]] = [None]

    if slic.begin_index:
        begin_raw = self.try_getting_int_literals(slic.begin_index)
        if begin_raw is None:
            return self.nonliteral_tuple_index_helper(left_type, slic)
        begin = begin_raw

    if slic.end_index:
        end_raw = self.try_getting_int_literals(slic.end_index)
        if end_raw is None:
            return self.nonliteral_tuple_index_helper(left_type, slic)
        end = end_raw

    if slic.stride:
        stride_raw = self.try_getting_int_literals(slic.stride)
        if stride_raw is None:
            return self.nonliteral_tuple_index_helper(left_type, slic)
        stride = stride_raw

    items: List[Type] = []
    for b, e, s in itertools.product(begin, end, stride):
        items.append(left_type.slice(b, e, s))
    return make_simplified_union(items)

</t>
<t tx="ekr.20220525082933.695">def try_getting_int_literals(self, index: Expression) -&gt; Optional[List[int]]:
    """If the given expression or type corresponds to an int literal
    or a union of int literals, returns a list of the underlying ints.
    Otherwise, returns None.

    Specifically, this function is guaranteed to return a list with
    one or more ints if one one the following is true:

    1. 'expr' is a IntExpr or a UnaryExpr backed by an IntExpr
    2. 'typ' is a LiteralType containing an int
    3. 'typ' is a UnionType containing only LiteralType of ints
    """
    if isinstance(index, IntExpr):
        return [index.value]
    elif isinstance(index, UnaryExpr):
        if index.op == '-':
            operand = index.expr
            if isinstance(operand, IntExpr):
                return [-1 * operand.value]
    typ = get_proper_type(self.accept(index))
    if isinstance(typ, Instance) and typ.last_known_value is not None:
        typ = typ.last_known_value
    if isinstance(typ, LiteralType) and isinstance(typ.value, int):
        return [typ.value]
    if isinstance(typ, UnionType):
        out = []
        for item in get_proper_types(typ.items):
            if isinstance(item, LiteralType) and isinstance(item.value, int):
                out.append(item.value)
            else:
                return None
        return out
    return None

</t>
<t tx="ekr.20220525082933.696">def nonliteral_tuple_index_helper(self, left_type: TupleType, index: Expression) -&gt; Type:
    index_type = self.accept(index)
    expected_type = UnionType.make_union([self.named_type('builtins.int'),
                                          self.named_type('builtins.slice')])
    if not self.chk.check_subtype(index_type, expected_type, index,
                                  message_registry.INVALID_TUPLE_INDEX_TYPE,
                                  'actual type', 'expected type'):
        return AnyType(TypeOfAny.from_error)
    else:
        union = make_simplified_union(left_type.items)
        if isinstance(index, SliceExpr):
            return self.chk.named_generic_type('builtins.tuple', [union])
        else:
            return union

</t>
<t tx="ekr.20220525082933.697">def visit_typeddict_index_expr(self, td_type: TypedDictType,
                               index: Expression,
                               ) -&gt; Type:
    if isinstance(index, (StrExpr, UnicodeExpr)):
        key_names = [index.value]
    else:
        typ = get_proper_type(self.accept(index))
        if isinstance(typ, UnionType):
            key_types: List[Type] = list(typ.items)
        else:
            key_types = [typ]

        key_names = []
        for key_type in get_proper_types(key_types):
            if isinstance(key_type, Instance) and key_type.last_known_value is not None:
                key_type = key_type.last_known_value

            if (isinstance(key_type, LiteralType)
                    and isinstance(key_type.value, str)
                    and key_type.fallback.type.fullname != 'builtins.bytes'):
                key_names.append(key_type.value)
            else:
                self.msg.typeddict_key_must_be_string_literal(td_type, index)
                return AnyType(TypeOfAny.from_error)

    value_types = []
    for key_name in key_names:
        value_type = td_type.items.get(key_name)
        if value_type is None:
            self.msg.typeddict_key_not_found(td_type, key_name, index)
            return AnyType(TypeOfAny.from_error)
        else:
            value_types.append(value_type)
    return make_simplified_union(value_types)

</t>
<t tx="ekr.20220525082933.698">def visit_enum_index_expr(
    self, enum_type: TypeInfo, index: Expression, context: Context
) -&gt; Type:
    string_type: Type = self.named_type("builtins.str")
    if self.chk.options.python_version[0] &lt; 3:
        string_type = UnionType.make_union([string_type,
                                            self.named_type('builtins.unicode')])
    self.chk.check_subtype(self.accept(index), string_type, context,
                           "Enum index should be a string", "actual index type")
    return Instance(enum_type, [])

</t>
<t tx="ekr.20220525082933.699">def visit_cast_expr(self, expr: CastExpr) -&gt; Type:
    """Type check a cast expression."""
    source_type = self.accept(expr.expr, type_context=AnyType(TypeOfAny.special_form),
                              allow_none_return=True, always_allow_any=True)
    target_type = expr.type
    options = self.chk.options
    if (options.warn_redundant_casts and not isinstance(get_proper_type(target_type), AnyType)
            and is_same_type(source_type, target_type)):
        self.msg.redundant_cast(target_type, expr)
    if options.disallow_any_unimported and has_any_from_unimported_type(target_type):
        self.msg.unimported_type_becomes_any("Target type of cast", target_type, expr)
    check_for_explicit_any(target_type, self.chk.options, self.chk.is_typeshed_stub, self.msg,
                           context=expr)
    return target_type

</t>
<t tx="ekr.20220525082933.7">def main() -&gt; None:
    prog, *args = argv

    if not set(args).issubset(cmds):
        print("usage:", prog, " ".join('[%s]' % k for k in cmds))
        print()
        print('Run the given tests. If given no arguments, run everything except mypyc-extra.')
        exit(1)

    if not args:
        args = DEFAULT_COMMANDS[:]

    status = 0

    if 'self' in args and 'lint' in args:
        # Perform lint and self check in parallel as it's faster.
        proc = start_background_cmd('lint')
        cmd_status = run_cmd('self')
        if cmd_status:
            status = cmd_status
        cmd_status = wait_background_cmd('lint', proc)
        if cmd_status:
            status = cmd_status
        args = [arg for arg in args if arg not in ('self', 'lint')]

    for arg in args:
        cmd_status = run_cmd(arg)
        if cmd_status:
            status = cmd_status

    exit(status)


</t>
<t tx="ekr.20220525082933.70">def make_cache(input_dir: str, sqlite: bool) -&gt; MetadataStore:
    if sqlite:
        return SqliteMetadataStore(input_dir)
    else:
        return FilesystemMetadataStore(input_dir)


</t>
<t tx="ekr.20220525082933.700">def visit_assert_type_expr(self, expr: AssertTypeExpr) -&gt; Type:
    source_type = self.accept(expr.expr, type_context=self.type_context[-1],
                              allow_none_return=True, always_allow_any=True)
    target_type = expr.type
    if not is_same_type(source_type, target_type):
        self.msg.assert_type_fail(source_type, target_type, expr)
    return source_type

</t>
<t tx="ekr.20220525082933.701">def visit_reveal_expr(self, expr: RevealExpr) -&gt; Type:
    """Type check a reveal_type expression."""
    if expr.kind == REVEAL_TYPE:
        assert expr.expr is not None
        revealed_type = self.accept(expr.expr, type_context=self.type_context[-1],
                                    allow_none_return=True)
        if not self.chk.current_node_deferred:
            self.msg.reveal_type(revealed_type, expr.expr)
            if not self.chk.in_checked_function():
                self.msg.note("'reveal_type' always outputs 'Any' in unchecked functions",
                              expr.expr)
        return revealed_type
    else:
        # REVEAL_LOCALS
        if not self.chk.current_node_deferred:
            # the RevealExpr contains a local_nodes attribute,
            # calculated at semantic analysis time. Use it to pull out the
            # corresponding subset of variables in self.chk.type_map
            names_to_types = {
                var_node.name: var_node.type for var_node in expr.local_nodes
            } if expr.local_nodes is not None else {}

            self.msg.reveal_locals(names_to_types, expr)
        return NoneType()

</t>
<t tx="ekr.20220525082933.702">def visit_type_application(self, tapp: TypeApplication) -&gt; Type:
    """Type check a type application (expr[type, ...]).

    There are two different options here, depending on whether expr refers
    to a type alias or directly to a generic class. In the first case we need
    to use a dedicated function typeanal.expand_type_aliases. This
    is due to the fact that currently type aliases machinery uses
    unbound type variables, while normal generics use bound ones;
    see TypeAlias docstring for more details.
    """
    if isinstance(tapp.expr, RefExpr) and isinstance(tapp.expr.node, TypeAlias):
        # Subscription of a (generic) alias in runtime context, expand the alias.
        item = expand_type_alias(tapp.expr.node, tapp.types, self.chk.fail,
                                 tapp.expr.node.no_args, tapp)
        item = get_proper_type(item)
        if isinstance(item, Instance):
            tp = type_object_type(item.type, self.named_type)
            return self.apply_type_arguments_to_callable(tp, item.args, tapp)
        else:
            self.chk.fail(message_registry.ONLY_CLASS_APPLICATION, tapp)
            return AnyType(TypeOfAny.from_error)
    # Type application of a normal generic class in runtime context.
    # This is typically used as `x = G[int]()`.
    tp = get_proper_type(self.accept(tapp.expr))
    if isinstance(tp, (CallableType, Overloaded)):
        if not tp.is_type_obj():
            self.chk.fail(message_registry.ONLY_CLASS_APPLICATION, tapp)
        return self.apply_type_arguments_to_callable(tp, tapp.types, tapp)
    if isinstance(tp, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=tp)
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082933.703">def visit_type_alias_expr(self, alias: TypeAliasExpr) -&gt; Type:
    """Right hand side of a type alias definition.

    It has the same type as if the alias itself was used in a runtime context.
    For example, here:

        A = reveal_type(List[T])
        reveal_type(A)

    both `reveal_type` instances will reveal the same type `def (...) -&gt; builtins.list[Any]`.
    Note that type variables are implicitly substituted with `Any`.
    """
    return self.alias_type_in_runtime_context(alias.node, alias.no_args,
                                              alias, alias_definition=True)

</t>
<t tx="ekr.20220525082933.704">def alias_type_in_runtime_context(self, alias: TypeAlias,
                                  no_args: bool, ctx: Context,
                                  *,
                                  alias_definition: bool = False) -&gt; Type:
    """Get type of a type alias (could be generic) in a runtime expression.

    Note that this function can be called only if the alias appears _not_
    as a target of type application, which is treated separately in the
    visit_type_application method. Some examples where this method is called are
    casts and instantiation:

        class LongName(Generic[T]): ...
        A = LongName[int]

        x = A()
        y = cast(A, ...)
    """
    if isinstance(alias.target, Instance) and alias.target.invalid:  # type: ignore
        # An invalid alias, error already has been reported
        return AnyType(TypeOfAny.from_error)
    # If this is a generic alias, we set all variables to `Any`.
    # For example:
    #     A = List[Tuple[T, T]]
    #     x = A() &lt;- same as List[Tuple[Any, Any]], see PEP 484.
    item = get_proper_type(set_any_tvars(alias, ctx.line, ctx.column))
    if isinstance(item, Instance):
        # Normally we get a callable type (or overloaded) with .is_type_obj() true
        # representing the class's constructor
        tp = type_object_type(item.type, self.named_type)
        if no_args:
            return tp
        return self.apply_type_arguments_to_callable(tp, item.args, ctx)
    elif (isinstance(item, TupleType) and
          # Tuple[str, int]() fails at runtime, only named tuples and subclasses work.
          tuple_fallback(item).type.fullname != 'builtins.tuple'):
        return type_object_type(tuple_fallback(item).type, self.named_type)
    elif isinstance(item, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=item)
    else:
        if alias_definition:
            return AnyType(TypeOfAny.special_form)
        # This type is invalid in most runtime contexts, give it an 'object' type.
        return self.named_type('builtins.object')

</t>
<t tx="ekr.20220525082933.705">def apply_type_arguments_to_callable(
    self, tp: Type, args: Sequence[Type], ctx: Context
) -&gt; Type:
    """Apply type arguments to a generic callable type coming from a type object.

    This will first perform type arguments count checks, report the
    error as needed, and return the correct kind of Any. As a special
    case this returns Any for non-callable types, because if type object type
    is not callable, then an error should be already reported.
    """
    tp = get_proper_type(tp)

    if isinstance(tp, CallableType):
        if len(tp.variables) != len(args):
            self.msg.incompatible_type_application(len(tp.variables),
                                                   len(args), ctx)
            return AnyType(TypeOfAny.from_error)
        return self.apply_generic_arguments(tp, args, ctx)
    if isinstance(tp, Overloaded):
        for it in tp.items:
            if len(it.variables) != len(args):
                self.msg.incompatible_type_application(len(it.variables),
                                                       len(args), ctx)
                return AnyType(TypeOfAny.from_error)
        return Overloaded([self.apply_generic_arguments(it, args, ctx)
                           for it in tp.items])
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082933.706">def visit_list_expr(self, e: ListExpr) -&gt; Type:
    """Type check a list expression [...]."""
    return self.check_lst_expr(e, 'builtins.list', '&lt;list&gt;')

</t>
<t tx="ekr.20220525082933.707">def visit_set_expr(self, e: SetExpr) -&gt; Type:
    return self.check_lst_expr(e, 'builtins.set', '&lt;set&gt;')

</t>
<t tx="ekr.20220525082933.708">def fast_container_type(
        self, e: Union[ListExpr, SetExpr, TupleExpr], container_fullname: str
) -&gt; Optional[Type]:
    """
    Fast path to determine the type of a list or set literal,
    based on the list of entries. This mostly impacts large
    module-level constant definitions.

    Limitations:
     - no active type context
     - no star expressions
     - the joined type of all entries must be an Instance or Tuple type
    """
    ctx = self.type_context[-1]
    if ctx:
        return None
    rt = self.resolved_type.get(e, None)
    if rt is not None:
        return rt if isinstance(rt, Instance) else None
    values: List[Type] = []
    for item in e.items:
        if isinstance(item, StarExpr):
            # fallback to slow path
            self.resolved_type[e] = NoneType()
            return None
        values.append(self.accept(item))
    vt = join.join_type_list(values)
    if not allow_fast_container_literal(vt):
        self.resolved_type[e] = NoneType()
        return None
    ct = self.chk.named_generic_type(container_fullname, [vt])
    self.resolved_type[e] = ct
    return ct

</t>
<t tx="ekr.20220525082933.709">def check_lst_expr(self, e: Union[ListExpr, SetExpr, TupleExpr], fullname: str,
                   tag: str) -&gt; Type:
    # fast path
    t = self.fast_container_type(e, fullname)
    if t:
        return t

    # Translate into type checking a generic function call.
    # Used for list and set expressions, as well as for tuples
    # containing star expressions that don't refer to a
    # Tuple. (Note: "lst" stands for list-set-tuple. :-)
    tv = TypeVarType('T', 'T', -1, [], self.object_type())
    constructor = CallableType(
        [tv],
        [nodes.ARG_STAR],
        [None],
        self.chk.named_generic_type(fullname, [tv]),
        self.named_type('builtins.function'),
        name=tag,
        variables=[tv])
    out = self.check_call(constructor,
                          [(i.expr if isinstance(i, StarExpr) else i)
                           for i in e.items],
                          [(nodes.ARG_STAR if isinstance(i, StarExpr) else nodes.ARG_POS)
                           for i in e.items],
                          e)[0]
    return remove_instance_last_known_values(out)

</t>
<t tx="ekr.20220525082933.71">def merge_deps(all: Dict[str, Set[str]], new: Dict[str, Set[str]]) -&gt; None:
    for k, v in new.items():
        all.setdefault(k, set()).update(v)


</t>
<t tx="ekr.20220525082933.710">def visit_tuple_expr(self, e: TupleExpr) -&gt; Type:
    """Type check a tuple expression."""
    # Try to determine type context for type inference.
    type_context = get_proper_type(self.type_context[-1])
    type_context_items = None
    if isinstance(type_context, UnionType):
        tuples_in_context = [t for t in get_proper_types(type_context.items)
                             if (isinstance(t, TupleType) and len(t.items) == len(e.items)) or
                             is_named_instance(t, 'builtins.tuple')]
        if len(tuples_in_context) == 1:
            type_context = tuples_in_context[0]
        else:
            # There are either no relevant tuples in the Union, or there is
            # more than one.  Either way, we can't decide on a context.
            pass

    if isinstance(type_context, TupleType):
        type_context_items = type_context.items
    elif type_context and is_named_instance(type_context, 'builtins.tuple'):
        assert isinstance(type_context, Instance)
        if type_context.args:
            type_context_items = [type_context.args[0]] * len(e.items)
    # NOTE: it's possible for the context to have a different
    # number of items than e.  In that case we use those context
    # items that match a position in e, and we'll worry about type
    # mismatches later.

    # Infer item types.  Give up if there's a star expression
    # that's not a Tuple.
    items: List[Type] = []
    j = 0  # Index into type_context_items; irrelevant if type_context_items is none
    for i in range(len(e.items)):
        item = e.items[i]
        if isinstance(item, StarExpr):
            # Special handling for star expressions.
            # TODO: If there's a context, and item.expr is a
            # TupleExpr, flatten it, so we can benefit from the
            # context?  Counterargument: Why would anyone write
            # (1, *(2, 3)) instead of (1, 2, 3) except in a test?
            tt = self.accept(item.expr)
            tt = get_proper_type(tt)
            if isinstance(tt, TupleType):
                items.extend(tt.items)
                j += len(tt.items)
            else:
                # A star expression that's not a Tuple.
                # Treat the whole thing as a variable-length tuple.
                return self.check_lst_expr(e, 'builtins.tuple', '&lt;tuple&gt;')
        else:
            if not type_context_items or j &gt;= len(type_context_items):
                tt = self.accept(item)
            else:
                tt = self.accept(item, type_context_items[j])
                j += 1
            items.append(tt)
    # This is a partial fallback item type. A precise type will be calculated on demand.
    fallback_item = AnyType(TypeOfAny.special_form)
    return TupleType(items, self.chk.named_generic_type('builtins.tuple', [fallback_item]))

</t>
<t tx="ekr.20220525082933.711">def fast_dict_type(self, e: DictExpr) -&gt; Optional[Type]:
    """
    Fast path to determine the type of a dict literal,
    based on the list of entries. This mostly impacts large
    module-level constant definitions.

    Limitations:
     - no active type context
     - only supported star expressions are other dict instances
     - the joined types of all keys and values must be Instance or Tuple types
    """
    ctx = self.type_context[-1]
    if ctx:
        return None
    rt = self.resolved_type.get(e, None)
    if rt is not None:
        return rt if isinstance(rt, Instance) else None
    keys: List[Type] = []
    values: List[Type] = []
    stargs: Optional[Tuple[Type, Type]] = None
    for key, value in e.items:
        if key is None:
            st = get_proper_type(self.accept(value))
            if (
                    isinstance(st, Instance)
                    and st.type.fullname == 'builtins.dict'
                    and len(st.args) == 2
            ):
                stargs = (st.args[0], st.args[1])
            else:
                self.resolved_type[e] = NoneType()
                return None
        else:
            keys.append(self.accept(key))
            values.append(self.accept(value))
    kt = join.join_type_list(keys)
    vt = join.join_type_list(values)
    if not (allow_fast_container_literal(kt) and allow_fast_container_literal(vt)):
        self.resolved_type[e] = NoneType()
        return None
    if stargs and (stargs[0] != kt or stargs[1] != vt):
        self.resolved_type[e] = NoneType()
        return None
    dt = self.chk.named_generic_type('builtins.dict', [kt, vt])
    self.resolved_type[e] = dt
    return dt

</t>
<t tx="ekr.20220525082933.712">def visit_dict_expr(self, e: DictExpr) -&gt; Type:
    """Type check a dict expression.

    Translate it into a call to dict(), with provisions for **expr.
    """
    # if the dict literal doesn't match TypedDict, check_typeddict_call_with_dict reports
    # an error, but returns the TypedDict type that matches the literal it found
    # that would cause a second error when that TypedDict type is returned upstream
    # to avoid the second error, we always return TypedDict type that was requested
    typeddict_context = self.find_typeddict_context(self.type_context[-1], e)
    if typeddict_context:
        self.check_typeddict_call_with_dict(
            callee=typeddict_context,
            kwargs=e,
            context=e
        )
        return typeddict_context.copy_modified()

    # fast path attempt
    dt = self.fast_dict_type(e)
    if dt:
        return dt

    # Collect function arguments, watching out for **expr.
    args: List[Expression] = []  # Regular "key: value"
    stargs: List[Expression] = []  # For "**expr"
    for key, value in e.items:
        if key is None:
            stargs.append(value)
        else:
            tup = TupleExpr([key, value])
            if key.line &gt;= 0:
                tup.line = key.line
                tup.column = key.column
            else:
                tup.line = value.line
                tup.column = value.column
            args.append(tup)
    # Define type variables (used in constructors below).
    kt = TypeVarType('KT', 'KT', -1, [], self.object_type())
    vt = TypeVarType('VT', 'VT', -2, [], self.object_type())
    rv = None
    # Call dict(*args), unless it's empty and stargs is not.
    if args or not stargs:
        # The callable type represents a function like this:
        #
        #   def &lt;unnamed&gt;(*v: Tuple[kt, vt]) -&gt; Dict[kt, vt]: ...
        constructor = CallableType(
            [TupleType([kt, vt], self.named_type('builtins.tuple'))],
            [nodes.ARG_STAR],
            [None],
            self.chk.named_generic_type('builtins.dict', [kt, vt]),
            self.named_type('builtins.function'),
            name='&lt;dict&gt;',
            variables=[kt, vt])
        rv = self.check_call(constructor, args, [nodes.ARG_POS] * len(args), e)[0]
    else:
        # dict(...) will be called below.
        pass
    # Call rv.update(arg) for each arg in **stargs,
    # except if rv isn't set yet, then set rv = dict(arg).
    if stargs:
        for arg in stargs:
            if rv is None:
                constructor = CallableType(
                    [self.chk.named_generic_type('typing.Mapping', [kt, vt])],
                    [nodes.ARG_POS],
                    [None],
                    self.chk.named_generic_type('builtins.dict', [kt, vt]),
                    self.named_type('builtins.function'),
                    name='&lt;list&gt;',
                    variables=[kt, vt])
                rv = self.check_call(constructor, [arg], [nodes.ARG_POS], arg)[0]
            else:
                self.check_method_call_by_name('update', rv, [arg], [nodes.ARG_POS], arg)
    assert rv is not None
    return rv

</t>
<t tx="ekr.20220525082933.713">def find_typeddict_context(self, context: Optional[Type],
                           dict_expr: DictExpr) -&gt; Optional[TypedDictType]:
    context = get_proper_type(context)
    if isinstance(context, TypedDictType):
        return context
    elif isinstance(context, UnionType):
        items = []
        for item in context.items:
            item_context = self.find_typeddict_context(item, dict_expr)
            if (item_context is not None
                    and self.match_typeddict_call_with_dict(
                        item_context, dict_expr, dict_expr)):
                items.append(item_context)
        if len(items) == 1:
            # Only one union item is valid TypedDict for the given dict_expr, so use the
            # context as it's unambiguous.
            return items[0]
        if len(items) &gt; 1:
            self.msg.typeddict_context_ambiguous(items, dict_expr)
    # No TypedDict type in context.
    return None

</t>
<t tx="ekr.20220525082933.714">def visit_lambda_expr(self, e: LambdaExpr) -&gt; Type:
    """Type check lambda expression."""
    self.chk.check_default_args(e, body_is_trivial=False)
    inferred_type, type_override = self.infer_lambda_type_using_context(e)
    if not inferred_type:
        self.chk.return_types.append(AnyType(TypeOfAny.special_form))
        # Type check everything in the body except for the final return
        # statement (it can contain tuple unpacking before return).
        with self.chk.scope.push_function(e):
            # Lambdas can have more than one element in body,
            # when we add "fictional" AssigmentStatement nodes, like in:
            # `lambda (a, b): a`
            for stmt in e.body.body[:-1]:
                stmt.accept(self.chk)
            # Only type check the return expression, not the return statement.
            # This is important as otherwise the following statements would be
            # considered unreachable. There's no useful type context.
            ret_type = self.accept(e.expr(), allow_none_return=True)
        fallback = self.named_type('builtins.function')
        self.chk.return_types.pop()
        return callable_type(e, fallback, ret_type)
    else:
        # Type context available.
        self.chk.return_types.append(inferred_type.ret_type)
        self.chk.check_func_item(e, type_override=type_override)
        if not self.chk.has_type(e.expr()):
            # TODO: return expression must be accepted before exiting function scope.
            self.accept(e.expr(), allow_none_return=True)
        ret_type = self.chk.lookup_type(e.expr())
        self.chk.return_types.pop()
        return replace_callable_return_type(inferred_type, ret_type)

</t>
<t tx="ekr.20220525082933.715">def infer_lambda_type_using_context(self, e: LambdaExpr) -&gt; Tuple[Optional[CallableType],
                                                                Optional[CallableType]]:
    """Try to infer lambda expression type using context.

    Return None if could not infer type.
    The second item in the return type is the type_override parameter for check_func_item.
    """
    # TODO also accept 'Any' context
    ctx = get_proper_type(self.type_context[-1])

    if isinstance(ctx, UnionType):
        callables = [t for t in get_proper_types(ctx.relevant_items())
                     if isinstance(t, CallableType)]
        if len(callables) == 1:
            ctx = callables[0]

    if not ctx or not isinstance(ctx, CallableType):
        return None, None

    # The context may have function type variables in it. We replace them
    # since these are the type variables we are ultimately trying to infer;
    # they must be considered as indeterminate. We use ErasedType since it
    # does not affect type inference results (it is for purposes like this
    # only).
    callable_ctx = get_proper_type(replace_meta_vars(ctx, ErasedType()))
    assert isinstance(callable_ctx, CallableType)

    if callable_ctx.type_guard is not None:
        # Lambda's return type cannot be treated as a `TypeGuard`,
        # because it is implicit. And `TypeGuard`s must be explicit.
        # See https://github.com/python/mypy/issues/9927
        return None, None

    arg_kinds = [arg.kind for arg in e.arguments]

    if callable_ctx.is_ellipsis_args or ctx.param_spec() is not None:
        # Fill in Any arguments to match the arguments of the lambda.
        callable_ctx = callable_ctx.copy_modified(
            is_ellipsis_args=False,
            arg_types=[AnyType(TypeOfAny.special_form)] * len(arg_kinds),
            arg_kinds=arg_kinds,
            arg_names=e.arg_names[:],
        )

    if ARG_STAR in arg_kinds or ARG_STAR2 in arg_kinds:
        # TODO treat this case appropriately
        return callable_ctx, None

    if callable_ctx.arg_kinds != arg_kinds:
        # Incompatible context; cannot use it to infer types.
        self.chk.fail(message_registry.CANNOT_INFER_LAMBDA_TYPE, e)
        return None, None

    return callable_ctx, callable_ctx

</t>
<t tx="ekr.20220525082933.716">def visit_super_expr(self, e: SuperExpr) -&gt; Type:
    """Type check a super expression (non-lvalue)."""

    # We have an expression like super(T, var).member

    # First compute the types of T and var
    types = self._super_arg_types(e)
    if isinstance(types, tuple):
        type_type, instance_type = types
    else:
        return types

    # Now get the MRO
    type_info = type_info_from_type(type_type)
    if type_info is None:
        self.chk.fail(message_registry.UNSUPPORTED_ARG_1_FOR_SUPER, e)
        return AnyType(TypeOfAny.from_error)

    instance_info = type_info_from_type(instance_type)
    if instance_info is None:
        self.chk.fail(message_registry.UNSUPPORTED_ARG_2_FOR_SUPER, e)
        return AnyType(TypeOfAny.from_error)

    mro = instance_info.mro

    # The base is the first MRO entry *after* type_info that has a member
    # with the right name
    try:
        index = mro.index(type_info)
    except ValueError:
        self.chk.fail(message_registry.SUPER_ARG_2_NOT_INSTANCE_OF_ARG_1, e)
        return AnyType(TypeOfAny.from_error)

    if len(mro) == index + 1:
        self.chk.fail(message_registry.TARGET_CLASS_HAS_NO_BASE_CLASS, e)
        return AnyType(TypeOfAny.from_error)

    for base in mro[index+1:]:
        if e.name in base.names or base == mro[-1]:
            if e.info and e.info.fallback_to_any and base == mro[-1]:
                # There's an undefined base class, and we're at the end of the
                # chain.  That's not an error.
                return AnyType(TypeOfAny.special_form)

            return analyze_member_access(name=e.name,
                                         typ=instance_type,
                                         is_lvalue=False,
                                         is_super=True,
                                         is_operator=False,
                                         original_type=instance_type,
                                         override_info=base,
                                         context=e,
                                         msg=self.msg,
                                         chk=self.chk,
                                         in_literal_context=self.is_literal_context())

    assert False, 'unreachable'

</t>
<t tx="ekr.20220525082933.717">def _super_arg_types(self, e: SuperExpr) -&gt; Union[Type, Tuple[Type, Type]]:
    """
    Computes the types of the type and instance expressions in super(T, instance), or the
    implicit ones for zero-argument super() expressions.  Returns a single type for the whole
    super expression when possible (for errors, anys), otherwise the pair of computed types.
    """

    if not self.chk.in_checked_function():
        return AnyType(TypeOfAny.unannotated)
    elif len(e.call.args) == 0:
        if self.chk.options.python_version[0] == 2:
            self.chk.fail(message_registry.TOO_FEW_ARGS_FOR_SUPER, e)
            return AnyType(TypeOfAny.from_error)
        elif not e.info:
            # This has already been reported by the semantic analyzer.
            return AnyType(TypeOfAny.from_error)
        elif self.chk.scope.active_class():
            self.chk.fail(message_registry.SUPER_OUTSIDE_OF_METHOD_NOT_SUPPORTED, e)
            return AnyType(TypeOfAny.from_error)

        # Zero-argument super() is like super(&lt;current class&gt;, &lt;self&gt;)
        current_type = fill_typevars(e.info)
        type_type: ProperType = TypeType(current_type)

        # Use the type of the self argument, in case it was annotated
        method = self.chk.scope.top_function()
        assert method is not None
        if method.arguments:
            instance_type: Type = method.arguments[0].variable.type or current_type
        else:
            self.chk.fail(message_registry.SUPER_ENCLOSING_POSITIONAL_ARGS_REQUIRED, e)
            return AnyType(TypeOfAny.from_error)
    elif ARG_STAR in e.call.arg_kinds:
        self.chk.fail(message_registry.SUPER_VARARGS_NOT_SUPPORTED, e)
        return AnyType(TypeOfAny.from_error)
    elif set(e.call.arg_kinds) != {ARG_POS}:
        self.chk.fail(message_registry.SUPER_POSITIONAL_ARGS_REQUIRED, e)
        return AnyType(TypeOfAny.from_error)
    elif len(e.call.args) == 1:
        self.chk.fail(message_registry.SUPER_WITH_SINGLE_ARG_NOT_SUPPORTED, e)
        return AnyType(TypeOfAny.from_error)
    elif len(e.call.args) == 2:
        type_type = get_proper_type(self.accept(e.call.args[0]))
        instance_type = self.accept(e.call.args[1])
    else:
        self.chk.fail(message_registry.TOO_MANY_ARGS_FOR_SUPER, e)
        return AnyType(TypeOfAny.from_error)

    # Imprecisely assume that the type is the current class
    if isinstance(type_type, AnyType):
        if e.info:
            type_type = TypeType(fill_typevars(e.info))
        else:
            return AnyType(TypeOfAny.from_another_any, source_any=type_type)
    elif isinstance(type_type, TypeType):
        type_item = type_type.item
        if isinstance(type_item, AnyType):
            if e.info:
                type_type = TypeType(fill_typevars(e.info))
            else:
                return AnyType(TypeOfAny.from_another_any, source_any=type_item)

    if (not isinstance(type_type, TypeType)
            and not (isinstance(type_type, FunctionLike) and type_type.is_type_obj())):
        self.msg.first_argument_for_super_must_be_type(type_type, e)
        return AnyType(TypeOfAny.from_error)

    # Imprecisely assume that the instance is of the current class
    instance_type = get_proper_type(instance_type)
    if isinstance(instance_type, AnyType):
        if e.info:
            instance_type = fill_typevars(e.info)
        else:
            return AnyType(TypeOfAny.from_another_any, source_any=instance_type)
    elif isinstance(instance_type, TypeType):
        instance_item = instance_type.item
        if isinstance(instance_item, AnyType):
            if e.info:
                instance_type = TypeType(fill_typevars(e.info))
            else:
                return AnyType(TypeOfAny.from_another_any, source_any=instance_item)

    return type_type, instance_type

</t>
<t tx="ekr.20220525082933.718">def visit_slice_expr(self, e: SliceExpr) -&gt; Type:
    expected = make_optional_type(self.named_type('builtins.int'))
    for index in [e.begin_index, e.end_index, e.stride]:
        if index:
            t = self.accept(index)
            self.chk.check_subtype(t, expected,
                                   index, message_registry.INVALID_SLICE_INDEX)
    return self.named_type('builtins.slice')

</t>
<t tx="ekr.20220525082933.719">def visit_list_comprehension(self, e: ListComprehension) -&gt; Type:
    return self.check_generator_or_comprehension(
        e.generator, 'builtins.list', '&lt;list-comprehension&gt;')

</t>
<t tx="ekr.20220525082933.72">def load(cache: MetadataStore, s: str) -&gt; Any:
    data = cache.read(s)
    obj = json.loads(data)
    if s.endswith(".meta.json"):
        # For meta files, zero out the mtimes and sort the
        # dependencies to avoid spurious conflicts
        obj["mtime"] = 0
        obj["data_mtime"] = 0
        if "dependencies" in obj:
            all_deps = obj["dependencies"] + obj["suppressed"]
            num_deps = len(obj["dependencies"])
            thing = list(zip(all_deps, obj["dep_prios"], obj["dep_lines"]))

            def unzip(x: Any) -&gt; Any:
                return zip(*x) if x else ((), (), ())

            obj["dependencies"], prios1, lines1 = unzip(sorted(thing[:num_deps]))
            obj["suppressed"], prios2, lines2 = unzip(sorted(thing[num_deps:]))
            obj["dep_prios"] = prios1 + prios2
            obj["dep_lines"] = lines1 + lines2
    if s.endswith(".deps.json"):
        # For deps files, sort the deps to avoid spurious mismatches
        for v in obj.values():
            v.sort()
    return obj


</t>
<t tx="ekr.20220525082933.720">def visit_set_comprehension(self, e: SetComprehension) -&gt; Type:
    return self.check_generator_or_comprehension(
        e.generator, 'builtins.set', '&lt;set-comprehension&gt;')

</t>
<t tx="ekr.20220525082933.721">def visit_generator_expr(self, e: GeneratorExpr) -&gt; Type:
    # If any of the comprehensions use async for, the expression will return an async generator
    # object
    if any(e.is_async):
        typ = 'typing.AsyncGenerator'
        # received type is always None in async generator expressions
        additional_args: List[Type] = [NoneType()]
    else:
        typ = 'typing.Generator'
        # received type and returned type are None
        additional_args = [NoneType(), NoneType()]
    return self.check_generator_or_comprehension(e, typ, '&lt;generator&gt;',
                                                 additional_args=additional_args)

</t>
<t tx="ekr.20220525082933.722">def check_generator_or_comprehension(self, gen: GeneratorExpr,
                                     type_name: str,
                                     id_for_messages: str,
                                     additional_args: Optional[List[Type]] = None) -&gt; Type:
    """Type check a generator expression or a list comprehension."""
    additional_args = additional_args or []
    with self.chk.binder.frame_context(can_skip=True, fall_through=0):
        self.check_for_comp(gen)

        # Infer the type of the list comprehension by using a synthetic generic
        # callable type.
        tv = TypeVarType('T', 'T', -1, [], self.object_type())
        tv_list: List[Type] = [tv]
        constructor = CallableType(
            tv_list,
            [nodes.ARG_POS],
            [None],
            self.chk.named_generic_type(type_name, tv_list + additional_args),
            self.chk.named_type('builtins.function'),
            name=id_for_messages,
            variables=[tv])
        return self.check_call(constructor, [gen.left_expr], [nodes.ARG_POS], gen)[0]

</t>
<t tx="ekr.20220525082933.723">def visit_dictionary_comprehension(self, e: DictionaryComprehension) -&gt; Type:
    """Type check a dictionary comprehension."""
    with self.chk.binder.frame_context(can_skip=True, fall_through=0):
        self.check_for_comp(e)

        # Infer the type of the list comprehension by using a synthetic generic
        # callable type.
        ktdef = TypeVarType('KT', 'KT', -1, [], self.object_type())
        vtdef = TypeVarType('VT', 'VT', -2, [], self.object_type())
        constructor = CallableType(
            [ktdef, vtdef],
            [nodes.ARG_POS, nodes.ARG_POS],
            [None, None],
            self.chk.named_generic_type('builtins.dict', [ktdef, vtdef]),
            self.chk.named_type('builtins.function'),
            name='&lt;dictionary-comprehension&gt;',
            variables=[ktdef, vtdef])
        return self.check_call(constructor,
                               [e.key, e.value], [nodes.ARG_POS, nodes.ARG_POS], e)[0]

</t>
<t tx="ekr.20220525082933.724">def check_for_comp(self, e: Union[GeneratorExpr, DictionaryComprehension]) -&gt; None:
    """Check the for_comp part of comprehensions. That is the part from 'for':
    ... for x in y if z

    Note: This adds the type information derived from the condlists to the current binder.
    """
    for index, sequence, conditions, is_async in zip(e.indices, e.sequences,
                                                     e.condlists, e.is_async):
        if is_async:
            _, sequence_type = self.chk.analyze_async_iterable_item_type(sequence)
        else:
            _, sequence_type = self.chk.analyze_iterable_item_type(sequence)
        self.chk.analyze_index_variables(index, sequence_type, True, e)
        for condition in conditions:
            self.accept(condition)

            # values are only part of the comprehension when all conditions are true
            true_map, false_map = self.chk.find_isinstance_check(condition)

            if true_map:
                self.chk.push_type_map(true_map)

            if codes.REDUNDANT_EXPR in self.chk.options.enabled_error_codes:
                if true_map is None:
                    self.msg.redundant_condition_in_comprehension(False, condition)
                elif false_map is None:
                    self.msg.redundant_condition_in_comprehension(True, condition)

</t>
<t tx="ekr.20220525082933.725">def visit_conditional_expr(self, e: ConditionalExpr, allow_none_return: bool = False) -&gt; Type:
    self.accept(e.cond)
    ctx = self.type_context[-1]

    # Gain type information from isinstance if it is there
    # but only for the current expression
    if_map, else_map = self.chk.find_isinstance_check(e.cond)
    if codes.REDUNDANT_EXPR in self.chk.options.enabled_error_codes:
        if if_map is None:
            self.msg.redundant_condition_in_if(False, e.cond)
        elif else_map is None:
            self.msg.redundant_condition_in_if(True, e.cond)

    if_type = self.analyze_cond_branch(if_map, e.if_expr, context=ctx,
                                       allow_none_return=allow_none_return)

    # we want to keep the narrowest value of if_type for union'ing the branches
    # however, it would be silly to pass a literal as a type context. Pass the
    # underlying fallback type instead.
    if_type_fallback = simple_literal_type(get_proper_type(if_type)) or if_type

    # Analyze the right branch using full type context and store the type
    full_context_else_type = self.analyze_cond_branch(else_map, e.else_expr, context=ctx,
                                                      allow_none_return=allow_none_return)

    if not mypy.checker.is_valid_inferred_type(if_type):
        # Analyze the right branch disregarding the left branch.
        else_type = full_context_else_type
        # we want to keep the narrowest value of else_type for union'ing the branches
        # however, it would be silly to pass a literal as a type context. Pass the
        # underlying fallback type instead.
        else_type_fallback = simple_literal_type(get_proper_type(else_type)) or else_type

        # If it would make a difference, re-analyze the left
        # branch using the right branch's type as context.
        if ctx is None or not is_equivalent(else_type_fallback, ctx):
            # TODO: If it's possible that the previous analysis of
            # the left branch produced errors that are avoided
            # using this context, suppress those errors.
            if_type = self.analyze_cond_branch(if_map, e.if_expr, context=else_type_fallback,
                                               allow_none_return=allow_none_return)

    elif if_type_fallback == ctx:
        # There is no point re-running the analysis if if_type is equal to ctx.
        # That would  be an exact duplicate of the work we just did.
        # This optimization is particularly important to avoid exponential blowup with nested
        # if/else expressions: https://github.com/python/mypy/issues/9591
        # TODO: would checking for is_proper_subtype also work and cover more cases?
        else_type = full_context_else_type
    else:
        # Analyze the right branch in the context of the left
        # branch's type.
        else_type = self.analyze_cond_branch(else_map, e.else_expr, context=if_type_fallback,
                                             allow_none_return=allow_none_return)

    # Only create a union type if the type context is a union, to be mostly
    # compatible with older mypy versions where we always did a join.
    #
    # TODO: Always create a union or at least in more cases?
    if isinstance(get_proper_type(self.type_context[-1]), UnionType):
        res = make_simplified_union([if_type, full_context_else_type])
    else:
        res = join.join_types(if_type, else_type)

    return res

</t>
<t tx="ekr.20220525082933.726">def analyze_cond_branch(self, map: Optional[Dict[Expression, Type]],
                        node: Expression, context: Optional[Type],
                        allow_none_return: bool = False) -&gt; Type:
    with self.chk.binder.frame_context(can_skip=True, fall_through=0):
        if map is None:
            # We still need to type check node, in case we want to
            # process it for isinstance checks later
            self.accept(node, type_context=context, allow_none_return=allow_none_return)
            return UninhabitedType()
        self.chk.push_type_map(map)
        return self.accept(node, type_context=context, allow_none_return=allow_none_return)

</t>
<t tx="ekr.20220525082933.727">def visit_backquote_expr(self, e: BackquoteExpr) -&gt; Type:
    self.accept(e.expr)
    return self.named_type('builtins.str')

</t>
<t tx="ekr.20220525082933.728">#
# Helpers
#

</t>
<t tx="ekr.20220525082933.729">def accept(self,
           node: Expression,
           type_context: Optional[Type] = None,
           allow_none_return: bool = False,
           always_allow_any: bool = False,
           ) -&gt; Type:
    """Type check a node in the given type context.  If allow_none_return
    is True and this expression is a call, allow it to return None.  This
    applies only to this expression and not any subexpressions.
    """
    if node in self.type_overrides:
        return self.type_overrides[node]
    self.type_context.append(type_context)
    try:
        if allow_none_return and isinstance(node, CallExpr):
            typ = self.visit_call_expr(node, allow_none_return=True)
        elif allow_none_return and isinstance(node, YieldFromExpr):
            typ = self.visit_yield_from_expr(node, allow_none_return=True)
        elif allow_none_return and isinstance(node, ConditionalExpr):
            typ = self.visit_conditional_expr(node, allow_none_return=True)
        elif allow_none_return and isinstance(node, AwaitExpr):
            typ = self.visit_await_expr(node, allow_none_return=True)
        else:
            typ = node.accept(self)
    except Exception as err:
        report_internal_error(err, self.chk.errors.file,
                              node.line, self.chk.errors, self.chk.options)

    self.type_context.pop()
    assert typ is not None
    self.chk.store_type(node, typ)

    if (self.chk.options.disallow_any_expr and
            not always_allow_any and
            not self.chk.is_stub and
            self.chk.in_checked_function() and
            has_any_type(typ) and not self.chk.current_node_deferred):
        self.msg.disallowed_any_type(typ, node)

    if not self.chk.in_checked_function() or self.chk.current_node_deferred:
        return AnyType(TypeOfAny.unannotated)
    else:
        return typ

</t>
<t tx="ekr.20220525082933.73">def main() -&gt; None:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--verbose", action="store_true", default=False, help="Increase verbosity"
    )
    parser.add_argument(
        "--sqlite", action="store_true", default=False, help="Use a sqlite cache"
    )
    parser.add_argument("input_dir1", help="Input directory for the cache")
    parser.add_argument("input_dir2", help="Input directory for the cache")
    parser.add_argument("output", help="Output file")
    args = parser.parse_args()

    cache1 = make_cache(args.input_dir1, args.sqlite)
    cache2 = make_cache(args.input_dir2, args.sqlite)

    type_misses: Dict[str, int] = defaultdict(int)
    type_hits: Dict[str, int] = defaultdict(int)

    updates: Dict[str, Optional[str]] = {}

    deps1: Dict[str, Set[str]] = {}
    deps2: Dict[str, Set[str]] = {}

    misses = hits = 0
    cache1_all = list(cache1.list_all())
    for s in cache1_all:
        obj1 = load(cache1, s)
        try:
            obj2 = load(cache2, s)
        except FileNotFoundError:
            obj2 = None

        typ = s.split(".")[-2]
        if obj1 != obj2:
            misses += 1
            type_misses[typ] += 1

            # Collect the dependencies instead of including them directly in the diff
            # so we can produce a much smaller direct diff of them.
            if ".deps." not in s:
                if obj2 is not None:
                    updates[s] = json.dumps(obj2)
                else:
                    updates[s] = None
            elif obj2:
                merge_deps(deps1, obj1)
                merge_deps(deps2, obj2)
        else:
            hits += 1
            type_hits[typ] += 1

    cache1_all_set = set(cache1_all)
    for s in cache2.list_all():
        if s not in cache1_all_set:
            updates[s] = cache2.read(s)

    # Compute what deps have been added and merge them all into the
    # @root deps file.
    new_deps = {k: deps1.get(k, set()) - deps2.get(k, set()) for k in deps2}
    new_deps = {k: v for k, v in new_deps.items() if v}
    try:
        root_deps = load(cache1, "@root.deps.json")
    except FileNotFoundError:
        root_deps = {}
    merge_deps(new_deps, root_deps)

    new_deps_json = {k: list(v) for k, v in new_deps.items() if v}
    updates["@root.deps.json"] = json.dumps(new_deps_json)

    # Drop updates to deps.meta.json for size reasons. The diff
    # applier will manually fix it up.
    updates.pop("./@deps.meta.json", None)
    updates.pop("@deps.meta.json", None)

    ###

    print("Generated incremental cache:", hits, "hits,", misses, "misses")
    if args.verbose:
        print("hits", type_hits)
        print("misses", type_misses)

    with open(args.output, "w") as f:
        json.dump(updates, f)


</t>
<t tx="ekr.20220525082933.730">def named_type(self, name: str) -&gt; Instance:
    """Return an instance type with type given by the name and no type
    arguments. Alias for TypeChecker.named_type.
    """
    return self.chk.named_type(name)

</t>
<t tx="ekr.20220525082933.731">def is_valid_var_arg(self, typ: Type) -&gt; bool:
    """Is a type valid as a *args argument?"""
    typ = get_proper_type(typ)
    return (isinstance(typ, TupleType) or
            is_subtype(typ, self.chk.named_generic_type('typing.Iterable',
                                                        [AnyType(TypeOfAny.special_form)])) or
            isinstance(typ, AnyType) or
            isinstance(typ, ParamSpecType))

</t>
<t tx="ekr.20220525082933.732">def is_valid_keyword_var_arg(self, typ: Type) -&gt; bool:
    """Is a type valid as a **kwargs argument?"""
    ret = (
            is_subtype(typ, self.chk.named_generic_type('typing.Mapping',
                [self.named_type('builtins.str'), AnyType(TypeOfAny.special_form)])) or
            is_subtype(typ, self.chk.named_generic_type('typing.Mapping',
                [UninhabitedType(), UninhabitedType()])) or
            isinstance(typ, ParamSpecType)
    )
    if self.chk.options.python_version[0] &lt; 3:
        ret = ret or is_subtype(typ, self.chk.named_generic_type('typing.Mapping',
            [self.named_type('builtins.unicode'), AnyType(TypeOfAny.special_form)]))
    return ret

</t>
<t tx="ekr.20220525082933.733">def has_member(self, typ: Type, member: str) -&gt; bool:
    """Does type have member with the given name?"""
    # TODO: refactor this to use checkmember.analyze_member_access, otherwise
    # these two should be carefully kept in sync.
    # This is much faster than analyze_member_access, though, and so using
    # it first as a filter is important for performance.
    typ = get_proper_type(typ)

    if isinstance(typ, TypeVarType):
        typ = get_proper_type(typ.upper_bound)
    if isinstance(typ, TupleType):
        typ = tuple_fallback(typ)
    if isinstance(typ, LiteralType):
        typ = typ.fallback
    if isinstance(typ, Instance):
        return typ.type.has_readable_member(member)
    if isinstance(typ, CallableType) and typ.is_type_obj():
        return typ.fallback.type.has_readable_member(member)
    elif isinstance(typ, AnyType):
        return True
    elif isinstance(typ, UnionType):
        result = all(self.has_member(x, member) for x in typ.relevant_items())
        return result
    elif isinstance(typ, TypeType):
        # Type[Union[X, ...]] is always normalized to Union[Type[X], ...],
        # so we don't need to care about unions here.
        item = typ.item
        if isinstance(item, TypeVarType):
            item = get_proper_type(item.upper_bound)
        if isinstance(item, TupleType):
            item = tuple_fallback(item)
        if isinstance(item, Instance) and item.type.metaclass_type is not None:
            return self.has_member(item.type.metaclass_type, member)
        if isinstance(item, AnyType):
            return True
        return False
    else:
        return False

</t>
<t tx="ekr.20220525082933.734">def not_ready_callback(self, name: str, context: Context) -&gt; None:
    """Called when we can't infer the type of a variable because it's not ready yet.

    Either defer type checking of the enclosing function to the next
    pass or report an error.
    """
    self.chk.handle_cannot_determine_type(name, context)

</t>
<t tx="ekr.20220525082933.735">def visit_yield_expr(self, e: YieldExpr) -&gt; Type:
    return_type = self.chk.return_types[-1]
    expected_item_type = self.chk.get_generator_yield_type(return_type, False)
    if e.expr is None:
        if (not isinstance(get_proper_type(expected_item_type), (NoneType, AnyType))
                and self.chk.in_checked_function()):
            self.chk.fail(message_registry.YIELD_VALUE_EXPECTED, e)
    else:
        actual_item_type = self.accept(e.expr, expected_item_type)
        self.chk.check_subtype(actual_item_type, expected_item_type, e,
                               message_registry.INCOMPATIBLE_TYPES_IN_YIELD,
                               'actual type', 'expected type')
    return self.chk.get_generator_receive_type(return_type, False)

</t>
<t tx="ekr.20220525082933.736">def visit_await_expr(self, e: AwaitExpr, allow_none_return: bool = False) -&gt; Type:
    expected_type = self.type_context[-1]
    if expected_type is not None:
        expected_type = self.chk.named_generic_type('typing.Awaitable', [expected_type])
    actual_type = get_proper_type(self.accept(e.expr, expected_type))
    if isinstance(actual_type, AnyType):
        return AnyType(TypeOfAny.from_another_any, source_any=actual_type)
    ret = self.check_awaitable_expr(actual_type, e,
                                    message_registry.INCOMPATIBLE_TYPES_IN_AWAIT)
    if not allow_none_return and isinstance(get_proper_type(ret), NoneType):
        self.chk.msg.does_not_return_value(None, e)
    return ret

</t>
<t tx="ekr.20220525082933.737">def check_awaitable_expr(self, t: Type, ctx: Context, msg: Union[str, ErrorMessage]) -&gt; Type:
    """Check the argument to `await` and extract the type of value.

    Also used by `async for` and `async with`.
    """
    if not self.chk.check_subtype(t, self.named_type('typing.Awaitable'), ctx,
                                  msg, 'actual type', 'expected type'):
        return AnyType(TypeOfAny.special_form)
    else:
        generator = self.check_method_call_by_name('__await__', t, [], [], ctx)[0]
        ret_type = self.chk.get_generator_return_type(generator, False)
        ret_type = get_proper_type(ret_type)
        if isinstance(ret_type, UninhabitedType) and not ret_type.ambiguous:
            self.chk.binder.unreachable()
        return ret_type

</t>
<t tx="ekr.20220525082933.738">def visit_yield_from_expr(self, e: YieldFromExpr, allow_none_return: bool = False) -&gt; Type:
    # NOTE: Whether `yield from` accepts an `async def` decorated
    # with `@types.coroutine` (or `@asyncio.coroutine`) depends on
    # whether the generator containing the `yield from` is itself
    # thus decorated.  But it accepts a generator regardless of
    # how it's decorated.
    return_type = self.chk.return_types[-1]
    # TODO: What should the context for the sub-expression be?
    # If the containing function has type Generator[X, Y, ...],
    # the context should be Generator[X, Y, T], where T is the
    # context of the 'yield from' itself (but it isn't known).
    subexpr_type = get_proper_type(self.accept(e.expr))

    # Check that the expr is an instance of Iterable and get the type of the iterator produced
    # by __iter__.
    if isinstance(subexpr_type, AnyType):
        iter_type: Type = AnyType(TypeOfAny.from_another_any, source_any=subexpr_type)
    elif self.chk.type_is_iterable(subexpr_type):
        if is_async_def(subexpr_type) and not has_coroutine_decorator(return_type):
            self.chk.msg.yield_from_invalid_operand_type(subexpr_type, e)

        any_type = AnyType(TypeOfAny.special_form)
        generic_generator_type = self.chk.named_generic_type('typing.Generator',
                                                             [any_type, any_type, any_type])
        iter_type, _ = self.check_method_call_by_name(
            '__iter__', subexpr_type, [], [], context=generic_generator_type)
    else:
        if not (is_async_def(subexpr_type) and has_coroutine_decorator(return_type)):
            self.chk.msg.yield_from_invalid_operand_type(subexpr_type, e)
            iter_type = AnyType(TypeOfAny.from_error)
        else:
            iter_type = self.check_awaitable_expr(
                subexpr_type, e, message_registry.INCOMPATIBLE_TYPES_IN_YIELD_FROM)

    # Check that the iterator's item type matches the type yielded by the Generator function
    # containing this `yield from` expression.
    expected_item_type = self.chk.get_generator_yield_type(return_type, False)
    actual_item_type = self.chk.get_generator_yield_type(iter_type, False)

    self.chk.check_subtype(actual_item_type, expected_item_type, e,
                           message_registry.INCOMPATIBLE_TYPES_IN_YIELD_FROM,
                           'actual type', 'expected type')

    # Determine the type of the entire yield from expression.
    iter_type = get_proper_type(iter_type)
    if (isinstance(iter_type, Instance) and
            iter_type.type.fullname == 'typing.Generator'):
        expr_type = self.chk.get_generator_return_type(iter_type, False)
    else:
        # Non-Generators don't return anything from `yield from` expressions.
        # However special-case Any (which might be produced by an error).
        actual_item_type = get_proper_type(actual_item_type)
        if isinstance(actual_item_type, AnyType):
            expr_type = AnyType(TypeOfAny.from_another_any, source_any=actual_item_type)
        else:
            # Treat `Iterator[X]` as a shorthand for `Generator[X, None, Any]`.
            expr_type = NoneType()

    if not allow_none_return and isinstance(get_proper_type(expr_type), NoneType):
        self.chk.msg.does_not_return_value(None, e)
    return expr_type

</t>
<t tx="ekr.20220525082933.739">def visit_temp_node(self, e: TempNode) -&gt; Type:
    return e.type

</t>
<t tx="ekr.20220525082933.74">@path C:/Repos/mypy/misc/
@language python
@tabwidth -4
#!/usr/bin/env python3
"""
Parse source files and print the abstract syntax trees.
"""

from typing import Tuple
import sys
import argparse

from mypy.errors import CompileError
from mypy.options import Options
from mypy import defaults
from mypy.parse import parse


@others
if __name__ == '__main__':
    main()
</t>
<t tx="ekr.20220525082933.740">def visit_type_var_expr(self, e: TypeVarExpr) -&gt; Type:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082933.741">def visit_paramspec_expr(self, e: ParamSpecExpr) -&gt; Type:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082933.742">def visit_type_var_tuple_expr(self, e: TypeVarTupleExpr) -&gt; Type:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082933.743">def visit_newtype_expr(self, e: NewTypeExpr) -&gt; Type:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082933.744">def visit_namedtuple_expr(self, e: NamedTupleExpr) -&gt; Type:
    tuple_type = e.info.tuple_type
    if tuple_type:
        if (self.chk.options.disallow_any_unimported and
                has_any_from_unimported_type(tuple_type)):
            self.msg.unimported_type_becomes_any("NamedTuple type", tuple_type, e)
        check_for_explicit_any(tuple_type, self.chk.options, self.chk.is_typeshed_stub,
                               self.msg, context=e)
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082933.745">def visit_enum_call_expr(self, e: EnumCallExpr) -&gt; Type:
    for name, value in zip(e.items, e.values):
        if value is not None:
            typ = self.accept(value)
            if not isinstance(get_proper_type(typ), AnyType):
                var = e.info.names[name].node
                if isinstance(var, Var):
                    # Inline TypeChecker.set_inferred_type(),
                    # without the lvalue.  (This doesn't really do
                    # much, since the value attribute is defined
                    # to have type Any in the typeshed stub.)
                    var.type = typ
                    var.is_inferred = True
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082933.746">def visit_typeddict_expr(self, e: TypedDictExpr) -&gt; Type:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082933.747">def visit__promote_expr(self, e: PromoteExpr) -&gt; Type:
    return e.type

</t>
<t tx="ekr.20220525082933.748">def visit_star_expr(self, e: StarExpr) -&gt; StarType:
    return StarType(self.accept(e.expr))

</t>
<t tx="ekr.20220525082933.749">def object_type(self) -&gt; Instance:
    """Return instance type 'object'."""
    return self.named_type('builtins.object')

</t>
<t tx="ekr.20220525082933.75">def dump(fname: str,
         python_version: Tuple[int, int],
         quiet: bool = False) -&gt; None:
    options = Options()
    options.python_version = python_version
    with open(fname, 'rb') as f:
        s = f.read()
        tree = parse(s, fname, None, errors=None, options=options)
        if not quiet:
            print(tree)


</t>
<t tx="ekr.20220525082933.750">def bool_type(self) -&gt; Instance:
    """Return instance type 'bool'."""
    return self.named_type('builtins.bool')

</t>
<t tx="ekr.20220525082933.751">@overload
def narrow_type_from_binder(self, expr: Expression, known_type: Type) -&gt; Type: ...

</t>
<t tx="ekr.20220525082933.752">@overload
def narrow_type_from_binder(self, expr: Expression, known_type: Type,
                            skip_non_overlapping: bool) -&gt; Optional[Type]: ...

</t>
<t tx="ekr.20220525082933.753">def narrow_type_from_binder(self, expr: Expression, known_type: Type,
                            skip_non_overlapping: bool = False) -&gt; Optional[Type]:
    """Narrow down a known type of expression using information in conditional type binder.

    If 'skip_non_overlapping' is True, return None if the type and restriction are
    non-overlapping.
    """
    if literal(expr) &gt;= LITERAL_TYPE:
        restriction = self.chk.binder.get(expr)
        # If the current node is deferred, some variables may get Any types that they
        # otherwise wouldn't have. We don't want to narrow down these since it may
        # produce invalid inferred Optional[Any] types, at least.
        if restriction and not (isinstance(get_proper_type(known_type), AnyType)
                                and self.chk.current_node_deferred):
            # Note: this call should match the one in narrow_declared_type().
            if (skip_non_overlapping and
                    not is_overlapping_types(known_type, restriction,
                                             prohibit_none_typevar_overlap=True)):
                return None
            return narrow_declared_type(known_type, restriction)
    return known_type


</t>
<t tx="ekr.20220525082933.754">def has_any_type(t: Type, ignore_in_type_obj: bool = False) -&gt; bool:
    """Whether t contains an Any type"""
    return t.accept(HasAnyType(ignore_in_type_obj))


</t>
<t tx="ekr.20220525082933.755">class HasAnyType(types.TypeQuery[bool]):
    @others
</t>
<t tx="ekr.20220525082933.756">def __init__(self, ignore_in_type_obj: bool) -&gt; None:
    super().__init__(any)
    self.ignore_in_type_obj = ignore_in_type_obj

</t>
<t tx="ekr.20220525082933.757">def visit_any(self, t: AnyType) -&gt; bool:
    return t.type_of_any != TypeOfAny.special_form  # special forms are not real Any types

</t>
<t tx="ekr.20220525082933.758">def visit_callable_type(self, t: CallableType) -&gt; bool:
    if self.ignore_in_type_obj and t.is_type_obj():
        return False
    return super().visit_callable_type(t)


</t>
<t tx="ekr.20220525082933.759">def has_coroutine_decorator(t: Type) -&gt; bool:
    """Whether t came from a function decorated with `@coroutine`."""
    t = get_proper_type(t)
    return isinstance(t, Instance) and t.type.fullname == 'typing.AwaitableGenerator'


</t>
<t tx="ekr.20220525082933.76">def main() -&gt; None:
    # Parse a file and dump the AST (or display errors).
    parser = argparse.ArgumentParser(
        description="Parse source files and print the abstract syntax tree (AST).",
    )
    parser.add_argument('--py2', action='store_true', help='parse FILEs as Python 2')
    parser.add_argument('--quiet', action='store_true', help='do not print AST')
    parser.add_argument('FILE', nargs='*', help='files to parse')
    args = parser.parse_args()

    if args.py2:
        pyversion = defaults.PYTHON2_VERSION
    else:
        pyversion = defaults.PYTHON3_VERSION

    status = 0
    for fname in args.FILE:
        try:
            dump(fname, pyversion, args.quiet)
        except CompileError as e:
            for msg in e.messages:
                sys.stderr.write('%s\n' % msg)
            status = 1
    sys.exit(status)


</t>
<t tx="ekr.20220525082933.760">def is_async_def(t: Type) -&gt; bool:
    """Whether t came from a function defined using `async def`."""
    # In check_func_def(), when we see a function decorated with
    # `@typing.coroutine` or `@async.coroutine`, we change the
    # return type to typing.AwaitableGenerator[...], so that its
    # type is compatible with either Generator or Awaitable.
    # But for the check here we need to know whether the original
    # function (before decoration) was an `async def`.  The
    # AwaitableGenerator type conveniently preserves the original
    # type as its 4th parameter (3rd when using 0-origin indexing
    # :-), so that we can recover that information here.
    # (We really need to see whether the original, undecorated
    # function was an `async def`, which is orthogonal to its
    # decorations.)
    t = get_proper_type(t)
    if (isinstance(t, Instance)
            and t.type.fullname == 'typing.AwaitableGenerator'
            and len(t.args) &gt;= 4):
        t = get_proper_type(t.args[3])
    return isinstance(t, Instance) and t.type.fullname == 'typing.Coroutine'


</t>
<t tx="ekr.20220525082933.761">def is_non_empty_tuple(t: Type) -&gt; bool:
    t = get_proper_type(t)
    return isinstance(t, TupleType) and bool(t.items)


</t>
<t tx="ekr.20220525082933.762">def is_duplicate_mapping(mapping: List[int],
                         actual_types: List[Type],
                         actual_kinds: List[ArgKind]) -&gt; bool:
    return (
        len(mapping) &gt; 1
        # Multiple actuals can map to the same formal if they both come from
        # varargs (*args and **kwargs); in this case at runtime it is possible
        # that here are no duplicates. We need to allow this, as the convention
        # f(..., *args, **kwargs) is common enough.
        and not (len(mapping) == 2
                 and actual_kinds[mapping[0]] == nodes.ARG_STAR
                 and actual_kinds[mapping[1]] == nodes.ARG_STAR2)
        # Multiple actuals can map to the same formal if there are multiple
        # **kwargs which cannot be mapped with certainty (non-TypedDict
        # **kwargs).
        and not all(actual_kinds[m] == nodes.ARG_STAR2 and
                    not isinstance(get_proper_type(actual_types[m]), TypedDictType)
                    for m in mapping)
    )


</t>
<t tx="ekr.20220525082933.763">def replace_callable_return_type(c: CallableType, new_ret_type: Type) -&gt; CallableType:
    """Return a copy of a callable type with a different return type."""
    return c.copy_modified(ret_type=new_ret_type)


</t>
<t tx="ekr.20220525082933.764">class ArgInferSecondPassQuery(types.TypeQuery[bool]):
    """Query whether an argument type should be inferred in the second pass.

    The result is True if the type has a type variable in a callable return
    type anywhere. For example, the result for Callable[[], T] is True if t is
    a type variable.
    """
    @others
</t>
<t tx="ekr.20220525082933.765">def __init__(self) -&gt; None:
    super().__init__(any)

</t>
<t tx="ekr.20220525082933.766">def visit_callable_type(self, t: CallableType) -&gt; bool:
    return self.query_types(t.arg_types) or t.accept(HasTypeVarQuery())


</t>
<t tx="ekr.20220525082933.767">class HasTypeVarQuery(types.TypeQuery[bool]):
    """Visitor for querying whether a type has a type variable component."""
    def __init__(self) -&gt; None:
        super().__init__(any)

    def visit_type_var(self, t: TypeVarType) -&gt; bool:
        return True


</t>
<t tx="ekr.20220525082933.768">def has_erased_component(t: Optional[Type]) -&gt; bool:
    return t is not None and t.accept(HasErasedComponentsQuery())


</t>
<t tx="ekr.20220525082933.769">class HasErasedComponentsQuery(types.TypeQuery[bool]):
    """Visitor for querying whether a type has an erased component."""
    def __init__(self) -&gt; None:
        super().__init__(any)

    def visit_erased_type(self, t: ErasedType) -&gt; bool:
        return True


</t>
<t tx="ekr.20220525082933.77">@path C:/Repos/mypy/misc/
@language python
@tabwidth -4
"""Fixer for lib2to3 that inserts mypy annotations into all methods.

The simplest way to run this is to copy it into lib2to3's "fixes"
subdirectory and then run "2to3 -f annotate" over your files.

The fixer transforms e.g.

  def foo(self, bar, baz=12):
      return bar + baz

into

  def foo(self, bar, baz=12):
      # type: (Any, int) -&gt; Any
      return bar + baz

It does not do type inference but it recognizes some basic default
argument values such as numbers and strings (and assumes their type
implies the argument type).

It also uses some basic heuristics to decide whether to ignore the
first argument:

  - always if it's named 'self'
  - if there's a @classmethod decorator

Finally, it knows that __init__() is supposed to return None.
"""


import os
import re

from lib2to3.fixer_base import BaseFix
from lib2to3.patcomp import compile_pattern
from lib2to3.pytree import Leaf, Node
from lib2to3.fixer_util import token, syms, touch_import


@others
</t>
<t tx="ekr.20220525082933.770">def has_uninhabited_component(t: Optional[Type]) -&gt; bool:
    return t is not None and t.accept(HasUninhabitedComponentsQuery())


</t>
<t tx="ekr.20220525082933.771">class HasUninhabitedComponentsQuery(types.TypeQuery[bool]):
    """Visitor for querying whether a type has an UninhabitedType component."""
    def __init__(self) -&gt; None:
        super().__init__(any)

    def visit_uninhabited_type(self, t: UninhabitedType) -&gt; bool:
        return True


</t>
<t tx="ekr.20220525082933.772">def arg_approximate_similarity(actual: Type, formal: Type) -&gt; bool:
    """Return if caller argument (actual) is roughly compatible with signature arg (formal).

    This function is deliberately loose and will report two types are similar
    as long as their "shapes" are plausibly the same.

    This is useful when we're doing error reporting: for example, if we're trying
    to select an overload alternative and there's no exact match, we can use
    this function to help us identify which alternative the user might have
    *meant* to match.
    """
    actual = get_proper_type(actual)
    formal = get_proper_type(formal)

    # Erase typevars: we'll consider them all to have the same "shape".
    if isinstance(actual, TypeVarType):
        actual = erase_to_union_or_bound(actual)
    if isinstance(formal, TypeVarType):
        formal = erase_to_union_or_bound(formal)

    @others
    if isinstance(formal, CallableType):
        if isinstance(actual, (CallableType, Overloaded, TypeType)):
            return True
    if is_typetype_like(actual) and is_typetype_like(formal):
        return True

    # Unions
    if isinstance(actual, UnionType):
        return any(arg_approximate_similarity(item, formal) for item in actual.relevant_items())
    if isinstance(formal, UnionType):
        return any(arg_approximate_similarity(actual, item) for item in formal.relevant_items())

    # TypedDicts
    if isinstance(actual, TypedDictType):
        if isinstance(formal, TypedDictType):
            return True
        return arg_approximate_similarity(actual.fallback, formal)

    # Instances
    # For instances, we mostly defer to the existing is_subtype check.
    if isinstance(formal, Instance):
        if isinstance(actual, CallableType):
            actual = actual.fallback
        if isinstance(actual, Overloaded):
            actual = actual.items[0].fallback
        if isinstance(actual, TupleType):
            actual = tuple_fallback(actual)
        if isinstance(actual, Instance) and formal.type in actual.type.mro:
            # Try performing a quick check as an optimization
            return True

    # Fall back to a standard subtype check for the remaining kinds of type.
    return is_subtype(erasetype.erase_type(actual), erasetype.erase_type(formal))


</t>
<t tx="ekr.20220525082933.773"># Callable or Type[...]-ish types
def is_typetype_like(typ: ProperType) -&gt; bool:
    return (isinstance(typ, TypeType)
            or (isinstance(typ, FunctionLike) and typ.is_type_obj())
            or (isinstance(typ, Instance) and typ.type.fullname == "builtins.type"))

</t>
<t tx="ekr.20220525082933.774">def any_causes_overload_ambiguity(items: List[CallableType],
                                  return_types: List[Type],
                                  arg_types: List[Type],
                                  arg_kinds: List[ArgKind],
                                  arg_names: Optional[Sequence[Optional[str]]]) -&gt; bool:
    """May an argument containing 'Any' cause ambiguous result type on call to overloaded function?

    Note that this sometimes returns True even if there is no ambiguity, since a correct
    implementation would be complex (and the call would be imprecisely typed due to Any
    types anyway).

    Args:
        items: Overload items matching the actual arguments
        arg_types: Actual argument types
        arg_kinds: Actual argument kinds
        arg_names: Actual argument names
    """
    if all_same_types(return_types):
        return False

    actual_to_formal = [
        map_formals_to_actuals(
            arg_kinds, arg_names, item.arg_kinds, item.arg_names, lambda i: arg_types[i])
        for item in items
    ]

    for arg_idx, arg_type in enumerate(arg_types):
        # We ignore Anys in type object callables as ambiguity
        # creators, since that can lead to falsely claiming ambiguity
        # for overloads between Type and Callable.
        if has_any_type(arg_type, ignore_in_type_obj=True):
            matching_formals_unfiltered = [(item_idx, lookup[arg_idx])
                                           for item_idx, lookup in enumerate(actual_to_formal)
                                           if lookup[arg_idx]]

            matching_returns = []
            matching_formals = []
            for item_idx, formals in matching_formals_unfiltered:
                matched_callable = items[item_idx]
                matching_returns.append(matched_callable.ret_type)

                # Note: if an actual maps to multiple formals of differing types within
                # a single callable, then we know at least one of those formals must be
                # a different type then the formal(s) in some other callable.
                # So it's safe to just append everything to the same list.
                for formal in formals:
                    matching_formals.append(matched_callable.arg_types[formal])
            if not all_same_types(matching_formals) and not all_same_types(matching_returns):
                # Any maps to multiple different types, and the return types of these items differ.
                return True
    return False


</t>
<t tx="ekr.20220525082933.775">def all_same_types(types: List[Type]) -&gt; bool:
    if len(types) == 0:
        return True
    return all(is_same_type(t, types[0]) for t in types[1:])


</t>
<t tx="ekr.20220525082933.776">def merge_typevars_in_callables_by_name(
        callables: Sequence[CallableType]) -&gt; Tuple[List[CallableType], List[TypeVarType]]:
    """Takes all the typevars present in the callables and 'combines' the ones with the same name.

    For example, suppose we have two callables with signatures "f(x: T, y: S) -&gt; T" and
    "f(x: List[Tuple[T, S]]) -&gt; Tuple[T, S]". Both callables use typevars named "T" and
    "S", but we treat them as distinct, unrelated typevars. (E.g. they could both have
    distinct ids.)

    If we pass in both callables into this function, it returns a list containing two
    new callables that are identical in signature, but use the same underlying TypeVarType
    for T and S.

    This is useful if we want to take the output lists and "merge" them into one callable
    in some way -- for example, when unioning together overloads.

    Returns both the new list of callables and a list of all distinct TypeVarType objects used.
    """
    output: List[CallableType] = []
    unique_typevars: Dict[str, TypeVarType] = {}
    variables: List[TypeVarType] = []

    for target in callables:
        if target.is_generic():
            target = freshen_function_type_vars(target)

            rename = {}  # Dict[TypeVarId, TypeVar]
            for tv in target.variables:
                name = tv.fullname
                if name not in unique_typevars:
                    # TODO(PEP612): fix for ParamSpecType
                    if isinstance(tv, ParamSpecType):
                        continue
                    assert isinstance(tv, TypeVarType)
                    unique_typevars[name] = tv
                    variables.append(tv)
                rename[tv.id] = unique_typevars[name]

            target = cast(CallableType, expand_type(target, rename))
        output.append(target)

    return output, variables


</t>
<t tx="ekr.20220525082933.777">def try_getting_literal(typ: Type) -&gt; ProperType:
    """If possible, get a more precise literal type for a given type."""
    typ = get_proper_type(typ)
    if isinstance(typ, Instance) and typ.last_known_value is not None:
        return typ.last_known_value
    return typ


</t>
<t tx="ekr.20220525082933.778">def is_expr_literal_type(node: Expression) -&gt; bool:
    """Returns 'true' if the given node is a Literal"""
    if isinstance(node, IndexExpr):
        base = node.base
        return isinstance(base, RefExpr) and base.fullname in LITERAL_TYPE_NAMES
    if isinstance(node, NameExpr):
        underlying = node.node
        return isinstance(underlying, TypeAlias) and isinstance(get_proper_type(underlying.target),
                                                                LiteralType)
    return False


</t>
<t tx="ekr.20220525082933.779">def has_bytes_component(typ: Type, py2: bool = False) -&gt; bool:
    """Is this one of builtin byte types, or a union that contains it?"""
    typ = get_proper_type(typ)
    if py2:
        byte_types = {'builtins.str', 'builtins.bytearray'}
    else:
        byte_types = {'builtins.bytes', 'builtins.bytearray'}
    if isinstance(typ, UnionType):
        return any(has_bytes_component(t) for t in typ.items)
    if isinstance(typ, Instance) and typ.type.fullname in byte_types:
        return True
    return False


</t>
<t tx="ekr.20220525082933.78">class FixAnnotate(BaseFix):

    # This fixer is compatible with the bottom matcher.
    BM_compatible = True

    # This fixer shouldn't run by default.
    explicit = True

    # The pattern to match.
    PATTERN = """
              funcdef&lt; 'def' name=any parameters&lt; '(' [args=any] ')' &gt; ':' suite=any+ &gt;
              """

    counter = None if not os.getenv('MAXFIXES') else int(os.getenv('MAXFIXES'))

    @others
</t>
<t tx="ekr.20220525082933.780">def type_info_from_type(typ: Type) -&gt; Optional[TypeInfo]:
    """Gets the TypeInfo for a type, indirecting through things like type variables and tuples."""
    typ = get_proper_type(typ)
    if isinstance(typ, FunctionLike) and typ.is_type_obj():
        return typ.type_object()
    if isinstance(typ, TypeType):
        typ = typ.item
    if isinstance(typ, TypeVarType):
        typ = get_proper_type(typ.upper_bound)
    if isinstance(typ, TupleType):
        typ = tuple_fallback(typ)
    if isinstance(typ, Instance):
        return typ.type

    # A complicated type. Too tricky, give up.
    # TODO: Do something more clever here.
    return None


</t>
<t tx="ekr.20220525082933.781">def is_operator_method(fullname: Optional[str]) -&gt; bool:
    if fullname is None:
        return False
    short_name = fullname.split('.')[-1]
    return (
        short_name in operators.op_methods.values() or
        short_name in operators.reverse_op_methods.values() or
        short_name in operators.unary_op_methods.values())


</t>
<t tx="ekr.20220525082933.782">def get_partial_instance_type(t: Optional[Type]) -&gt; Optional[PartialType]:
    if t is None or not isinstance(t, PartialType) or t.type is None:
        return None
    return t
</t>
<t tx="ekr.20220525082933.783">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Type checking of attribute access"""

from typing import cast, Callable, Optional, Union, Sequence
from typing_extensions import TYPE_CHECKING

from mypy.types import (
    Type, Instance, AnyType, TupleType, TypedDictType, CallableType, FunctionLike,
    TypeVarLikeType, Overloaded, TypeVarType, UnionType, PartialType, TypeOfAny, LiteralType,
    DeletedType, NoneType, TypeType, has_type_vars, get_proper_type, ProperType, ParamSpecType,
    ENUM_REMOVED_PROPS
)
from mypy.nodes import (
    TypeInfo, FuncBase, Var, FuncDef, SymbolNode, SymbolTable, Context,
    MypyFile, TypeVarExpr, ARG_POS, ARG_STAR, ARG_STAR2, Decorator,
    OverloadedFuncDef, TypeAlias, TempNode, is_final_node,
    SYMBOL_FUNCBASE_TYPES, IndexExpr
)
from mypy.messages import MessageBuilder
from mypy.maptype import map_instance_to_supertype
from mypy.expandtype import expand_type_by_instance, freshen_function_type_vars
from mypy.erasetype import erase_typevars
from mypy.plugin import AttributeContext
from mypy.typeanal import set_any_tvars
from mypy import message_registry
from mypy import subtypes
from mypy import meet
from mypy.typeops import (
    tuple_fallback, bind_self, erase_to_bound, class_callable, type_object_type_from_function,
    make_simplified_union, function_type,
)

if TYPE_CHECKING:  # import for forward declaration only
    import mypy.checker

from mypy import state


@others
</t>
<t tx="ekr.20220525082933.784">class MemberContext:
    """Information and objects needed to type check attribute access.

    Look at the docstring of analyze_member_access for more information.
    """

    @others
</t>
<t tx="ekr.20220525082933.785">def __init__(self,
             is_lvalue: bool,
             is_super: bool,
             is_operator: bool,
             original_type: Type,
             context: Context,
             msg: MessageBuilder,
             chk: 'mypy.checker.TypeChecker',
             self_type: Optional[Type],
             module_symbol_table: Optional[SymbolTable] = None) -&gt; None:
    self.is_lvalue = is_lvalue
    self.is_super = is_super
    self.is_operator = is_operator
    self.original_type = original_type
    self.self_type = self_type or original_type
    self.context = context  # Error context
    self.msg = msg
    self.chk = chk
    self.module_symbol_table = module_symbol_table

</t>
<t tx="ekr.20220525082933.786">def named_type(self, name: str) -&gt; Instance:
    return self.chk.named_type(name)

</t>
<t tx="ekr.20220525082933.787">def not_ready_callback(self, name: str, context: Context) -&gt; None:
    self.chk.handle_cannot_determine_type(name, context)

</t>
<t tx="ekr.20220525082933.788">def copy_modified(self, *, messages: Optional[MessageBuilder] = None,
                  self_type: Optional[Type] = None,
                  is_lvalue: Optional[bool] = None) -&gt; 'MemberContext':
    mx = MemberContext(self.is_lvalue, self.is_super, self.is_operator,
                       self.original_type, self.context, self.msg, self.chk,
                       self.self_type, self.module_symbol_table)
    if messages is not None:
        mx.msg = messages
    if self_type is not None:
        mx.self_type = self_type
    if is_lvalue is not None:
        mx.is_lvalue = is_lvalue
    return mx


</t>
<t tx="ekr.20220525082933.789">def analyze_member_access(name: str,
                          typ: Type,
                          context: Context,
                          is_lvalue: bool,
                          is_super: bool,
                          is_operator: bool,
                          msg: MessageBuilder, *,
                          original_type: Type,
                          chk: 'mypy.checker.TypeChecker',
                          override_info: Optional[TypeInfo] = None,
                          in_literal_context: bool = False,
                          self_type: Optional[Type] = None,
                          module_symbol_table: Optional[SymbolTable] = None) -&gt; Type:
    """Return the type of attribute 'name' of 'typ'.

    The actual implementation is in '_analyze_member_access' and this docstring
    also applies to it.

    This is a general operation that supports various different variations:

      1. lvalue or non-lvalue access (setter or getter access)
      2. supertype access when using super() (is_super == True and
         'override_info' should refer to the supertype)

    'original_type' is the most precise inferred or declared type of the base object
    that we have available. When looking for an attribute of 'typ', we may perform
    recursive calls targeting the fallback type, and 'typ' may become some supertype
    of 'original_type'. 'original_type' is always preserved as the 'typ' type used in
    the initial, non-recursive call. The 'self_type' is a component of 'original_type'
    to which generic self should be bound (a narrower type that has a fallback to instance).
    Currently this is used only for union types.

    'module_symbol_table' is passed to this function if 'typ' is actually a module
    and we want to keep track of the available attributes of the module (since they
    are not available via the type object directly)
    """
    mx = MemberContext(is_lvalue,
                       is_super,
                       is_operator,
                       original_type,
                       context,
                       msg,
                       chk=chk,
                       self_type=self_type,
                       module_symbol_table=module_symbol_table)
    result = _analyze_member_access(name, typ, mx, override_info)
    possible_literal = get_proper_type(result)
    if (in_literal_context and isinstance(possible_literal, Instance) and
            possible_literal.last_known_value is not None):
        return possible_literal.last_known_value
    else:
        return result


</t>
<t tx="ekr.20220525082933.79">def transform(self, node, results):
    if FixAnnotate.counter is not None:
        if FixAnnotate.counter &lt;= 0:
            return
    suite = results['suite']
    children = suite[0].children

    # NOTE: I've reverse-engineered the structure of the parse tree.
    # It's always a list of nodes, the first of which contains the
    # entire suite.  Its children seem to be:
    #
    #   [0] NEWLINE
    #   [1] INDENT
    #   [2...n-2] statements (the first may be a docstring)
    #   [n-1] DEDENT
    #
    # Comments before the suite are part of the INDENT's prefix.
    #
    # "Compact" functions (e.g. "def foo(x, y): return max(x, y)")
    # have a different structure that isn't matched by PATTERN.

    ## print('-'*60)
    ## print(node)
    ## for i, ch in enumerate(children):
    ##     print(i, repr(ch.prefix), repr(ch))

    # Check if there's already an annotation.
    for ch in children:
        if ch.prefix.lstrip().startswith('# type:'):
            return  # There's already a # type: comment here; don't change anything.

    # Compute the annotation
    annot = self.make_annotation(node, results)

    # Insert '# type: {annot}' comment.
    # For reference, see lib2to3/fixes/fix_tuple_params.py in stdlib.
    if len(children) &gt;= 2 and children[1].type == token.INDENT:
        children[1].prefix = '{}# type: {}\n{}'.format(children[1].value, annot, children[1].prefix)
        children[1].changed()
        if FixAnnotate.counter is not None:
            FixAnnotate.counter -= 1

    # Also add 'from typing import Any' at the top.
    if 'Any' in annot:
        touch_import('typing', 'Any', node)

</t>
<t tx="ekr.20220525082933.790">def _analyze_member_access(name: str,
                           typ: Type,
                           mx: MemberContext,
                           override_info: Optional[TypeInfo] = None) -&gt; Type:
    # TODO: This and following functions share some logic with subtypes.find_member;
    #       consider refactoring.
    typ = get_proper_type(typ)
    if isinstance(typ, Instance):
        return analyze_instance_member_access(name, typ, mx, override_info)
    elif isinstance(typ, AnyType):
        # The base object has dynamic type.
        return AnyType(TypeOfAny.from_another_any, source_any=typ)
    elif isinstance(typ, UnionType):
        return analyze_union_member_access(name, typ, mx)
    elif isinstance(typ, FunctionLike) and typ.is_type_obj():
        return analyze_type_callable_member_access(name, typ, mx)
    elif isinstance(typ, TypeType):
        return analyze_type_type_member_access(name, typ, mx, override_info)
    elif isinstance(typ, TupleType):
        # Actually look up from the fallback instance type.
        return _analyze_member_access(name, tuple_fallback(typ), mx, override_info)
    elif isinstance(typ, (LiteralType, FunctionLike)):
        # Actually look up from the fallback instance type.
        return _analyze_member_access(name, typ.fallback, mx, override_info)
    elif isinstance(typ, TypedDictType):
        return analyze_typeddict_access(name, typ, mx, override_info)
    elif isinstance(typ, NoneType):
        return analyze_none_member_access(name, typ, mx)
    elif isinstance(typ, TypeVarLikeType):
        return _analyze_member_access(name, typ.upper_bound, mx, override_info)
    elif isinstance(typ, DeletedType):
        mx.msg.deleted_as_rvalue(typ, mx.context)
        return AnyType(TypeOfAny.from_error)
    if mx.chk.should_suppress_optional_error([typ]):
        return AnyType(TypeOfAny.from_error)
    return mx.msg.has_no_attr(mx.original_type, typ, name, mx.context, mx.module_symbol_table)


</t>
<t tx="ekr.20220525082933.791"># The several functions that follow implement analyze_member_access for various
# types and aren't documented individually.


</t>
<t tx="ekr.20220525082933.792">def analyze_instance_member_access(name: str,
                                   typ: Instance,
                                   mx: MemberContext,
                                   override_info: Optional[TypeInfo]) -&gt; Type:
    if name == '__init__' and not mx.is_super:
        # Accessing __init__ in statically typed code would compromise
        # type safety unless used via super().
        mx.msg.fail(message_registry.CANNOT_ACCESS_INIT, mx.context)
        return AnyType(TypeOfAny.from_error)

    # The base object has an instance type.

    info = typ.type
    if override_info:
        info = override_info

    if (state.find_occurrences and
            info.name == state.find_occurrences[0] and
            name == state.find_occurrences[1]):
        mx.msg.note("Occurrence of '{}.{}'".format(*state.find_occurrences), mx.context)

    # Look up the member. First look up the method dictionary.
    method = info.get_method(name)
    if method and not isinstance(method, Decorator):
        if method.is_property:
            assert isinstance(method, OverloadedFuncDef)
            first_item = cast(Decorator, method.items[0])
            return analyze_var(name, first_item.var, typ, info, mx)
        if mx.is_lvalue:
            mx.msg.cant_assign_to_method(mx.context)
        signature = function_type(method, mx.named_type('builtins.function'))
        signature = freshen_function_type_vars(signature)
        if name == '__new__':
            # __new__ is special and behaves like a static method -- don't strip
            # the first argument.
            pass
        else:
            if isinstance(signature, FunctionLike) and name != '__call__':
                # TODO: use proper treatment of special methods on unions instead
                #       of this hack here and below (i.e. mx.self_type).
                dispatched_type = meet.meet_types(mx.original_type, typ)
                signature = check_self_arg(signature, dispatched_type, method.is_class,
                                           mx.context, name, mx.msg)
            signature = bind_self(signature, mx.self_type, is_classmethod=method.is_class)
        typ = map_instance_to_supertype(typ, method.info)
        member_type = expand_type_by_instance(signature, typ)
        freeze_type_vars(member_type)
        return member_type
    else:
        # Not a method.
        return analyze_member_var_access(name, typ, info, mx)


</t>
<t tx="ekr.20220525082933.793">def analyze_type_callable_member_access(name: str,
                                        typ: FunctionLike,
                                        mx: MemberContext) -&gt; Type:
    # Class attribute.
    # TODO super?
    ret_type = typ.items[0].ret_type
    assert isinstance(ret_type, ProperType)
    if isinstance(ret_type, TupleType):
        ret_type = tuple_fallback(ret_type)
    if isinstance(ret_type, Instance):
        if not mx.is_operator:
            # When Python sees an operator (eg `3 == 4`), it automatically translates that
            # into something like `int.__eq__(3, 4)` instead of `(3).__eq__(4)` as an
            # optimization.
            #
            # While it normally it doesn't matter which of the two versions are used, it
            # does cause inconsistencies when working with classes. For example, translating
            # `int == int` to `int.__eq__(int)` would not work since `int.__eq__` is meant to
            # compare two int _instances_. What we really want is `type(int).__eq__`, which
            # is meant to compare two types or classes.
            #
            # This check makes sure that when we encounter an operator, we skip looking up
            # the corresponding method in the current instance to avoid this edge case.
            # See https://github.com/python/mypy/pull/1787 for more info.
            # TODO: do not rely on same type variables being present in all constructor overloads.
            result = analyze_class_attribute_access(ret_type, name, mx,
                                                    original_vars=typ.items[0].variables)
            if result:
                return result
        # Look up from the 'type' type.
        return _analyze_member_access(name, typ.fallback, mx)
    else:
        assert False, f'Unexpected type {ret_type!r}'


</t>
<t tx="ekr.20220525082933.794">def analyze_type_type_member_access(name: str,
                                    typ: TypeType,
                                    mx: MemberContext,
                                    override_info: Optional[TypeInfo]) -&gt; Type:
    # Similar to analyze_type_callable_attribute_access.
    item = None
    fallback = mx.named_type('builtins.type')
    if isinstance(typ.item, Instance):
        item = typ.item
    elif isinstance(typ.item, AnyType):
        with mx.msg.filter_errors():
            return _analyze_member_access(name, fallback, mx, override_info)
    elif isinstance(typ.item, TypeVarType):
        upper_bound = get_proper_type(typ.item.upper_bound)
        if isinstance(upper_bound, Instance):
            item = upper_bound
        elif isinstance(upper_bound, TupleType):
            item = tuple_fallback(upper_bound)
        elif isinstance(upper_bound, AnyType):
            with mx.msg.filter_errors():
                return _analyze_member_access(name, fallback, mx, override_info)
    elif isinstance(typ.item, TupleType):
        item = tuple_fallback(typ.item)
    elif isinstance(typ.item, FunctionLike) and typ.item.is_type_obj():
        item = typ.item.fallback
    elif isinstance(typ.item, TypeType):
        # Access member on metaclass object via Type[Type[C]]
        if isinstance(typ.item.item, Instance):
            item = typ.item.item.type.metaclass_type
    ignore_messages = False
    if item and not mx.is_operator:
        # See comment above for why operators are skipped
        result = analyze_class_attribute_access(item, name, mx, override_info)
        if result:
            if not (isinstance(get_proper_type(result), AnyType) and item.type.fallback_to_any):
                return result
            else:
                # We don't want errors on metaclass lookup for classes with Any fallback
                ignore_messages = True
    if item is not None:
        fallback = item.type.metaclass_type or fallback

    with mx.msg.filter_errors(filter_errors=ignore_messages):
        return _analyze_member_access(name, fallback, mx, override_info)


</t>
<t tx="ekr.20220525082933.795">def analyze_union_member_access(name: str, typ: UnionType, mx: MemberContext) -&gt; Type:
    with mx.msg.disable_type_names():
        results = []
        for subtype in typ.relevant_items():
            # Self types should be bound to every individual item of a union.
            item_mx = mx.copy_modified(self_type=subtype)
            results.append(_analyze_member_access(name, subtype, item_mx))
    return make_simplified_union(results)


</t>
<t tx="ekr.20220525082933.796">def analyze_none_member_access(name: str, typ: NoneType, mx: MemberContext) -&gt; Type:
    is_python_3 = mx.chk.options.python_version[0] &gt;= 3
    # In Python 2 "None" has exactly the same attributes as "object". Python 3 adds a single
    # extra attribute, "__bool__".
    if is_python_3 and name == '__bool__':
        literal_false = LiteralType(False, fallback=mx.named_type('builtins.bool'))
        return CallableType(arg_types=[],
                            arg_kinds=[],
                            arg_names=[],
                            ret_type=literal_false,
                            fallback=mx.named_type('builtins.function'))
    elif mx.chk.should_suppress_optional_error([typ]):
        return AnyType(TypeOfAny.from_error)
    else:
        return _analyze_member_access(name, mx.named_type('builtins.object'), mx)


</t>
<t tx="ekr.20220525082933.797">def analyze_member_var_access(name: str,
                              itype: Instance,
                              info: TypeInfo,
                              mx: MemberContext) -&gt; Type:
    """Analyse attribute access that does not target a method.

    This is logically part of analyze_member_access and the arguments are similar.

    original_type is the type of E in the expression E.var
    """
    # It was not a method. Try looking up a variable.
    v = lookup_member_var_or_accessor(info, name, mx.is_lvalue)

    vv = v
    if isinstance(vv, Decorator):
        # The associated Var node of a decorator contains the type.
        v = vv.var

    if isinstance(vv, TypeInfo):
        # If the associated variable is a TypeInfo synthesize a Var node for
        # the purposes of type checking.  This enables us to type check things
        # like accessing class attributes on an inner class.
        v = Var(name, type=type_object_type(vv, mx.named_type))
        v.info = info

    if isinstance(vv, TypeAlias) and isinstance(get_proper_type(vv.target), Instance):
        # Similar to the above TypeInfo case, we allow using
        # qualified type aliases in runtime context if it refers to an
        # instance type. For example:
        #     class C:
        #         A = List[int]
        #     x = C.A() &lt;- this is OK
        typ = instance_alias_type(vv, mx.named_type)
        v = Var(name, type=typ)
        v.info = info

    if isinstance(v, Var):
        implicit = info[name].implicit

        # An assignment to final attribute is always an error,
        # independently of types.
        if mx.is_lvalue and not mx.chk.get_final_context():
            check_final_member(name, info, mx.msg, mx.context)

        return analyze_var(name, v, itype, info, mx, implicit=implicit)
    elif isinstance(v, FuncDef):
        assert False, "Did not expect a function"
    elif (not v and name not in ['__getattr__', '__setattr__', '__getattribute__'] and
          not mx.is_operator and mx.module_symbol_table is None):
        # Above we skip ModuleType.__getattr__ etc. if we have a
        # module symbol table, since the symbol table allows precise
        # checking.
        if not mx.is_lvalue:
            for method_name in ('__getattribute__', '__getattr__'):
                method = info.get_method(method_name)

                # __getattribute__ is defined on builtins.object and returns Any, so without
                # the guard this search will always find object.__getattribute__ and conclude
                # that the attribute exists
                if method and method.info.fullname != 'builtins.object':
                    bound_method = analyze_decorator_or_funcbase_access(
                        defn=method, itype=itype, info=info,
                        self_type=mx.self_type, name=method_name, mx=mx)
                    typ = map_instance_to_supertype(itype, method.info)
                    getattr_type = get_proper_type(expand_type_by_instance(bound_method, typ))
                    if isinstance(getattr_type, CallableType):
                        result = getattr_type.ret_type
                    else:
                        result = getattr_type

                    # Call the attribute hook before returning.
                    fullname = f'{method.info.fullname}.{name}'
                    hook = mx.chk.plugin.get_attribute_hook(fullname)
                    if hook:
                        result = hook(AttributeContext(get_proper_type(mx.original_type),
                                                       result, mx.context, mx.chk))
                    return result
        else:
            setattr_meth = info.get_method('__setattr__')
            if setattr_meth and setattr_meth.info.fullname != 'builtins.object':
                bound_type = analyze_decorator_or_funcbase_access(
                    defn=setattr_meth, itype=itype, info=info,
                    self_type=mx.self_type, name=name,
                    mx=mx.copy_modified(is_lvalue=False))
                typ = map_instance_to_supertype(itype, setattr_meth.info)
                setattr_type = get_proper_type(expand_type_by_instance(bound_type, typ))
                if isinstance(setattr_type, CallableType) and len(setattr_type.arg_types) &gt; 0:
                    return setattr_type.arg_types[-1]

    if itype.type.fallback_to_any:
        return AnyType(TypeOfAny.special_form)

    # Could not find the member.
    if mx.is_super:
        mx.msg.undefined_in_superclass(name, mx.context)
        return AnyType(TypeOfAny.from_error)
    else:
        if mx.chk and mx.chk.should_suppress_optional_error([itype]):
            return AnyType(TypeOfAny.from_error)
        return mx.msg.has_no_attr(
            mx.original_type, itype, name, mx.context, mx.module_symbol_table
        )


</t>
<t tx="ekr.20220525082933.798">def check_final_member(name: str, info: TypeInfo, msg: MessageBuilder, ctx: Context) -&gt; None:
    """Give an error if the name being assigned was declared as final."""
    for base in info.mro:
        sym = base.names.get(name)
        if sym and is_final_node(sym.node):
            msg.cant_assign_to_final(name, attr_assign=True, ctx=ctx)


</t>
<t tx="ekr.20220525082933.799">def analyze_descriptor_access(descriptor_type: Type,
                              mx: MemberContext) -&gt; Type:
    """Type check descriptor access.

    Arguments:
        descriptor_type: The type of the descriptor attribute being accessed
            (the type of ``f`` in ``a.f`` when ``f`` is a descriptor).
        mx: The current member access context.
    Return:
        The return type of the appropriate ``__get__`` overload for the descriptor.
    """
    instance_type = get_proper_type(mx.original_type)
    descriptor_type = get_proper_type(descriptor_type)

    if isinstance(descriptor_type, UnionType):
        # Map the access over union types
        return make_simplified_union([
            analyze_descriptor_access(typ, mx)
            for typ in descriptor_type.items
        ])
    elif not isinstance(descriptor_type, Instance):
        return descriptor_type

    if not descriptor_type.type.has_readable_member('__get__'):
        return descriptor_type

    dunder_get = descriptor_type.type.get_method('__get__')
    if dunder_get is None:
        mx.msg.fail(message_registry.DESCRIPTOR_GET_NOT_CALLABLE.format(descriptor_type),
                    mx.context)
        return AnyType(TypeOfAny.from_error)

    bound_method = analyze_decorator_or_funcbase_access(
        defn=dunder_get, itype=descriptor_type, info=descriptor_type.type,
        self_type=descriptor_type, name='__set__', mx=mx)

    typ = map_instance_to_supertype(descriptor_type, dunder_get.info)
    dunder_get_type = expand_type_by_instance(bound_method, typ)

    if isinstance(instance_type, FunctionLike) and instance_type.is_type_obj():
        owner_type = instance_type.items[0].ret_type
        instance_type = NoneType()
    elif isinstance(instance_type, TypeType):
        owner_type = instance_type.item
        instance_type = NoneType()
    else:
        owner_type = instance_type

    callable_name = mx.chk.expr_checker.method_fullname(descriptor_type, "__get__")
    dunder_get_type = mx.chk.expr_checker.transform_callee_type(
        callable_name, dunder_get_type,
        [TempNode(instance_type, context=mx.context),
         TempNode(TypeType.make_normalized(owner_type), context=mx.context)],
        [ARG_POS, ARG_POS], mx.context, object_type=descriptor_type,
    )

    _, inferred_dunder_get_type = mx.chk.expr_checker.check_call(
        dunder_get_type,
        [TempNode(instance_type, context=mx.context),
         TempNode(TypeType.make_normalized(owner_type), context=mx.context)],
        [ARG_POS, ARG_POS], mx.context, object_type=descriptor_type,
        callable_name=callable_name)

    inferred_dunder_get_type = get_proper_type(inferred_dunder_get_type)
    if isinstance(inferred_dunder_get_type, AnyType):
        # check_call failed, and will have reported an error
        return inferred_dunder_get_type

    if not isinstance(inferred_dunder_get_type, CallableType):
        mx.msg.fail(message_registry.DESCRIPTOR_GET_NOT_CALLABLE.format(descriptor_type),
                    mx.context)
        return AnyType(TypeOfAny.from_error)

    return inferred_dunder_get_type.ret_type


</t>
<t tx="ekr.20220525082933.80">def make_annotation(self, node, results):
    name = results['name']
    assert isinstance(name, Leaf), repr(name)
    assert name.type == token.NAME, repr(name)
    decorators = self.get_decorators(node)
    is_method = self.is_method(node)
    if name.value == '__init__' or not self.has_return_exprs(node):
        restype = 'None'
    else:
        restype = 'Any'
    args = results.get('args')
    argtypes = []
    if isinstance(args, Node):
        children = args.children
    elif isinstance(args, Leaf):
        children = [args]
    else:
        children = []
    # Interpret children according to the following grammar:
    # (('*'|'**')? NAME ['=' expr] ','?)*
    stars = inferred_type = ''
    in_default = False
    at_start = True
    for child in children:
        if isinstance(child, Leaf):
            if child.value in ('*', '**'):
                stars += child.value
            elif child.type == token.NAME and not in_default:
                if not is_method or not at_start or 'staticmethod' in decorators:
                    inferred_type = 'Any'
                else:
                    # Always skip the first argument if it's named 'self'.
                    # Always skip the first argument of a class method.
                    if  child.value == 'self' or 'classmethod' in decorators:
                        pass
                    else:
                        inferred_type = 'Any'
            elif child.value == '=':
                in_default = True
            elif in_default and child.value != ',':
                if child.type == token.NUMBER:
                    if re.match(r'\d+[lL]?$', child.value):
                        inferred_type = 'int'
                    else:
                        inferred_type = 'float'  # TODO: complex?
                elif child.type == token.STRING:
                    if child.value.startswith(('u', 'U')):
                        inferred_type = 'unicode'
                    else:
                        inferred_type = 'str'
                elif child.type == token.NAME and child.value in ('True', 'False'):
                    inferred_type = 'bool'
            elif child.value == ',':
                if inferred_type:
                    argtypes.append(stars + inferred_type)
                # Reset
                stars = inferred_type = ''
                in_default = False
                at_start = False
    if inferred_type:
        argtypes.append(stars + inferred_type)
    return '(' + ', '.join(argtypes) + ') -&gt; ' + restype

</t>
<t tx="ekr.20220525082933.800">def instance_alias_type(alias: TypeAlias,
                        named_type: Callable[[str], Instance]) -&gt; Type:
    """Type of a type alias node targeting an instance, when appears in runtime context.

    As usual, we first erase any unbound type variables to Any.
    """
    target: Type = get_proper_type(alias.target)
    assert isinstance(get_proper_type(target),
                      Instance), "Must be called only with aliases to classes"
    target = get_proper_type(set_any_tvars(alias, alias.line, alias.column))
    assert isinstance(target, Instance)
    tp = type_object_type(target.type, named_type)
    return expand_type_by_instance(tp, target)


</t>
<t tx="ekr.20220525082933.801">def analyze_var(name: str,
                var: Var,
                itype: Instance,
                info: TypeInfo,
                mx: MemberContext, *,
                implicit: bool = False) -&gt; Type:
    """Analyze access to an attribute via a Var node.

    This is conceptually part of analyze_member_access and the arguments are similar.

    itype is the class object in which var is defined
    original_type is the type of E in the expression E.var
    if implicit is True, the original Var was created as an assignment to self
    """
    # Found a member variable.
    itype = map_instance_to_supertype(itype, var.info)
    typ = var.type
    if typ:
        if isinstance(typ, PartialType):
            return mx.chk.handle_partial_var_type(typ, mx.is_lvalue, var, mx.context)
        if mx.is_lvalue and var.is_property and not var.is_settable_property:
            # TODO allow setting attributes in subclass (although it is probably an error)
            mx.msg.read_only_property(name, itype.type, mx.context)
        if mx.is_lvalue and var.is_classvar:
            mx.msg.cant_assign_to_classvar(name, mx.context)
        t = get_proper_type(expand_type_by_instance(typ, itype))
        result: Type = t
        typ = get_proper_type(typ)
        if var.is_initialized_in_class and isinstance(typ, FunctionLike) and not typ.is_type_obj():
            if mx.is_lvalue:
                if var.is_property:
                    if not var.is_settable_property:
                        mx.msg.read_only_property(name, itype.type, mx.context)
                else:
                    mx.msg.cant_assign_to_method(mx.context)

            if not var.is_staticmethod:
                # Class-level function objects and classmethods become bound methods:
                # the former to the instance, the latter to the class.
                functype = typ
                # Use meet to narrow original_type to the dispatched type.
                # For example, assume
                # * A.f: Callable[[A1], None] where A1 &lt;: A (maybe A1 == A)
                # * B.f: Callable[[B1], None] where B1 &lt;: B (maybe B1 == B)
                # * x: Union[A1, B1]
                # In `x.f`, when checking `x` against A1 we assume x is compatible with A
                # and similarly for B1 when checking against B
                dispatched_type = meet.meet_types(mx.original_type, itype)
                signature = freshen_function_type_vars(functype)
                signature = check_self_arg(signature, dispatched_type, var.is_classmethod,
                                           mx.context, name, mx.msg)
                signature = bind_self(signature, mx.self_type, var.is_classmethod)
                expanded_signature = get_proper_type(expand_type_by_instance(signature, itype))
                freeze_type_vars(expanded_signature)
                if var.is_property:
                    # A property cannot have an overloaded type =&gt; the cast is fine.
                    assert isinstance(expanded_signature, CallableType)
                    result = expanded_signature.ret_type
                else:
                    result = expanded_signature
    else:
        if not var.is_ready:
            mx.not_ready_callback(var.name, mx.context)
        # Implicit 'Any' type.
        result = AnyType(TypeOfAny.special_form)
    fullname = f'{var.info.fullname}.{name}'
    hook = mx.chk.plugin.get_attribute_hook(fullname)
    if result and not mx.is_lvalue and not implicit:
        result = analyze_descriptor_access(result, mx)
    if hook:
        result = hook(AttributeContext(get_proper_type(mx.original_type),
                                       result, mx.context, mx.chk))
    return result


</t>
<t tx="ekr.20220525082933.802">def freeze_type_vars(member_type: Type) -&gt; None:
    if not isinstance(member_type, ProperType):
        return
    if isinstance(member_type, CallableType):
        for v in member_type.variables:
            v.id.meta_level = 0
    if isinstance(member_type, Overloaded):
        for it in member_type.items:
            for v in it.variables:
                v.id.meta_level = 0


</t>
<t tx="ekr.20220525082933.803">def lookup_member_var_or_accessor(info: TypeInfo, name: str,
                                  is_lvalue: bool) -&gt; Optional[SymbolNode]:
    """Find the attribute/accessor node that refers to a member of a type."""
    # TODO handle lvalues
    node = info.get(name)
    if node:
        return node.node
    else:
        return None


</t>
<t tx="ekr.20220525082933.804">def check_self_arg(functype: FunctionLike,
                   dispatched_arg_type: Type,
                   is_classmethod: bool,
                   context: Context, name: str,
                   msg: MessageBuilder) -&gt; FunctionLike:
    """Check that an instance has a valid type for a method with annotated 'self'.

    For example if the method is defined as:
        class A:
            def f(self: S) -&gt; T: ...
    then for 'x.f' we check that meet(type(x), A) &lt;: S. If the method is overloaded, we
    select only overloads items that satisfy this requirement. If there are no matching
    overloads, an error is generated.

    Note: dispatched_arg_type uses a meet to select a relevant item in case if the
    original type of 'x' is a union. This is done because several special methods
    treat union types in ad-hoc manner, so we can't use MemberContext.self_type yet.
    """
    items = functype.items
    if not items:
        return functype
    new_items = []
    if is_classmethod:
        dispatched_arg_type = TypeType.make_normalized(dispatched_arg_type)
    for item in items:
        if not item.arg_types or item.arg_kinds[0] not in (ARG_POS, ARG_STAR):
            # No positional first (self) argument (*args is okay).
            msg.no_formal_self(name, item, context)
            # This is pretty bad, so just return the original signature if
            # there is at least one such error.
            return functype
        else:
            selfarg = item.arg_types[0]
            if subtypes.is_subtype(dispatched_arg_type, erase_typevars(erase_to_bound(selfarg))):
                new_items.append(item)
            elif isinstance(selfarg, ParamSpecType):
                # TODO: This is not always right. What's the most reasonable thing to do here?
                new_items.append(item)
    if not new_items:
        # Choose first item for the message (it may be not very helpful for overloads).
        msg.incompatible_self_argument(name, dispatched_arg_type, items[0],
                                       is_classmethod, context)
        return functype
    if len(new_items) == 1:
        return new_items[0]
    return Overloaded(new_items)


</t>
<t tx="ekr.20220525082933.805">def analyze_class_attribute_access(itype: Instance,
                                   name: str,
                                   mx: MemberContext,
                                   override_info: Optional[TypeInfo] = None,
                                   original_vars: Optional[Sequence[TypeVarLikeType]] = None
                                   ) -&gt; Optional[Type]:
    """Analyze access to an attribute on a class object.

    itype is the return type of the class object callable, original_type is the type
    of E in the expression E.var, original_vars are type variables of the class callable
    (for generic classes).
    """
    info = itype.type
    if override_info:
        info = override_info

    fullname = '{}.{}'.format(info.fullname, name)
    hook = mx.chk.plugin.get_class_attribute_hook(fullname)

    node = info.get(name)
    if not node:
        if info.fallback_to_any:
            return apply_class_attr_hook(mx, hook, AnyType(TypeOfAny.special_form))
        return None

    is_decorated = isinstance(node.node, Decorator)
    is_method = is_decorated or isinstance(node.node, FuncBase)
    if mx.is_lvalue:
        if is_method:
            mx.msg.cant_assign_to_method(mx.context)
        if isinstance(node.node, TypeInfo):
            mx.msg.fail(message_registry.CANNOT_ASSIGN_TO_TYPE, mx.context)

    # If a final attribute was declared on `self` in `__init__`, then it
    # can't be accessed on the class object.
    if node.implicit and isinstance(node.node, Var) and node.node.is_final:
        mx.msg.fail(message_registry.CANNOT_ACCESS_FINAL_INSTANCE_ATTR
                    .format(node.node.name), mx.context)

    # An assignment to final attribute on class object is also always an error,
    # independently of types.
    if mx.is_lvalue and not mx.chk.get_final_context():
        check_final_member(name, info, mx.msg, mx.context)

    if info.is_enum and not (mx.is_lvalue or is_decorated or is_method):
        enum_class_attribute_type = analyze_enum_class_attribute_access(itype, name, mx)
        if enum_class_attribute_type:
            return apply_class_attr_hook(mx, hook, enum_class_attribute_type)

    t = node.type
    if t:
        if isinstance(t, PartialType):
            symnode = node.node
            assert isinstance(symnode, Var)
            return apply_class_attr_hook(mx, hook,
                                         mx.chk.handle_partial_var_type(t, mx.is_lvalue, symnode,
                                                                        mx.context))

        # Find the class where method/variable was defined.
        if isinstance(node.node, Decorator):
            super_info: Optional[TypeInfo] = node.node.var.info
        elif isinstance(node.node, (Var, SYMBOL_FUNCBASE_TYPES)):
            super_info = node.node.info
        else:
            super_info = None

        # Map the type to how it would look as a defining class. For example:
        #     class C(Generic[T]): ...
        #     class D(C[Tuple[T, S]]): ...
        #     D[int, str].method()
        # Here itype is D[int, str], isuper is C[Tuple[int, str]].
        if not super_info:
            isuper = None
        else:
            isuper = map_instance_to_supertype(itype, super_info)

        if isinstance(node.node, Var):
            assert isuper is not None
            # Check if original variable type has type variables. For example:
            #     class C(Generic[T]):
            #         x: T
            #     C.x  # Error, ambiguous access
            #     C[int].x  # Also an error, since C[int] is same as C at runtime
            if isinstance(t, TypeVarType) or has_type_vars(t):
                # Exception: access on Type[...], including first argument of class methods is OK.
                if not isinstance(get_proper_type(mx.original_type), TypeType) or node.implicit:
                    if node.node.is_classvar:
                        message = message_registry.GENERIC_CLASS_VAR_ACCESS
                    else:
                        message = message_registry.GENERIC_INSTANCE_VAR_CLASS_ACCESS
                    mx.msg.fail(message, mx.context)

            # Erase non-mapped variables, but keep mapped ones, even if there is an error.
            # In the above example this means that we infer following types:
            #     C.x -&gt; Any
            #     C[int].x -&gt; int
            t = erase_typevars(expand_type_by_instance(t, isuper))

        is_classmethod = ((is_decorated and cast(Decorator, node.node).func.is_class)
                          or (isinstance(node.node, FuncBase) and node.node.is_class))
        t = get_proper_type(t)
        if isinstance(t, FunctionLike) and is_classmethod:
            t = check_self_arg(t, mx.self_type, False, mx.context, name, mx.msg)
        result = add_class_tvars(t, isuper, is_classmethod,
                                 mx.self_type, original_vars=original_vars)
        if not mx.is_lvalue:
            result = analyze_descriptor_access(result, mx)

        return apply_class_attr_hook(mx, hook, result)
    elif isinstance(node.node, Var):
        mx.not_ready_callback(name, mx.context)
        return AnyType(TypeOfAny.special_form)

    if isinstance(node.node, TypeVarExpr):
        mx.msg.fail(message_registry.CANNOT_USE_TYPEVAR_AS_EXPRESSION.format(
                    info.name, name), mx.context)
        return AnyType(TypeOfAny.from_error)

    if isinstance(node.node, TypeInfo):
        return type_object_type(node.node, mx.named_type)

    if isinstance(node.node, MypyFile):
        # Reference to a module object.
        return mx.named_type('types.ModuleType')

    if (isinstance(node.node, TypeAlias) and
            isinstance(get_proper_type(node.node.target), Instance)):
        return instance_alias_type(node.node, mx.named_type)

    if is_decorated:
        assert isinstance(node.node, Decorator)
        if node.node.type:
            return apply_class_attr_hook(mx, hook, node.node.type)
        else:
            mx.not_ready_callback(name, mx.context)
            return AnyType(TypeOfAny.from_error)
    else:
        assert isinstance(node.node, FuncBase)
        typ = function_type(node.node, mx.named_type('builtins.function'))
        # Note: if we are accessing class method on class object, the cls argument is bound.
        # Annotated and/or explicit class methods go through other code paths above, for
        # unannotated implicit class methods we do this here.
        if node.node.is_class:
            typ = bind_self(typ, is_classmethod=True)
        return apply_class_attr_hook(mx, hook, typ)


</t>
<t tx="ekr.20220525082933.806">def apply_class_attr_hook(mx: MemberContext,
                          hook: Optional[Callable[[AttributeContext], Type]],
                          result: Type,
                          ) -&gt; Optional[Type]:
    if hook:
        result = hook(AttributeContext(get_proper_type(mx.original_type),
                                       result, mx.context, mx.chk))
    return result


</t>
<t tx="ekr.20220525082933.807">def analyze_enum_class_attribute_access(itype: Instance,
                                        name: str,
                                        mx: MemberContext,
                                        ) -&gt; Optional[Type]:
    # Skip these since Enum will remove it
    if name in ENUM_REMOVED_PROPS:
        return mx.msg.has_no_attr(
            mx.original_type, itype, name, mx.context, mx.module_symbol_table
        )
    # For other names surrendered by underscores, we don't make them Enum members
    if name.startswith('__') and name.endswith("__") and name.replace('_', '') != '':
        return None

    enum_literal = LiteralType(name, fallback=itype)
    return itype.copy_modified(last_known_value=enum_literal)


</t>
<t tx="ekr.20220525082933.808">def analyze_typeddict_access(name: str, typ: TypedDictType,
                             mx: MemberContext, override_info: Optional[TypeInfo]) -&gt; Type:
    if name == '__setitem__':
        if isinstance(mx.context, IndexExpr):
            # Since we can get this during `a['key'] = ...`
            # it is safe to assume that the context is `IndexExpr`.
            item_type = mx.chk.expr_checker.visit_typeddict_index_expr(
                typ, mx.context.index)
        else:
            # It can also be `a.__setitem__(...)` direct call.
            # In this case `item_type` can be `Any`,
            # because we don't have args available yet.
            # TODO: check in `default` plugin that `__setitem__` is correct.
            item_type = AnyType(TypeOfAny.implementation_artifact)
        return CallableType(
            arg_types=[mx.chk.named_type('builtins.str'), item_type],
            arg_kinds=[ARG_POS, ARG_POS],
            arg_names=[None, None],
            ret_type=NoneType(),
            fallback=mx.chk.named_type('builtins.function'),
            name=name,
        )
    elif name == '__delitem__':
        return CallableType(
            arg_types=[mx.chk.named_type('builtins.str')],
            arg_kinds=[ARG_POS],
            arg_names=[None],
            ret_type=NoneType(),
            fallback=mx.chk.named_type('builtins.function'),
            name=name,
        )
    return _analyze_member_access(name, typ.fallback, mx, override_info)


</t>
<t tx="ekr.20220525082933.809">def add_class_tvars(t: ProperType, isuper: Optional[Instance],
                    is_classmethod: bool,
                    original_type: Type,
                    original_vars: Optional[Sequence[TypeVarLikeType]] = None) -&gt; Type:
    """Instantiate type variables during analyze_class_attribute_access,
    e.g T and Q in the following:

    class A(Generic[T]):
        @classmethod
        def foo(cls: Type[Q]) -&gt; Tuple[T, Q]: ...

    class B(A[str]): pass
    B.foo()

    Args:
        t: Declared type of the method (or property)
        isuper: Current instance mapped to the superclass where method was defined, this
            is usually done by map_instance_to_supertype()
        is_classmethod: True if this method is decorated with @classmethod
        original_type: The value of the type B in the expression B.foo() or the corresponding
            component in case of a union (this is used to bind the self-types)
        original_vars: Type variables of the class callable on which the method was accessed
    Returns:
        Expanded method type with added type variables (when needed).
    """
    # TODO: verify consistency between Q and T

    # We add class type variables if the class method is accessed on class object
    # without applied type arguments, this matches the behavior of __init__().
    # For example (continuing the example in docstring):
    #     A       # The type of callable is def [T] () -&gt; A[T], _not_ def () -&gt; A[Any]
    #     A[int]  # The type of callable is def () -&gt; A[int]
    # and
    #     A.foo       # The type is generic def [T] () -&gt; Tuple[T, A[T]]
    #     A[int].foo  # The type is non-generic def () -&gt; Tuple[int, A[int]]
    #
    # This behaviour is useful for defining alternative constructors for generic classes.
    # To achieve such behaviour, we add the class type variables that are still free
    # (i.e. appear in the return type of the class object on which the method was accessed).
    if isinstance(t, CallableType):
        tvars = original_vars if original_vars is not None else []
        if is_classmethod:
            t = freshen_function_type_vars(t)
            t = bind_self(t, original_type, is_classmethod=True)
            assert isuper is not None
            t = cast(CallableType, expand_type_by_instance(t, isuper))
            freeze_type_vars(t)
        return t.copy_modified(variables=list(tvars) + list(t.variables))
    elif isinstance(t, Overloaded):
        return Overloaded([cast(CallableType, add_class_tvars(item, isuper,
                                                              is_classmethod, original_type,
                                                              original_vars=original_vars))
                           for item in t.items])
    if isuper is not None:
        t = cast(ProperType, expand_type_by_instance(t, isuper))
    return t


</t>
<t tx="ekr.20220525082933.81"># The parse tree has a different shape when there is a single
# decorator vs. when there are multiple decorators.
DECORATED = "decorated&lt; (d=decorator | decorators&lt; dd=decorator+ &gt;) funcdef &gt;"
decorated = compile_pattern(DECORATED)

</t>
<t tx="ekr.20220525082933.810">def type_object_type(info: TypeInfo, named_type: Callable[[str], Instance]) -&gt; ProperType:
    """Return the type of a type object.

    For a generic type G with type variables T and S the type is generally of form

      Callable[..., G[T, S]]

    where ... are argument types for the __init__/__new__ method (without the self
    argument). Also, the fallback type will be 'type' instead of 'function'.
    """

    # We take the type from whichever of __init__ and __new__ is first
    # in the MRO, preferring __init__ if there is a tie.
    init_method = info.get('__init__')
    new_method = info.get('__new__')
    if not init_method or not is_valid_constructor(init_method.node):
        # Must be an invalid class definition.
        return AnyType(TypeOfAny.from_error)
    # There *should* always be a __new__ method except the test stubs
    # lack it, so just copy init_method in that situation
    new_method = new_method or init_method
    if not is_valid_constructor(new_method.node):
        # Must be an invalid class definition.
        return AnyType(TypeOfAny.from_error)

    # The two is_valid_constructor() checks ensure this.
    assert isinstance(new_method.node, (SYMBOL_FUNCBASE_TYPES, Decorator))
    assert isinstance(init_method.node, (SYMBOL_FUNCBASE_TYPES, Decorator))

    init_index = info.mro.index(init_method.node.info)
    new_index = info.mro.index(new_method.node.info)

    fallback = info.metaclass_type or named_type('builtins.type')
    if init_index &lt; new_index:
        method: Union[FuncBase, Decorator] = init_method.node
        is_new = False
    elif init_index &gt; new_index:
        method = new_method.node
        is_new = True
    else:
        if init_method.node.info.fullname == 'builtins.object':
            # Both are defined by object.  But if we've got a bogus
            # base class, we can't know for sure, so check for that.
            if info.fallback_to_any:
                # Construct a universal callable as the prototype.
                any_type = AnyType(TypeOfAny.special_form)
                sig = CallableType(arg_types=[any_type, any_type],
                                   arg_kinds=[ARG_STAR, ARG_STAR2],
                                   arg_names=["_args", "_kwds"],
                                   ret_type=any_type,
                                   fallback=named_type('builtins.function'))
                return class_callable(sig, info, fallback, None, is_new=False)

        # Otherwise prefer __init__ in a tie. It isn't clear that this
        # is the right thing, but __new__ caused problems with
        # typeshed (#5647).
        method = init_method.node
        is_new = False
    # Construct callable type based on signature of __init__. Adjust
    # return type and insert type arguments.
    if isinstance(method, FuncBase):
        t = function_type(method, fallback)
    else:
        assert isinstance(method.type, ProperType)
        assert isinstance(method.type, FunctionLike)  # is_valid_constructor() ensures this
        t = method.type
    return type_object_type_from_function(t, info, method.info, fallback, is_new)


</t>
<t tx="ekr.20220525082933.811">def analyze_decorator_or_funcbase_access(
    defn: Union[Decorator, FuncBase],
    itype: Instance,
    info: TypeInfo,
    self_type: Optional[Type],
    name: str,
    mx: MemberContext,
) -&gt; Type:
    """Analyzes the type behind method access.

    The function itself can possibly be decorated.
    See: https://github.com/python/mypy/issues/10409
    """
    if isinstance(defn, Decorator):
        return analyze_var(name, defn.var, itype, info, mx)
    return bind_self(
        function_type(defn,  mx.chk.named_type('builtins.function')),
        original_type=self_type,
    )


</t>
<t tx="ekr.20220525082933.812">def is_valid_constructor(n: Optional[SymbolNode]) -&gt; bool:
    """Does this node represents a valid constructor method?

    This includes normal functions, overloaded functions, and decorators
    that return a callable type.
    """
    if isinstance(n, FuncBase):
        return True
    if isinstance(n, Decorator):
        return isinstance(get_proper_type(n.type), FunctionLike)
    return False
</t>
<t tx="ekr.20220525082933.813">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Pattern checker. This file is conceptually part of TypeChecker."""

from collections import defaultdict
from typing import List, Optional, Tuple, Dict, NamedTuple, Set, Union
from typing_extensions import Final

import mypy.checker
from mypy.checkmember import analyze_member_access
from mypy.expandtype import expand_type_by_instance
from mypy.join import join_types
from mypy.literals import literal_hash
from mypy.maptype import map_instance_to_supertype
from mypy.meet import narrow_declared_type
from mypy import message_registry
from mypy.messages import MessageBuilder
from mypy.nodes import Expression, ARG_POS, TypeAlias, TypeInfo, Var, NameExpr
from mypy.patterns import (
    Pattern, AsPattern, OrPattern, ValuePattern, SequencePattern, StarredPattern, MappingPattern,
    ClassPattern, SingletonPattern
)
from mypy.plugin import Plugin
from mypy.subtypes import is_subtype
from mypy.typeops import try_getting_str_literals_from_type, make_simplified_union, \
    coerce_to_literal
from mypy.types import (
    LiteralType, ProperType, AnyType, TypeOfAny, Instance, Type, UninhabitedType, get_proper_type,
    TypedDictType, TupleType, NoneType, UnionType
)
from mypy.typevars import fill_typevars
from mypy.visitor import PatternVisitor

self_match_type_names: Final = [
    "builtins.bool",
    "builtins.bytearray",
    "builtins.bytes",
    "builtins.dict",
    "builtins.float",
    "builtins.frozenset",
    "builtins.int",
    "builtins.list",
    "builtins.set",
    "builtins.str",
    "builtins.tuple",
]

non_sequence_match_type_names: Final = [
    "builtins.str",
    "builtins.bytes",
    "builtins.bytearray"
]


@others
</t>
<t tx="ekr.20220525082933.814"># For every Pattern a PatternType can be calculated. This requires recursively calculating
# the PatternTypes of the sub-patterns first.
# Using the data in the PatternType the match subject and captured names can be narrowed/inferred.
class PatternType(NamedTuple):
    type: Type  # The type the match subject can be narrowed to
    rest_type: Type  # The remaining type if the pattern didn't match
    captures: Dict[Expression, Type]  # The variables captured by the pattern


</t>
<t tx="ekr.20220525082933.815">class PatternChecker(PatternVisitor[PatternType]):
    """Pattern checker.

    This class checks if a pattern can match a type, what the type can be narrowed to, and what
    type capture patterns should be inferred as.
    """

    # Some services are provided by a TypeChecker instance.
    chk: 'mypy.checker.TypeChecker'
    # This is shared with TypeChecker, but stored also here for convenience.
    msg: MessageBuilder
    # Currently unused
    plugin: Plugin
    # The expression being matched against the pattern
    subject: Expression

    subject_type: Type
    # Type of the subject to check the (sub)pattern against
    type_context: List[Type]
    # Types that match against self instead of their __match_args__ if used as a class pattern
    # Filled in from self_match_type_names
    self_match_types: List[Type]
    # Types that are sequences, but don't match sequence patterns. Filled in from
    # non_sequence_match_type_names
    non_sequence_match_types: List[Type]

    @others
</t>
<t tx="ekr.20220525082933.816">def __init__(self,
             chk: 'mypy.checker.TypeChecker',
             msg: MessageBuilder, plugin: Plugin
             ) -&gt; None:
    self.chk = chk
    self.msg = msg
    self.plugin = plugin

    self.type_context = []
    self.self_match_types = self.generate_types_from_names(self_match_type_names)
    self.non_sequence_match_types = self.generate_types_from_names(
        non_sequence_match_type_names
    )

</t>
<t tx="ekr.20220525082933.817">def accept(self, o: Pattern, type_context: Type) -&gt; PatternType:
    self.type_context.append(type_context)
    result = o.accept(self)
    self.type_context.pop()

    return result

</t>
<t tx="ekr.20220525082933.818">def visit_as_pattern(self, o: AsPattern) -&gt; PatternType:
    current_type = self.type_context[-1]
    if o.pattern is not None:
        pattern_type = self.accept(o.pattern, current_type)
        typ, rest_type, type_map = pattern_type
    else:
        typ, rest_type, type_map = current_type, UninhabitedType(), {}

    if not is_uninhabited(typ) and o.name is not None:
        typ, _ = self.chk.conditional_types_with_intersection(current_type,
                                                              [get_type_range(typ)],
                                                              o,
                                                              default=current_type)
        if not is_uninhabited(typ):
            type_map[o.name] = typ

    return PatternType(typ, rest_type, type_map)

</t>
<t tx="ekr.20220525082933.819">def visit_or_pattern(self, o: OrPattern) -&gt; PatternType:
    current_type = self.type_context[-1]

    #
    # Check all the subpatterns
    #
    pattern_types = []
    for pattern in o.patterns:
        pattern_type = self.accept(pattern, current_type)
        pattern_types.append(pattern_type)
        current_type = pattern_type.rest_type

    #
    # Collect the final type
    #
    types = []
    for pattern_type in pattern_types:
        if not is_uninhabited(pattern_type.type):
            types.append(pattern_type.type)

    #
    # Check the capture types
    #
    capture_types: Dict[Var, List[Tuple[Expression, Type]]] = defaultdict(list)
    # Collect captures from the first subpattern
    for expr, typ in pattern_types[0].captures.items():
        node = get_var(expr)
        capture_types[node].append((expr, typ))

    # Check if other subpatterns capture the same names
    for i, pattern_type in enumerate(pattern_types[1:]):
        vars = {get_var(expr) for expr, _ in pattern_type.captures.items()}
        if capture_types.keys() != vars:
            self.msg.fail(message_registry.OR_PATTERN_ALTERNATIVE_NAMES, o.patterns[i])
        for expr, typ in pattern_type.captures.items():
            node = get_var(expr)
            capture_types[node].append((expr, typ))

    captures: Dict[Expression, Type] = {}
    for var, capture_list in capture_types.items():
        typ = UninhabitedType()
        for _, other in capture_list:
            typ = join_types(typ, other)

        captures[capture_list[0][0]] = typ

    union_type = make_simplified_union(types)
    return PatternType(union_type, current_type, captures)

</t>
<t tx="ekr.20220525082933.82">def get_decorators(self, node):
    """Return a list of decorators found on a function definition.

    This is a list of strings; only simple decorators
    (e.g. @staticmethod) are returned.

    If the function is undecorated or only non-simple decorators
    are found, return [].
    """
    if node.parent is None:
        return []
    results = {}
    if not self.decorated.match(node.parent, results):
        return []
    decorators = results.get('dd') or [results['d']]
    decs = []
    for d in decorators:
        for child in d.children:
            if isinstance(child, Leaf) and child.type == token.NAME:
                decs.append(child.value)
    return decs

</t>
<t tx="ekr.20220525082933.820">def visit_value_pattern(self, o: ValuePattern) -&gt; PatternType:
    current_type = self.type_context[-1]
    typ = self.chk.expr_checker.accept(o.expr)
    typ = coerce_to_literal(typ)
    narrowed_type, rest_type = self.chk.conditional_types_with_intersection(
        current_type,
        [get_type_range(typ)],
        o,
        default=current_type
    )
    if not isinstance(get_proper_type(narrowed_type), (LiteralType, UninhabitedType)):
        return PatternType(narrowed_type, UnionType.make_union([narrowed_type, rest_type]), {})
    return PatternType(narrowed_type, rest_type, {})

</t>
<t tx="ekr.20220525082933.821">def visit_singleton_pattern(self, o: SingletonPattern) -&gt; PatternType:
    current_type = self.type_context[-1]
    value: Union[bool, None] = o.value
    if isinstance(value, bool):
        typ = self.chk.expr_checker.infer_literal_expr_type(value, "builtins.bool")
    elif value is None:
        typ = NoneType()
    else:
        assert False

    narrowed_type, rest_type = self.chk.conditional_types_with_intersection(
        current_type,
        [get_type_range(typ)],
        o,
        default=current_type
    )
    return PatternType(narrowed_type, rest_type, {})

</t>
<t tx="ekr.20220525082933.822">def visit_sequence_pattern(self, o: SequencePattern) -&gt; PatternType:
    #
    # check for existence of a starred pattern
    #
    current_type = get_proper_type(self.type_context[-1])
    if not self.can_match_sequence(current_type):
        return self.early_non_match()
    star_positions = [i for i, p in enumerate(o.patterns) if isinstance(p, StarredPattern)]
    star_position: Optional[int] = None
    if len(star_positions) == 1:
        star_position = star_positions[0]
    elif len(star_positions) &gt;= 2:
        assert False, "Parser should prevent multiple starred patterns"
    required_patterns = len(o.patterns)
    if star_position is not None:
        required_patterns -= 1

    #
    # get inner types of original type
    #
    if isinstance(current_type, TupleType):
        inner_types = current_type.items
        size_diff = len(inner_types) - required_patterns
        if size_diff &lt; 0:
            return self.early_non_match()
        elif size_diff &gt; 0 and star_position is None:
            return self.early_non_match()
    else:
        inner_type = self.get_sequence_type(current_type)
        if inner_type is None:
            inner_type = self.chk.named_type("builtins.object")
        inner_types = [inner_type] * len(o.patterns)

    #
    # match inner patterns
    #
    contracted_new_inner_types: List[Type] = []
    contracted_rest_inner_types: List[Type] = []
    captures: Dict[Expression, Type] = {}

    contracted_inner_types = self.contract_starred_pattern_types(inner_types,
                                                                 star_position,
                                                                 required_patterns)
    can_match = True
    for p, t in zip(o.patterns, contracted_inner_types):
        pattern_type = self.accept(p, t)
        typ, rest, type_map = pattern_type
        if is_uninhabited(typ):
            can_match = False
        else:
            contracted_new_inner_types.append(typ)
            contracted_rest_inner_types.append(rest)
        self.update_type_map(captures, type_map)
    new_inner_types = self.expand_starred_pattern_types(contracted_new_inner_types,
                                                        star_position,
                                                        len(inner_types))
    rest_inner_types = self.expand_starred_pattern_types(contracted_rest_inner_types,
                                                         star_position,
                                                         len(inner_types))

    #
    # Calculate new type
    #
    new_type: Type
    rest_type: Type = current_type
    if not can_match:
        new_type = UninhabitedType()
    elif isinstance(current_type, TupleType):
        narrowed_inner_types = []
        inner_rest_types = []
        for inner_type, new_inner_type in zip(inner_types, new_inner_types):
            narrowed_inner_type, inner_rest_type = \
                self.chk.conditional_types_with_intersection(
                    new_inner_type,
                    [get_type_range(inner_type)],
                    o,
                    default=new_inner_type
                )
            narrowed_inner_types.append(narrowed_inner_type)
            inner_rest_types.append(inner_rest_type)
        if all(not is_uninhabited(typ) for typ in narrowed_inner_types):
            new_type = TupleType(narrowed_inner_types, current_type.partial_fallback)
        else:
            new_type = UninhabitedType()

        if all(is_uninhabited(typ) for typ in inner_rest_types):
            # All subpatterns always match, so we can apply negative narrowing
            rest_type = TupleType(rest_inner_types, current_type.partial_fallback)
    else:
        new_inner_type = UninhabitedType()
        for typ in new_inner_types:
            new_inner_type = join_types(new_inner_type, typ)
        new_type = self.construct_sequence_child(current_type, new_inner_type)
        if is_subtype(new_type, current_type):
            new_type, _ = self.chk.conditional_types_with_intersection(
                current_type,
                [get_type_range(new_type)],
                o,
                default=current_type
            )
        else:
            new_type = current_type
    return PatternType(new_type, rest_type, captures)

</t>
<t tx="ekr.20220525082933.823">def get_sequence_type(self, t: Type) -&gt; Optional[Type]:
    t = get_proper_type(t)
    if isinstance(t, AnyType):
        return AnyType(TypeOfAny.from_another_any, t)
    if isinstance(t, UnionType):
        items = [self.get_sequence_type(item) for item in t.items]
        not_none_items = [item for item in items if item is not None]
        if len(not_none_items) &gt; 0:
            return make_simplified_union(not_none_items)
        else:
            return None

    if self.chk.type_is_iterable(t) and isinstance(t, Instance):
        return self.chk.iterable_item_type(t)
    else:
        return None

</t>
<t tx="ekr.20220525082933.824">def contract_starred_pattern_types(self,
                                   types: List[Type],
                                   star_pos: Optional[int],
                                   num_patterns: int
                                   ) -&gt; List[Type]:
    """
    Contracts a list of types in a sequence pattern depending on the position of a starred
    capture pattern.

    For example if the sequence pattern [a, *b, c] is matched against types [bool, int, str,
    bytes] the contracted types are [bool, Union[int, str], bytes].

    If star_pos in None the types are returned unchanged.
    """
    if star_pos is None:
        return types
    new_types = types[:star_pos]
    star_length = len(types) - num_patterns
    new_types.append(make_simplified_union(types[star_pos:star_pos+star_length]))
    new_types += types[star_pos+star_length:]

    return new_types

</t>
<t tx="ekr.20220525082933.825">def expand_starred_pattern_types(self,
                                 types: List[Type],
                                 star_pos: Optional[int],
                                 num_types: int
                                 ) -&gt; List[Type]:
    """Undoes the contraction done by contract_starred_pattern_types.

    For example if the sequence pattern is [a, *b, c] and types [bool, int, str] are extended
    to length 4 the result is [bool, int, int, str].
    """
    if star_pos is None:
        return types
    new_types = types[:star_pos]
    star_length = num_types - len(types) + 1
    new_types += [types[star_pos]] * star_length
    new_types += types[star_pos+1:]

    return new_types

</t>
<t tx="ekr.20220525082933.826">def visit_starred_pattern(self, o: StarredPattern) -&gt; PatternType:
    captures: Dict[Expression, Type] = {}
    if o.capture is not None:
        list_type = self.chk.named_generic_type('builtins.list', [self.type_context[-1]])
        captures[o.capture] = list_type
    return PatternType(self.type_context[-1], UninhabitedType(), captures)

</t>
<t tx="ekr.20220525082933.827">def visit_mapping_pattern(self, o: MappingPattern) -&gt; PatternType:
    current_type = get_proper_type(self.type_context[-1])
    can_match = True
    captures: Dict[Expression, Type] = {}
    for key, value in zip(o.keys, o.values):
        inner_type = self.get_mapping_item_type(o, current_type, key)
        if inner_type is None:
            can_match = False
            inner_type = self.chk.named_type("builtins.object")
        pattern_type = self.accept(value, inner_type)
        if is_uninhabited(pattern_type.type):
            can_match = False
        else:
            self.update_type_map(captures, pattern_type.captures)

    if o.rest is not None:
        mapping = self.chk.named_type("typing.Mapping")
        if is_subtype(current_type, mapping) and isinstance(current_type, Instance):
            mapping_inst = map_instance_to_supertype(current_type, mapping.type)
            dict_typeinfo = self.chk.lookup_typeinfo("builtins.dict")
            rest_type = Instance(dict_typeinfo, mapping_inst.args)
        else:
            object_type = self.chk.named_type("builtins.object")
            rest_type = self.chk.named_generic_type("builtins.dict",
                                                    [object_type, object_type])

        captures[o.rest] = rest_type

    if can_match:
        # We can't narrow the type here, as Mapping key is invariant.
        new_type = self.type_context[-1]
    else:
        new_type = UninhabitedType()
    return PatternType(new_type, current_type, captures)

</t>
<t tx="ekr.20220525082933.828">def get_mapping_item_type(self,
                          pattern: MappingPattern,
                          mapping_type: Type,
                          key: Expression
                          ) -&gt; Optional[Type]:
    mapping_type = get_proper_type(mapping_type)
    if isinstance(mapping_type, TypedDictType):
        with self.msg.filter_errors() as local_errors:
            result: Optional[Type] = self.chk.expr_checker.visit_typeddict_index_expr(
                mapping_type, key)
            has_local_errors = local_errors.has_new_errors()
        # If we can't determine the type statically fall back to treating it as a normal
        # mapping
        if has_local_errors:
            with self.msg.filter_errors() as local_errors:
                result = self.get_simple_mapping_item_type(pattern,
                                                           mapping_type,
                                                           key)

                if local_errors.has_new_errors():
                    result = None
    else:
        with self.msg.filter_errors():
            result = self.get_simple_mapping_item_type(pattern,
                                                       mapping_type,
                                                       key)
    return result

</t>
<t tx="ekr.20220525082933.829">def get_simple_mapping_item_type(self,
                                 pattern: MappingPattern,
                                 mapping_type: Type,
                                 key: Expression
                                 ) -&gt; Type:
    result, _ = self.chk.expr_checker.check_method_call_by_name('__getitem__',
                                                                mapping_type,
                                                                [key],
                                                                [ARG_POS],
                                                                pattern)
    return result

</t>
<t tx="ekr.20220525082933.83">def is_method(self, node):
    """Return whether the node occurs (directly) inside a class."""
    node = node.parent
    while node is not None:
        if node.type == syms.classdef:
            return True
        if node.type == syms.funcdef:
            return False
        node = node.parent
    return False

</t>
<t tx="ekr.20220525082933.830">def visit_class_pattern(self, o: ClassPattern) -&gt; PatternType:
    current_type = get_proper_type(self.type_context[-1])

    #
    # Check class type
    #
    type_info = o.class_ref.node
    if type_info is None:
        return PatternType(AnyType(TypeOfAny.from_error), AnyType(TypeOfAny.from_error), {})
    if isinstance(type_info, TypeAlias) and not type_info.no_args:
        self.msg.fail(message_registry.CLASS_PATTERN_GENERIC_TYPE_ALIAS, o)
        return self.early_non_match()
    if isinstance(type_info, TypeInfo):
        any_type = AnyType(TypeOfAny.implementation_artifact)
        typ: Type = Instance(type_info, [any_type] * len(type_info.defn.type_vars))
    elif isinstance(type_info, TypeAlias):
        typ = type_info.target
    else:
        if isinstance(type_info, Var):
            name = str(type_info.type)
        else:
            name = type_info.name
        self.msg.fail(message_registry.CLASS_PATTERN_TYPE_REQUIRED.format(name), o.class_ref)
        return self.early_non_match()

    new_type, rest_type = self.chk.conditional_types_with_intersection(
        current_type, [get_type_range(typ)], o, default=current_type
    )
    if is_uninhabited(new_type):
        return self.early_non_match()
    # TODO: Do I need this?
    narrowed_type = narrow_declared_type(current_type, new_type)

    #
    # Convert positional to keyword patterns
    #
    keyword_pairs: List[Tuple[Optional[str], Pattern]] = []
    match_arg_set: Set[str] = set()

    captures: Dict[Expression, Type] = {}

    if len(o.positionals) != 0:
        if self.should_self_match(typ):
            if len(o.positionals) &gt; 1:
                self.msg.fail(message_registry.CLASS_PATTERN_TOO_MANY_POSITIONAL_ARGS, o)
            pattern_type = self.accept(o.positionals[0], narrowed_type)
            if not is_uninhabited(pattern_type.type):
                return PatternType(pattern_type.type,
                                   join_types(rest_type, pattern_type.rest_type),
                                   pattern_type.captures)
            captures = pattern_type.captures
        else:
            with self.msg.filter_errors() as local_errors:
                match_args_type = analyze_member_access("__match_args__", typ, o,
                                                        False, False, False,
                                                        self.msg,
                                                        original_type=typ,
                                                        chk=self.chk)
                has_local_errors = local_errors.has_new_errors()
            if has_local_errors:
                self.msg.fail(message_registry.MISSING_MATCH_ARGS.format(typ), o)
                return self.early_non_match()

            proper_match_args_type = get_proper_type(match_args_type)
            if isinstance(proper_match_args_type, TupleType):
                match_arg_names = get_match_arg_names(proper_match_args_type)

                if len(o.positionals) &gt; len(match_arg_names):
                    self.msg.fail(message_registry.CLASS_PATTERN_TOO_MANY_POSITIONAL_ARGS, o)
                    return self.early_non_match()
            else:
                match_arg_names = [None] * len(o.positionals)

            for arg_name, pos in zip(match_arg_names, o.positionals):
                keyword_pairs.append((arg_name, pos))
                if arg_name is not None:
                    match_arg_set.add(arg_name)

    #
    # Check for duplicate patterns
    #
    keyword_arg_set = set()
    has_duplicates = False
    for key, value in zip(o.keyword_keys, o.keyword_values):
        keyword_pairs.append((key, value))
        if key in match_arg_set:
            self.msg.fail(
                message_registry.CLASS_PATTERN_KEYWORD_MATCHES_POSITIONAL.format(key),
                value
            )
            has_duplicates = True
        elif key in keyword_arg_set:
            self.msg.fail(message_registry.CLASS_PATTERN_DUPLICATE_KEYWORD_PATTERN.format(key),
                          value)
            has_duplicates = True
        keyword_arg_set.add(key)

    if has_duplicates:
        return self.early_non_match()

    #
    # Check keyword patterns
    #
    can_match = True
    for keyword, pattern in keyword_pairs:
        key_type: Optional[Type] = None
        with self.msg.filter_errors() as local_errors:
            if keyword is not None:
                key_type = analyze_member_access(keyword,
                                                 narrowed_type,
                                                 pattern,
                                                 False,
                                                 False,
                                                 False,
                                                 self.msg,
                                                 original_type=new_type,
                                                 chk=self.chk)
            else:
                key_type = AnyType(TypeOfAny.from_error)
            has_local_errors = local_errors.has_new_errors()
        if has_local_errors or key_type is None:
            key_type = AnyType(TypeOfAny.from_error)
            self.msg.fail(message_registry.CLASS_PATTERN_UNKNOWN_KEYWORD.format(typ, keyword),
                          pattern)

        inner_type, inner_rest_type, inner_captures = self.accept(pattern, key_type)
        if is_uninhabited(inner_type):
            can_match = False
        else:
            self.update_type_map(captures, inner_captures)
            if not is_uninhabited(inner_rest_type):
                rest_type = current_type

    if not can_match:
        new_type = UninhabitedType()
    return PatternType(new_type, rest_type, captures)

</t>
<t tx="ekr.20220525082933.831">def should_self_match(self, typ: Type) -&gt; bool:
    typ = get_proper_type(typ)
    if isinstance(typ, Instance) and typ.type.is_named_tuple:
        return False
    for other in self.self_match_types:
        if is_subtype(typ, other):
            return True
    return False

</t>
<t tx="ekr.20220525082933.832">def can_match_sequence(self, typ: ProperType) -&gt; bool:
    if isinstance(typ, UnionType):
        return any(self.can_match_sequence(get_proper_type(item)) for item in typ.items)
    for other in self.non_sequence_match_types:
        # We have to ignore promotions, as memoryview should match, but bytes,
        # which it can be promoted to, shouldn't
        if is_subtype(typ, other, ignore_promotions=True):
            return False
    sequence = self.chk.named_type("typing.Sequence")
    # If the static type is more general than sequence the actual type could still match
    return is_subtype(typ, sequence) or is_subtype(sequence, typ)

</t>
<t tx="ekr.20220525082933.833">def generate_types_from_names(self, type_names: List[str]) -&gt; List[Type]:
    types: List[Type] = []
    for name in type_names:
        try:
            types.append(self.chk.named_type(name))
        except KeyError as e:
            # Some built in types are not defined in all test cases
            if not name.startswith('builtins.'):
                raise e
            pass

    return types

</t>
<t tx="ekr.20220525082933.834">def update_type_map(self,
                    original_type_map: Dict[Expression, Type],
                    extra_type_map: Dict[Expression, Type]
                    ) -&gt; None:
    # Calculating this would not be needed if TypeMap directly used literal hashes instead of
    # expressions, as suggested in the TODO above it's definition
    already_captured = {literal_hash(expr) for expr in original_type_map}
    for expr, typ in extra_type_map.items():
        if literal_hash(expr) in already_captured:
            node = get_var(expr)
            self.msg.fail(message_registry.MULTIPLE_ASSIGNMENTS_IN_PATTERN.format(node.name),
                          expr)
        else:
            original_type_map[expr] = typ

</t>
<t tx="ekr.20220525082933.835">def construct_sequence_child(self, outer_type: Type, inner_type: Type) -&gt; Type:
    """
    If outer_type is a child class of typing.Sequence returns a new instance of
    outer_type, that is a Sequence of inner_type. If outer_type is not a child class of
    typing.Sequence just returns a Sequence of inner_type

    For example:
    construct_sequence_child(List[int], str) = List[str]
    """
    proper_type = get_proper_type(outer_type)
    if isinstance(proper_type, UnionType):
        types = [
            self.construct_sequence_child(item, inner_type) for item in proper_type.items
            if self.can_match_sequence(get_proper_type(item))
        ]
        return make_simplified_union(types)
    sequence = self.chk.named_generic_type("typing.Sequence", [inner_type])
    if is_subtype(outer_type, self.chk.named_type("typing.Sequence")):
        proper_type = get_proper_type(outer_type)
        assert isinstance(proper_type, Instance)
        empty_type = fill_typevars(proper_type.type)
        partial_type = expand_type_by_instance(empty_type, sequence)
        return expand_type_by_instance(partial_type, proper_type)
    else:
        return sequence

</t>
<t tx="ekr.20220525082933.836">def early_non_match(self) -&gt; PatternType:
    return PatternType(UninhabitedType(), self.type_context[-1], {})


</t>
<t tx="ekr.20220525082933.837">def get_match_arg_names(typ: TupleType) -&gt; List[Optional[str]]:
    args: List[Optional[str]] = []
    for item in typ.items:
        values = try_getting_str_literals_from_type(item)
        if values is None or len(values) != 1:
            args.append(None)
        else:
            args.append(values[0])
    return args


</t>
<t tx="ekr.20220525082933.838">def get_var(expr: Expression) -&gt; Var:
    """
    Warning: this in only true for expressions captured by a match statement.
    Don't call it from anywhere else
    """
    assert isinstance(expr, NameExpr)
    node = expr.node
    assert isinstance(node, Var)
    return node


</t>
<t tx="ekr.20220525082933.839">def get_type_range(typ: Type) -&gt; 'mypy.checker.TypeRange':
    typ = get_proper_type(typ)
    if (isinstance(typ, Instance)
            and typ.last_known_value
            and isinstance(typ.last_known_value.value, bool)):
        typ = typ.last_known_value
    return mypy.checker.TypeRange(typ, is_upper_bound=False)


</t>
<t tx="ekr.20220525082933.84">RETURN_EXPR = "return_stmt&lt; 'return' any &gt;"
return_expr = compile_pattern(RETURN_EXPR)

</t>
<t tx="ekr.20220525082933.840">def is_uninhabited(typ: Type) -&gt; bool:
    return isinstance(get_proper_type(typ), UninhabitedType)
</t>
<t tx="ekr.20220525082933.841">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""
Format expression type checker.

This file is conceptually part of ExpressionChecker and TypeChecker. Main functionality
is located in StringFormatterChecker.check_str_format_call() for '{}'.format(), and in
StringFormatterChecker.check_str_interpolation() for printf-style % interpolation.

Note that although at runtime format strings are parsed using custom parsers,
here we use a regexp-based approach. This way we 99% match runtime behaviour while keeping
implementation simple.
"""

import re

from typing import (
    cast, List, Tuple, Dict, Callable, Union, Optional, Pattern, Match, Set
)
from typing_extensions import Final, TYPE_CHECKING, TypeAlias as _TypeAlias

from mypy.errors import Errors
from mypy.types import (
    Type, AnyType, TupleType, Instance, UnionType, TypeOfAny, get_proper_type, TypeVarType,
    LiteralType, get_proper_types
)
from mypy.nodes import (
    StrExpr, BytesExpr, UnicodeExpr, TupleExpr, DictExpr, Context, Expression, StarExpr, CallExpr,
    IndexExpr, MemberExpr, TempNode, ARG_POS, ARG_STAR, ARG_NAMED, ARG_STAR2,
    Node, MypyFile, ExpressionStmt, NameExpr, IntExpr
)
import mypy.errorcodes as codes

if TYPE_CHECKING:
    # break import cycle only needed for mypy
    import mypy.checker
    import mypy.checkexpr
from mypy import message_registry
from mypy.messages import MessageBuilder
from mypy.maptype import map_instance_to_supertype
from mypy.typeops import custom_special_method
from mypy.subtypes import is_subtype
from mypy.parse import parse

FormatStringExpr: _TypeAlias = Union[StrExpr, BytesExpr, UnicodeExpr]
Checkers: _TypeAlias = Tuple[Callable[[Expression], None], Callable[[Type], bool]]
MatchMap: _TypeAlias = Dict[Tuple[int, int], Match[str]]  # span -&gt; match


@others
</t>
<t tx="ekr.20220525082933.842">def compile_format_re() -&gt; Pattern[str]:
    """Construct regexp to match format conversion specifiers in % interpolation.

    See https://docs.python.org/3/library/stdtypes.html#printf-style-string-formatting
    The regexp is intentionally a bit wider to report better errors.
    """
    key_re = r'(\((?P&lt;key&gt;[^)]*)\))?'  # (optional) parenthesised sequence of characters.
    flags_re = r'(?P&lt;flags&gt;[#0\-+ ]*)'  # (optional) sequence of flags.
    width_re = r'(?P&lt;width&gt;[1-9][0-9]*|\*)?'  # (optional) minimum field width (* or numbers).
    precision_re = r'(?:\.(?P&lt;precision&gt;\*|[0-9]+)?)?'  # (optional) . followed by * of numbers.
    length_mod_re = r'[hlL]?'  # (optional) length modifier (unused).
    type_re = r'(?P&lt;type&gt;.)?'  # conversion type.
    format_re = '%' + key_re + flags_re + width_re + precision_re + length_mod_re + type_re
    return re.compile(format_re)


</t>
<t tx="ekr.20220525082933.843">def compile_new_format_re(custom_spec: bool) -&gt; Pattern[str]:
    """Construct regexps to match format conversion specifiers in str.format() calls.

    See After https://docs.python.org/3/library/string.html#formatspec for
    specifications. The regexps are intentionally wider, to report better errors,
    instead of just not matching.
    """

    # Field (optional) is an integer/identifier possibly followed by several .attr and [index].
    field = r'(?P&lt;field&gt;(?P&lt;key&gt;[^.[!:]*)([^:!]+)?)'

    # Conversion (optional) is ! followed by one of letters for forced repr(), str(), or ascii().
    conversion = r'(?P&lt;conversion&gt;![^:])?'

    # Format specification (optional) follows its own mini-language:
    if not custom_spec:
        # Fill and align is valid for all builtin types.
        fill_align = r'(?P&lt;fill_align&gt;.?[&lt;&gt;=^])?'
        # Number formatting options are only valid for int, float, complex, and Decimal,
        # except if only width is given (it is valid for all types).
        # This contains sign, flags (sign, # and/or 0), width, grouping (_ or ,) and precision.
        num_spec = r'(?P&lt;flags&gt;[+\- ]?#?0?)(?P&lt;width&gt;\d+)?[_,]?(?P&lt;precision&gt;\.\d+)?'
        # The last element is type.
        conv_type = r'(?P&lt;type&gt;.)?'  # only some are supported, but we want to give a better error
        format_spec = r'(?P&lt;format_spec&gt;:' + fill_align + num_spec + conv_type + r')?'
    else:
        # Custom types can define their own form_spec using __format__().
        format_spec = r'(?P&lt;format_spec&gt;:.*)?'

    return re.compile(field + conversion + format_spec)


</t>
<t tx="ekr.20220525082933.844">FORMAT_RE: Final = compile_format_re()
FORMAT_RE_NEW: Final = compile_new_format_re(False)
FORMAT_RE_NEW_CUSTOM: Final = compile_new_format_re(True)
DUMMY_FIELD_NAME: Final = "__dummy_name__"

# Format types supported by str.format() for builtin classes.
SUPPORTED_TYPES_NEW: Final = {"b", "c", "d", "e", "E", "f", "F",
                              "g", "G", "n", "o", "s", "x", "X", "%"}

# Types that require either int or float.
NUMERIC_TYPES_OLD: Final = {"d", "i", "o", "u", "x", "X", "e", "E", "f", "F", "g", "G"}
NUMERIC_TYPES_NEW: Final = {"b", "d", "o", "e", "E", "f", "F", "g", "G", "n", "x", "X", "%"}

# These types accept _only_ int.
REQUIRE_INT_OLD: Final = {"o", "x", "X"}
REQUIRE_INT_NEW: Final = {"b", "d", "o", "x", "X"}

# These types fall back to SupportsFloat with % (other fall back to SupportsInt)
FLOAT_TYPES: Final = {"e", "E", "f", "F", "g", "G"}


</t>
<t tx="ekr.20220525082933.845">class ConversionSpecifier:
    @others
</t>
<t tx="ekr.20220525082933.846">def __init__(self, match: Match[str],
             start_pos: int = -1,
             non_standard_format_spec: bool = False) -&gt; None:

    self.whole_seq = match.group()
    self.start_pos = start_pos

    m_dict = match.groupdict()
    self.key = m_dict.get('key')

    # Replace unmatched optional groups with empty matches (for convenience).
    self.conv_type = m_dict.get('type', '')
    self.flags = m_dict.get('flags', '')
    self.width = m_dict.get('width', '')
    self.precision = m_dict.get('precision', '')

    # Used only for str.format() calls (it may be custom for types with __format__()).
    self.format_spec = m_dict.get('format_spec')
    self.non_standard_format_spec = non_standard_format_spec
    # Used only for str.format() calls.
    self.conversion = m_dict.get('conversion')
    # Full formatted expression (i.e. key plus following attributes and/or indexes).
    # Used only for str.format() calls.
    self.field = m_dict.get('field')

</t>
<t tx="ekr.20220525082933.847">def has_key(self) -&gt; bool:
    return self.key is not None

</t>
<t tx="ekr.20220525082933.848">def has_star(self) -&gt; bool:
    return self.width == '*' or self.precision == '*'


</t>
<t tx="ekr.20220525082933.849">def parse_conversion_specifiers(format_str: str) -&gt; List[ConversionSpecifier]:
    """Parse c-printf-style format string into list of conversion specifiers."""
    specifiers: List[ConversionSpecifier] = []
    for m in re.finditer(FORMAT_RE, format_str):
        specifiers.append(ConversionSpecifier(m, start_pos=m.start()))
    return specifiers


</t>
<t tx="ekr.20220525082933.85">def has_return_exprs(self, node):
    """Traverse the tree below node looking for 'return expr'.

    Return True if at least 'return expr' is found, False if not.
    (If both 'return' and 'return expr' are found, return True.)
    """
    results = {}
    if self.return_expr.match(node, results):
        return True
    for child in node.children:
        if child.type not in (syms.funcdef, syms.classdef):
            if self.has_return_exprs(child):
                return True
    return False
</t>
<t tx="ekr.20220525082933.850">def parse_format_value(format_value: str, ctx: Context, msg: MessageBuilder,
                       nested: bool = False) -&gt; Optional[List[ConversionSpecifier]]:
    """Parse format string into list of conversion specifiers.

    The specifiers may be nested (two levels maximum), in this case they are ordered as
    '{0:{1}}, {2:{3}{4}}'. Return None in case of an error.
    """
    top_targets = find_non_escaped_targets(format_value, ctx, msg)
    if top_targets is None:
        return None

    result: List[ConversionSpecifier] = []
    for target, start_pos in top_targets:
        match = FORMAT_RE_NEW.fullmatch(target)
        if match:
            conv_spec = ConversionSpecifier(match, start_pos=start_pos)
        else:
            custom_match = FORMAT_RE_NEW_CUSTOM.fullmatch(target)
            if custom_match:
                conv_spec = ConversionSpecifier(
                    custom_match, start_pos=start_pos,
                    non_standard_format_spec=True)
            else:
                msg.fail('Invalid conversion specifier in format string',
                         ctx, code=codes.STRING_FORMATTING)
                return None

        if conv_spec.key and ('{' in conv_spec.key or '}' in conv_spec.key):
            msg.fail('Conversion value must not contain { or }',
                     ctx, code=codes.STRING_FORMATTING)
            return None
        result.append(conv_spec)

        # Parse nested conversions that are allowed in format specifier.
        if (conv_spec.format_spec and conv_spec.non_standard_format_spec and
                ('{' in conv_spec.format_spec or '}' in conv_spec.format_spec)):
            if nested:
                msg.fail('Formatting nesting must be at most two levels deep',
                         ctx, code=codes.STRING_FORMATTING)
                return None
            sub_conv_specs = parse_format_value(conv_spec.format_spec, ctx, msg,
                                                nested=True)
            if sub_conv_specs is None:
                return None
            result.extend(sub_conv_specs)
    return result


</t>
<t tx="ekr.20220525082933.851">def find_non_escaped_targets(format_value: str, ctx: Context,
                             msg: MessageBuilder) -&gt; Optional[List[Tuple[str, int]]]:
    """Return list of raw (un-parsed) format specifiers in format string.

    Format specifiers don't include enclosing braces. We don't use regexp for
    this because they don't work well with nested/repeated patterns
    (both greedy and non-greedy), and these are heavily used internally for
    representation of f-strings.

    Return None in case of an error.
    """
    result = []
    next_spec = ''
    pos = 0
    nesting = 0
    while pos &lt; len(format_value):
        c = format_value[pos]
        if not nesting:
            # Skip any paired '{{' and '}}', enter nesting on '{', report error on '}'.
            if c == '{':
                if pos &lt; len(format_value) - 1 and format_value[pos + 1] == '{':
                    pos += 1
                else:
                    nesting = 1
            if c == '}':
                if pos &lt; len(format_value) - 1 and format_value[pos + 1] == '}':
                    pos += 1
                else:
                    msg.fail('Invalid conversion specifier in format string:'
                             ' unexpected }', ctx, code=codes.STRING_FORMATTING)
                    return None
        else:
            # Adjust nesting level, then either continue adding chars or move on.
            if c == '{':
                nesting += 1
            if c == '}':
                nesting -= 1
            if nesting:
                next_spec += c
            else:
                result.append((next_spec, pos - len(next_spec)))
                next_spec = ''
        pos += 1
    if nesting:
        msg.fail('Invalid conversion specifier in format string:'
                 ' unmatched {', ctx, code=codes.STRING_FORMATTING)
        return None
    return result


</t>
<t tx="ekr.20220525082933.852">class StringFormatterChecker:
    """String interpolation/formatter type checker.

    This class works closely together with checker.ExpressionChecker.
    """

    # Some services are provided by a TypeChecker instance.
    chk: "mypy.checker.TypeChecker"
    # This is shared with TypeChecker, but stored also here for convenience.
    msg: MessageBuilder
    # Some services are provided by a ExpressionChecker instance.
    exprchk: "mypy.checkexpr.ExpressionChecker"

    @others
</t>
<t tx="ekr.20220525082933.853">def __init__(self,
             exprchk: 'mypy.checkexpr.ExpressionChecker',
             chk: 'mypy.checker.TypeChecker',
             msg: MessageBuilder) -&gt; None:
    """Construct an expression type checker."""
    self.chk = chk
    self.exprchk = exprchk
    self.msg = msg
    # This flag is used to track Python 2 corner case where for example
    # '%s, %d' % (u'abc', 42) returns u'abc, 42' (i.e. unicode, not a string).
    self.unicode_upcast = False

</t>
<t tx="ekr.20220525082933.854">def check_str_format_call(self, call: CallExpr, format_value: str) -&gt; None:
    """Perform more precise checks for str.format() calls when possible.

    Currently the checks are performed for:
      * Actual string literals
      * Literal types with string values
      * Final names with string values

    The checks that we currently perform:
      * Check generic validity (e.g. unmatched { or }, and {} in invalid positions)
      * Check consistency of specifiers' auto-numbering
      * Verify that replacements can be found for all conversion specifiers,
        and all arguments were used
      * Non-standard format specs are only allowed for types with custom __format__
      * Type check replacements with accessors applied (if any).
      * Verify that specifier type is known and matches replacement type
      * Perform special checks for some specifier types:
        - 'c' requires a single character string
        - 's' must not accept bytes
        - non-empty flags are only allowed for numeric types
    """
    conv_specs = parse_format_value(format_value, call, self.msg)
    if conv_specs is None:
        return
    if not self.auto_generate_keys(conv_specs, call):
        return
    self.check_specs_in_format_call(call, conv_specs, format_value)

</t>
<t tx="ekr.20220525082933.855">def check_specs_in_format_call(self, call: CallExpr,
                               specs: List[ConversionSpecifier], format_value: str) -&gt; None:
    """Perform pairwise checks for conversion specifiers vs their replacements.

    The core logic for format checking is implemented in this method.
    """
    assert all(s.key for s in specs), "Keys must be auto-generated first!"
    replacements = self.find_replacements_in_call(call, [cast(str, s.key) for s in specs])
    assert len(replacements) == len(specs)
    for spec, repl in zip(specs, replacements):
        repl = self.apply_field_accessors(spec, repl, ctx=call)
        actual_type = repl.type if isinstance(repl, TempNode) else self.chk.lookup_type(repl)
        assert actual_type is not None

        # Special case custom formatting.
        if (spec.format_spec and spec.non_standard_format_spec and
                # Exclude "dynamic" specifiers (i.e. containing nested formatting).
                not ('{' in spec.format_spec or '}' in spec.format_spec)):
            if (not custom_special_method(actual_type, '__format__', check_all=True) or
                    spec.conversion):
                # TODO: add support for some custom specs like datetime?
                self.msg.fail('Unrecognized format'
                              ' specification "{}"'.format(spec.format_spec[1:]),
                              call, code=codes.STRING_FORMATTING)
                continue
        # Adjust expected and actual types.
        if not spec.conv_type:
            expected_type: Optional[Type] = AnyType(TypeOfAny.special_form)
        else:
            assert isinstance(call.callee, MemberExpr)
            if isinstance(call.callee.expr, (StrExpr, UnicodeExpr)):
                format_str = call.callee.expr
            else:
                format_str = StrExpr(format_value)
            expected_type = self.conversion_type(spec.conv_type, call, format_str,
                                                 format_call=True)
        if spec.conversion is not None:
            # If the explicit conversion is given, then explicit conversion is called _first_.
            if spec.conversion[1] not in 'rsa':
                self.msg.fail('Invalid conversion type "{}",'
                              ' must be one of "r", "s" or "a"'.format(spec.conversion[1]),
                              call, code=codes.STRING_FORMATTING)
            actual_type = self.named_type('builtins.str')

        # Perform the checks for given types.
        if expected_type is None:
            continue

        a_type = get_proper_type(actual_type)
        actual_items = (get_proper_types(a_type.items) if isinstance(a_type, UnionType)
                        else [a_type])
        for a_type in actual_items:
            if custom_special_method(a_type, '__format__'):
                continue
            self.check_placeholder_type(a_type, expected_type, call)
            self.perform_special_format_checks(spec, call, repl, a_type, expected_type)

</t>
<t tx="ekr.20220525082933.856">def perform_special_format_checks(self, spec: ConversionSpecifier, call: CallExpr,
                                  repl: Expression, actual_type: Type,
                                  expected_type: Type) -&gt; None:
    # TODO: try refactoring to combine this logic with % formatting.
    if spec.conv_type == 'c':
        if isinstance(repl, (StrExpr, BytesExpr)) and len(repl.value) != 1:
            self.msg.requires_int_or_char(call, format_call=True)
        c_typ = get_proper_type(self.chk.lookup_type(repl))
        if isinstance(c_typ, Instance) and c_typ.last_known_value:
            c_typ = c_typ.last_known_value
        if isinstance(c_typ, LiteralType) and isinstance(c_typ.value, str):
            if len(c_typ.value) != 1:
                self.msg.requires_int_or_char(call, format_call=True)
    if (not spec.conv_type or spec.conv_type == 's') and not spec.conversion:
        if self.chk.options.python_version &gt;= (3, 0):
            if (has_type_component(actual_type, 'builtins.bytes') and
                    not custom_special_method(actual_type, '__str__')):
                self.msg.fail(
                    'On Python 3 formatting "b\'abc\'" with "{}" '
                    'produces "b\'abc\'", not "abc"; '
                    'use "{!r}" if this is desired behavior',
                    call, code=codes.STR_BYTES_PY3)
    if spec.flags:
        numeric_types = UnionType([self.named_type('builtins.int'),
                                   self.named_type('builtins.float')])
        if (spec.conv_type and spec.conv_type not in NUMERIC_TYPES_NEW or
                not spec.conv_type and not is_subtype(actual_type, numeric_types) and
                not custom_special_method(actual_type, '__format__')):
            self.msg.fail('Numeric flags are only allowed for numeric types', call,
                          code=codes.STRING_FORMATTING)

</t>
<t tx="ekr.20220525082933.857">def find_replacements_in_call(self, call: CallExpr,
                              keys: List[str]) -&gt; List[Expression]:
    """Find replacement expression for every specifier in str.format() call.

    In case of an error use TempNode(AnyType).
    """
    result: List[Expression] = []
    used: Set[Expression] = set()
    for key in keys:
        if key.isdecimal():
            expr = self.get_expr_by_position(int(key), call)
            if not expr:
                self.msg.fail('Cannot find replacement for positional'
                              ' format specifier {}'.format(key), call,
                              code=codes.STRING_FORMATTING)
                expr = TempNode(AnyType(TypeOfAny.from_error))
        else:
            expr = self.get_expr_by_name(key, call)
            if not expr:
                self.msg.fail('Cannot find replacement for named'
                              ' format specifier "{}"'.format(key), call,
                              code=codes.STRING_FORMATTING)
                expr = TempNode(AnyType(TypeOfAny.from_error))
        result.append(expr)
        if not isinstance(expr, TempNode):
            used.add(expr)
    # Strictly speaking not using all replacements is not a type error, but most likely
    # a typo in user code, so we show an error like we do for % formatting.
    total_explicit = len([kind for kind in call.arg_kinds if kind in (ARG_POS, ARG_NAMED)])
    if len(used) &lt; total_explicit:
        self.msg.too_many_string_formatting_arguments(call)
    return result

</t>
<t tx="ekr.20220525082933.858">def get_expr_by_position(self, pos: int, call: CallExpr) -&gt; Optional[Expression]:
    """Get positional replacement expression from '{0}, {1}'.format(x, y, ...) call.

    If the type is from *args, return TempNode(&lt;item type&gt;). Return None in case of
    an error.
    """
    pos_args = [arg for arg, kind in zip(call.args, call.arg_kinds) if kind == ARG_POS]
    if pos &lt; len(pos_args):
        return pos_args[pos]
    star_args = [arg for arg, kind in zip(call.args, call.arg_kinds) if kind == ARG_STAR]
    if not star_args:
        return None

    # Fall back to *args when present in call.
    star_arg = star_args[0]
    varargs_type = get_proper_type(self.chk.lookup_type(star_arg))
    if (not isinstance(varargs_type, Instance) or not
            varargs_type.type.has_base('typing.Sequence')):
        # Error should be already reported.
        return TempNode(AnyType(TypeOfAny.special_form))
    iter_info = self.chk.named_generic_type('typing.Sequence',
                                            [AnyType(TypeOfAny.special_form)]).type
    return TempNode(map_instance_to_supertype(varargs_type, iter_info).args[0])

</t>
<t tx="ekr.20220525082933.859">def get_expr_by_name(self, key: str, call: CallExpr) -&gt; Optional[Expression]:
    """Get named replacement expression from '{name}'.format(name=...) call.

    If the type is from **kwargs, return TempNode(&lt;item type&gt;). Return None in case of
    an error.
    """
    named_args = [arg for arg, kind, name in zip(call.args, call.arg_kinds, call.arg_names)
                  if kind == ARG_NAMED and name == key]
    if named_args:
        return named_args[0]
    star_args_2 = [arg for arg, kind in zip(call.args, call.arg_kinds) if kind == ARG_STAR2]
    if not star_args_2:
        return None
    star_arg_2 = star_args_2[0]
    kwargs_type = get_proper_type(self.chk.lookup_type(star_arg_2))
    if (not isinstance(kwargs_type, Instance) or not
            kwargs_type.type.has_base('typing.Mapping')):
        # Error should be already reported.
        return TempNode(AnyType(TypeOfAny.special_form))
    any_type = AnyType(TypeOfAny.special_form)
    mapping_info = self.chk.named_generic_type('typing.Mapping',
                                               [any_type, any_type]).type
    return TempNode(map_instance_to_supertype(kwargs_type, mapping_info).args[1])

</t>
<t tx="ekr.20220525082933.86">@path C:/Repos/mypy/misc/
@language python
@tabwidth -4
#!/usr/bin/env python3
"""
This file compares the output and runtime of running normal vs incremental mode
on the history of any arbitrary git repo as a way of performing a sanity check
to make sure incremental mode is working correctly and efficiently.

It does so by first running mypy without incremental mode on the specified range
of commits to find the expected result, then rewinds back to the first commit and
re-runs mypy on the commits with incremental mode enabled to make sure it returns
the same results.

This script will download and test the official mypy repo by default. Running:

    python3 misc/incremental_checker.py last 30

is equivalent to running

    python3 misc/incremental_checker.py last 30 \\
            --repo_url https://github.com/python/mypy.git \\
            --file-path mypy

You can chose to run this script against a specific commit id or against the
last n commits.

To run this script against the last 30 commits:

    python3 misc/incremental_checker.py last 30

To run this script starting from the commit id 2a432b:

    python3 misc/incremental_checker.py commit 2a432b
"""

from typing import Any, Dict, List, Optional, Tuple

from argparse import ArgumentParser, RawDescriptionHelpFormatter, Namespace
import base64
import json
import os
import random
import re
import shutil
import subprocess
import sys
import textwrap
import time


CACHE_PATH = ".incremental_checker_cache.json"
MYPY_REPO_URL = "https://github.com/python/mypy.git"
MYPY_TARGET_FILE = "mypy"
DAEMON_CMD = ["python3", "-m", "mypy.dmypy"]

JsonDict = Dict[str, Any]


@others
if __name__ == '__main__':
    main()
</t>
<t tx="ekr.20220525082933.860">def auto_generate_keys(self, all_specs: List[ConversionSpecifier],
                       ctx: Context) -&gt; bool:
    """Translate '{} {name} {}' to '{0} {name} {1}'.

    Return True if generation was successful, otherwise report an error and return false.
    """
    some_defined = any(s.key and s.key.isdecimal() for s in all_specs)
    all_defined = all(bool(s.key) for s in all_specs)
    if some_defined and not all_defined:
        self.msg.fail('Cannot combine automatic field numbering and'
                      ' manual field specification', ctx, code=codes.STRING_FORMATTING)
        return False
    if all_defined:
        return True
    next_index = 0
    for spec in all_specs:
        if not spec.key:
            str_index = str(next_index)
            spec.key = str_index
            # Update also the full field (i.e. turn {.x} into {0.x}).
            if not spec.field:
                spec.field = str_index
            else:
                spec.field = str_index + spec.field
            next_index += 1
    return True

</t>
<t tx="ekr.20220525082933.861">def apply_field_accessors(self, spec: ConversionSpecifier, repl: Expression,
                          ctx: Context) -&gt; Expression:
    """Transform and validate expr in '{.attr[item]}'.format(expr) into expr.attr['item'].

    If validation fails, return TempNode(AnyType).
    """
    assert spec.key, "Keys must be auto-generated first!"
    if spec.field == spec.key:
        return repl
    assert spec.field

    temp_errors = Errors()
    dummy = DUMMY_FIELD_NAME + spec.field[len(spec.key):]
    temp_ast: Node = parse(
        dummy, fnam="&lt;format&gt;", module=None, options=self.chk.options, errors=temp_errors
    )
    if temp_errors.is_errors():
        self.msg.fail(f'Syntax error in format specifier "{spec.field}"',
                      ctx, code=codes.STRING_FORMATTING)
        return TempNode(AnyType(TypeOfAny.from_error))

    # These asserts are guaranteed by the original regexp.
    assert isinstance(temp_ast, MypyFile)
    temp_ast = temp_ast.defs[0]
    assert isinstance(temp_ast, ExpressionStmt)
    temp_ast = temp_ast.expr
    if not self.validate_and_transform_accessors(temp_ast, repl, spec, ctx=ctx):
        return TempNode(AnyType(TypeOfAny.from_error))

    # Check if there are any other errors (like missing members).
    # TODO: fix column to point to actual start of the format specifier _within_ string.
    temp_ast.line = ctx.line
    temp_ast.column = ctx.column
    self.exprchk.accept(temp_ast)
    return temp_ast

</t>
<t tx="ekr.20220525082933.862">def validate_and_transform_accessors(self, temp_ast: Expression, original_repl: Expression,
                                     spec: ConversionSpecifier, ctx: Context) -&gt; bool:
    """Validate and transform (in-place) format field accessors.

    On error, report it and return False. The transformations include replacing the dummy
    variable with actual replacement expression and translating any name expressions in an
    index into strings, so that this will work:

        class User(TypedDict):
            name: str
            id: int
        u: User
        '{[id]:d} -&gt; {[name]}'.format(u)
    """
    if not isinstance(temp_ast, (MemberExpr, IndexExpr)):
        self.msg.fail('Only index and member expressions are allowed in'
                      ' format field accessors; got "{}"'.format(spec.field),
                      ctx, code=codes.STRING_FORMATTING)
        return False
    if isinstance(temp_ast, MemberExpr):
        node = temp_ast.expr
    else:
        node = temp_ast.base
        if not isinstance(temp_ast.index, (NameExpr, IntExpr)):
            assert spec.key, "Call this method only after auto-generating keys!"
            assert spec.field
            self.msg.fail('Invalid index expression in format field'
                          ' accessor "{}"'.format(spec.field[len(spec.key):]), ctx,
                          code=codes.STRING_FORMATTING)
            return False
        if isinstance(temp_ast.index, NameExpr):
            temp_ast.index = StrExpr(temp_ast.index.name)
    if isinstance(node, NameExpr) and node.name == DUMMY_FIELD_NAME:
        # Replace it with the actual replacement expression.
        assert isinstance(temp_ast, (IndexExpr, MemberExpr))  # XXX: this is redundant
        if isinstance(temp_ast, IndexExpr):
            temp_ast.base = original_repl
        else:
            temp_ast.expr = original_repl
        return True
    node.line = ctx.line
    node.column = ctx.column
    return self.validate_and_transform_accessors(node, original_repl=original_repl,
                                                 spec=spec, ctx=ctx)

</t>
<t tx="ekr.20220525082933.863"># TODO: In Python 3, the bytes formatting has a more restricted set of options
#       compared to string formatting.
def check_str_interpolation(self,
                            expr: FormatStringExpr,
                            replacements: Expression) -&gt; Type:
    """Check the types of the 'replacements' in a string interpolation
    expression: str % replacements.
    """
    self.exprchk.accept(expr)
    specifiers = parse_conversion_specifiers(expr.value)
    has_mapping_keys = self.analyze_conversion_specifiers(specifiers, expr)
    if isinstance(expr, BytesExpr) and (3, 0) &lt;= self.chk.options.python_version &lt; (3, 5):
        self.msg.fail('Bytes formatting is only supported in Python 3.5 and later',
                      replacements, code=codes.STRING_FORMATTING)
        return AnyType(TypeOfAny.from_error)

    self.unicode_upcast = False
    if has_mapping_keys is None:
        pass  # Error was reported
    elif has_mapping_keys:
        self.check_mapping_str_interpolation(specifiers, replacements, expr)
    else:
        self.check_simple_str_interpolation(specifiers, replacements, expr)

    if isinstance(expr, BytesExpr):
        return self.named_type('builtins.bytes')
    elif isinstance(expr, UnicodeExpr):
        return self.named_type('builtins.unicode')
    elif isinstance(expr, StrExpr):
        if self.unicode_upcast:
            return self.named_type('builtins.unicode')
        return self.named_type('builtins.str')
    else:
        assert False

</t>
<t tx="ekr.20220525082933.864">def analyze_conversion_specifiers(self, specifiers: List[ConversionSpecifier],
                                  context: Context) -&gt; Optional[bool]:
    has_star = any(specifier.has_star() for specifier in specifiers)
    has_key = any(specifier.has_key() for specifier in specifiers)
    all_have_keys = all(
        specifier.has_key() or specifier.conv_type == '%' for specifier in specifiers
    )

    if has_key and has_star:
        self.msg.string_interpolation_with_star_and_key(context)
        return None
    if has_key and not all_have_keys:
        self.msg.string_interpolation_mixing_key_and_non_keys(context)
        return None
    return has_key

</t>
<t tx="ekr.20220525082933.865">def check_simple_str_interpolation(self, specifiers: List[ConversionSpecifier],
                                   replacements: Expression, expr: FormatStringExpr) -&gt; None:
    """Check % string interpolation with positional specifiers '%s, %d' % ('yes, 42')."""
    checkers = self.build_replacement_checkers(specifiers, replacements, expr)
    if checkers is None:
        return

    rhs_type = get_proper_type(self.accept(replacements))
    rep_types: List[Type] = []
    if isinstance(rhs_type, TupleType):
        rep_types = rhs_type.items
    elif isinstance(rhs_type, AnyType):
        return
    elif isinstance(rhs_type, Instance) and rhs_type.type.fullname == 'builtins.tuple':
        # Assume that an arbitrary-length tuple has the right number of items.
        rep_types = [rhs_type.args[0]] * len(checkers)
    elif isinstance(rhs_type, UnionType):
        for typ in rhs_type.relevant_items():
            temp_node = TempNode(typ)
            temp_node.line = replacements.line
            self.check_simple_str_interpolation(specifiers, temp_node, expr)
        return
    else:
        rep_types = [rhs_type]

    if len(checkers) &gt; len(rep_types):
        # Only check the fix-length Tuple type. Other Iterable types would skip.
        if (is_subtype(rhs_type, self.chk.named_type("typing.Iterable")) and
                not isinstance(rhs_type, TupleType)):
            return
        else:
            self.msg.too_few_string_formatting_arguments(replacements)
    elif len(checkers) &lt; len(rep_types):
        self.msg.too_many_string_formatting_arguments(replacements)
    else:
        if len(checkers) == 1:
            check_node, check_type = checkers[0]
            if isinstance(rhs_type, TupleType) and len(rhs_type.items) == 1:
                check_type(rhs_type.items[0])
            else:
                check_node(replacements)
        elif (isinstance(replacements, TupleExpr)
              and not any(isinstance(item, StarExpr) for item in replacements.items)):
            for checks, rep_node in zip(checkers, replacements.items):
                check_node, check_type = checks
                check_node(rep_node)
        else:
            for checks, rep_type in zip(checkers, rep_types):
                check_node, check_type = checks
                check_type(rep_type)

</t>
<t tx="ekr.20220525082933.866">def check_mapping_str_interpolation(self, specifiers: List[ConversionSpecifier],
                                    replacements: Expression,
                                    expr: FormatStringExpr) -&gt; None:
    """Check % string interpolation with names specifiers '%(name)s' % {'name': 'John'}."""
    if (isinstance(replacements, DictExpr) and
            all(isinstance(k, (StrExpr, BytesExpr, UnicodeExpr))
                for k, v in replacements.items)):
        mapping: Dict[str, Type] = {}
        for k, v in replacements.items:
            if self.chk.options.python_version &gt;= (3, 0) and isinstance(expr, BytesExpr):
                # Special case: for bytes formatting keys must be bytes.
                if not isinstance(k, BytesExpr):
                    self.msg.fail('Dictionary keys in bytes formatting must be bytes,'
                                  ' not strings', expr, code=codes.STRING_FORMATTING)
            key_str = cast(FormatStringExpr, k).value
            mapping[key_str] = self.accept(v)

        for specifier in specifiers:
            if specifier.conv_type == '%':
                # %% is allowed in mappings, no checking is required
                continue
            assert specifier.key is not None
            if specifier.key not in mapping:
                self.msg.key_not_in_mapping(specifier.key, replacements)
                return
            rep_type = mapping[specifier.key]
            assert specifier.conv_type is not None
            expected_type = self.conversion_type(specifier.conv_type, replacements, expr)
            if expected_type is None:
                return
            self.chk.check_subtype(rep_type, expected_type, replacements,
                                   message_registry.INCOMPATIBLE_TYPES_IN_STR_INTERPOLATION,
                                   'expression has type',
                                   f'placeholder with key \'{specifier.key}\' has type',
                                   code=codes.STRING_FORMATTING)
            if specifier.conv_type == 's':
                self.check_s_special_cases(expr, rep_type, expr)
    else:
        rep_type = self.accept(replacements)
        dict_type = self.build_dict_type(expr)
        self.chk.check_subtype(rep_type, dict_type, replacements,
                               message_registry.FORMAT_REQUIRES_MAPPING,
                               'expression has type', 'expected type for mapping is',
                               code=codes.STRING_FORMATTING)

</t>
<t tx="ekr.20220525082933.867">def build_dict_type(self, expr: FormatStringExpr) -&gt; Type:
    """Build expected mapping type for right operand in % formatting."""
    any_type = AnyType(TypeOfAny.special_form)
    if self.chk.options.python_version &gt;= (3, 0):
        if isinstance(expr, BytesExpr):
            bytes_type = self.chk.named_generic_type('builtins.bytes', [])
            return self.chk.named_generic_type('typing.Mapping',
                                               [bytes_type, any_type])
        elif isinstance(expr, StrExpr):
            str_type = self.chk.named_generic_type('builtins.str', [])
            return self.chk.named_generic_type('typing.Mapping',
                                               [str_type, any_type])
        else:
            assert False, "There should not be UnicodeExpr on Python 3"
    else:
        str_type = self.chk.named_generic_type('builtins.str', [])
        unicode_type = self.chk.named_generic_type('builtins.unicode', [])
        str_map = self.chk.named_generic_type('typing.Mapping',
                                              [str_type, any_type])
        unicode_map = self.chk.named_generic_type('typing.Mapping',
                                                  [unicode_type, any_type])
        return UnionType.make_union([str_map, unicode_map])

</t>
<t tx="ekr.20220525082933.868">def build_replacement_checkers(self, specifiers: List[ConversionSpecifier],
                               context: Context, expr: FormatStringExpr
                               ) -&gt; Optional[List[Checkers]]:
    checkers: List[Checkers] = []
    for specifier in specifiers:
        checker = self.replacement_checkers(specifier, context, expr)
        if checker is None:
            return None
        checkers.extend(checker)
    return checkers

</t>
<t tx="ekr.20220525082933.869">def replacement_checkers(self, specifier: ConversionSpecifier, context: Context,
                         expr: FormatStringExpr) -&gt; Optional[List[Checkers]]:
    """Returns a list of tuples of two functions that check whether a replacement is
    of the right type for the specifier. The first function takes a node and checks
    its type in the right type context. The second function just checks a type.
    """
    checkers: List[Checkers] = []

    if specifier.width == '*':
        checkers.append(self.checkers_for_star(context))
    if specifier.precision == '*':
        checkers.append(self.checkers_for_star(context))

    if specifier.conv_type == 'c':
        c = self.checkers_for_c_type(specifier.conv_type, context, expr)
        if c is None:
            return None
        checkers.append(c)
    elif specifier.conv_type is not None and specifier.conv_type != '%':
        c = self.checkers_for_regular_type(specifier.conv_type, context, expr)
        if c is None:
            return None
        checkers.append(c)
    return checkers

</t>
<t tx="ekr.20220525082933.87">def print_offset(text: str, indent_length: int = 4) -&gt; None:
    print()
    print(textwrap.indent(text, ' ' * indent_length))
    print()


</t>
<t tx="ekr.20220525082933.870">def checkers_for_star(self, context: Context) -&gt; Checkers:
    """Returns a tuple of check functions that check whether, respectively,
    a node or a type is compatible with a star in a conversion specifier.
    """
    expected = self.named_type('builtins.int')

    def check_type(type: Type) -&gt; bool:
        expected = self.named_type('builtins.int')
        return self.chk.check_subtype(type, expected, context, '* wants int',
                                      code=codes.STRING_FORMATTING)

    def check_expr(expr: Expression) -&gt; None:
        type = self.accept(expr, expected)
        check_type(type)

    return check_expr, check_type

</t>
<t tx="ekr.20220525082933.871">def check_placeholder_type(self, typ: Type, expected_type: Type, context: Context) -&gt; bool:
    return self.chk.check_subtype(typ, expected_type, context,
                                  message_registry.INCOMPATIBLE_TYPES_IN_STR_INTERPOLATION,
                                  'expression has type', 'placeholder has type',
                                  code=codes.STRING_FORMATTING)

</t>
<t tx="ekr.20220525082933.872">def checkers_for_regular_type(self, conv_type: str,
                              context: Context,
                              expr: FormatStringExpr) -&gt; Optional[Checkers]:
    """Returns a tuple of check functions that check whether, respectively,
    a node or a type is compatible with 'type'. Return None in case of an error.
    """
    expected_type = self.conversion_type(conv_type, context, expr)
    if expected_type is None:
        return None

    def check_type(typ: Type) -&gt; bool:
        assert expected_type is not None
        ret = self.check_placeholder_type(typ, expected_type, context)
        if ret and conv_type == 's':
            ret = self.check_s_special_cases(expr, typ, context)
        return ret

    def check_expr(expr: Expression) -&gt; None:
        type = self.accept(expr, expected_type)
        check_type(type)

    return check_expr, check_type

</t>
<t tx="ekr.20220525082933.873">def check_s_special_cases(self, expr: FormatStringExpr, typ: Type, context: Context) -&gt; bool:
    """Additional special cases for %s in bytes vs string context."""
    if isinstance(expr, StrExpr):
        # Couple special cases for string formatting.
        if self.chk.options.python_version &gt;= (3, 0):
            if has_type_component(typ, 'builtins.bytes'):
                self.msg.fail(
                    'On Python 3 formatting "b\'abc\'" with "%s" '
                    'produces "b\'abc\'", not "abc"; '
                    'use "%r" if this is desired behavior',
                    context, code=codes.STR_BYTES_PY3)
                return False
        if self.chk.options.python_version &lt; (3, 0):
            if has_type_component(typ, 'builtins.unicode'):
                self.unicode_upcast = True
    if isinstance(expr, BytesExpr):
        # A special case for bytes formatting: b'%s' actually requires bytes on Python 3.
        if self.chk.options.python_version &gt;= (3, 0):
            if has_type_component(typ, 'builtins.str'):
                self.msg.fail("On Python 3 b'%s' requires bytes, not string", context,
                              code=codes.STRING_FORMATTING)
                return False
    return True

</t>
<t tx="ekr.20220525082933.874">def checkers_for_c_type(self, type: str,
                        context: Context,
                        format_expr: FormatStringExpr) -&gt; Optional[Checkers]:
    """Returns a tuple of check functions that check whether, respectively,
    a node or a type is compatible with 'type' that is a character type.
    """
    expected_type = self.conversion_type(type, context, format_expr)
    if expected_type is None:
        return None

    def check_type(type: Type) -&gt; bool:
        assert expected_type is not None
        if self.chk.options.python_version &gt;= (3, 0) and isinstance(format_expr, BytesExpr):
            err_msg = '"%c" requires an integer in range(256) or a single byte'
        else:
            err_msg = '"%c" requires int or char'
        return self.chk.check_subtype(type, expected_type, context, err_msg,
                                      'expression has type',
                                      code=codes.STRING_FORMATTING)

    def check_expr(expr: Expression) -&gt; None:
        """int, or str with length 1"""
        type = self.accept(expr, expected_type)
        # We need further check with expr to make sure that
        # it has exact one char or one single byte.
        if check_type(type):
            # Python 3 doesn't support b'%c' % str
            if (self.chk.options.python_version &gt;= (3, 0)
                    and isinstance(format_expr, BytesExpr)
                    and isinstance(expr, BytesExpr) and len(expr.value) != 1):
                self.msg.requires_int_or_single_byte(context)
            # In Python 2, b'%c' is the same as '%c'
            elif isinstance(expr, (StrExpr, BytesExpr)) and len(expr.value) != 1:
                self.msg.requires_int_or_char(context)

    return check_expr, check_type

</t>
<t tx="ekr.20220525082933.875">def conversion_type(self, p: str, context: Context, expr: FormatStringExpr,
                    format_call: bool = False) -&gt; Optional[Type]:
    """Return the type that is accepted for a string interpolation conversion specifier type.

    Note that both Python's float (e.g. %f) and integer (e.g. %d)
    specifier types accept both float and integers.

    The 'format_call' argument indicates whether this type came from % interpolation or from
    a str.format() call, the meaning of few formatting types are different.
    """
    NUMERIC_TYPES = NUMERIC_TYPES_NEW if format_call else NUMERIC_TYPES_OLD
    INT_TYPES = REQUIRE_INT_NEW if format_call else REQUIRE_INT_OLD
    if p == 'b' and not format_call:
        if self.chk.options.python_version &lt; (3, 5):
            self.msg.fail('Format character "b" is only supported in Python 3.5 and later',
                          context, code=codes.STRING_FORMATTING)
            return None
        if not isinstance(expr, BytesExpr):
            self.msg.fail('Format character "b" is only supported on bytes patterns', context,
                          code=codes.STRING_FORMATTING)
            return None
        return self.named_type('builtins.bytes')
    elif p == 'a':
        if self.chk.options.python_version &lt; (3, 0):
            self.msg.fail('Format character "a" is only supported in Python 3', context,
                          code=codes.STRING_FORMATTING)
            return None
        # TODO: return type object?
        return AnyType(TypeOfAny.special_form)
    elif p in ['s', 'r']:
        return AnyType(TypeOfAny.special_form)
    elif p in NUMERIC_TYPES:
        if p in INT_TYPES:
            numeric_types = [self.named_type('builtins.int')]
        else:
            numeric_types = [self.named_type('builtins.int'),
                             self.named_type('builtins.float')]
            if not format_call:
                if p in FLOAT_TYPES:
                    numeric_types.append(self.named_type('typing.SupportsFloat'))
                else:
                    numeric_types.append(self.named_type('typing.SupportsInt'))
        return UnionType.make_union(numeric_types)
    elif p in ['c']:
        if isinstance(expr, BytesExpr):
            return UnionType([self.named_type('builtins.int'),
                              self.named_type('builtins.bytes')])
        else:
            return UnionType([self.named_type('builtins.int'),
                              self.named_type('builtins.str')])
    else:
        self.msg.unsupported_placeholder(p, context)
        return None

</t>
<t tx="ekr.20220525082933.876">#
# Helpers
#

</t>
<t tx="ekr.20220525082933.877">def named_type(self, name: str) -&gt; Instance:
    """Return an instance type with type given by the name and no type
    arguments. Alias for TypeChecker.named_type.
    """
    return self.chk.named_type(name)

</t>
<t tx="ekr.20220525082933.878">def accept(self, expr: Expression, context: Optional[Type] = None) -&gt; Type:
    """Type check a node. Alias for TypeChecker.accept."""
    return self.chk.expr_checker.accept(expr, context)


</t>
<t tx="ekr.20220525082933.879">def has_type_component(typ: Type, fullname: str) -&gt; bool:
    """Is this a specific instance type, or a union that contains it?

    We use this ad-hoc function instead of a proper visitor or subtype check
    because some str vs bytes errors are strictly speaking not runtime errors,
    but rather highly counter-intuitive behavior. This is similar to what is used for
    --strict-equality.
    """
    typ = get_proper_type(typ)
    if isinstance(typ, Instance):
        return typ.type.has_base(fullname)
    elif isinstance(typ, TypeVarType):
        return (has_type_component(typ.upper_bound, fullname) or
                any(has_type_component(v, fullname) for v in typ.values))
    elif isinstance(typ, UnionType):
        return any(has_type_component(t, fullname) for t in typ.relevant_items())
    return False
</t>
<t tx="ekr.20220525082933.88">def delete_folder(folder_path: str) -&gt; None:
    if os.path.exists(folder_path):
        shutil.rmtree(folder_path)


</t>
<t tx="ekr.20220525082933.880">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
import argparse
import configparser
import glob as fileglob
from io import StringIO
import os
import re
import sys

if sys.version_info &gt;= (3, 11):
    import tomllib
else:
    import tomli as tomllib

from typing import (Any, Callable, Dict, List, Mapping, MutableMapping,  Optional, Sequence,
                    TextIO, Tuple, Union)
from typing_extensions import Final, TypeAlias as _TypeAlias

from mypy import defaults
from mypy.options import Options, PER_MODULE_OPTIONS

_CONFIG_VALUE_TYPES: _TypeAlias = Union[
    str, bool, int, float, Dict[str, str], List[str], Tuple[int, int],
]
_INI_PARSER_CALLABLE: _TypeAlias = Callable[[Any], _CONFIG_VALUE_TYPES]


@others
</t>
<t tx="ekr.20220525082933.881">def parse_version(v: Union[str, float]) -&gt; Tuple[int, int]:
    m = re.match(r'\A(\d)\.(\d+)\Z', str(v))
    if not m:
        raise argparse.ArgumentTypeError(
            f"Invalid python version '{v}' (expected format: 'x.y')")
    major, minor = int(m.group(1)), int(m.group(2))
    if major == 2:
        if minor != 7:
            raise argparse.ArgumentTypeError(
                f"Python 2.{minor} is not supported (must be 2.7)")
    elif major == 3:
        if minor &lt; defaults.PYTHON3_VERSION_MIN[1]:
            msg = "Python 3.{0} is not supported (must be {1}.{2} or higher)".format(
                minor,
                *defaults.PYTHON3_VERSION_MIN
            )

            if isinstance(v, float):
                msg += ". You may need to put quotes around your Python version"

            raise argparse.ArgumentTypeError(msg)
    else:
        raise argparse.ArgumentTypeError(
            f"Python major version '{major}' out of range (must be 2 or 3)")
    return major, minor


</t>
<t tx="ekr.20220525082933.882">def try_split(v: Union[str, Sequence[str]], split_regex: str = '[,]') -&gt; List[str]:
    """Split and trim a str or list of str into a list of str"""
    if isinstance(v, str):
        return [p.strip() for p in re.split(split_regex, v)]

    return [p.strip() for p in v]


</t>
<t tx="ekr.20220525082933.883">def expand_path(path: str) -&gt; str:
    """Expand the user home directory and any environment variables contained within
    the provided path.
    """

    return os.path.expandvars(os.path.expanduser(path))


</t>
<t tx="ekr.20220525082933.884">def str_or_array_as_list(v: Union[str, Sequence[str]]) -&gt; List[str]:
    if isinstance(v, str):
        return [v.strip()] if v.strip() else []
    return [p.strip() for p in v if p.strip()]


</t>
<t tx="ekr.20220525082933.885">def split_and_match_files_list(paths: Sequence[str]) -&gt; List[str]:
    """Take a list of files/directories (with support for globbing through the glob library).

    Where a path/glob matches no file, we still include the raw path in the resulting list.

    Returns a list of file paths
    """
    expanded_paths = []

    for path in paths:
        path = expand_path(path.strip())
        globbed_files = fileglob.glob(path, recursive=True)
        if globbed_files:
            expanded_paths.extend(globbed_files)
        else:
            expanded_paths.append(path)

    return expanded_paths


</t>
<t tx="ekr.20220525082933.886">def split_and_match_files(paths: str) -&gt; List[str]:
    """Take a string representing a list of files/directories (with support for globbing
    through the glob library).

    Where a path/glob matches no file, we still include the raw path in the resulting list.

    Returns a list of file paths
    """

    return split_and_match_files_list(paths.split(','))


</t>
<t tx="ekr.20220525082933.887">def check_follow_imports(choice: str) -&gt; str:
    choices = ['normal', 'silent', 'skip', 'error']
    if choice not in choices:
        raise argparse.ArgumentTypeError(
            "invalid choice '{}' (choose from {})".format(
                choice,
                ', '.join(f"'{x}'" for x in choices)))
    return choice


</t>
<t tx="ekr.20220525082933.888"># For most options, the type of the default value set in options.py is
# sufficient, and we don't have to do anything here.  This table
# exists to specify types for values initialized to None or container
# types.
ini_config_types: Final[Dict[str, _INI_PARSER_CALLABLE]] = {
    'python_version': parse_version,
    'strict_optional_whitelist': lambda s: s.split(),
    'custom_typing_module': str,
    'custom_typeshed_dir': expand_path,
    'mypy_path': lambda s: [expand_path(p.strip()) for p in re.split('[,:]', s)],
    'files': split_and_match_files,
    'quickstart_file': expand_path,
    'junit_xml': expand_path,
    # These two are for backwards compatibility
    'silent_imports': bool,
    'almost_silent': bool,
    'follow_imports': check_follow_imports,
    'no_site_packages': bool,
    'plugins': lambda s: [p.strip() for p in s.split(',')],
    'always_true': lambda s: [p.strip() for p in s.split(',')],
    'always_false': lambda s: [p.strip() for p in s.split(',')],
    'disable_error_code': lambda s: [p.strip() for p in s.split(',')],
    'enable_error_code': lambda s: [p.strip() for p in s.split(',')],
    'package_root': lambda s: [p.strip() for p in s.split(',')],
    'cache_dir': expand_path,
    'python_executable': expand_path,
    'strict': bool,
    'exclude': lambda s: [s.strip()],
}

# Reuse the ini_config_types and overwrite the diff
toml_config_types: Final[Dict[str, _INI_PARSER_CALLABLE]] = ini_config_types.copy()
toml_config_types.update({
    'python_version': parse_version,
    'strict_optional_whitelist': try_split,
    'mypy_path': lambda s: [expand_path(p) for p in try_split(s, '[,:]')],
    'files': lambda s: split_and_match_files_list(try_split(s)),
    'follow_imports': lambda s: check_follow_imports(str(s)),
    'plugins': try_split,
    'always_true': try_split,
    'always_false': try_split,
    'disable_error_code': try_split,
    'enable_error_code': try_split,
    'package_root': try_split,
    'exclude': str_or_array_as_list,
})


</t>
<t tx="ekr.20220525082933.889">def parse_config_file(options: Options, set_strict_flags: Callable[[], None],
                      filename: Optional[str],
                      stdout: Optional[TextIO] = None,
                      stderr: Optional[TextIO] = None) -&gt; None:
    """Parse a config file into an Options object.

    Errors are written to stderr but are not fatal.

    If filename is None, fall back to default config files.
    """
    stdout = stdout or sys.stdout
    stderr = stderr or sys.stderr

    if filename is not None:
        config_files: Tuple[str, ...] = (filename,)
    else:
        config_files = tuple(map(os.path.expanduser, defaults.CONFIG_FILES))

    config_parser = configparser.RawConfigParser()

    for config_file in config_files:
        if not os.path.exists(config_file):
            continue
        try:
            if is_toml(config_file):
                with open(config_file, "rb") as f:
                    toml_data = tomllib.load(f)
                # Filter down to just mypy relevant toml keys
                toml_data = toml_data.get('tool', {})
                if 'mypy' not in toml_data:
                    continue
                toml_data = {'mypy': toml_data['mypy']}
                parser: MutableMapping[str, Any] = destructure_overrides(toml_data)
                config_types = toml_config_types
            else:
                config_parser.read(config_file)
                parser = config_parser
                config_types = ini_config_types
        except (tomllib.TOMLDecodeError, configparser.Error, ConfigTOMLValueError) as err:
            print(f"{config_file}: {err}", file=stderr)
        else:
            if config_file in defaults.SHARED_CONFIG_FILES and 'mypy' not in parser:
                continue
            file_read = config_file
            options.config_file = file_read
            break
    else:
        return

    os.environ['MYPY_CONFIG_FILE_DIR'] = os.path.dirname(
            os.path.abspath(config_file))

    if 'mypy' not in parser:
        if filename or file_read not in defaults.SHARED_CONFIG_FILES:
            print(f"{file_read}: No [mypy] section in config file", file=stderr)
    else:
        section = parser['mypy']
        prefix = f"{file_read}: [mypy]: "
        updates, report_dirs = parse_section(
            prefix, options, set_strict_flags, section, config_types, stderr)
        for k, v in updates.items():
            setattr(options, k, v)
        options.report_dirs.update(report_dirs)

    for name, section in parser.items():
        if name.startswith('mypy-'):
            prefix = get_prefix(file_read, name)
            updates, report_dirs = parse_section(
                prefix, options, set_strict_flags, section, config_types, stderr)
            if report_dirs:
                print("%sPer-module sections should not specify reports (%s)" %
                      (prefix, ', '.join(s + '_report' for s in sorted(report_dirs))),
                      file=stderr)
            if set(updates) - PER_MODULE_OPTIONS:
                print("%sPer-module sections should only specify per-module flags (%s)" %
                      (prefix, ', '.join(sorted(set(updates) - PER_MODULE_OPTIONS))),
                      file=stderr)
                updates = {k: v for k, v in updates.items() if k in PER_MODULE_OPTIONS}
            globs = name[5:]
            for glob in globs.split(','):
                # For backwards compatibility, replace (back)slashes with dots.
                glob = glob.replace(os.sep, '.')
                if os.altsep:
                    glob = glob.replace(os.altsep, '.')

                if (any(c in glob for c in '?[]!') or
                        any('*' in x and x != '*' for x in glob.split('.'))):
                    print("%sPatterns must be fully-qualified module names, optionally "
                          "with '*' in some components (e.g spam.*.eggs.*)"
                          % prefix,
                          file=stderr)
                else:
                    options.per_module_options[glob] = updates


</t>
<t tx="ekr.20220525082933.89">def execute(command: List[str], fail_on_error: bool = True) -&gt; Tuple[str, str, int]:
    proc = subprocess.Popen(
        ' '.join(command),
        stderr=subprocess.PIPE,
        stdout=subprocess.PIPE,
        shell=True)
    stdout_bytes, stderr_bytes = proc.communicate()  # type: Tuple[bytes, bytes]
    stdout, stderr = stdout_bytes.decode('utf-8'), stderr_bytes.decode('utf-8')
    if fail_on_error and proc.returncode != 0:
        print('EXECUTED COMMAND:', repr(command))
        print('RETURN CODE:', proc.returncode)
        print()
        print('STDOUT:')
        print_offset(stdout)
        print('STDERR:')
        print_offset(stderr)
        raise RuntimeError('Unexpected error from external tool.')
    return stdout, stderr, proc.returncode


</t>
<t tx="ekr.20220525082933.890">def get_prefix(file_read: str, name: str) -&gt; str:
    if is_toml(file_read):
        module_name_str = 'module = "%s"' % '-'.join(name.split('-')[1:])
    else:
        module_name_str = name

    return f'{file_read}: [{module_name_str}]: '


</t>
<t tx="ekr.20220525082933.891">def is_toml(filename: str) -&gt; bool:
    return filename.lower().endswith('.toml')


</t>
<t tx="ekr.20220525082933.892">def destructure_overrides(toml_data: Dict[str, Any]) -&gt; Dict[str, Any]:
    """Take the new [[tool.mypy.overrides]] section array in the pyproject.toml file,
    and convert it back to a flatter structure that the existing config_parser can handle.

    E.g. the following pyproject.toml file:

        [[tool.mypy.overrides]]
        module = [
            "a.b",
            "b.*"
        ]
        disallow_untyped_defs = true

        [[tool.mypy.overrides]]
        module = 'c'
        disallow_untyped_defs = false

    Would map to the following config dict that it would have gotten from parsing an equivalent
    ini file:

        {
            "mypy-a.b": {
                disallow_untyped_defs = true,
            },
            "mypy-b.*": {
                disallow_untyped_defs = true,
            },
            "mypy-c": {
                disallow_untyped_defs: false,
            },
        }
    """
    if 'overrides' not in toml_data['mypy']:
        return toml_data

    if not isinstance(toml_data['mypy']['overrides'], list):
        raise ConfigTOMLValueError("tool.mypy.overrides sections must be an array. Please make "
                         "sure you are using double brackets like so: [[tool.mypy.overrides]]")

    result = toml_data.copy()
    for override in result['mypy']['overrides']:
        if 'module' not in override:
            raise ConfigTOMLValueError("toml config file contains a [[tool.mypy.overrides]] "
                             "section, but no module to override was specified.")

        if isinstance(override['module'], str):
            modules = [override['module']]
        elif isinstance(override['module'], list):
            modules = override['module']
        else:
            raise ConfigTOMLValueError("toml config file contains a [[tool.mypy.overrides]] "
                             "section with a module value that is not a string or a list of "
                             "strings")

        for module in modules:
            module_overrides = override.copy()
            del module_overrides['module']
            old_config_name = f'mypy-{module}'
            if old_config_name not in result:
                result[old_config_name] = module_overrides
            else:
                for new_key, new_value in module_overrides.items():
                    if (new_key in result[old_config_name] and
                            result[old_config_name][new_key] != new_value):
                        raise ConfigTOMLValueError("toml config file contains "
                                         "[[tool.mypy.overrides]] sections with conflicting "
                                         "values. Module '%s' has two different values for '%s'"
                                         % (module, new_key))
                    result[old_config_name][new_key] = new_value

    del result['mypy']['overrides']
    return result


</t>
<t tx="ekr.20220525082933.893">def parse_section(prefix: str, template: Options,
                  set_strict_flags: Callable[[], None],
                  section: Mapping[str, Any],
                  config_types: Dict[str, Any],
                  stderr: TextIO = sys.stderr
                  ) -&gt; Tuple[Dict[str, object], Dict[str, str]]:
    """Parse one section of a config file.

    Returns a dict of option values encountered, and a dict of report directories.
    """
    results: Dict[str, object] = {}
    report_dirs: Dict[str, str] = {}
    for key in section:
        invert = False
        options_key = key
        if key in config_types:
            ct = config_types[key]
        else:
            dv = None
            # We have to keep new_semantic_analyzer in Options
            # for plugin compatibility but it is not a valid option anymore.
            assert hasattr(template, 'new_semantic_analyzer')
            if key != 'new_semantic_analyzer':
                dv = getattr(template, key, None)
            if dv is None:
                if key.endswith('_report'):
                    report_type = key[:-7].replace('_', '-')
                    if report_type in defaults.REPORTER_NAMES:
                        report_dirs[report_type] = str(section[key])
                    else:
                        print(f"{prefix}Unrecognized report type: {key}",
                              file=stderr)
                    continue
                if key.startswith('x_'):
                    pass  # Don't complain about `x_blah` flags
                elif key.startswith('no_') and hasattr(template, key[3:]):
                    options_key = key[3:]
                    invert = True
                elif key.startswith('allow') and hasattr(template, 'dis' + key):
                    options_key = 'dis' + key
                    invert = True
                elif key.startswith('disallow') and hasattr(template, key[3:]):
                    options_key = key[3:]
                    invert = True
                elif key == 'strict':
                    pass  # Special handling below
                else:
                    print(f"{prefix}Unrecognized option: {key} = {section[key]}",
                          file=stderr)
                if invert:
                    dv = getattr(template, options_key, None)
                else:
                    continue
            ct = type(dv)
        v: Any = None
        try:
            if ct is bool:
                if isinstance(section, dict):
                    v = convert_to_boolean(section.get(key))
                else:
                    v = section.getboolean(key)  # type: ignore[attr-defined]  # Until better stub
                if invert:
                    v = not v
            elif callable(ct):
                if invert:
                    print(f"{prefix}Can not invert non-boolean key {options_key}",
                          file=stderr)
                    continue
                try:
                    v = ct(section.get(key))
                except argparse.ArgumentTypeError as err:
                    print(f"{prefix}{key}: {err}", file=stderr)
                    continue
            else:
                print(f"{prefix}Don't know what type {key} should have", file=stderr)
                continue
        except ValueError as err:
            print(f"{prefix}{key}: {err}", file=stderr)
            continue
        if key == 'strict':
            if v:
                set_strict_flags()
            continue
        if key == 'silent_imports':
            print("%ssilent_imports has been replaced by "
                  "ignore_missing_imports=True; follow_imports=skip" % prefix, file=stderr)
            if v:
                if 'ignore_missing_imports' not in results:
                    results['ignore_missing_imports'] = True
                if 'follow_imports' not in results:
                    results['follow_imports'] = 'skip'
        if key == 'almost_silent':
            print("%salmost_silent has been replaced by "
                  "follow_imports=error" % prefix, file=stderr)
            if v:
                if 'follow_imports' not in results:
                    results['follow_imports'] = 'error'
        results[options_key] = v
    return results, report_dirs


</t>
<t tx="ekr.20220525082933.894">def convert_to_boolean(value: Optional[Any]) -&gt; bool:
    """Return a boolean value translating from other types if necessary."""
    if isinstance(value, bool):
        return value
    if not isinstance(value, str):
        value = str(value)
    if value.lower() not in configparser.RawConfigParser.BOOLEAN_STATES:
        raise ValueError(f'Not a boolean: {value}')
    return configparser.RawConfigParser.BOOLEAN_STATES[value.lower()]


</t>
<t tx="ekr.20220525082933.895">def split_directive(s: str) -&gt; Tuple[List[str], List[str]]:
    """Split s on commas, except during quoted sections.

    Returns the parts and a list of error messages."""
    parts = []
    cur: List[str] = []
    errors = []
    i = 0
    while i &lt; len(s):
        if s[i] == ',':
            parts.append(''.join(cur).strip())
            cur = []
        elif s[i] == '"':
            i += 1
            while i &lt; len(s) and s[i] != '"':
                cur.append(s[i])
                i += 1
            if i == len(s):
                errors.append("Unterminated quote in configuration comment")
                cur.clear()
        else:
            cur.append(s[i])
        i += 1
    if cur:
        parts.append(''.join(cur).strip())

    return parts, errors


</t>
<t tx="ekr.20220525082933.896">def mypy_comments_to_config_map(line: str,
                                template: Options) -&gt; Tuple[Dict[str, str], List[str]]:
    """Rewrite the mypy comment syntax into ini file syntax.

    Returns
    """
    options = {}
    entries, errors = split_directive(line)
    for entry in entries:
        if '=' not in entry:
            name = entry
            value = None
        else:
            name, value = (x.strip() for x in entry.split('=', 1))

        name = name.replace('-', '_')
        if value is None:
            value = 'True'
        options[name] = value

    return options, errors


</t>
<t tx="ekr.20220525082933.897">def parse_mypy_comments(
        args: List[Tuple[int, str]],
        template: Options) -&gt; Tuple[Dict[str, object], List[Tuple[int, str]]]:
    """Parse a collection of inline mypy: configuration comments.

    Returns a dictionary of options to be applied and a list of error messages
    generated.
    """

    errors: List[Tuple[int, str]] = []
    sections = {}

    for lineno, line in args:
        # In order to easily match the behavior for bools, we abuse configparser.
        # Oddly, the only way to get the SectionProxy object with the getboolean
        # method is to create a config parser.
        parser = configparser.RawConfigParser()
        options, parse_errors = mypy_comments_to_config_map(line, template)
        parser['dummy'] = options
        errors.extend((lineno, x) for x in parse_errors)

        stderr = StringIO()
        strict_found = False

        def set_strict_flags() -&gt; None:
            nonlocal strict_found
            strict_found = True

        new_sections, reports = parse_section(
            '', template, set_strict_flags, parser['dummy'], ini_config_types, stderr=stderr)
        errors.extend((lineno, x) for x in stderr.getvalue().strip().split('\n') if x)
        if reports:
            errors.append((lineno, "Reports not supported in inline configuration"))
        if strict_found:
            errors.append((lineno,
                           'Setting "strict" not supported in inline configuration: specify it in '
                           'a configuration file instead, or set individual inline flags '
                           '(see "mypy -h" for the list of flags enabled in strict mode)'))

        sections.update(new_sections)

    return sections, errors


</t>
<t tx="ekr.20220525082933.898">def get_config_module_names(filename: Optional[str], modules: List[str]) -&gt; str:
    if not filename or not modules:
        return ''

    if not is_toml(filename):
        return ", ".join(f"[mypy-{module}]" for module in modules)

    return "module = ['%s']" % ("', '".join(sorted(modules)))


</t>
<t tx="ekr.20220525082933.899">class ConfigTOMLValueError(ValueError):
    pass
</t>
<t tx="ekr.20220525082933.90">def ensure_environment_is_ready(mypy_path: str, temp_repo_path: str, mypy_cache_path: str) -&gt; None:
    os.chdir(mypy_path)
    delete_folder(temp_repo_path)
    delete_folder(mypy_cache_path)


</t>
<t tx="ekr.20220525082933.900">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Type inference constraints."""

from typing import TYPE_CHECKING, Iterable, List, Optional, Sequence
from typing_extensions import Final

from mypy.types import (
    CallableType, Type, TypeVisitor, UnboundType, AnyType, NoneType, TypeVarType, Instance,
    TupleType, TypedDictType, UnionType, Overloaded, ErasedType, PartialType, DeletedType,
    UninhabitedType, TypeType, TypeVarId, TypeQuery, is_named_instance, TypeOfAny, LiteralType,
    ProperType, ParamSpecType, get_proper_type, TypeAliasType, is_union_with_any,
    UnpackType, callable_with_ellipsis, Parameters, TUPLE_LIKE_INSTANCE_NAMES, TypeVarTupleType,
)
from mypy.maptype import map_instance_to_supertype
import mypy.subtypes
import mypy.sametypes
import mypy.typeops
from mypy.erasetype import erase_typevars
from mypy.nodes import COVARIANT, CONTRAVARIANT, ArgKind
from mypy.argmap import ArgTypeExpander
from mypy.typestate import TypeState

if TYPE_CHECKING:
    from mypy.infer import ArgumentInferContext

SUBTYPE_OF: Final = 0
SUPERTYPE_OF: Final = 1


@others
</t>
<t tx="ekr.20220525082933.901">class Constraint:
    """A representation of a type constraint.

    It can be either T &lt;: type or T :&gt; type (T is a type variable).
    """

    type_var: TypeVarId
    op = 0           # SUBTYPE_OF or SUPERTYPE_OF
    target: Type

    @others
</t>
<t tx="ekr.20220525082933.902">def __init__(self, type_var: TypeVarId, op: int, target: Type) -&gt; None:
    self.type_var = type_var
    self.op = op
    self.target = target

</t>
<t tx="ekr.20220525082933.903">def __repr__(self) -&gt; str:
    op_str = '&lt;:'
    if self.op == SUPERTYPE_OF:
        op_str = ':&gt;'
    return f'{self.type_var} {op_str} {self.target}'


</t>
<t tx="ekr.20220525082933.904">def infer_constraints_for_callable(
        callee: CallableType,
        arg_types: Sequence[Optional[Type]],
        arg_kinds: List[ArgKind],
        formal_to_actual: List[List[int]],
        context: 'ArgumentInferContext') -&gt; List[Constraint]:
    """Infer type variable constraints for a callable and actual arguments.

    Return a list of constraints.
    """
    constraints: List[Constraint] = []
    mapper = ArgTypeExpander(context)

    for i, actuals in enumerate(formal_to_actual):
        for actual in actuals:
            actual_arg_type = arg_types[actual]
            if actual_arg_type is None:
                continue

            actual_type = mapper.expand_actual_type(actual_arg_type, arg_kinds[actual],
                                                    callee.arg_names[i], callee.arg_kinds[i])
            c = infer_constraints(callee.arg_types[i], actual_type, SUPERTYPE_OF)
            constraints.extend(c)

    return constraints


</t>
<t tx="ekr.20220525082933.905">def infer_constraints(template: Type, actual: Type,
                      direction: int) -&gt; List[Constraint]:
    """Infer type constraints.

    Match a template type, which may contain type variable references,
    recursively against a type which does not contain (the same) type
    variable references. The result is a list of type constrains of
    form 'T is a supertype/subtype of x', where T is a type variable
    present in the template and x is a type without reference to type
    variables present in the template.

    Assume T and S are type variables. Now the following results can be
    calculated (read as '(template, actual) --&gt; result'):

      (T, X)            --&gt;  T :&gt; X
      (X[T], X[Y])      --&gt;  T &lt;: Y and T :&gt; Y
      ((T, T), (X, Y))  --&gt;  T :&gt; X and T :&gt; Y
      ((T, S), (X, Y))  --&gt;  T :&gt; X and S :&gt; Y
      (X[T], Any)       --&gt;  T &lt;: Any and T :&gt; Any

    The constraints are represented as Constraint objects.
    """
    if any(get_proper_type(template) == get_proper_type(t) for t in TypeState._inferring):
        return []
    if isinstance(template, TypeAliasType) and template.is_recursive:
        # This case requires special care because it may cause infinite recursion.
        TypeState._inferring.append(template)
        res = _infer_constraints(template, actual, direction)
        TypeState._inferring.pop()
        return res
    return _infer_constraints(template, actual, direction)


</t>
<t tx="ekr.20220525082933.906">def _infer_constraints(template: Type, actual: Type,
                       direction: int) -&gt; List[Constraint]:

    orig_template = template
    template = get_proper_type(template)
    actual = get_proper_type(actual)

    # Type inference shouldn't be affected by whether union types have been simplified.
    # We however keep any ErasedType items, so that the caller will see it when using
    # checkexpr.has_erased_component().
    if isinstance(template, UnionType):
        template = mypy.typeops.make_simplified_union(template.items, keep_erased=True)
    if isinstance(actual, UnionType):
        actual = mypy.typeops.make_simplified_union(actual.items, keep_erased=True)

    # Ignore Any types from the type suggestion engine to avoid them
    # causing us to infer Any in situations where a better job could
    # be done otherwise. (This can produce false positives but that
    # doesn't really matter because it is all heuristic anyway.)
    if isinstance(actual, AnyType) and actual.type_of_any == TypeOfAny.suggestion_engine:
        return []

    # If the template is simply a type variable, emit a Constraint directly.
    # We need to handle this case before handling Unions for two reasons:
    #  1. "T &lt;: Union[U1, U2]" is not equivalent to "T &lt;: U1 or T &lt;: U2",
    #     because T can itself be a union (notably, Union[U1, U2] itself).
    #  2. "T :&gt; Union[U1, U2]" is logically equivalent to "T :&gt; U1 and
    #     T :&gt; U2", but they are not equivalent to the constraint solver,
    #     which never introduces new Union types (it uses join() instead).
    if isinstance(template, TypeVarType):
        return [Constraint(template.id, direction, actual)]

    # Now handle the case of either template or actual being a Union.
    # For a Union to be a subtype of another type, every item of the Union
    # must be a subtype of that type, so concatenate the constraints.
    if direction == SUBTYPE_OF and isinstance(template, UnionType):
        res = []
        for t_item in template.items:
            res.extend(infer_constraints(t_item, actual, direction))
        return res
    if direction == SUPERTYPE_OF and isinstance(actual, UnionType):
        res = []
        for a_item in actual.items:
            res.extend(infer_constraints(orig_template, a_item, direction))
        return res

    # Now the potential subtype is known not to be a Union or a type
    # variable that we are solving for. In that case, for a Union to
    # be a supertype of the potential subtype, some item of the Union
    # must be a supertype of it.
    if direction == SUBTYPE_OF and isinstance(actual, UnionType):
        # If some of items is not a complete type, disregard that.
        items = simplify_away_incomplete_types(actual.items)
        # We infer constraints eagerly -- try to find constraints for a type
        # variable if possible. This seems to help with some real-world
        # use cases.
        return any_constraints(
            [infer_constraints_if_possible(template, a_item, direction)
             for a_item in items],
            eager=True)
    if direction == SUPERTYPE_OF and isinstance(template, UnionType):
        # When the template is a union, we are okay with leaving some
        # type variables indeterminate. This helps with some special
        # cases, though this isn't very principled.
        return any_constraints(
            [infer_constraints_if_possible(t_item, actual, direction)
             for t_item in template.items],
            eager=False)

    # Remaining cases are handled by ConstraintBuilderVisitor.
    return template.accept(ConstraintBuilderVisitor(actual, direction))


</t>
<t tx="ekr.20220525082933.907">def infer_constraints_if_possible(template: Type, actual: Type,
                                  direction: int) -&gt; Optional[List[Constraint]]:
    """Like infer_constraints, but return None if the input relation is
    known to be unsatisfiable, for example if template=List[T] and actual=int.
    (In this case infer_constraints would return [], just like it would for
    an automatically satisfied relation like template=List[T] and actual=object.)
    """
    if (direction == SUBTYPE_OF and
            not mypy.subtypes.is_subtype(erase_typevars(template), actual)):
        return None
    if (direction == SUPERTYPE_OF and
            not mypy.subtypes.is_subtype(actual, erase_typevars(template))):
        return None
    if (direction == SUPERTYPE_OF and isinstance(template, TypeVarType) and
            not mypy.subtypes.is_subtype(actual, erase_typevars(template.upper_bound))):
        # This is not caught by the above branch because of the erase_typevars() call,
        # that would return 'Any' for a type variable.
        return None
    return infer_constraints(template, actual, direction)


</t>
<t tx="ekr.20220525082933.908">def select_trivial(options: Sequence[Optional[List[Constraint]]]) -&gt; List[List[Constraint]]:
    """Select only those lists where each item is a constraint against Any."""
    res = []
    for option in options:
        if option is None:
            continue
        if all(isinstance(get_proper_type(c.target), AnyType) for c in option):
            res.append(option)
    return res


</t>
<t tx="ekr.20220525082933.909">def merge_with_any(constraint: Constraint) -&gt; Constraint:
    """Transform a constraint target into a union with given Any type."""
    target = constraint.target
    if is_union_with_any(target):
        # Do not produce redundant unions.
        return constraint
    # TODO: if we will support multiple sources Any, use this here instead.
    any_type = AnyType(TypeOfAny.implementation_artifact)
    return Constraint(
        constraint.type_var,
        constraint.op,
        UnionType.make_union([target, any_type], target.line, target.column),
    )


</t>
<t tx="ekr.20220525082933.91">def initialize_repo(repo_url: str, temp_repo_path: str, branch: str) -&gt; None:
    print(f"Cloning repo {repo_url} to {temp_repo_path}")
    execute(["git", "clone", repo_url, temp_repo_path])
    if branch is not None:
        print(f"Checking out branch {branch}")
        execute(["git", "-C", temp_repo_path, "checkout", branch])


</t>
<t tx="ekr.20220525082933.910">def any_constraints(options: List[Optional[List[Constraint]]], eager: bool) -&gt; List[Constraint]:
    """Deduce what we can from a collection of constraint lists.

    It's a given that at least one of the lists must be satisfied. A
    None element in the list of options represents an unsatisfiable
    constraint and is ignored.  Ignore empty constraint lists if eager
    is true -- they are always trivially satisfiable.
    """
    if eager:
        valid_options = [option for option in options if option]
    else:
        valid_options = [option for option in options if option is not None]

    if not valid_options:
        return []

    if len(valid_options) == 1:
        return valid_options[0]

    if all(is_same_constraints(valid_options[0], c) for c in valid_options[1:]):
        # Multiple sets of constraints that are all the same. Just pick any one of them.
        return valid_options[0]

    if all(is_similar_constraints(valid_options[0], c) for c in valid_options[1:]):
        # All options have same structure. In this case we can merge-in trivial
        # options (i.e. those that only have Any) and try again.
        # TODO: More generally, if a given (variable, direction) pair appears in
        # every option, combine the bounds with meet/join always, not just for Any.
        trivial_options = select_trivial(valid_options)
        if trivial_options and len(trivial_options) &lt; len(valid_options):
            merged_options = []
            for option in valid_options:
                if option in trivial_options:
                    continue
                if option is not None:
                    merged_option: Optional[List[Constraint]] = [
                        merge_with_any(c) for c in option
                    ]
                else:
                    merged_option = None
                merged_options.append(merged_option)
            return any_constraints([option for option in merged_options], eager)
    # Otherwise, there are either no valid options or multiple, inconsistent valid
    # options. Give up and deduce nothing.
    return []


</t>
<t tx="ekr.20220525082933.911">def is_same_constraints(x: List[Constraint], y: List[Constraint]) -&gt; bool:
    for c1 in x:
        if not any(is_same_constraint(c1, c2) for c2 in y):
            return False
    for c1 in y:
        if not any(is_same_constraint(c1, c2) for c2 in x):
            return False
    return True


</t>
<t tx="ekr.20220525082933.912">def is_same_constraint(c1: Constraint, c2: Constraint) -&gt; bool:
    # Ignore direction when comparing constraints against Any.
    skip_op_check = (
        isinstance(get_proper_type(c1.target), AnyType) and
        isinstance(get_proper_type(c2.target), AnyType)
    )
    return (c1.type_var == c2.type_var
            and (c1.op == c2.op or skip_op_check)
            and mypy.sametypes.is_same_type(c1.target, c2.target))


</t>
<t tx="ekr.20220525082933.913">def is_similar_constraints(x: List[Constraint], y: List[Constraint]) -&gt; bool:
    """Check that two lists of constraints have similar structure.

    This means that each list has same type variable plus direction pairs (i.e we
    ignore the target). Except for constraints where target is Any type, there
    we ignore direction as well.
    """
    return _is_similar_constraints(x, y) and _is_similar_constraints(y, x)


</t>
<t tx="ekr.20220525082933.914">def _is_similar_constraints(x: List[Constraint], y: List[Constraint]) -&gt; bool:
    """Check that every constraint in the first list has a similar one in the second.

    See docstring above for definition of similarity.
    """
    for c1 in x:
        has_similar = False
        for c2 in y:
            # Ignore direction when either constraint is against Any.
            skip_op_check = (
                isinstance(get_proper_type(c1.target), AnyType) or
                isinstance(get_proper_type(c2.target), AnyType)
            )
            if c1.type_var == c2.type_var and (c1.op == c2.op or skip_op_check):
                has_similar = True
                break
        if not has_similar:
            return False
    return True


</t>
<t tx="ekr.20220525082933.915">def simplify_away_incomplete_types(types: Iterable[Type]) -&gt; List[Type]:
    complete = [typ for typ in types if is_complete_type(typ)]
    if complete:
        return complete
    else:
        return list(types)


</t>
<t tx="ekr.20220525082933.916">def is_complete_type(typ: Type) -&gt; bool:
    """Is a type complete?

    A complete doesn't have uninhabited type components or (when not in strict
    optional mode) None components.
    """
    return typ.accept(CompleteTypeVisitor())


</t>
<t tx="ekr.20220525082933.917">class CompleteTypeVisitor(TypeQuery[bool]):
    def __init__(self) -&gt; None:
        super().__init__(all)

    def visit_uninhabited_type(self, t: UninhabitedType) -&gt; bool:
        return False


</t>
<t tx="ekr.20220525082933.918">class ConstraintBuilderVisitor(TypeVisitor[List[Constraint]]):
    """Visitor class for inferring type constraints."""

    # The type that is compared against a template
    # TODO: The value may be None. Is that actually correct?
    actual: ProperType

    @others
</t>
<t tx="ekr.20220525082933.919">def __init__(self, actual: ProperType, direction: int) -&gt; None:
    # Direction must be SUBTYPE_OF or SUPERTYPE_OF.
    self.actual = actual
    self.direction = direction

</t>
<t tx="ekr.20220525082933.92">def get_commits(repo_folder_path: str, commit_range: str) -&gt; List[Tuple[str, str]]:
    raw_data, _stderr, _errcode = execute([
        "git", "-C", repo_folder_path, "log", "--reverse", "--oneline", commit_range])
    output = []
    for line in raw_data.strip().split('\n'):
        commit_id, _, message = line.partition(' ')
        output.append((commit_id, message))
    return output


</t>
<t tx="ekr.20220525082933.920"># Trivial leaf types

</t>
<t tx="ekr.20220525082933.921">def visit_unbound_type(self, template: UnboundType) -&gt; List[Constraint]:
    return []

</t>
<t tx="ekr.20220525082933.922">def visit_any(self, template: AnyType) -&gt; List[Constraint]:
    return []

</t>
<t tx="ekr.20220525082933.923">def visit_none_type(self, template: NoneType) -&gt; List[Constraint]:
    return []

</t>
<t tx="ekr.20220525082933.924">def visit_uninhabited_type(self, template: UninhabitedType) -&gt; List[Constraint]:
    return []

</t>
<t tx="ekr.20220525082933.925">def visit_erased_type(self, template: ErasedType) -&gt; List[Constraint]:
    return []

</t>
<t tx="ekr.20220525082933.926">def visit_deleted_type(self, template: DeletedType) -&gt; List[Constraint]:
    return []

</t>
<t tx="ekr.20220525082933.927">def visit_literal_type(self, template: LiteralType) -&gt; List[Constraint]:
    return []

</t>
<t tx="ekr.20220525082933.928"># Errors

</t>
<t tx="ekr.20220525082933.929">def visit_partial_type(self, template: PartialType) -&gt; List[Constraint]:
    # We can't do anything useful with a partial type here.
    assert False, "Internal error"

</t>
<t tx="ekr.20220525082933.93">def get_commits_starting_at(repo_folder_path: str, start_commit: str) -&gt; List[Tuple[str, str]]:
    print(f"Fetching commits starting at {start_commit}")
    return get_commits(repo_folder_path, f'{start_commit}^..HEAD')


</t>
<t tx="ekr.20220525082933.930"># Non-trivial leaf type

</t>
<t tx="ekr.20220525082933.931">def visit_type_var(self, template: TypeVarType) -&gt; List[Constraint]:
    assert False, ("Unexpected TypeVarType in ConstraintBuilderVisitor"
                   " (should have been handled in infer_constraints)")

</t>
<t tx="ekr.20220525082933.932">def visit_param_spec(self, template: ParamSpecType) -&gt; List[Constraint]:
    # Can't infer ParamSpecs from component values (only via Callable[P, T]).
    return []

</t>
<t tx="ekr.20220525082933.933">def visit_type_var_tuple(self, template: TypeVarTupleType) -&gt; List[Constraint]:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082933.934">def visit_unpack_type(self, template: UnpackType) -&gt; List[Constraint]:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082933.935">def visit_parameters(self, template: Parameters) -&gt; List[Constraint]:
    # constraining Any against C[P] turns into infer_against_any([P], Any)
    # ... which seems like the only case this can happen. Better to fail loudly.
    if isinstance(self.actual, AnyType):
        return self.infer_against_any(template.arg_types, self.actual)
    raise RuntimeError("Parameters cannot be constrained to")

</t>
<t tx="ekr.20220525082933.936"># Non-leaf types

</t>
<t tx="ekr.20220525082933.937">def visit_instance(self, template: Instance) -&gt; List[Constraint]:
    original_actual = actual = self.actual
    res: List[Constraint] = []
    if isinstance(actual, (CallableType, Overloaded)) and template.type.is_protocol:
        if template.type.protocol_members == ['__call__']:
            # Special case: a generic callback protocol
            if not any(mypy.sametypes.is_same_type(template, t)
                       for t in template.type.inferring):
                template.type.inferring.append(template)
                call = mypy.subtypes.find_member('__call__', template, actual,
                                                 is_operator=True)
                assert call is not None
                if mypy.subtypes.is_subtype(actual, erase_typevars(call)):
                    subres = infer_constraints(call, actual, self.direction)
                    res.extend(subres)
                template.type.inferring.pop()
                return res
    if isinstance(actual, CallableType) and actual.fallback is not None:
        actual = actual.fallback
    if isinstance(actual, Overloaded) and actual.fallback is not None:
        actual = actual.fallback
    if isinstance(actual, TypedDictType):
        actual = actual.as_anonymous().fallback
    if isinstance(actual, LiteralType):
        actual = actual.fallback
    if isinstance(actual, Instance):
        instance = actual
        erased = erase_typevars(template)
        assert isinstance(erased, Instance)  # type: ignore
        # We always try nominal inference if possible,
        # it is much faster than the structural one.
        if (self.direction == SUBTYPE_OF and
                template.type.has_base(instance.type.fullname)):
            mapped = map_instance_to_supertype(template, instance.type)
            tvars = mapped.type.defn.type_vars
            # N.B: We use zip instead of indexing because the lengths might have
            # mismatches during daemon reprocessing.
            for tvar, mapped_arg, instance_arg in zip(tvars, mapped.args, instance.args):
                # TODO(PEP612): More ParamSpec work (or is Parameters the only thing accepted)
                if isinstance(tvar, TypeVarType):
                    # The constraints for generic type parameters depend on variance.
                    # Include constraints from both directions if invariant.
                    if tvar.variance != CONTRAVARIANT:
                        res.extend(infer_constraints(
                            mapped_arg, instance_arg, self.direction))
                    if tvar.variance != COVARIANT:
                        res.extend(infer_constraints(
                            mapped_arg, instance_arg, neg_op(self.direction)))
                elif isinstance(tvar, ParamSpecType) and isinstance(mapped_arg, ParamSpecType):
                    suffix = get_proper_type(instance_arg)

                    if isinstance(suffix, CallableType):
                        prefix = mapped_arg.prefix
                        from_concat = bool(prefix.arg_types) or suffix.from_concatenate
                        suffix = suffix.copy_modified(from_concatenate=from_concat)

                    if isinstance(suffix, Parameters) or isinstance(suffix, CallableType):
                        # no such thing as variance for ParamSpecs
                        # TODO: is there a case I am missing?
                        # TODO: constraints between prefixes
                        prefix = mapped_arg.prefix
                        suffix = suffix.copy_modified(
                            suffix.arg_types[len(prefix.arg_types):],
                            suffix.arg_kinds[len(prefix.arg_kinds):],
                            suffix.arg_names[len(prefix.arg_names):])
                        res.append(Constraint(mapped_arg.id, SUPERTYPE_OF, suffix))
                    elif isinstance(suffix, ParamSpecType):
                        res.append(Constraint(mapped_arg.id, SUPERTYPE_OF, suffix))

            return res
        elif (self.direction == SUPERTYPE_OF and
                instance.type.has_base(template.type.fullname)):
            mapped = map_instance_to_supertype(instance, template.type)
            tvars = template.type.defn.type_vars
            # N.B: We use zip instead of indexing because the lengths might have
            # mismatches during daemon reprocessing.
            for tvar, mapped_arg, template_arg in zip(tvars, mapped.args, template.args):
                if isinstance(tvar, TypeVarType):
                    # The constraints for generic type parameters depend on variance.
                    # Include constraints from both directions if invariant.
                    if tvar.variance != CONTRAVARIANT:
                        res.extend(infer_constraints(
                            template_arg, mapped_arg, self.direction))
                    if tvar.variance != COVARIANT:
                        res.extend(infer_constraints(
                            template_arg, mapped_arg, neg_op(self.direction)))
                elif (isinstance(tvar, ParamSpecType) and
                      isinstance(template_arg, ParamSpecType)):
                    suffix = get_proper_type(mapped_arg)

                    if isinstance(suffix, CallableType):
                        prefix = template_arg.prefix
                        from_concat = bool(prefix.arg_types) or suffix.from_concatenate
                        suffix = suffix.copy_modified(from_concatenate=from_concat)

                    if isinstance(suffix, Parameters) or isinstance(suffix, CallableType):
                        # no such thing as variance for ParamSpecs
                        # TODO: is there a case I am missing?
                        # TODO: constraints between prefixes
                        prefix = template_arg.prefix

                        suffix = suffix.copy_modified(
                            suffix.arg_types[len(prefix.arg_types):],
                            suffix.arg_kinds[len(prefix.arg_kinds):],
                            suffix.arg_names[len(prefix.arg_names):])
                        res.append(Constraint(template_arg.id, SUPERTYPE_OF, suffix))
                    elif isinstance(suffix, ParamSpecType):
                        res.append(Constraint(template_arg.id, SUPERTYPE_OF, suffix))
            return res
        if (template.type.is_protocol and self.direction == SUPERTYPE_OF and
                # We avoid infinite recursion for structural subtypes by checking
                # whether this type already appeared in the inference chain.
                # This is a conservative way to break the inference cycles.
                # It never produces any "false" constraints but gives up soon
                # on purely structural inference cycles, see #3829.
                # Note that we use is_protocol_implementation instead of is_subtype
                # because some type may be considered a subtype of a protocol
                # due to _promote, but still not implement the protocol.
                not any(mypy.sametypes.is_same_type(template, t)
                        for t in template.type.inferring) and
                mypy.subtypes.is_protocol_implementation(instance, erased)):
            template.type.inferring.append(template)
            res.extend(self.infer_constraints_from_protocol_members(
                instance, template, original_actual, template))
            template.type.inferring.pop()
            return res
        elif (instance.type.is_protocol and self.direction == SUBTYPE_OF and
              # We avoid infinite recursion for structural subtypes also here.
              not any(mypy.sametypes.is_same_type(instance, i)
                      for i in instance.type.inferring) and
              mypy.subtypes.is_protocol_implementation(erased, instance)):
            instance.type.inferring.append(instance)
            res.extend(self.infer_constraints_from_protocol_members(
                instance, template, template, instance))
            instance.type.inferring.pop()
            return res
    if isinstance(actual, AnyType):
        return self.infer_against_any(template.args, actual)
    if (isinstance(actual, TupleType)
            and is_named_instance(template, TUPLE_LIKE_INSTANCE_NAMES)
            and self.direction == SUPERTYPE_OF):
        for item in actual.items:
            cb = infer_constraints(template.args[0], item, SUPERTYPE_OF)
            res.extend(cb)
        return res
    elif isinstance(actual, TupleType) and self.direction == SUPERTYPE_OF:
        return infer_constraints(template,
                                 mypy.typeops.tuple_fallback(actual),
                                 self.direction)
    elif isinstance(actual, TypeVarType):
        if not actual.values:
            return infer_constraints(template, actual.upper_bound, self.direction)
        return []
    else:
        return []

</t>
<t tx="ekr.20220525082933.938">def infer_constraints_from_protocol_members(self,
                                            instance: Instance, template: Instance,
                                            subtype: Type, protocol: Instance,
                                            ) -&gt; List[Constraint]:
    """Infer constraints for situations where either 'template' or 'instance' is a protocol.

    The 'protocol' is the one of two that is an instance of protocol type, 'subtype'
    is the type used to bind self during inference. Currently, we just infer constrains for
    every protocol member type (both ways for settable members).
    """
    res = []
    for member in protocol.type.protocol_members:
        inst = mypy.subtypes.find_member(member, instance, subtype)
        temp = mypy.subtypes.find_member(member, template, subtype)
        if inst is None or temp is None:
            return []  # See #11020
        # The above is safe since at this point we know that 'instance' is a subtype
        # of (erased) 'template', therefore it defines all protocol members
        res.extend(infer_constraints(temp, inst, self.direction))
        if (mypy.subtypes.IS_SETTABLE in
                mypy.subtypes.get_member_flags(member, protocol.type)):
            # Settable members are invariant, add opposite constraints
            res.extend(infer_constraints(temp, inst, neg_op(self.direction)))
    return res

</t>
<t tx="ekr.20220525082933.939">def visit_callable_type(self, template: CallableType) -&gt; List[Constraint]:
    if isinstance(self.actual, CallableType):
        res: List[Constraint] = []
        cactual = self.actual
        param_spec = template.param_spec()
        if param_spec is None:
            # FIX verify argument counts
            # FIX what if one of the functions is generic

            # We can't infer constraints from arguments if the template is Callable[..., T]
            # (with literal '...').
            if not template.is_ellipsis_args:
                # The lengths should match, but don't crash (it will error elsewhere).
                for t, a in zip(template.arg_types, cactual.arg_types):
                    # Negate direction due to function argument type contravariance.
                    res.extend(infer_constraints(t, a, neg_op(self.direction)))
        else:
            # sometimes, it appears we try to get constraints between two paramspec callables?
            # TODO: Direction
            # TODO: check the prefixes match
            prefix = param_spec.prefix
            prefix_len = len(prefix.arg_types)
            cactual_ps = cactual.param_spec()

            if not cactual_ps:
                res.append(Constraint(param_spec.id,
                                      SUBTYPE_OF,
                                      cactual.copy_modified(
                                        arg_types=cactual.arg_types[prefix_len:],
                                        arg_kinds=cactual.arg_kinds[prefix_len:],
                                        arg_names=cactual.arg_names[prefix_len:],
                                        ret_type=NoneType())))
            else:
                res.append(Constraint(param_spec.id, SUBTYPE_OF, cactual_ps))

            # compare prefixes
            cactual_prefix = cactual.copy_modified(
                arg_types=cactual.arg_types[:prefix_len],
                arg_kinds=cactual.arg_kinds[:prefix_len],
                arg_names=cactual.arg_names[:prefix_len])

            # TODO: see above "FIX" comments for param_spec is None case
            # TODO: this assume positional arguments
            for t, a in zip(prefix.arg_types, cactual_prefix.arg_types):
                res.extend(infer_constraints(t, a, neg_op(self.direction)))

        template_ret_type, cactual_ret_type = template.ret_type, cactual.ret_type
        if template.type_guard is not None:
            template_ret_type = template.type_guard
        if cactual.type_guard is not None:
            cactual_ret_type = cactual.type_guard

        res.extend(infer_constraints(template_ret_type, cactual_ret_type,
                                     self.direction))
        return res
    elif isinstance(self.actual, AnyType):
        param_spec = template.param_spec()
        any_type = AnyType(TypeOfAny.from_another_any, source_any=self.actual)
        if param_spec is None:
            # FIX what if generic
            res = self.infer_against_any(template.arg_types, self.actual)
        else:
            res = [Constraint(param_spec.id,
                              SUBTYPE_OF,
                              callable_with_ellipsis(any_type, any_type, template.fallback))]
        res.extend(infer_constraints(template.ret_type, any_type, self.direction))
        return res
    elif isinstance(self.actual, Overloaded):
        return self.infer_against_overloaded(self.actual, template)
    elif isinstance(self.actual, TypeType):
        return infer_constraints(template.ret_type, self.actual.item, self.direction)
    elif isinstance(self.actual, Instance):
        # Instances with __call__ method defined are considered structural
        # subtypes of Callable with a compatible signature.
        call = mypy.subtypes.find_member('__call__', self.actual, self.actual,
                                         is_operator=True)
        if call:
            return infer_constraints(template, call, self.direction)
        else:
            return []
    else:
        return []

</t>
<t tx="ekr.20220525082933.94">def get_nth_commit(repo_folder_path: str, n: int) -&gt; Tuple[str, str]:
    print(f"Fetching last {n} commits (or all, if there are fewer commits than n)")
    return get_commits(repo_folder_path, f'-{n}')[0]


</t>
<t tx="ekr.20220525082933.940">def infer_against_overloaded(self, overloaded: Overloaded,
                             template: CallableType) -&gt; List[Constraint]:
    # Create constraints by matching an overloaded type against a template.
    # This is tricky to do in general. We cheat by only matching against
    # the first overload item that is callable compatible. This
    # seems to work somewhat well, but we should really use a more
    # reliable technique.
    item = find_matching_overload_item(overloaded, template)
    return infer_constraints(template, item, self.direction)

</t>
<t tx="ekr.20220525082933.941">def visit_tuple_type(self, template: TupleType) -&gt; List[Constraint]:
    actual = self.actual
    # TODO: Support other items in the tuple besides Unpack
    # TODO: Support subclasses of Tuple
    is_varlength_tuple = (
        isinstance(actual, Instance)
        and actual.type.fullname == "builtins.tuple"
    )
    if len(template.items) == 1:
        item = get_proper_type(template.items[0])
        if isinstance(item, UnpackType):
            unpacked_type = get_proper_type(item.type)
            if isinstance(unpacked_type, TypeVarTupleType):
                if (
                    isinstance(actual, (TupleType, AnyType))
                    or is_varlength_tuple
                ):
                    return [Constraint(
                        type_var=unpacked_type.id,
                        op=self.direction,
                        target=actual,
                    )]

    if isinstance(actual, TupleType) and len(actual.items) == len(template.items):
        res: List[Constraint] = []
        for i in range(len(template.items)):
            res.extend(infer_constraints(template.items[i],
                                         actual.items[i],
                                         self.direction))
        return res
    elif isinstance(actual, AnyType):
        return self.infer_against_any(template.items, actual)
    else:
        return []

</t>
<t tx="ekr.20220525082933.942">def visit_typeddict_type(self, template: TypedDictType) -&gt; List[Constraint]:
    actual = self.actual
    if isinstance(actual, TypedDictType):
        res: List[Constraint] = []
        # NOTE: Non-matching keys are ignored. Compatibility is checked
        #       elsewhere so this shouldn't be unsafe.
        for (item_name, template_item_type, actual_item_type) in template.zip(actual):
            res.extend(infer_constraints(template_item_type,
                                         actual_item_type,
                                         self.direction))
        return res
    elif isinstance(actual, AnyType):
        return self.infer_against_any(template.items.values(), actual)
    else:
        return []

</t>
<t tx="ekr.20220525082933.943">def visit_union_type(self, template: UnionType) -&gt; List[Constraint]:
    assert False, ("Unexpected UnionType in ConstraintBuilderVisitor"
                   " (should have been handled in infer_constraints)")

</t>
<t tx="ekr.20220525082933.944">def visit_type_alias_type(self, template: TypeAliasType) -&gt; List[Constraint]:
    assert False, f"This should be never called, got {template}"

</t>
<t tx="ekr.20220525082933.945">def infer_against_any(self, types: Iterable[Type], any_type: AnyType) -&gt; List[Constraint]:
    res: List[Constraint] = []
    for t in types:
        # Note that we ignore variance and simply always use the
        # original direction. This is because for Any targets direction is
        # irrelevant in most cases, see e.g. is_same_constraint().
        res.extend(infer_constraints(t, any_type, self.direction))
    return res

</t>
<t tx="ekr.20220525082933.946">def visit_overloaded(self, template: Overloaded) -&gt; List[Constraint]:
    if isinstance(self.actual, CallableType):
        items = find_matching_overload_items(template, self.actual)
    else:
        items = template.items
    res: List[Constraint] = []
    for t in items:
        res.extend(infer_constraints(t, self.actual, self.direction))
    return res

</t>
<t tx="ekr.20220525082933.947">def visit_type_type(self, template: TypeType) -&gt; List[Constraint]:
    if isinstance(self.actual, CallableType):
        return infer_constraints(template.item, self.actual.ret_type, self.direction)
    elif isinstance(self.actual, Overloaded):
        return infer_constraints(template.item, self.actual.items[0].ret_type,
                                 self.direction)
    elif isinstance(self.actual, TypeType):
        return infer_constraints(template.item, self.actual.item, self.direction)
    elif isinstance(self.actual, AnyType):
        return infer_constraints(template.item, self.actual, self.direction)
    else:
        return []


</t>
<t tx="ekr.20220525082933.948">def neg_op(op: int) -&gt; int:
    """Map SubtypeOf to SupertypeOf and vice versa."""

    if op == SUBTYPE_OF:
        return SUPERTYPE_OF
    elif op == SUPERTYPE_OF:
        return SUBTYPE_OF
    else:
        raise ValueError(f'Invalid operator {op}')


</t>
<t tx="ekr.20220525082933.949">def find_matching_overload_item(overloaded: Overloaded, template: CallableType) -&gt; CallableType:
    """Disambiguate overload item against a template."""
    items = overloaded.items
    for item in items:
        # Return type may be indeterminate in the template, so ignore it when performing a
        # subtype check.
        if mypy.subtypes.is_callable_compatible(item, template,
                                                is_compat=mypy.subtypes.is_subtype,
                                                ignore_return=True):
            return item
    # Fall back to the first item if we can't find a match. This is totally arbitrary --
    # maybe we should just bail out at this point.
    return items[0]


</t>
<t tx="ekr.20220525082933.95">def run_mypy(target_file_path: Optional[str],
             mypy_cache_path: str,
             mypy_script: Optional[str],
             *,
             incremental: bool = False,
             daemon: bool = False,
             verbose: bool = False) -&gt; Tuple[float, str, Dict[str, Any]]:
    """Runs mypy against `target_file_path` and returns what mypy prints to stdout as a string.

    If `incremental` is set to True, this function will use store and retrieve all caching data
    inside `mypy_cache_path`. If `verbose` is set to True, this function will pass the "-v -v"
    flags to mypy to make it output debugging information.

    If `daemon` is True, we use daemon mode; the daemon must be started and stopped by the caller.
    """
    stats = {}  # type: Dict[str, Any]
    if daemon:
        command = DAEMON_CMD + ["check", "-v"]
    else:
        if mypy_script is None:
            command = ["python3", "-m", "mypy"]
        else:
            command = [mypy_script]
        command.extend(["--cache-dir", mypy_cache_path])
        if incremental:
            command.append("--incremental")
        if verbose:
            command.extend(["-v", "-v"])
    if target_file_path is not None:
        command.append(target_file_path)
    start = time.time()
    output, stderr, _ = execute(command, False)
    if stderr != "":
        output = stderr
    else:
        if daemon:
            output, stats = filter_daemon_stats(output)
    runtime = time.time() - start
    return runtime, output, stats


</t>
<t tx="ekr.20220525082933.950">def find_matching_overload_items(overloaded: Overloaded,
                                 template: CallableType) -&gt; List[CallableType]:
    """Like find_matching_overload_item, but return all matches, not just the first."""
    items = overloaded.items
    res = []
    for item in items:
        # Return type may be indeterminate in the template, so ignore it when performing a
        # subtype check.
        if mypy.subtypes.is_callable_compatible(item, template,
                                                is_compat=mypy.subtypes.is_subtype,
                                                ignore_return=True):
            res.append(item)
    if not res:
        # Falling back to all items if we can't find a match is pretty arbitrary, but
        # it maintains backward compatibility.
        res = items[:]
    return res
</t>
<t tx="ekr.20220525082933.951">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from typing import Any, cast

from mypy.types import (
    ProperType, UnboundType, AnyType, NoneType, UninhabitedType, ErasedType, DeletedType,
    Instance, TypeVarType, ParamSpecType, PartialType, CallableType, TupleType, TypedDictType,
    LiteralType, UnionType, Overloaded, TypeType, TypeAliasType, UnpackType, Parameters,
    TypeVarTupleType
)
from mypy.type_visitor import TypeVisitor


@others
</t>
<t tx="ekr.20220525082933.952">def copy_type(t: ProperType) -&gt; ProperType:
    """Create a shallow copy of a type.

    This can be used to mutate the copy with truthiness information.

    Classes compiled with mypyc don't support copy.copy(), so we need
    a custom implementation.
    """
    return t.accept(TypeShallowCopier())


</t>
<t tx="ekr.20220525082933.953">class TypeShallowCopier(TypeVisitor[ProperType]):
    @others
</t>
<t tx="ekr.20220525082933.954">def visit_unbound_type(self, t: UnboundType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20220525082933.955">def visit_any(self, t: AnyType) -&gt; ProperType:
    return self.copy_common(t, AnyType(t.type_of_any, t.source_any, t.missing_import_name))

</t>
<t tx="ekr.20220525082933.956">def visit_none_type(self, t: NoneType) -&gt; ProperType:
    return self.copy_common(t, NoneType())

</t>
<t tx="ekr.20220525082933.957">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; ProperType:
    dup = UninhabitedType(t.is_noreturn)
    dup.ambiguous = t.ambiguous
    return self.copy_common(t, dup)

</t>
<t tx="ekr.20220525082933.958">def visit_erased_type(self, t: ErasedType) -&gt; ProperType:
    return self.copy_common(t, ErasedType())

</t>
<t tx="ekr.20220525082933.959">def visit_deleted_type(self, t: DeletedType) -&gt; ProperType:
    return self.copy_common(t, DeletedType(t.source))

</t>
<t tx="ekr.20220525082933.96">def filter_daemon_stats(output: str) -&gt; Tuple[str, Dict[str, Any]]:
    stats = {}  # type: Dict[str, Any]
    lines = output.splitlines()
    output_lines = []
    for line in lines:
        m = re.match(r'(\w+)\s+:\s+(.*)', line)
        if m:
            key, value = m.groups()
            stats[key] = value
        else:
            output_lines.append(line)
    if output_lines:
        output_lines.append('\n')
    return '\n'.join(output_lines), stats


</t>
<t tx="ekr.20220525082933.960">def visit_instance(self, t: Instance) -&gt; ProperType:
    dup = Instance(t.type, t.args, last_known_value=t.last_known_value)
    dup.invalid = t.invalid
    return self.copy_common(t, dup)

</t>
<t tx="ekr.20220525082933.961">def visit_type_var(self, t: TypeVarType) -&gt; ProperType:
    dup = TypeVarType(
        t.name,
        t.fullname,
        t.id,
        values=t.values,
        upper_bound=t.upper_bound,
        variance=t.variance,
    )
    return self.copy_common(t, dup)

</t>
<t tx="ekr.20220525082933.962">def visit_param_spec(self, t: ParamSpecType) -&gt; ProperType:
    dup = ParamSpecType(t.name, t.fullname, t.id, t.flavor, t.upper_bound, prefix=t.prefix)
    return self.copy_common(t, dup)

</t>
<t tx="ekr.20220525082933.963">def visit_parameters(self, t: Parameters) -&gt; ProperType:
    dup = Parameters(t.arg_types, t.arg_kinds, t.arg_names,
                     variables=t.variables,
                     is_ellipsis_args=t.is_ellipsis_args)
    return self.copy_common(t, dup)

</t>
<t tx="ekr.20220525082933.964">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; ProperType:
    dup = TypeVarTupleType(t.name, t.fullname, t.id, t.upper_bound)
    return self.copy_common(t, dup)

</t>
<t tx="ekr.20220525082933.965">def visit_unpack_type(self, t: UnpackType) -&gt; ProperType:
    dup = UnpackType(t.type)
    return self.copy_common(t, dup)

</t>
<t tx="ekr.20220525082933.966">def visit_partial_type(self, t: PartialType) -&gt; ProperType:
    return self.copy_common(t, PartialType(t.type, t.var, t.value_type))

</t>
<t tx="ekr.20220525082933.967">def visit_callable_type(self, t: CallableType) -&gt; ProperType:
    return self.copy_common(t, t.copy_modified())

</t>
<t tx="ekr.20220525082933.968">def visit_tuple_type(self, t: TupleType) -&gt; ProperType:
    return self.copy_common(t, TupleType(t.items, t.partial_fallback, implicit=t.implicit))

</t>
<t tx="ekr.20220525082933.969">def visit_typeddict_type(self, t: TypedDictType) -&gt; ProperType:
    return self.copy_common(t, TypedDictType(t.items, t.required_keys, t.fallback))

</t>
<t tx="ekr.20220525082933.97">def start_daemon(mypy_cache_path: str) -&gt; None:
    cmd = DAEMON_CMD + ["restart", "--log-file", "./@incr-chk-logs", "--", "--cache-dir", mypy_cache_path]
    execute(cmd)


</t>
<t tx="ekr.20220525082933.970">def visit_literal_type(self, t: LiteralType) -&gt; ProperType:
    return self.copy_common(t, LiteralType(value=t.value, fallback=t.fallback))

</t>
<t tx="ekr.20220525082933.971">def visit_union_type(self, t: UnionType) -&gt; ProperType:
    return self.copy_common(t, UnionType(t.items))

</t>
<t tx="ekr.20220525082933.972">def visit_overloaded(self, t: Overloaded) -&gt; ProperType:
    return self.copy_common(t, Overloaded(items=t.items))

</t>
<t tx="ekr.20220525082933.973">def visit_type_type(self, t: TypeType) -&gt; ProperType:
    # Use cast since the type annotations in TypeType are imprecise.
    return self.copy_common(t, TypeType(cast(Any, t.item)))

</t>
<t tx="ekr.20220525082933.974">def visit_type_alias_type(self, t: TypeAliasType) -&gt; ProperType:
    assert False, "only ProperTypes supported"

</t>
<t tx="ekr.20220525082933.975">def copy_common(self, t: ProperType, t2: ProperType) -&gt; ProperType:
    t2.line = t.line
    t2.column = t.column
    t2.can_be_false = t.can_be_false
    t2.can_be_true = t.can_be_true
    return t2
</t>
<t tx="ekr.20220525082933.976">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
import os

from typing_extensions import Final

PYTHON2_VERSION: Final = (2, 7)
PYTHON3_VERSION: Final = (3, 6)
PYTHON3_VERSION_MIN: Final = (3, 4)
CACHE_DIR: Final = ".mypy_cache"
CONFIG_FILE: Final = ["mypy.ini", ".mypy.ini"]
PYPROJECT_CONFIG_FILES: Final = [
    "pyproject.toml",
]
SHARED_CONFIG_FILES: Final = [
    "setup.cfg",
]
USER_CONFIG_FILES: Final = [
    "~/.config/mypy/config",
    "~/.mypy.ini",
]
if os.environ.get("XDG_CONFIG_HOME"):
    USER_CONFIG_FILES.insert(0, os.path.join(os.environ["XDG_CONFIG_HOME"], "mypy/config"))

CONFIG_FILES: Final = (
    CONFIG_FILE + PYPROJECT_CONFIG_FILES + SHARED_CONFIG_FILES + USER_CONFIG_FILES
)

# This must include all reporters defined in mypy.report. This is defined here
# to make reporter names available without importing mypy.report -- this speeds
# up startup.
REPORTER_NAMES: Final = [
    "linecount",
    "any-exprs",
    "linecoverage",
    "memory-xml",
    "cobertura-xml",
    "xml",
    "xslt-html",
    "xslt-txt",
    "html",
    "txt",
    "lineprecision",
]

# Threshold after which we sometimes filter out most errors to avoid very
# verbose output
MANY_ERRORS_THRESHOLD: Final = 200
</t>
<t tx="ekr.20220525082933.977">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
import sys

from typing import Any, Callable

if sys.platform == 'win32':
    import ctypes
    from ctypes.wintypes import DWORD, HANDLE
    import subprocess

    PROCESS_QUERY_LIMITED_INFORMATION = ctypes.c_ulong(0x1000)

    kernel32 = ctypes.windll.kernel32
    OpenProcess: Callable[[DWORD, int, int], HANDLE] = kernel32.OpenProcess
    GetExitCodeProcess: Callable[[HANDLE, Any], int] = kernel32.GetExitCodeProcess
else:
    import os
    import signal


@others
</t>
<t tx="ekr.20220525082933.978">def alive(pid: int) -&gt; bool:
    """Is the process alive?"""
    if sys.platform == 'win32':
        # why can't anything be easy...
        status = DWORD()
        handle = OpenProcess(PROCESS_QUERY_LIMITED_INFORMATION,
                             0,
                             pid)
        GetExitCodeProcess(handle, ctypes.byref(status))
        return status.value == 259  # STILL_ACTIVE
    else:
        try:
            os.kill(pid, 0)
        except OSError:
            return False
        return True


</t>
<t tx="ekr.20220525082933.979">def kill(pid: int) -&gt; None:
    """Kill the process."""
    if sys.platform == 'win32':
        subprocess.check_output(f"taskkill /pid {pid} /f /t")
    else:
        os.kill(pid, signal.SIGKILL)
</t>
<t tx="ekr.20220525082933.98">def stop_daemon() -&gt; None:
    execute(DAEMON_CMD + ["stop"])


</t>
<t tx="ekr.20220525082933.980">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Server for mypy daemon mode.

This implements a daemon process which keeps useful state in memory
to enable fine-grained incremental reprocessing of changes.
"""

import argparse
import base64
import io
import json
import os
import pickle
import subprocess
import sys
import time
import traceback
from contextlib import redirect_stderr, redirect_stdout

from typing import AbstractSet, Any, Callable, Dict, List, Optional, Sequence, Tuple, Set
from typing_extensions import Final

import mypy.build
import mypy.errors
import mypy.main
from mypy.find_sources import create_source_list, InvalidSourceList
from mypy.server.update import FineGrainedBuildManager, refresh_suppressed_submodules
from mypy.dmypy_util import receive
from mypy.ipc import IPCServer
from mypy.fscache import FileSystemCache
from mypy.fswatcher import FileSystemWatcher, FileData
from mypy.modulefinder import BuildSource, compute_search_paths, FindModuleCache, SearchPaths
from mypy.options import Options
from mypy.suggestions import SuggestionFailure, SuggestionEngine
from mypy.typestate import reset_global_state
from mypy.version import __version__
from mypy.util import FancyFormatter, count_stats

MEM_PROFILE: Final = False  # If True, dump memory profile after initialization

if sys.platform == 'win32':
    from subprocess import STARTUPINFO

    def daemonize(options: Options,
                  status_file: str,
                  timeout: Optional[int] = None,
                  log_file: Optional[str] = None) -&gt; int:
        """Create the daemon process via "dmypy daemon" and pass options via command line

        When creating the daemon grandchild, we create it in a new console, which is
        started hidden. We cannot use DETACHED_PROCESS since it will cause console windows
        to pop up when starting. See
        https://github.com/python/cpython/pull/4150#issuecomment-340215696
        for more on why we can't have nice things.

        It also pickles the options to be unpickled by mypy.
        """
        command = [sys.executable, '-m', 'mypy.dmypy', '--status-file', status_file, 'daemon']
        pickled_options = pickle.dumps((options.snapshot(), timeout, log_file))
        command.append(f'--options-data="{base64.b64encode(pickled_options).decode()}"')
        info = STARTUPINFO()
        info.dwFlags = 0x1  # STARTF_USESHOWWINDOW aka use wShowWindow's value
        info.wShowWindow = 0  # SW_HIDE aka make the window invisible
        try:
            subprocess.Popen(command,
                             creationflags=0x10,  # CREATE_NEW_CONSOLE
                             startupinfo=info)
            return 0
        except subprocess.CalledProcessError as e:
            return e.returncode

else:
    def _daemonize_cb(func: Callable[[], None], log_file: Optional[str] = None) -&gt; int:
        """Arrange to call func() in a grandchild of the current process.

        Return 0 for success, exit status for failure, negative if
        subprocess killed by signal.
        """
        # See https://stackoverflow.com/questions/473620/how-do-you-create-a-daemon-in-python
        sys.stdout.flush()
        sys.stderr.flush()
        pid = os.fork()
        if pid:
            # Parent process: wait for child in case things go bad there.
            npid, sts = os.waitpid(pid, 0)
            sig = sts &amp; 0xff
            if sig:
                print("Child killed by signal", sig)
                return -sig
            sts = sts &gt;&gt; 8
            if sts:
                print("Child exit status", sts)
            return sts
        # Child process: do a bunch of UNIX stuff and then fork a grandchild.
        try:
            os.setsid()  # Detach controlling terminal
            os.umask(0o27)
            devnull = os.open('/dev/null', os.O_RDWR)
            os.dup2(devnull, 0)
            os.dup2(devnull, 1)
            os.dup2(devnull, 2)
            os.close(devnull)
            pid = os.fork()
            if pid:
                # Child is done, exit to parent.
                os._exit(0)
            # Grandchild: run the server.
            if log_file:
                sys.stdout = sys.stderr = open(log_file, 'a', buffering=1)
                fd = sys.stdout.fileno()
                os.dup2(fd, 2)
                os.dup2(fd, 1)
            func()
        finally:
            # Make sure we never get back into the caller.
            os._exit(1)

    def daemonize(options: Options,
                  status_file: str,
                  timeout: Optional[int] = None,
                  log_file: Optional[str] = None) -&gt; int:
        """Run the mypy daemon in a grandchild of the current process

        Return 0 for success, exit status for failure, negative if
        subprocess killed by signal.
        """
        return _daemonize_cb(Server(options, status_file, timeout).serve, log_file)

# Server code.

CONNECTION_NAME: Final = "dmypy"


@others
</t>
<t tx="ekr.20220525082933.981">def process_start_options(flags: List[str], allow_sources: bool) -&gt; Options:
    _, options = mypy.main.process_options(
        ['-i'] + flags, require_targets=False, server_options=True
    )
    if options.report_dirs:
        print("dmypy: Ignoring report generation settings. Start/restart cannot generate reports.")
    if options.junit_xml:
        print("dmypy: Ignoring report generation settings. "
              "Start/restart does not support --junit-xml. Pass it to check/recheck instead")
        options.junit_xml = None
    if not options.incremental:
        sys.exit("dmypy: start/restart should not disable incremental mode")
    if options.follow_imports not in ('skip', 'error', 'normal'):
        sys.exit("dmypy: follow-imports=silent not supported")
    return options


</t>
<t tx="ekr.20220525082933.982">def ignore_suppressed_imports(module: str) -&gt; bool:
    """Can we skip looking for newly unsuppressed imports to module?"""
    # Various submodules of 'encodings' can be suppressed, since it
    # uses module-level '__getattr__'. Skip them since there are many
    # of them, and following imports to them is kind of pointless.
    return module.startswith('encodings.')


</t>
<t tx="ekr.20220525082933.983">ModulePathPair = Tuple[str, str]
ModulePathPairs = List[ModulePathPair]
ChangesAndRemovals = Tuple[ModulePathPairs, ModulePathPairs]


</t>
<t tx="ekr.20220525082933.984">class Server:

    # NOTE: the instance is constructed in the parent process but
    # serve() is called in the grandchild (by daemonize()).

    @others
</t>
<t tx="ekr.20220525082933.985">def __init__(self, options: Options,
             status_file: str,
             timeout: Optional[int] = None) -&gt; None:
    """Initialize the server with the desired mypy flags."""
    self.options = options
    # Snapshot the options info before we muck with it, to detect changes
    self.options_snapshot = options.snapshot()
    self.timeout = timeout
    self.fine_grained_manager: Optional[FineGrainedBuildManager] = None

    if os.path.isfile(status_file):
        os.unlink(status_file)

    self.fscache = FileSystemCache()

    options.raise_exceptions = True
    options.incremental = True
    options.fine_grained_incremental = True
    options.show_traceback = True
    if options.use_fine_grained_cache:
        # Using fine_grained_cache implies generating and caring
        # about the fine grained cache
        options.cache_fine_grained = True
    else:
        options.cache_dir = os.devnull
    # Fine-grained incremental doesn't support general partial types
    # (details in https://github.com/python/mypy/issues/4492)
    options.local_partial_types = True
    self.status_file = status_file

    # Since the object is created in the parent process we can check
    # the output terminal options here.
    self.formatter = FancyFormatter(sys.stdout, sys.stderr, options.show_error_codes)

</t>
<t tx="ekr.20220525082933.986">def _response_metadata(self) -&gt; Dict[str, str]:
    py_version = f'{self.options.python_version[0]}_{self.options.python_version[1]}'
    return {
        'platform': self.options.platform,
        'python_version': py_version,
    }

</t>
<t tx="ekr.20220525082933.987">def serve(self) -&gt; None:
    """Serve requests, synchronously (no thread or fork)."""
    command = None
    server = IPCServer(CONNECTION_NAME, self.timeout)
    try:
        with open(self.status_file, 'w') as f:
            json.dump({'pid': os.getpid(), 'connection_name': server.connection_name}, f)
            f.write('\n')  # I like my JSON with a trailing newline
        while True:
            with server:
                data = receive(server)
                resp: Dict[str, Any] = {}
                if 'command' not in data:
                    resp = {'error': "No command found in request"}
                else:
                    command = data['command']
                    if not isinstance(command, str):
                        resp = {'error': "Command is not a string"}
                    else:
                        command = data.pop('command')
                        try:
                            resp = self.run_command(command, data)
                        except Exception:
                            # If we are crashing, report the crash to the client
                            tb = traceback.format_exception(*sys.exc_info())
                            resp = {'error': "Daemon crashed!\n" + "".join(tb)}
                            resp.update(self._response_metadata())
                            server.write(json.dumps(resp).encode('utf8'))
                            raise
                try:
                    resp.update(self._response_metadata())
                    server.write(json.dumps(resp).encode('utf8'))
                except OSError:
                    pass  # Maybe the client hung up
                if command == 'stop':
                    reset_global_state()
                    sys.exit(0)
    finally:
        # If the final command is something other than a clean
        # stop, remove the status file. (We can't just
        # simplify the logic and always remove the file, since
        # that could cause us to remove a future server's
        # status file.)
        if command != 'stop':
            os.unlink(self.status_file)
        try:
            server.cleanup()  # try to remove the socket dir on Linux
        except OSError:
            pass
        exc_info = sys.exc_info()
        if exc_info[0] and exc_info[0] is not SystemExit:
            traceback.print_exception(*exc_info)

</t>
<t tx="ekr.20220525082933.988">def run_command(self, command: str, data: Dict[str, object]) -&gt; Dict[str, object]:
    """Run a specific command from the registry."""
    key = 'cmd_' + command
    method = getattr(self.__class__, key, None)
    if method is None:
        return {'error': f"Unrecognized command '{command}'"}
    else:
        if command not in {'check', 'recheck', 'run'}:
            # Only the above commands use some error formatting.
            del data['is_tty']
            del data['terminal_width']
        return method(self, **data)

</t>
<t tx="ekr.20220525082933.989"># Command functions (run in the server via RPC).

</t>
<t tx="ekr.20220525082933.99">def load_cache(incremental_cache_path: str = CACHE_PATH) -&gt; JsonDict:
    if os.path.exists(incremental_cache_path):
        with open(incremental_cache_path) as stream:
            return json.load(stream)
    else:
        return {}


</t>
<t tx="ekr.20220525082933.990">def cmd_status(self, fswatcher_dump_file: Optional[str] = None) -&gt; Dict[str, object]:
    """Return daemon status."""
    res: Dict[str, object] = {}
    res.update(get_meminfo())
    if fswatcher_dump_file:
        data = self.fswatcher.dump_file_data() if hasattr(self, 'fswatcher') else {}
        # Using .dumps and then writing was noticeably faster than using dump
        s = json.dumps(data)
        with open(fswatcher_dump_file, 'w') as f:
            f.write(s)
    return res

</t>
<t tx="ekr.20220525082933.991">def cmd_stop(self) -&gt; Dict[str, object]:
    """Stop daemon."""
    # We need to remove the status file *before* we complete the
    # RPC. Otherwise a race condition exists where a subsequent
    # command can see a status file from a dying server and think
    # it is a live one.
    os.unlink(self.status_file)
    return {}

</t>
<t tx="ekr.20220525082933.992">def cmd_run(self, version: str, args: Sequence[str],
            is_tty: bool, terminal_width: int) -&gt; Dict[str, object]:
    """Check a list of files, triggering a restart if needed."""
    stderr = io.StringIO()
    stdout = io.StringIO()
    try:
        # Process options can exit on improper arguments, so we need to catch that and
        # capture stderr so the client can report it
        with redirect_stderr(stderr):
            with redirect_stdout(stdout):
                sources, options = mypy.main.process_options(
                    ['-i'] + list(args),
                    require_targets=True,
                    server_options=True,
                    fscache=self.fscache,
                    program='mypy-daemon',
                    header=argparse.SUPPRESS)
        # Signal that we need to restart if the options have changed
        if self.options_snapshot != options.snapshot():
            return {'restart': 'configuration changed'}
        if __version__ != version:
            return {'restart': 'mypy version changed'}
        if self.fine_grained_manager:
            manager = self.fine_grained_manager.manager
            start_plugins_snapshot = manager.plugins_snapshot
            _, current_plugins_snapshot = mypy.build.load_plugins(
                options, manager.errors, sys.stdout, extra_plugins=()
            )
            if current_plugins_snapshot != start_plugins_snapshot:
                return {'restart': 'plugins changed'}
    except InvalidSourceList as err:
        return {'out': '', 'err': str(err), 'status': 2}
    except SystemExit as e:
        return {'out': stdout.getvalue(), 'err': stderr.getvalue(), 'status': e.code}
    return self.check(sources, is_tty, terminal_width)

</t>
<t tx="ekr.20220525082933.993">def cmd_check(self, files: Sequence[str],
              is_tty: bool, terminal_width: int) -&gt; Dict[str, object]:
    """Check a list of files."""
    try:
        sources = create_source_list(files, self.options, self.fscache)
    except InvalidSourceList as err:
        return {'out': '', 'err': str(err), 'status': 2}
    return self.check(sources, is_tty, terminal_width)

</t>
<t tx="ekr.20220525082933.994">def cmd_recheck(self,
                is_tty: bool,
                terminal_width: int,
                remove: Optional[List[str]] = None,
                update: Optional[List[str]] = None) -&gt; Dict[str, object]:
    """Check the same list of files we checked most recently.

    If remove/update is given, they modify the previous list;
    if all are None, stat() is called for each file in the previous list.
    """
    t0 = time.time()
    if not self.fine_grained_manager:
        return {'error': "Command 'recheck' is only valid after a 'check' command"}
    sources = self.previous_sources
    if remove:
        removals = set(remove)
        sources = [s for s in sources if s.path and s.path not in removals]
    if update:
        known = {s.path for s in sources if s.path}
        added = [p for p in update if p not in known]
        try:
            added_sources = create_source_list(added, self.options, self.fscache)
        except InvalidSourceList as err:
            return {'out': '', 'err': str(err), 'status': 2}
        sources = sources + added_sources  # Make a copy!
    t1 = time.time()
    manager = self.fine_grained_manager.manager
    manager.log(f"fine-grained increment: cmd_recheck: {t1 - t0:.3f}s")
    if not self.following_imports():
        messages = self.fine_grained_increment(sources, remove, update)
    else:
        assert remove is None and update is None
        messages = self.fine_grained_increment_follow_imports(sources)
    res = self.increment_output(messages, sources, is_tty, terminal_width)
    self.flush_caches()
    self.update_stats(res)
    return res

</t>
<t tx="ekr.20220525082933.995">def check(self, sources: List[BuildSource],
          is_tty: bool, terminal_width: int) -&gt; Dict[str, Any]:
    """Check using fine-grained incremental mode.

    If is_tty is True format the output nicely with colors and summary line
    (unless disabled in self.options). Also pass the terminal_width to formatter.
    """
    if not self.fine_grained_manager:
        res = self.initialize_fine_grained(sources, is_tty, terminal_width)
    else:
        if not self.following_imports():
            messages = self.fine_grained_increment(sources)
        else:
            messages = self.fine_grained_increment_follow_imports(sources)
        res = self.increment_output(messages, sources, is_tty, terminal_width)
    self.flush_caches()
    self.update_stats(res)
    return res

</t>
<t tx="ekr.20220525082933.996">def flush_caches(self) -&gt; None:
    self.fscache.flush()
    if self.fine_grained_manager:
        self.fine_grained_manager.flush_cache()

</t>
<t tx="ekr.20220525082933.997">def update_stats(self, res: Dict[str, Any]) -&gt; None:
    if self.fine_grained_manager:
        manager = self.fine_grained_manager.manager
        manager.dump_stats()
        res['stats'] = manager.stats
        manager.stats = {}

</t>
<t tx="ekr.20220525082933.998">def following_imports(self) -&gt; bool:
    """Are we following imports?"""
    # TODO: What about silent?
    return self.options.follow_imports == 'normal'

</t>
<t tx="ekr.20220525082933.999">def initialize_fine_grained(self, sources: List[BuildSource],
                            is_tty: bool, terminal_width: int) -&gt; Dict[str, Any]:
    self.fswatcher = FileSystemWatcher(self.fscache)
    t0 = time.time()
    self.update_sources(sources)
    t1 = time.time()
    try:
        result = mypy.build.build(sources=sources,
                                  options=self.options,
                                  fscache=self.fscache)
    except mypy.errors.CompileError as e:
        output = ''.join(s + '\n' for s in e.messages)
        if e.use_stdout:
            out, err = output, ''
        else:
            out, err = '', output
        return {'out': out, 'err': err, 'status': 2}
    messages = result.errors
    self.fine_grained_manager = FineGrainedBuildManager(result)

    if self.following_imports():
        sources = find_all_sources_in_build(self.fine_grained_manager.graph, sources)
        self.update_sources(sources)

    self.previous_sources = sources

    # If we are using the fine-grained cache, build hasn't actually done
    # the typechecking on the updated files yet.
    # Run a fine-grained update starting from the cached data
    if result.used_cache:
        t2 = time.time()
        # Pull times and hashes out of the saved_cache and stick them into
        # the fswatcher, so we pick up the changes.
        for state in self.fine_grained_manager.graph.values():
            meta = state.meta
            if meta is None: continue
            assert state.path is not None
            self.fswatcher.set_file_data(
                state.path,
                FileData(st_mtime=float(meta.mtime), st_size=meta.size, hash=meta.hash))

        changed, removed = self.find_changed(sources)
        changed += self.find_added_suppressed(self.fine_grained_manager.graph, set(),
                                              self.fine_grained_manager.manager.search_paths)

        # Find anything that has had its dependency list change
        for state in self.fine_grained_manager.graph.values():
            if not state.is_fresh():
                assert state.path is not None
                changed.append((state.id, state.path))

        t3 = time.time()
        # Run an update
        messages = self.fine_grained_manager.update(changed, removed)

        if self.following_imports():
            # We need to do another update to any new files found by following imports.
            messages = self.fine_grained_increment_follow_imports(sources)

        t4 = time.time()
        self.fine_grained_manager.manager.add_stats(
            update_sources_time=t1 - t0,
            build_time=t2 - t1,
            find_changes_time=t3 - t2,
            fg_update_time=t4 - t3,
            files_changed=len(removed) + len(changed))

    else:
        # Stores the initial state of sources as a side effect.
        self.fswatcher.find_changed()

    if MEM_PROFILE:
        from mypy.memprofile import print_memory_profile
        print_memory_profile(run_gc=False)

    status = 1 if messages else 0
    messages = self.pretty_messages(messages, len(sources), is_tty, terminal_width)
    return {'out': ''.join(s + '\n' for s in messages), 'err': '', 'status': status}

</t>
<t tx="ekr.20220525082934.1">def expand_type(typ: Type, env: Mapping[TypeVarId, Type]) -&gt; Type:
    """Substitute any type variable references in a type given by a type
    environment.
    """
    # TODO: use an overloaded signature? (ProperType stays proper after expansion.)
    return typ.accept(ExpandTypeVisitor(env))


</t>
<t tx="ekr.20220525082934.10">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082934.100"># BinOp(expr left, operator op, expr right)
def visit_BinOp(self, n: ast3.BinOp) -&gt; OpExpr:
    op = self.from_operator(n.op)

    if op is None:
        raise RuntimeError('cannot translate BinOp ' + str(type(n.op)))

    e = OpExpr(op, self.visit(n.left), self.visit(n.right))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1000">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_for_stmt(self)


</t>
<t tx="ekr.20220525082934.1001">class ReturnStmt(Statement):
    __slots__ = ('expr',)

    expr: Optional[Expression]

    @others
</t>
<t tx="ekr.20220525082934.1002">def __init__(self, expr: Optional[Expression]) -&gt; None:
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20220525082934.1003">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_return_stmt(self)


</t>
<t tx="ekr.20220525082934.1004">class AssertStmt(Statement):
    __slots__ = ('expr', 'msg')

    expr: Expression
    msg: Optional[Expression]

    @others
</t>
<t tx="ekr.20220525082934.1005">def __init__(self, expr: Expression, msg: Optional[Expression] = None) -&gt; None:
    super().__init__()
    self.expr = expr
    self.msg = msg

</t>
<t tx="ekr.20220525082934.1006">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_assert_stmt(self)


</t>
<t tx="ekr.20220525082934.1007">class DelStmt(Statement):
    __slots__ = ('expr',)

    expr: Lvalue

    @others
</t>
<t tx="ekr.20220525082934.1008">def __init__(self, expr: Lvalue) -&gt; None:
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20220525082934.1009">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_del_stmt(self)


</t>
<t tx="ekr.20220525082934.101"># UnaryOp(unaryop op, expr operand)
def visit_UnaryOp(self, n: ast3.UnaryOp) -&gt; UnaryExpr:
    op = None
    if isinstance(n.op, ast3.Invert):
        op = '~'
    elif isinstance(n.op, ast3.Not):
        op = 'not'
    elif isinstance(n.op, ast3.UAdd):
        op = '+'
    elif isinstance(n.op, ast3.USub):
        op = '-'

    if op is None:
        raise RuntimeError('cannot translate UnaryOp ' + str(type(n.op)))

    e = UnaryExpr(op, self.visit(n.operand))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1010">class BreakStmt(Statement):
    __slots__ = ()

    def accept(self, visitor: StatementVisitor[T]) -&gt; T:
        return visitor.visit_break_stmt(self)


</t>
<t tx="ekr.20220525082934.1011">class ContinueStmt(Statement):
    __slots__ = ()

    def accept(self, visitor: StatementVisitor[T]) -&gt; T:
        return visitor.visit_continue_stmt(self)


</t>
<t tx="ekr.20220525082934.1012">class PassStmt(Statement):
    __slots__ = ()

    def accept(self, visitor: StatementVisitor[T]) -&gt; T:
        return visitor.visit_pass_stmt(self)


</t>
<t tx="ekr.20220525082934.1013">class IfStmt(Statement):
    __slots__ = ('expr', 'body', 'else_body')

    expr: List[Expression]
    body: List[Block]
    else_body: Optional[Block]

    @others
</t>
<t tx="ekr.20220525082934.1014">def __init__(self, expr: List[Expression], body: List[Block],
             else_body: Optional[Block]) -&gt; None:
    super().__init__()
    self.expr = expr
    self.body = body
    self.else_body = else_body

</t>
<t tx="ekr.20220525082934.1015">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_if_stmt(self)


</t>
<t tx="ekr.20220525082934.1016">class RaiseStmt(Statement):
    __slots__ = ('expr', 'from_expr', 'legacy_mode')

    # Plain 'raise' is a valid statement.
    expr: Optional[Expression]
    from_expr: Optional[Expression]
    # Is set when python2 has `raise exc, msg, traceback`.
    legacy_mode: bool

    @others
</t>
<t tx="ekr.20220525082934.1017">def __init__(self, expr: Optional[Expression], from_expr: Optional[Expression]) -&gt; None:
    super().__init__()
    self.expr = expr
    self.from_expr = from_expr
    self.legacy_mode = False

</t>
<t tx="ekr.20220525082934.1018">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_raise_stmt(self)


</t>
<t tx="ekr.20220525082934.1019">class TryStmt(Statement):
    __slots__ = ('body', 'types', 'vars', 'handlers', 'else_body', 'finally_body')

    body: Block  # Try body
    # Plain 'except:' also possible
    types: List[Optional[Expression]]  # Except type expressions
    vars: List[Optional["NameExpr"]]  # Except variable names
    handlers: List[Block]  # Except bodies
    else_body: Optional[Block]
    finally_body: Optional[Block]

    @others
</t>
<t tx="ekr.20220525082934.102"># Lambda(arguments args, expr body)
def visit_Lambda(self, n: ast3.Lambda) -&gt; LambdaExpr:
    body = ast3.Return(n.body)
    body.lineno = n.body.lineno
    body.col_offset = n.body.col_offset

    e = LambdaExpr(self.transform_args(n.args, n.lineno),
                   self.as_required_block([body], n.lineno))
    e.set_line(n.lineno, n.col_offset)  # Overrides set_line -- can't use self.set_line
    return e

</t>
<t tx="ekr.20220525082934.1020">def __init__(self, body: Block, vars: List['Optional[NameExpr]'],
             types: List[Optional[Expression]],
             handlers: List[Block], else_body: Optional[Block],
             finally_body: Optional[Block]) -&gt; None:
    super().__init__()
    self.body = body
    self.vars = vars
    self.types = types
    self.handlers = handlers
    self.else_body = else_body
    self.finally_body = finally_body

</t>
<t tx="ekr.20220525082934.1021">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_try_stmt(self)


</t>
<t tx="ekr.20220525082934.1022">class WithStmt(Statement):
    __slots__ = ('expr', 'target', 'unanalyzed_type',
                 'analyzed_types', 'body', 'is_async')

    expr: List[Expression]
    target: List[Optional[Lvalue]]
    # Type given by type comments for target, can be None
    unanalyzed_type: Optional["mypy.types.Type"]
    # Semantically analyzed types from type comment (TypeList type expanded)
    analyzed_types: List["mypy.types.Type"]
    body: Block
    is_async: bool  # True if `async with ...` (PEP 492, Python 3.5)

    @others
</t>
<t tx="ekr.20220525082934.1023">def __init__(self, expr: List[Expression], target: List[Optional[Lvalue]],
             body: Block, target_type: 'Optional[mypy.types.Type]' = None) -&gt; None:
    super().__init__()
    self.expr = expr
    self.target = target
    self.unanalyzed_type = target_type
    self.analyzed_types = []
    self.body = body
    self.is_async = False

</t>
<t tx="ekr.20220525082934.1024">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_with_stmt(self)


</t>
<t tx="ekr.20220525082934.1025">class MatchStmt(Statement):
    subject: Expression
    patterns: List['Pattern']
    guards: List[Optional[Expression]]
    bodies: List[Block]

    @others
</t>
<t tx="ekr.20220525082934.1026">def __init__(self, subject: Expression, patterns: List['Pattern'],
             guards: List[Optional[Expression]], bodies: List[Block]) -&gt; None:
    super().__init__()
    assert len(patterns) == len(guards) == len(bodies)
    self.subject = subject
    self.patterns = patterns
    self.guards = guards
    self.bodies = bodies

</t>
<t tx="ekr.20220525082934.1027">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_match_stmt(self)


</t>
<t tx="ekr.20220525082934.1028">class PrintStmt(Statement):
    """Python 2 print statement"""

    __slots__ = ('args', 'newline', 'target')

    args: List[Expression]
    newline: bool
    # The file-like target object (given using &gt;&gt;).
    target: Optional[Expression]

    @others
</t>
<t tx="ekr.20220525082934.1029">def __init__(self,
             args: List[Expression],
             newline: bool,
             target: Optional[Expression] = None) -&gt; None:
    super().__init__()
    self.args = args
    self.newline = newline
    self.target = target

</t>
<t tx="ekr.20220525082934.103"># IfExp(expr test, expr body, expr orelse)
def visit_IfExp(self, n: ast3.IfExp) -&gt; ConditionalExpr:
    e = ConditionalExpr(self.visit(n.test),
                        self.visit(n.body),
                        self.visit(n.orelse))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1030">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_print_stmt(self)


</t>
<t tx="ekr.20220525082934.1031">class ExecStmt(Statement):
    """Python 2 exec statement"""

    __slots__ = ('expr', 'globals', 'locals')

    expr: Expression
    globals: Optional[Expression]
    locals: Optional[Expression]

    @others
</t>
<t tx="ekr.20220525082934.1032">def __init__(self, expr: Expression,
             globals: Optional[Expression],
             locals: Optional[Expression]) -&gt; None:
    super().__init__()
    self.expr = expr
    self.globals = globals
    self.locals = locals

</t>
<t tx="ekr.20220525082934.1033">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_exec_stmt(self)


</t>
<t tx="ekr.20220525082934.1034"># Expressions


</t>
<t tx="ekr.20220525082934.1035">class IntExpr(Expression):
    """Integer literal"""

    __slots__ = ('value',)

    value: int  # 0 by default

    @others
</t>
<t tx="ekr.20220525082934.1036">def __init__(self, value: int) -&gt; None:
    super().__init__()
    self.value = value

</t>
<t tx="ekr.20220525082934.1037">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_int_expr(self)


</t>
<t tx="ekr.20220525082934.1038"># How mypy uses StrExpr, BytesExpr, and UnicodeExpr:
# In Python 2 mode:
# b'x', 'x' -&gt; StrExpr
# u'x' -&gt; UnicodeExpr
# BytesExpr is unused
#
# In Python 3 mode:
# b'x' -&gt; BytesExpr
# 'x', u'x' -&gt; StrExpr
# UnicodeExpr is unused

</t>
<t tx="ekr.20220525082934.1039">class StrExpr(Expression):
    """String literal"""

    __slots__ = ('value', 'from_python_3')

    value: str  # '' by default

    # Keeps track of whether this string originated from Python 2 source code vs
    # Python 3 source code. We need to keep track of this information so we can
    # correctly handle types that have "nested strings". For example, consider this
    # type alias, where we have a forward reference to a literal type:
    #
    #     Alias = List["Literal['foo']"]
    #
    # When parsing this, we need to know whether the outer string and alias came from
    # Python 2 code vs Python 3 code so we can determine whether the inner `Literal['foo']`
    # is meant to be `Literal[u'foo']` or `Literal[b'foo']`.
    #
    # This field keeps track of that information.
    from_python_3: bool

    @others
</t>
<t tx="ekr.20220525082934.104"># Dict(expr* keys, expr* values)
def visit_Dict(self, n: ast3.Dict) -&gt; DictExpr:
    e = DictExpr(list(zip(self.translate_opt_expr_list(n.keys),
                          self.translate_expr_list(n.values))))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1040">def __init__(self, value: str, from_python_3: bool = False) -&gt; None:
    super().__init__()
    self.value = value
    self.from_python_3 = from_python_3

</t>
<t tx="ekr.20220525082934.1041">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_str_expr(self)


</t>
<t tx="ekr.20220525082934.1042">class BytesExpr(Expression):
    """Bytes literal"""

    __slots__ = ('value',)

    # Note: we deliberately do NOT use bytes here because it ends up
    # unnecessarily complicating a lot of the result logic. For example,
    # we'd have to worry about converting the bytes into a format we can
    # easily serialize/deserialize to and from JSON, would have to worry
    # about turning the bytes into a human-readable representation in
    # error messages...
    #
    # It's more convenient to just store the human-readable representation
    # from the very start.
    value: str

    @others
</t>
<t tx="ekr.20220525082934.1043">def __init__(self, value: str) -&gt; None:
    super().__init__()
    self.value = value

</t>
<t tx="ekr.20220525082934.1044">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_bytes_expr(self)


</t>
<t tx="ekr.20220525082934.1045">class UnicodeExpr(Expression):
    """Unicode literal (Python 2.x)"""

    __slots__ = ('value',)

    value: str

    @others
</t>
<t tx="ekr.20220525082934.1046">def __init__(self, value: str) -&gt; None:
    super().__init__()
    self.value = value

</t>
<t tx="ekr.20220525082934.1047">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_unicode_expr(self)


</t>
<t tx="ekr.20220525082934.1048">class FloatExpr(Expression):
    """Float literal"""

    __slots__ = ('value',)

    value: float  # 0.0 by default

    @others
</t>
<t tx="ekr.20220525082934.1049">def __init__(self, value: float) -&gt; None:
    super().__init__()
    self.value = value

</t>
<t tx="ekr.20220525082934.105"># Set(expr* elts)
def visit_Set(self, n: ast3.Set) -&gt; SetExpr:
    e = SetExpr(self.translate_expr_list(n.elts))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1050">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_float_expr(self)


</t>
<t tx="ekr.20220525082934.1051">class ComplexExpr(Expression):
    """Complex literal"""

    __slots__ = ('value',)

    value: complex

    @others
</t>
<t tx="ekr.20220525082934.1052">def __init__(self, value: complex) -&gt; None:
    super().__init__()
    self.value = value

</t>
<t tx="ekr.20220525082934.1053">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_complex_expr(self)


</t>
<t tx="ekr.20220525082934.1054">class EllipsisExpr(Expression):
    """Ellipsis (...)"""

    __slots__ = ()

    def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
        return visitor.visit_ellipsis(self)


</t>
<t tx="ekr.20220525082934.1055">class StarExpr(Expression):
    """Star expression"""

    __slots__ = ('expr', 'valid')

    expr: Expression
    valid: bool

    @others
</t>
<t tx="ekr.20220525082934.1056">def __init__(self, expr: Expression) -&gt; None:
    super().__init__()
    self.expr = expr

    # Whether this starred expression is used in a tuple/list and as lvalue
    self.valid = False

</t>
<t tx="ekr.20220525082934.1057">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_star_expr(self)


</t>
<t tx="ekr.20220525082934.1058">class RefExpr(Expression):
    """Abstract base class for name-like constructs"""

    __slots__ = ('kind', 'node', 'fullname', 'is_new_def', 'is_inferred_def', 'is_alias_rvalue',
                 'type_guard')

    @others
</t>
<t tx="ekr.20220525082934.1059">def __init__(self) -&gt; None:
    super().__init__()
    # LDEF/GDEF/MDEF/... (None if not available)
    self.kind: Optional[int] = None
    # Var, FuncDef or TypeInfo that describes this
    self.node: Optional[SymbolNode] = None
    # Fully qualified name (or name if not global)
    self.fullname: Optional[str] = None
    # Does this define a new name?
    self.is_new_def = False
    # Does this define a new name with inferred type?
    #
    # For members, after semantic analysis, this does not take base
    # classes into consideration at all; the type checker deals with these.
    self.is_inferred_def = False
    # Is this expression appears as an rvalue of a valid type alias definition?
    self.is_alias_rvalue = False
    # Cache type guard from callable_type.type_guard
    self.type_guard: Optional["mypy.types.Type"] = None


</t>
<t tx="ekr.20220525082934.106"># ListComp(expr elt, comprehension* generators)
def visit_ListComp(self, n: ast3.ListComp) -&gt; ListComprehension:
    e = ListComprehension(self.visit_GeneratorExp(cast(ast3.GeneratorExp, n)))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1060">class NameExpr(RefExpr):
    """Name expression

    This refers to a local name, global name or a module.
    """

    __slots__ = ('name', 'is_special_form')

    @others
</t>
<t tx="ekr.20220525082934.1061">def __init__(self, name: str) -&gt; None:
    super().__init__()
    self.name = name  # Name referred to (may be qualified)
    # Is this a l.h.s. of a special form assignment like typed dict or type variable?
    self.is_special_form = False

</t>
<t tx="ekr.20220525082934.1062">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_name_expr(self)

</t>
<t tx="ekr.20220525082934.1063">def serialize(self) -&gt; JsonDict:
    assert False, f"Serializing NameExpr: {self}"


</t>
<t tx="ekr.20220525082934.1064">class MemberExpr(RefExpr):
    """Member access expression x.y"""

    __slots__ = ('expr', 'name', 'def_var')

    @others
</t>
<t tx="ekr.20220525082934.1065">def __init__(self, expr: Expression, name: str) -&gt; None:
    super().__init__()
    self.expr = expr
    self.name = name
    # The variable node related to a definition through 'self.x = &lt;initializer&gt;'.
    # The nodes of other kinds of member expressions are resolved during type checking.
    self.def_var: Optional[Var] = None

</t>
<t tx="ekr.20220525082934.1066">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_member_expr(self)


</t>
<t tx="ekr.20220525082934.1067"># Kinds of arguments
@unique
class ArgKind(Enum):
    # Positional argument
    ARG_POS = 0
    # Positional, optional argument (functions only, not calls)
    ARG_OPT = 1
    # *arg argument
    ARG_STAR = 2
    # Keyword argument x=y in call, or keyword-only function arg
    ARG_NAMED = 3
    # **arg argument
    ARG_STAR2 = 4
    # In an argument list, keyword-only and also optional
    ARG_NAMED_OPT = 5

    @others
</t>
<t tx="ekr.20220525082934.1068">def is_positional(self, star: bool = False) -&gt; bool:
    return (
        self == ARG_POS
        or self == ARG_OPT
        or (star and self == ARG_STAR)
    )

</t>
<t tx="ekr.20220525082934.1069">def is_named(self, star: bool = False) -&gt; bool:
    return (
        self == ARG_NAMED
        or self == ARG_NAMED_OPT
        or (star and self == ARG_STAR2)
    )

</t>
<t tx="ekr.20220525082934.107"># SetComp(expr elt, comprehension* generators)
def visit_SetComp(self, n: ast3.SetComp) -&gt; SetComprehension:
    e = SetComprehension(self.visit_GeneratorExp(cast(ast3.GeneratorExp, n)))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1070">def is_required(self) -&gt; bool:
    return self == ARG_POS or self == ARG_NAMED

</t>
<t tx="ekr.20220525082934.1071">def is_optional(self) -&gt; bool:
    return self == ARG_OPT or self == ARG_NAMED_OPT

</t>
<t tx="ekr.20220525082934.1072">def is_star(self) -&gt; bool:
    return self == ARG_STAR or self == ARG_STAR2


</t>
<t tx="ekr.20220525082934.1073">ARG_POS: Final = ArgKind.ARG_POS
ARG_OPT: Final = ArgKind.ARG_OPT
ARG_STAR: Final = ArgKind.ARG_STAR
ARG_NAMED: Final = ArgKind.ARG_NAMED
ARG_STAR2: Final = ArgKind.ARG_STAR2
ARG_NAMED_OPT: Final = ArgKind.ARG_NAMED_OPT


</t>
<t tx="ekr.20220525082934.1074">class CallExpr(Expression):
    """Call expression.

    This can also represent several special forms that are syntactically calls
    such as cast(...) and None  # type: ....
    """

    __slots__ = ('callee', 'args', 'arg_kinds', 'arg_names', 'analyzed')

    @others
</t>
<t tx="ekr.20220525082934.1075">def __init__(self,
             callee: Expression,
             args: List[Expression],
             arg_kinds: List[ArgKind],
             arg_names: List[Optional[str]],
             analyzed: Optional[Expression] = None) -&gt; None:
    super().__init__()
    if not arg_names:
        arg_names = [None] * len(args)

    self.callee = callee
    self.args = args
    self.arg_kinds = arg_kinds  # ARG_ constants
    # Each name can be None if not a keyword argument.
    self.arg_names: List[Optional[str]] = arg_names
    # If not None, the node that represents the meaning of the CallExpr. For
    # cast(...) this is a CastExpr.
    self.analyzed = analyzed

</t>
<t tx="ekr.20220525082934.1076">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_call_expr(self)


</t>
<t tx="ekr.20220525082934.1077">class YieldFromExpr(Expression):
    __slots__ = ('expr',)

    expr: Expression

    @others
</t>
<t tx="ekr.20220525082934.1078">def __init__(self, expr: Expression) -&gt; None:
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20220525082934.1079">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_yield_from_expr(self)


</t>
<t tx="ekr.20220525082934.108"># DictComp(expr key, expr value, comprehension* generators)
def visit_DictComp(self, n: ast3.DictComp) -&gt; DictionaryComprehension:
    targets = [self.visit(c.target) for c in n.generators]
    iters = [self.visit(c.iter) for c in n.generators]
    ifs_list = [self.translate_expr_list(c.ifs) for c in n.generators]
    is_async = [bool(c.is_async) for c in n.generators]
    e = DictionaryComprehension(self.visit(n.key),
                                self.visit(n.value),
                                targets,
                                iters,
                                ifs_list,
                                is_async)
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1080">class YieldExpr(Expression):
    __slots__ = ('expr',)

    expr: Optional[Expression]

    @others
</t>
<t tx="ekr.20220525082934.1081">def __init__(self, expr: Optional[Expression]) -&gt; None:
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20220525082934.1082">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_yield_expr(self)


</t>
<t tx="ekr.20220525082934.1083">class IndexExpr(Expression):
    """Index expression x[y].

    Also wraps type application such as List[int] as a special form.
    """

    __slots__ = ('base', 'index', 'method_type', 'analyzed')

    base: Expression
    index: Expression
    # Inferred __getitem__ method type
    method_type: Optional["mypy.types.Type"]
    # If not None, this is actually semantically a type application
    # Class[type, ...] or a type alias initializer.
    analyzed: Union["TypeApplication", "TypeAliasExpr", None]

    @others
</t>
<t tx="ekr.20220525082934.1084">def __init__(self, base: Expression, index: Expression) -&gt; None:
    super().__init__()
    self.base = base
    self.index = index
    self.method_type = None
    self.analyzed = None

</t>
<t tx="ekr.20220525082934.1085">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_index_expr(self)


</t>
<t tx="ekr.20220525082934.1086">class UnaryExpr(Expression):
    """Unary operation"""

    __slots__ = ('op', 'expr', 'method_type')

    op: str  # TODO: Enum?
    expr: Expression
    # Inferred operator method type
    method_type: Optional["mypy.types.Type"]

    @others
</t>
<t tx="ekr.20220525082934.1087">def __init__(self, op: str, expr: Expression) -&gt; None:
    super().__init__()
    self.op = op
    self.expr = expr
    self.method_type = None

</t>
<t tx="ekr.20220525082934.1088">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_unary_expr(self)


</t>
<t tx="ekr.20220525082934.1089">class AssignmentExpr(Expression):
    """Assignment expressions in Python 3.8+, like "a := 2"."""

    __slots__ = ('target', 'value')

    @others
</t>
<t tx="ekr.20220525082934.109"># GeneratorExp(expr elt, comprehension* generators)
def visit_GeneratorExp(self, n: ast3.GeneratorExp) -&gt; GeneratorExpr:
    targets = [self.visit(c.target) for c in n.generators]
    iters = [self.visit(c.iter) for c in n.generators]
    ifs_list = [self.translate_expr_list(c.ifs) for c in n.generators]
    is_async = [bool(c.is_async) for c in n.generators]
    e = GeneratorExpr(self.visit(n.elt),
                      targets,
                      iters,
                      ifs_list,
                      is_async)
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1090">def __init__(self, target: Expression, value: Expression) -&gt; None:
    super().__init__()
    self.target = target
    self.value = value

</t>
<t tx="ekr.20220525082934.1091">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_assignment_expr(self)


</t>
<t tx="ekr.20220525082934.1092">class OpExpr(Expression):
    """Binary operation (other than . or [] or comparison operators,
    which have specific nodes)."""

    __slots__ = ('op', 'left', 'right',
                 'method_type', 'right_always', 'right_unreachable')

    op: str  # TODO: Enum?
    left: Expression
    right: Expression
    # Inferred type for the operator method type (when relevant).
    method_type: Optional["mypy.types.Type"]
    # Per static analysis only: Is the right side going to be evaluated every time?
    right_always: bool
    # Per static analysis only: Is the right side unreachable?
    right_unreachable: bool

    @others
</t>
<t tx="ekr.20220525082934.1093">def __init__(self, op: str, left: Expression, right: Expression) -&gt; None:
    super().__init__()
    self.op = op
    self.left = left
    self.right = right
    self.method_type = None
    self.right_always = False
    self.right_unreachable = False

</t>
<t tx="ekr.20220525082934.1094">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_op_expr(self)


</t>
<t tx="ekr.20220525082934.1095">class ComparisonExpr(Expression):
    """Comparison expression (e.g. a &lt; b &gt; c &lt; d)."""

    __slots__ = ('operators', 'operands', 'method_types')

    operators: List[str]
    operands: List[Expression]
    # Inferred type for the operator methods (when relevant; None for 'is').
    method_types: List[Optional["mypy.types.Type"]]

    @others
</t>
<t tx="ekr.20220525082934.1096">def __init__(self, operators: List[str], operands: List[Expression]) -&gt; None:
    super().__init__()
    self.operators = operators
    self.operands = operands
    self.method_types = []

</t>
<t tx="ekr.20220525082934.1097">def pairwise(self) -&gt; Iterator[Tuple[str, Expression, Expression]]:
    """If this comparison expr is "a &lt; b is c == d", yields the sequence
    ("&lt;", a, b), ("is", b, c), ("==", c, d)
    """
    for i, operator in enumerate(self.operators):
        yield operator, self.operands[i], self.operands[i + 1]

</t>
<t tx="ekr.20220525082934.1098">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_comparison_expr(self)


</t>
<t tx="ekr.20220525082934.1099">class SliceExpr(Expression):
    """Slice expression (e.g. 'x:y', 'x:', '::2' or ':').

    This is only valid as index in index expressions.
    """

    __slots__ = ('begin_index', 'end_index', 'stride')

    begin_index: Optional[Expression]
    end_index: Optional[Expression]
    stride: Optional[Expression]

    @others
</t>
<t tx="ekr.20220525082934.11">def visit_deleted_type(self, t: DeletedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082934.110"># Await(expr value)
def visit_Await(self, n: ast3.Await) -&gt; AwaitExpr:
    v = self.visit(n.value)
    e = AwaitExpr(v)
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1100">def __init__(self, begin_index: Optional[Expression],
             end_index: Optional[Expression],
             stride: Optional[Expression]) -&gt; None:
    super().__init__()
    self.begin_index = begin_index
    self.end_index = end_index
    self.stride = stride

</t>
<t tx="ekr.20220525082934.1101">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_slice_expr(self)


</t>
<t tx="ekr.20220525082934.1102">class CastExpr(Expression):
    """Cast expression cast(type, expr)."""

    __slots__ = ('expr', 'type')

    expr: Expression
    type: "mypy.types.Type"

    @others
</t>
<t tx="ekr.20220525082934.1103">def __init__(self, expr: Expression, typ: 'mypy.types.Type') -&gt; None:
    super().__init__()
    self.expr = expr
    self.type = typ

</t>
<t tx="ekr.20220525082934.1104">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_cast_expr(self)


</t>
<t tx="ekr.20220525082934.1105">class AssertTypeExpr(Expression):
    """Represents a typing.assert_type(expr, type) call."""
    __slots__ = ('expr', 'type')

    expr: Expression
    type: "mypy.types.Type"

    @others
</t>
<t tx="ekr.20220525082934.1106">def __init__(self, expr: Expression, typ: 'mypy.types.Type') -&gt; None:
    super().__init__()
    self.expr = expr
    self.type = typ

</t>
<t tx="ekr.20220525082934.1107">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_assert_type_expr(self)


</t>
<t tx="ekr.20220525082934.1108">class RevealExpr(Expression):
    """Reveal type expression reveal_type(expr) or reveal_locals() expression."""

    __slots__ = ('expr', 'kind', 'local_nodes')

    expr: Optional[Expression]
    kind: int
    local_nodes: Optional[List[Var]]

    @others
</t>
<t tx="ekr.20220525082934.1109">def __init__(
        self, kind: int,
        expr: Optional[Expression] = None,
        local_nodes: 'Optional[List[Var]]' = None) -&gt; None:
    super().__init__()
    self.expr = expr
    self.kind = kind
    self.local_nodes = local_nodes

</t>
<t tx="ekr.20220525082934.111"># Yield(expr? value)
def visit_Yield(self, n: ast3.Yield) -&gt; YieldExpr:
    e = YieldExpr(self.visit(n.value))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1110">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_reveal_expr(self)


</t>
<t tx="ekr.20220525082934.1111">class SuperExpr(Expression):
    """Expression super().name"""

    __slots__ = ('name', 'info', 'call')

    name: str
    info: Optional["TypeInfo"]  # Type that contains this super expression
    call: CallExpr  # The expression super(...)

    @others
</t>
<t tx="ekr.20220525082934.1112">def __init__(self, name: str, call: CallExpr) -&gt; None:
    super().__init__()
    self.name = name
    self.call = call
    self.info = None

</t>
<t tx="ekr.20220525082934.1113">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_super_expr(self)


</t>
<t tx="ekr.20220525082934.1114">class LambdaExpr(FuncItem, Expression):
    """Lambda expression"""

    @others
</t>
<t tx="ekr.20220525082934.1115">@property
def name(self) -&gt; str:
    return '&lt;lambda&gt;'

</t>
<t tx="ekr.20220525082934.1116">def expr(self) -&gt; Expression:
    """Return the expression (the body) of the lambda."""
    ret = cast(ReturnStmt, self.body.body[-1])
    expr = ret.expr
    assert expr is not None  # lambda can't have empty body
    return expr

</t>
<t tx="ekr.20220525082934.1117">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_lambda_expr(self)

</t>
<t tx="ekr.20220525082934.1118">def is_dynamic(self) -&gt; bool:
    return False


</t>
<t tx="ekr.20220525082934.1119">class ListExpr(Expression):
    """List literal expression [...]."""

    __slots__ = ('items',)

    items: List[Expression]

    @others
</t>
<t tx="ekr.20220525082934.112"># YieldFrom(expr value)
def visit_YieldFrom(self, n: ast3.YieldFrom) -&gt; YieldFromExpr:
    e = YieldFromExpr(self.visit(n.value))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1120">def __init__(self, items: List[Expression]) -&gt; None:
    super().__init__()
    self.items = items

</t>
<t tx="ekr.20220525082934.1121">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_list_expr(self)


</t>
<t tx="ekr.20220525082934.1122">class DictExpr(Expression):
    """Dictionary literal expression {key: value, ...}."""

    __slots__ = ('items',)

    items: List[Tuple[Optional[Expression], Expression]]

    @others
</t>
<t tx="ekr.20220525082934.1123">def __init__(self, items: List[Tuple[Optional[Expression], Expression]]) -&gt; None:
    super().__init__()
    self.items = items

</t>
<t tx="ekr.20220525082934.1124">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_dict_expr(self)


</t>
<t tx="ekr.20220525082934.1125">class TupleExpr(Expression):
    """Tuple literal expression (..., ...)

    Also lvalue sequences (..., ...) and [..., ...]"""

    __slots__ = ('items',)

    items: List[Expression]

    @others
</t>
<t tx="ekr.20220525082934.1126">def __init__(self, items: List[Expression]) -&gt; None:
    super().__init__()
    self.items = items

</t>
<t tx="ekr.20220525082934.1127">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_tuple_expr(self)


</t>
<t tx="ekr.20220525082934.1128">class SetExpr(Expression):
    """Set literal expression {value, ...}."""

    __slots__ = ('items',)

    items: List[Expression]

    @others
</t>
<t tx="ekr.20220525082934.1129">def __init__(self, items: List[Expression]) -&gt; None:
    super().__init__()
    self.items = items

</t>
<t tx="ekr.20220525082934.113"># Compare(expr left, cmpop* ops, expr* comparators)
def visit_Compare(self, n: ast3.Compare) -&gt; ComparisonExpr:
    operators = [self.from_comp_operator(o) for o in n.ops]
    operands = self.translate_expr_list([n.left] + n.comparators)
    e = ComparisonExpr(operators, operands)
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1130">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_set_expr(self)


</t>
<t tx="ekr.20220525082934.1131">class GeneratorExpr(Expression):
    """Generator expression ... for ... in ... [ for ...  in ... ] [ if ... ]."""

    __slots__ = ('left_expr', 'sequences', 'condlists', 'is_async', 'indices')

    left_expr: Expression
    sequences: List[Expression]
    condlists: List[List[Expression]]
    is_async: List[bool]
    indices: List[Lvalue]

    @others
</t>
<t tx="ekr.20220525082934.1132">def __init__(self, left_expr: Expression, indices: List[Lvalue],
             sequences: List[Expression], condlists: List[List[Expression]],
             is_async: List[bool]) -&gt; None:
    super().__init__()
    self.left_expr = left_expr
    self.sequences = sequences
    self.condlists = condlists
    self.indices = indices
    self.is_async = is_async

</t>
<t tx="ekr.20220525082934.1133">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_generator_expr(self)


</t>
<t tx="ekr.20220525082934.1134">class ListComprehension(Expression):
    """List comprehension (e.g. [x + 1 for x in a])"""

    __slots__ = ('generator',)

    generator: GeneratorExpr

    @others
</t>
<t tx="ekr.20220525082934.1135">def __init__(self, generator: GeneratorExpr) -&gt; None:
    super().__init__()
    self.generator = generator

</t>
<t tx="ekr.20220525082934.1136">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_list_comprehension(self)


</t>
<t tx="ekr.20220525082934.1137">class SetComprehension(Expression):
    """Set comprehension (e.g. {x + 1 for x in a})"""

    __slots__ = ('generator',)

    generator: GeneratorExpr

    @others
</t>
<t tx="ekr.20220525082934.1138">def __init__(self, generator: GeneratorExpr) -&gt; None:
    super().__init__()
    self.generator = generator

</t>
<t tx="ekr.20220525082934.1139">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_set_comprehension(self)


</t>
<t tx="ekr.20220525082934.114"># Call(expr func, expr* args, keyword* keywords)
# keyword = (identifier? arg, expr value)
def visit_Call(self, n: Call) -&gt; CallExpr:
    args = n.args
    keywords = n.keywords
    keyword_names = [k.arg for k in keywords]
    arg_types = self.translate_expr_list(
        [a.value if isinstance(a, Starred) else a for a in args] +
        [k.value for k in keywords])
    arg_kinds = ([ARG_STAR if type(a) is Starred else ARG_POS for a in args] +
                 [ARG_STAR2 if arg is None else ARG_NAMED for arg in keyword_names])
    e = CallExpr(self.visit(n.func),
                 arg_types,
                 arg_kinds,
                 cast('List[Optional[str]]', [None] * len(args)) + keyword_names)
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1140">class DictionaryComprehension(Expression):
    """Dictionary comprehension (e.g. {k: v for k, v in a}"""

    __slots__ = ('key', 'value', 'sequences', 'condlists', 'is_async', 'indices')

    key: Expression
    value: Expression
    sequences: List[Expression]
    condlists: List[List[Expression]]
    is_async: List[bool]
    indices: List[Lvalue]

    @others
</t>
<t tx="ekr.20220525082934.1141">def __init__(self, key: Expression, value: Expression, indices: List[Lvalue],
             sequences: List[Expression], condlists: List[List[Expression]],
             is_async: List[bool]) -&gt; None:
    super().__init__()
    self.key = key
    self.value = value
    self.sequences = sequences
    self.condlists = condlists
    self.indices = indices
    self.is_async = is_async

</t>
<t tx="ekr.20220525082934.1142">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_dictionary_comprehension(self)


</t>
<t tx="ekr.20220525082934.1143">class ConditionalExpr(Expression):
    """Conditional expression (e.g. x if y else z)"""

    __slots__ = ('cond', 'if_expr', 'else_expr')

    cond: Expression
    if_expr: Expression
    else_expr: Expression

    @others
</t>
<t tx="ekr.20220525082934.1144">def __init__(self, cond: Expression, if_expr: Expression, else_expr: Expression) -&gt; None:
    super().__init__()
    self.cond = cond
    self.if_expr = if_expr
    self.else_expr = else_expr

</t>
<t tx="ekr.20220525082934.1145">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_conditional_expr(self)


</t>
<t tx="ekr.20220525082934.1146">class BackquoteExpr(Expression):
    """Python 2 expression `...`."""

    __slots__ = ('expr',)

    expr: Expression

    @others
</t>
<t tx="ekr.20220525082934.1147">def __init__(self, expr: Expression) -&gt; None:
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20220525082934.1148">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_backquote_expr(self)


</t>
<t tx="ekr.20220525082934.1149">class TypeApplication(Expression):
    """Type application expr[type, ...]"""

    __slots__ = ('expr', 'types')

    expr: Expression
    types: List["mypy.types.Type"]

    @others
</t>
<t tx="ekr.20220525082934.115"># Constant(object value) -- a constant, in Python 3.8.
def visit_Constant(self, n: Constant) -&gt; Any:
    val = n.value
    e: Any = None
    if val is None:
        e = NameExpr('None')
    elif isinstance(val, str):
        e = StrExpr(n.s)
    elif isinstance(val, bytes):
        e = BytesExpr(bytes_to_human_readable_repr(n.s))
    elif isinstance(val, bool):  # Must check before int!
        e = NameExpr(str(val))
    elif isinstance(val, int):
        e = IntExpr(val)
    elif isinstance(val, float):
        e = FloatExpr(val)
    elif isinstance(val, complex):
        e = ComplexExpr(val)
    elif val is Ellipsis:
        e = EllipsisExpr()
    else:
        raise RuntimeError('Constant not implemented for ' + str(type(val)))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1150">def __init__(self, expr: Expression, types: List['mypy.types.Type']) -&gt; None:
    super().__init__()
    self.expr = expr
    self.types = types

</t>
<t tx="ekr.20220525082934.1151">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_type_application(self)


</t>
<t tx="ekr.20220525082934.1152"># Variance of a type variable. For example, T in the definition of
# List[T] is invariant, so List[int] is not a subtype of List[object],
# and also List[object] is not a subtype of List[int].
#
# The T in Iterable[T] is covariant, so Iterable[int] is a subtype of
# Iterable[object], but not vice versa.
#
# If T is contravariant in Foo[T], Foo[object] is a subtype of
# Foo[int], but not vice versa.
INVARIANT: Final = 0
COVARIANT: Final = 1
CONTRAVARIANT: Final = 2


</t>
<t tx="ekr.20220525082934.1153">class TypeVarLikeExpr(SymbolNode, Expression):
    """Base class for TypeVarExpr, ParamSpecExpr and TypeVarTupleExpr.

    Note that they are constructed by the semantic analyzer.
    """

    __slots__ = ('_name', '_fullname', 'upper_bound', 'variance')

    _name: str
    _fullname: str
    # Upper bound: only subtypes of upper_bound are valid as values. By default
    # this is 'object', meaning no restriction.
    upper_bound: "mypy.types.Type"
    # Variance of the type variable. Invariant is the default.
    # TypeVar(..., covariant=True) defines a covariant type variable.
    # TypeVar(..., contravariant=True) defines a contravariant type
    # variable.
    variance: int

    @others
</t>
<t tx="ekr.20220525082934.1154">def __init__(
    self, name: str, fullname: str, upper_bound: 'mypy.types.Type', variance: int = INVARIANT
) -&gt; None:
    super().__init__()
    self._name = name
    self._fullname = fullname
    self.upper_bound = upper_bound
    self.variance = variance

</t>
<t tx="ekr.20220525082934.1155">@property
def name(self) -&gt; str:
    return self._name

</t>
<t tx="ekr.20220525082934.1156">@property
def fullname(self) -&gt; str:
    return self._fullname


</t>
<t tx="ekr.20220525082934.1157">class TypeVarExpr(TypeVarLikeExpr):
    """Type variable expression TypeVar(...).

    This is also used to represent type variables in symbol tables.

    A type variable is not valid as a type unless bound in a TypeVarLikeScope.
    That happens within:

     1. a generic class that uses the type variable as a type argument or
     2. a generic function that refers to the type variable in its signature.
    """

    __slots__ = ('values',)

    # Value restriction: only types in the list are valid as values. If the
    # list is empty, there is no restriction.
    values: List["mypy.types.Type"]

    @others
</t>
<t tx="ekr.20220525082934.1158">def __init__(self, name: str, fullname: str,
             values: List['mypy.types.Type'],
             upper_bound: 'mypy.types.Type',
             variance: int = INVARIANT) -&gt; None:
    super().__init__(name, fullname, upper_bound, variance)
    self.values = values

</t>
<t tx="ekr.20220525082934.1159">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_type_var_expr(self)

</t>
<t tx="ekr.20220525082934.116"># Num(object n) -- a number as a PyObject.
def visit_Num(self, n: ast3.Num) -&gt; Union[IntExpr, FloatExpr, ComplexExpr]:
    # The n field has the type complex, but complex isn't *really*
    # a parent of int and float, and this causes isinstance below
    # to think that the complex branch is always picked. Avoid
    # this by throwing away the type.
    val: object = n.n
    if isinstance(val, int):
        e: Union[IntExpr, FloatExpr, ComplexExpr] = IntExpr(val)
    elif isinstance(val, float):
        e = FloatExpr(val)
    elif isinstance(val, complex):
        e = ComplexExpr(val)
    else:
        raise RuntimeError('num not implemented for ' + str(type(val)))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1160">def serialize(self) -&gt; JsonDict:
    return {'.class': 'TypeVarExpr',
            'name': self._name,
            'fullname': self._fullname,
            'values': [t.serialize() for t in self.values],
            'upper_bound': self.upper_bound.serialize(),
            'variance': self.variance,
            }

</t>
<t tx="ekr.20220525082934.1161">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'TypeVarExpr':
    assert data['.class'] == 'TypeVarExpr'
    return TypeVarExpr(data['name'],
                       data['fullname'],
                       [mypy.types.deserialize_type(v) for v in data['values']],
                       mypy.types.deserialize_type(data['upper_bound']),
                       data['variance'])


</t>
<t tx="ekr.20220525082934.1162">class ParamSpecExpr(TypeVarLikeExpr):
    __slots__ = ()

    @others
</t>
<t tx="ekr.20220525082934.1163">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_paramspec_expr(self)

</t>
<t tx="ekr.20220525082934.1164">def serialize(self) -&gt; JsonDict:
    return {
        '.class': 'ParamSpecExpr',
        'name': self._name,
        'fullname': self._fullname,
        'upper_bound': self.upper_bound.serialize(),
        'variance': self.variance,
    }

</t>
<t tx="ekr.20220525082934.1165">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'ParamSpecExpr':
    assert data['.class'] == 'ParamSpecExpr'
    return ParamSpecExpr(
        data['name'],
        data['fullname'],
        mypy.types.deserialize_type(data['upper_bound']),
        data['variance']
    )


</t>
<t tx="ekr.20220525082934.1166">class TypeVarTupleExpr(TypeVarLikeExpr):
    """Type variable tuple expression TypeVarTuple(...)."""

    __slots__ = ()

    @others
</t>
<t tx="ekr.20220525082934.1167">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_type_var_tuple_expr(self)

</t>
<t tx="ekr.20220525082934.1168">def serialize(self) -&gt; JsonDict:
    return {
        '.class': 'TypeVarTupleExpr',
        'name': self._name,
        'fullname': self._fullname,
        'upper_bound': self.upper_bound.serialize(),
        'variance': self.variance,
    }

</t>
<t tx="ekr.20220525082934.1169">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'TypeVarTupleExpr':
    assert data['.class'] == 'TypeVarTupleExpr'
    return TypeVarTupleExpr(
        data['name'],
        data['fullname'],
        mypy.types.deserialize_type(data['upper_bound']),
        data['variance']
    )


</t>
<t tx="ekr.20220525082934.117"># Str(string s)
def visit_Str(self, n: Str) -&gt; Union[UnicodeExpr, StrExpr]:
    # Hack: assume all string literals in Python 2 stubs are normal
    # strs (i.e. not unicode).  All stubs are parsed with the Python 3
    # parser, which causes unprefixed string literals to be interpreted
    # as unicode instead of bytes.  This hack is generally okay,
    # because mypy considers str literals to be compatible with
    # unicode.
    e = StrExpr(n.s)
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1170">class TypeAliasExpr(Expression):
    """Type alias expression (rvalue)."""

    __slots__ = ('type', 'tvars', 'no_args', 'node')

    # The target type.
    type: "mypy.types.Type"
    # Names of unbound type variables used to define the alias
    tvars: List[str]
    # Whether this alias was defined in bare form. Used to distinguish
    # between
    #     A = List
    # and
    #     A = List[Any]
    no_args: bool
    node: 'TypeAlias'

    @others
</t>
<t tx="ekr.20220525082934.1171">def __init__(self, node: 'TypeAlias') -&gt; None:
    super().__init__()
    self.type = node.target
    self.tvars = node.alias_tvars
    self.no_args = node.no_args
    self.node = node

</t>
<t tx="ekr.20220525082934.1172">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_type_alias_expr(self)


</t>
<t tx="ekr.20220525082934.1173">class NamedTupleExpr(Expression):
    """Named tuple expression namedtuple(...) or NamedTuple(...)."""

    __slots__ = ('info', 'is_typed')

    # The class representation of this named tuple (its tuple_type attribute contains
    # the tuple item types)
    info: "TypeInfo"
    is_typed: bool  # whether this class was created with typing.NamedTuple

    @others
</t>
<t tx="ekr.20220525082934.1174">def __init__(self, info: 'TypeInfo', is_typed: bool = False) -&gt; None:
    super().__init__()
    self.info = info
    self.is_typed = is_typed

</t>
<t tx="ekr.20220525082934.1175">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_namedtuple_expr(self)


</t>
<t tx="ekr.20220525082934.1176">class TypedDictExpr(Expression):
    """Typed dict expression TypedDict(...)."""

    __slots__ = ('info',)

    # The class representation of this typed dict
    info: "TypeInfo"

    @others
</t>
<t tx="ekr.20220525082934.1177">def __init__(self, info: 'TypeInfo') -&gt; None:
    super().__init__()
    self.info = info

</t>
<t tx="ekr.20220525082934.1178">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_typeddict_expr(self)


</t>
<t tx="ekr.20220525082934.1179">class EnumCallExpr(Expression):
    """Named tuple expression Enum('name', 'val1 val2 ...')."""

    __slots__ = ('info', 'items', 'values')

    # The class representation of this enumerated type
    info: "TypeInfo"
    # The item names (for debugging)
    items: List[str]
    values: List[Optional[Expression]]

    @others
</t>
<t tx="ekr.20220525082934.118"># JoinedStr(expr* values)
def visit_JoinedStr(self, n: ast3.JoinedStr) -&gt; Expression:
    # Each of n.values is a str or FormattedValue; we just concatenate
    # them all using ''.join.
    empty_string = StrExpr('')
    empty_string.set_line(n.lineno, n.col_offset)
    strs_to_join = ListExpr(self.translate_expr_list(n.values))
    strs_to_join.set_line(empty_string)
    # Don't make unnecessary join call if there is only one str to join
    if len(strs_to_join.items) == 1:
        return self.set_line(strs_to_join.items[0], n)
    join_method = MemberExpr(empty_string, 'join')
    join_method.set_line(empty_string)
    result_expression = CallExpr(join_method,
                                 [strs_to_join],
                                 [ARG_POS],
                                 [None])
    return self.set_line(result_expression, n)

</t>
<t tx="ekr.20220525082934.1180">def __init__(self, info: 'TypeInfo', items: List[str],
             values: List[Optional[Expression]]) -&gt; None:
    super().__init__()
    self.info = info
    self.items = items
    self.values = values

</t>
<t tx="ekr.20220525082934.1181">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_enum_call_expr(self)


</t>
<t tx="ekr.20220525082934.1182">class PromoteExpr(Expression):
    """Ducktype class decorator expression _promote(...)."""

    __slots__ = ('type',)

    type: "mypy.types.Type"

    @others
</t>
<t tx="ekr.20220525082934.1183">def __init__(self, type: 'mypy.types.Type') -&gt; None:
    super().__init__()
    self.type = type

</t>
<t tx="ekr.20220525082934.1184">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit__promote_expr(self)


</t>
<t tx="ekr.20220525082934.1185">class NewTypeExpr(Expression):
    """NewType expression NewType(...)."""

    __slots__ = ('name', 'old_type', 'info')

    name: str
    # The base type (the second argument to NewType)
    old_type: Optional["mypy.types.Type"]
    # The synthesized class representing the new type (inherits old_type)
    info: Optional["TypeInfo"]

    @others
</t>
<t tx="ekr.20220525082934.1186">def __init__(self, name: str, old_type: 'Optional[mypy.types.Type]', line: int,
             column: int) -&gt; None:
    super().__init__(line=line, column=column)
    self.name = name
    self.old_type = old_type
    self.info = None

</t>
<t tx="ekr.20220525082934.1187">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_newtype_expr(self)


</t>
<t tx="ekr.20220525082934.1188">class AwaitExpr(Expression):
    """Await expression (await ...)."""

    __slots__ = ('expr',)

    expr: Expression

    @others
</t>
<t tx="ekr.20220525082934.1189">def __init__(self, expr: Expression) -&gt; None:
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20220525082934.119"># FormattedValue(expr value)
def visit_FormattedValue(self, n: ast3.FormattedValue) -&gt; Expression:
    # A FormattedValue is a component of a JoinedStr, or it can exist
    # on its own. We translate them to individual '{}'.format(value)
    # calls. Format specifier and conversion information is passed along
    # to allow mypyc to support f-strings with format specifiers and conversions.
    val_exp = self.visit(n.value)
    val_exp.set_line(n.lineno, n.col_offset)
    conv_str = '' if n.conversion is None or n.conversion &lt; 0 else '!' + chr(n.conversion)
    format_string = StrExpr('{' + conv_str + ':{}}')
    format_spec_exp = self.visit(n.format_spec) if n.format_spec is not None else StrExpr('')
    format_string.set_line(n.lineno, n.col_offset)
    format_method = MemberExpr(format_string, 'format')
    format_method.set_line(format_string)
    result_expression = CallExpr(format_method,
                                 [val_exp, format_spec_exp],
                                 [ARG_POS, ARG_POS],
                                 [None, None])
    return self.set_line(result_expression, n)

</t>
<t tx="ekr.20220525082934.1190">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_await_expr(self)


</t>
<t tx="ekr.20220525082934.1191"># Constants


</t>
<t tx="ekr.20220525082934.1192">class TempNode(Expression):
    """Temporary dummy node used during type checking.

    This node is not present in the original program; it is just an artifact
    of the type checker implementation. It only represents an opaque node with
    some fixed type.
    """

    __slots__ = ('type', 'no_rhs')

    type: "mypy.types.Type"
    # Is this TempNode used to indicate absence of a right hand side in an annotated assignment?
    # (e.g. for 'x: int' the rvalue is TempNode(AnyType(TypeOfAny.special_form), no_rhs=True))
    no_rhs: bool

    @others
</t>
<t tx="ekr.20220525082934.1193">def __init__(self,
             typ: 'mypy.types.Type',
             no_rhs: bool = False,
             *,
             context: Optional[Context] = None) -&gt; None:
    """Construct a dummy node; optionally borrow line/column from context object."""
    super().__init__()
    self.type = typ
    self.no_rhs = no_rhs
    if context is not None:
        self.line = context.line
        self.column = context.column

</t>
<t tx="ekr.20220525082934.1194">def __repr__(self) -&gt; str:
    return 'TempNode:%d(%s)' % (self.line, str(self.type))

</t>
<t tx="ekr.20220525082934.1195">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    return visitor.visit_temp_node(self)


</t>
<t tx="ekr.20220525082934.1196">class TypeInfo(SymbolNode):
    """The type structure of a single class.

    Each TypeInfo corresponds one-to-one to a ClassDef, which
    represents the AST of the class.

    In type-theory terms, this is a "type constructor", and if the
    class is generic then it will be a type constructor of higher kind.
    Where the class is used in an actual type, it's in the form of an
    Instance, which amounts to a type application of the tycon to
    the appropriate number of arguments.
    """

    __slots__ = (
        '_fullname', 'module_name', 'defn', 'mro', '_mro_refs', 'bad_mro', 'is_final',
        'declared_metaclass', 'metaclass_type', 'names', 'is_abstract',
        'is_protocol', 'runtime_protocol', 'abstract_attributes',
        'deletable_attributes', 'slots', 'assuming', 'assuming_proper',
        'inferring', 'is_enum', 'fallback_to_any', 'type_vars', 'has_param_spec_type',
        'bases', '_promote', 'tuple_type', 'is_named_tuple', 'typeddict_type',
        'is_newtype', 'is_intersection', 'metadata',
    )

    _fullname: Bogus[str]  # Fully qualified name
    # Fully qualified name for the module this type was defined in. This
    # information is also in the fullname, but is harder to extract in the
    # case of nested class definitions.
    module_name: str
    defn: ClassDef  # Corresponding ClassDef
    # Method Resolution Order: the order of looking up attributes. The first
    # value always to refers to this class.
    mro: List["TypeInfo"]
    # Used to stash the names of the mro classes temporarily between
    # deserialization and fixup. See deserialize() for why.
    _mro_refs: Optional[List[str]]
    bad_mro: bool  # Could not construct full MRO
    is_final: bool

    declared_metaclass: Optional["mypy.types.Instance"]
    metaclass_type: Optional["mypy.types.Instance"]

    names: "SymbolTable"  # Names defined directly in this type
    is_abstract: bool                      # Does the class have any abstract attributes?
    is_protocol: bool                      # Is this a protocol class?
    runtime_protocol: bool                 # Does this protocol support isinstance checks?
    abstract_attributes: List[str]
    deletable_attributes: List[str]  # Used by mypyc only
    # Does this type have concrete `__slots__` defined?
    # If class does not have `__slots__` defined then it is `None`,
    # if it has empty `__slots__` then it is an empty set.
    slots: Optional[Set[str]]

    # The attributes 'assuming' and 'assuming_proper' represent structural subtype matrices.
    #
    # In languages with structural subtyping, one can keep a global subtype matrix like this:
    #   . A B C .
    #   A 1 0 0
    #   B 1 1 1
    #   C 1 0 1
    #   .
    # where 1 indicates that the type in corresponding row is a subtype of the type
    # in corresponding column. This matrix typically starts filled with all 1's and
    # a typechecker tries to "disprove" every subtyping relation using atomic (or nominal) types.
    # However, we don't want to keep this huge global state. Instead, we keep the subtype
    # information in the form of list of pairs (subtype, supertype) shared by all 'Instance's
    # with given supertype's TypeInfo. When we enter a subtype check we push a pair in this list
    # thus assuming that we started with 1 in corresponding matrix element. Such algorithm allows
    # to treat recursive and mutually recursive protocols and other kinds of complex situations.
    #
    # If concurrent/parallel type checking will be added in future,
    # then there should be one matrix per thread/process to avoid false negatives
    # during the type checking phase.
    assuming: List[Tuple["mypy.types.Instance", "mypy.types.Instance"]]
    assuming_proper: List[Tuple["mypy.types.Instance", "mypy.types.Instance"]]
    # Ditto for temporary 'inferring' stack of recursive constraint inference.
    # It contains Instance's of protocol types that appeared as an argument to
    # constraints.infer_constraints(). We need 'inferring' to avoid infinite recursion for
    # recursive and mutually recursive protocols.
    #
    # We make 'assuming' and 'inferring' attributes here instead of passing they as kwargs,
    # since this would require to pass them in many dozens of calls. In particular,
    # there is a dependency infer_constraint -&gt; is_subtype -&gt; is_callable_subtype -&gt;
    # -&gt; infer_constraints.
    inferring: List["mypy.types.Instance"]
    # 'inferring' and 'assuming' can't be made sets, since we need to use
    # is_same_type to correctly treat unions.

    # Classes inheriting from Enum shadow their true members with a __getattr__, so we
    # have to treat them as a special case.
    is_enum: bool
    # If true, any unknown attributes should have type 'Any' instead
    # of generating a type error.  This would be true if there is a
    # base class with type 'Any', but other use cases may be
    # possible. This is similar to having __getattr__ that returns Any
    # (and __setattr__), but without the __getattr__ method.
    fallback_to_any: bool

    # Information related to type annotations.

    # Generic type variable names (full names)
    type_vars: List[str]

    # Whether this class has a ParamSpec type variable
    has_param_spec_type: bool

    # Direct base classes.
    bases: List["mypy.types.Instance"]

    # Another type which this type will be treated as a subtype of,
    # even though it's not a subclass in Python.  The non-standard
    # `@_promote` decorator introduces this, and there are also
    # several builtin examples, in particular `int` -&gt; `float`.
    _promote: Optional["mypy.types.Type"]

    # Representation of a Tuple[...] base class, if the class has any
    # (e.g., for named tuples). If this is not None, the actual Type
    # object used for this class is not an Instance but a TupleType;
    # the corresponding Instance is set as the fallback type of the
    # tuple type.
    tuple_type: Optional["mypy.types.TupleType"]

    # Is this a named tuple type?
    is_named_tuple: bool

    # If this class is defined by the TypedDict type constructor,
    # then this is not None.
    typeddict_type: Optional["mypy.types.TypedDictType"]

    # Is this a newtype type?
    is_newtype: bool

    # Is this a synthesized intersection type?
    is_intersection: bool

    # This is a dictionary that will be serialized and un-serialized as is.
    # It is useful for plugins to add their data to save in the cache.
    metadata: Dict[str, JsonDict]

    FLAGS: Final = [
        'is_abstract', 'is_enum', 'fallback_to_any', 'is_named_tuple',
        'is_newtype', 'is_protocol', 'runtime_protocol', 'is_final',
        'is_intersection',
    ]

    @others
</t>
<t tx="ekr.20220525082934.1197">def __init__(self, names: 'SymbolTable', defn: ClassDef, module_name: str) -&gt; None:
    """Initialize a TypeInfo."""
    super().__init__()
    self._fullname = defn.fullname
    self.names = names
    self.defn = defn
    self.module_name = module_name
    self.type_vars = []
    self.has_param_spec_type = False
    self.bases = []
    self.mro = []
    self._mro_refs = None
    self.bad_mro = False
    self.declared_metaclass = None
    self.metaclass_type = None
    self.is_abstract = False
    self.abstract_attributes = []
    self.deletable_attributes = []
    self.slots = None
    self.assuming = []
    self.assuming_proper = []
    self.inferring = []
    self.is_protocol = False
    self.runtime_protocol = False
    self.add_type_vars()
    self.is_final = False
    self.is_enum = False
    self.fallback_to_any = False
    self._promote = None
    self.tuple_type = None
    self.is_named_tuple = False
    self.typeddict_type = None
    self.is_newtype = False
    self.is_intersection = False
    self.metadata = {}

</t>
<t tx="ekr.20220525082934.1198">def add_type_vars(self) -&gt; None:
    if self.defn.type_vars:
        for vd in self.defn.type_vars:
            if isinstance(vd, mypy.types.ParamSpecType):
                self.has_param_spec_type = True
            self.type_vars.append(vd.name)

</t>
<t tx="ekr.20220525082934.1199">@property
def name(self) -&gt; str:
    """Short name."""
    return self.defn.name

</t>
<t tx="ekr.20220525082934.12">def visit_erased_type(self, t: ErasedType) -&gt; Type:
    # Should not get here.
    raise RuntimeError()

</t>
<t tx="ekr.20220525082934.120"># Bytes(bytes s)
def visit_Bytes(self, n: ast3.Bytes) -&gt; Union[BytesExpr, StrExpr]:
    e = BytesExpr(bytes_to_human_readable_repr(n.s))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1200">@property
def fullname(self) -&gt; Bogus[str]:
    return self._fullname

</t>
<t tx="ekr.20220525082934.1201">def is_generic(self) -&gt; bool:
    """Is the type generic (i.e. does it have type variables)?"""
    return len(self.type_vars) &gt; 0

</t>
<t tx="ekr.20220525082934.1202">def get(self, name: str) -&gt; 'Optional[SymbolTableNode]':
    for cls in self.mro:
        n = cls.names.get(name)
        if n:
            return n
    return None

</t>
<t tx="ekr.20220525082934.1203">def get_containing_type_info(self, name: str) -&gt; 'Optional[TypeInfo]':
    for cls in self.mro:
        if name in cls.names:
            return cls
    return None

</t>
<t tx="ekr.20220525082934.1204">@property
def protocol_members(self) -&gt; List[str]:
    # Protocol members are names of all attributes/methods defined in a protocol
    # and in all its supertypes (except for 'object').
    members: Set[str] = set()
    assert self.mro, "This property can be only accessed after MRO is (re-)calculated"
    for base in self.mro[:-1]:  # we skip "object" since everyone implements it
        if base.is_protocol:
            for name in base.names:
                members.add(name)
    return sorted(list(members))

</t>
<t tx="ekr.20220525082934.1205">def __getitem__(self, name: str) -&gt; 'SymbolTableNode':
    n = self.get(name)
    if n:
        return n
    else:
        raise KeyError(name)

</t>
<t tx="ekr.20220525082934.1206">def __repr__(self) -&gt; str:
    return f'&lt;TypeInfo {self.fullname}&gt;'

</t>
<t tx="ekr.20220525082934.1207">def __bool__(self) -&gt; bool:
    # We defined this here instead of just overriding it in
    # FakeInfo so that mypyc can generate a direct call instead of
    # using the generic bool handling.
    return not isinstance(self, FakeInfo)

</t>
<t tx="ekr.20220525082934.1208">def has_readable_member(self, name: str) -&gt; bool:
    return self.get(name) is not None

</t>
<t tx="ekr.20220525082934.1209">def get_method(self, name: str) -&gt; Union[FuncBase, Decorator, None]:
    for cls in self.mro:
        if name in cls.names:
            node = cls.names[name].node
            if isinstance(node, FuncBase):
                return node
            elif isinstance(node, Decorator):  # Two `if`s make `mypyc` happy
                return node
            else:
                return None
    return None

</t>
<t tx="ekr.20220525082934.121"># NameConstant(singleton value)
def visit_NameConstant(self, n: NameConstant) -&gt; NameExpr:
    e = NameExpr(str(n.value))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1210">def calculate_metaclass_type(self) -&gt; 'Optional[mypy.types.Instance]':
    declared = self.declared_metaclass
    if declared is not None and not declared.type.has_base('builtins.type'):
        return declared
    if self._fullname == 'builtins.type':
        return mypy.types.Instance(self, [])
    candidates = [s.declared_metaclass
                  for s in self.mro
                  if s.declared_metaclass is not None
                  and s.declared_metaclass.type is not None]
    for c in candidates:
        if all(other.type in c.type.mro for other in candidates):
            return c
    return None

</t>
<t tx="ekr.20220525082934.1211">def is_metaclass(self) -&gt; bool:
    return (self.has_base('builtins.type') or self.fullname == 'abc.ABCMeta' or
            self.fallback_to_any)

</t>
<t tx="ekr.20220525082934.1212">def has_base(self, fullname: str) -&gt; bool:
    """Return True if type has a base type with the specified name.

    This can be either via extension or via implementation.
    """
    for cls in self.mro:
        if cls.fullname == fullname:
            return True
    return False

</t>
<t tx="ekr.20220525082934.1213">def direct_base_classes(self) -&gt; 'List[TypeInfo]':
    """Return a direct base classes.

    Omit base classes of other base classes.
    """
    return [base.type for base in self.bases]

</t>
<t tx="ekr.20220525082934.1214">def __str__(self) -&gt; str:
    """Return a string representation of the type.

    This includes the most important information about the type.
    """
    return self.dump()

</t>
<t tx="ekr.20220525082934.1215">def dump(self,
         str_conv: 'Optional[mypy.strconv.StrConv]' = None,
         type_str_conv: 'Optional[mypy.types.TypeStrVisitor]' = None) -&gt; str:
    """Return a string dump of the contents of the TypeInfo."""
    if not str_conv:
        str_conv = mypy.strconv.StrConv()
    base: str = ""

    def type_str(typ: 'mypy.types.Type') -&gt; str:
        if type_str_conv:
            return typ.accept(type_str_conv)
        return str(typ)

    head = 'TypeInfo' + str_conv.format_id(self)
    if self.bases:
        base = f"Bases({', '.join(type_str(base) for base in self.bases)})"
    mro = 'Mro({})'.format(', '.join(item.fullname + str_conv.format_id(item)
                                     for item in self.mro))
    names = []
    for name in sorted(self.names):
        description = name + str_conv.format_id(self.names[name].node)
        node = self.names[name].node
        if isinstance(node, Var) and node.type:
            description += f' ({type_str(node.type)})'
        names.append(description)
    items = [
        f'Name({self.fullname})',
        base,
        mro,
        ('Names', names),
    ]
    if self.declared_metaclass:
        items.append(f'DeclaredMetaclass({type_str(self.declared_metaclass)})')
    if self.metaclass_type:
        items.append(f'MetaclassType({type_str(self.metaclass_type)})')
    return mypy.strconv.dump_tagged(
        items,
        head,
        str_conv=str_conv)

</t>
<t tx="ekr.20220525082934.1216">def serialize(self) -&gt; JsonDict:
    # NOTE: This is where all ClassDefs originate, so there shouldn't be duplicates.
    data = {'.class': 'TypeInfo',
            'module_name': self.module_name,
            'fullname': self.fullname,
            'names': self.names.serialize(self.fullname),
            'defn': self.defn.serialize(),
            'abstract_attributes': self.abstract_attributes,
            'type_vars': self.type_vars,
            'has_param_spec_type': self.has_param_spec_type,
            'bases': [b.serialize() for b in self.bases],
            'mro': [c.fullname for c in self.mro],
            '_promote': None if self._promote is None else self._promote.serialize(),
            'declared_metaclass': (None if self.declared_metaclass is None
                                   else self.declared_metaclass.serialize()),
            'metaclass_type':
                None if self.metaclass_type is None else self.metaclass_type.serialize(),
            'tuple_type': None if self.tuple_type is None else self.tuple_type.serialize(),
            'typeddict_type':
                None if self.typeddict_type is None else self.typeddict_type.serialize(),
            'flags': get_flags(self, TypeInfo.FLAGS),
            'metadata': self.metadata,
            'slots': list(sorted(self.slots)) if self.slots is not None else None,
            'deletable_attributes': self.deletable_attributes,
            }
    return data

</t>
<t tx="ekr.20220525082934.1217">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'TypeInfo':
    names = SymbolTable.deserialize(data['names'])
    defn = ClassDef.deserialize(data['defn'])
    module_name = data['module_name']
    ti = TypeInfo(names, defn, module_name)
    ti._fullname = data['fullname']
    # TODO: Is there a reason to reconstruct ti.subtypes?
    ti.abstract_attributes = data['abstract_attributes']
    ti.type_vars = data['type_vars']
    ti.has_param_spec_type = data['has_param_spec_type']
    ti.bases = [mypy.types.Instance.deserialize(b) for b in data['bases']]
    ti._promote = (None if data['_promote'] is None
                   else mypy.types.deserialize_type(data['_promote']))
    ti.declared_metaclass = (None if data['declared_metaclass'] is None
                             else mypy.types.Instance.deserialize(data['declared_metaclass']))
    ti.metaclass_type = (None if data['metaclass_type'] is None
                         else mypy.types.Instance.deserialize(data['metaclass_type']))
    # NOTE: ti.mro will be set in the fixup phase based on these
    # names.  The reason we need to store the mro instead of just
    # recomputing it from base classes has to do with a subtle
    # point about fine-grained incremental: the cache files might
    # not be loaded until after a class in the mro has changed its
    # bases, which causes the mro to change. If we recomputed our
    # mro, we would compute the *new* mro, which leaves us with no
    # way to detect that the mro has changed! Thus we need to make
    # sure to load the original mro so that once the class is
    # rechecked, it can tell that the mro has changed.
    ti._mro_refs = data['mro']
    ti.tuple_type = (None if data['tuple_type'] is None
                     else mypy.types.TupleType.deserialize(data['tuple_type']))
    ti.typeddict_type = (None if data['typeddict_type'] is None
                        else mypy.types.TypedDictType.deserialize(data['typeddict_type']))
    ti.metadata = data['metadata']
    ti.slots = set(data['slots']) if data['slots'] is not None else None
    ti.deletable_attributes = data['deletable_attributes']
    set_flags(ti, data['flags'])
    return ti


</t>
<t tx="ekr.20220525082934.1218">class FakeInfo(TypeInfo):

    __slots__ = ('msg',)

    @others
</t>
<t tx="ekr.20220525082934.1219"># types.py defines a single instance of this class, called types.NOT_READY.
# This instance is used as a temporary placeholder in the process of de-serialization
# of 'Instance' types. The de-serialization happens in two steps: In the first step,
# Instance.type is set to NOT_READY. In the second step (in fixup.py) it is replaced by
# an actual TypeInfo. If you see the assertion error below, then most probably something
# went wrong during the second step and an 'Instance' that raised this error was not fixed.
# Note:
# 'None' is not used as a dummy value for two reasons:
# 1. This will require around 80-100 asserts to make 'mypy --strict-optional mypy'
#    pass cleanly.
# 2. If NOT_READY value is accidentally used somewhere, it will be obvious where the value
#    is from, whereas a 'None' value could come from anywhere.
#
# Additionally, this serves as a more general-purpose placeholder
# for missing TypeInfos in a number of places where the excuses
# for not being Optional are a little weaker.
#
# TypeInfo defines a __bool__ method that returns False for FakeInfo
# so that it can be conveniently tested against in the same way that it
# would be if things were properly optional.
def __init__(self, msg: str) -&gt; None:
    self.msg = msg

</t>
<t tx="ekr.20220525082934.122"># Ellipsis
def visit_Ellipsis(self, n: ast3_Ellipsis) -&gt; EllipsisExpr:
    e = EllipsisExpr()
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1220">def __getattribute__(self, attr: str) -&gt; None:
    # Handle __class__ so that isinstance still works...
    if attr == '__class__':
        return object.__getattribute__(self, attr)
    raise AssertionError(object.__getattribute__(self, 'msg'))


</t>
<t tx="ekr.20220525082934.1221">VAR_NO_INFO: Final[TypeInfo] = FakeInfo("Var is lacking info")
CLASSDEF_NO_INFO: Final[TypeInfo] = FakeInfo("ClassDef is lacking info")
FUNC_NO_INFO: Final[TypeInfo] = FakeInfo("FuncBase for non-methods lack info")


</t>
<t tx="ekr.20220525082934.1222">class TypeAlias(SymbolNode):
    """
    A symbol node representing a type alias.

    Type alias is a static concept, in contrast to variables with types
    like Type[...]. Namely:
        * type aliases
            - can be used in type context (annotations)
            - cannot be re-assigned
        * variables with type Type[...]
            - cannot be used in type context
            - but can be re-assigned

    An alias can be defined only by an assignment to a name (not any other lvalues).

    Such assignment defines an alias by default. To define a variable,
    an explicit Type[...] annotation is required. As an exception,
    at non-global scope non-subscripted rvalue creates a variable even without
    an annotation. This exception exists to accommodate the common use case of
    class-valued attributes. See SemanticAnalyzerPass2.check_and_set_up_type_alias
    for details.

    Aliases can be generic. Currently, mypy uses unbound type variables for
    generic aliases and identifies them by name. Essentially, type aliases
    work as macros that expand textually. The definition and expansion rules are
    following:

        1. An alias targeting a generic class without explicit variables act as
        the given class (this doesn't apply to Tuple and Callable, which are not proper
        classes but special type constructors):

            A = List
            AA = List[Any]

            x: A  # same as List[Any]
            x: A[int]  # same as List[int]

            x: AA  # same as List[Any]
            x: AA[int]  # Error!

            C = Callable  # Same as Callable[..., Any]
            T = Tuple  # Same as Tuple[Any, ...]

        2. An alias using explicit type variables in its rvalue expects
        replacements (type arguments) for these variables. If missing, they
        are treated as Any, like for other generics:

            B = List[Tuple[T, T]]

            x: B  # same as List[Tuple[Any, Any]]
            x: B[int]  # same as List[Tuple[int, int]]

            def f(x: B[T]) -&gt; T: ...  # without T, Any would be used here

        3. An alias can be defined using another aliases. In the definition
        rvalue the Any substitution doesn't happen for top level unsubscripted
        generic classes:

            A = List
            B = A  # here A is expanded to List, _not_ List[Any],
                   # to match the Python runtime behaviour
            x: B[int]  # same as List[int]
            C = List[A]  # this expands to List[List[Any]]

            AA = List[T]
            D = AA  # here AA expands to List[Any]
            x: D[int]  # Error!

    Note: the fact that we support aliases like `A = List` means that the target
    type will be initially an instance type with wrong number of type arguments.
    Such instances are all fixed in the third pass of semantic analyzis.
    We therefore store the difference between `List` and `List[Any]` rvalues (targets)
    using the `no_args` flag. See also TypeAliasExpr.no_args.

    Meaning of other fields:

    target: The target type. For generic aliases contains unbound type variables
        as nested types.
    _fullname: Qualified name of this type alias. This is used in particular
        to track fine grained dependencies from aliases.
    alias_tvars: Names of unbound type variables used to define this alias.
    normalized: Used to distinguish between `A = List`, and `A = list`. Both
        are internally stored using `builtins.list` (because `typing.List` is
        itself an alias), while the second cannot be subscripted because of
        Python runtime limitation.
    line and column: Line an column on the original alias definition.
    eager: If True, immediately expand alias when referred to (useful for aliases
        within functions that can't be looked up from the symbol table)
    """
    __slots__ = ('target', '_fullname', 'alias_tvars', 'no_args', 'normalized',
                 'line', 'column', '_is_recursive', 'eager')

    @others
</t>
<t tx="ekr.20220525082934.1223">def __init__(self, target: 'mypy.types.Type', fullname: str, line: int, column: int,
             *,
             alias_tvars: Optional[List[str]] = None,
             no_args: bool = False,
             normalized: bool = False,
             eager: bool = False) -&gt; None:
    self._fullname = fullname
    self.target = target
    if alias_tvars is None:
        alias_tvars = []
    self.alias_tvars = alias_tvars
    self.no_args = no_args
    self.normalized = normalized
    # This attribute is manipulated by TypeAliasType. If non-None,
    # it is the cached value.
    self._is_recursive: Optional[bool] = None
    self.eager = eager
    super().__init__(line, column)

</t>
<t tx="ekr.20220525082934.1224">@property
def name(self) -&gt; str:
    return self._fullname.split('.')[-1]

</t>
<t tx="ekr.20220525082934.1225">@property
def fullname(self) -&gt; str:
    return self._fullname

</t>
<t tx="ekr.20220525082934.1226">def serialize(self) -&gt; JsonDict:
    data: JsonDict = {
        ".class": "TypeAlias",
        "fullname": self._fullname,
        "target": self.target.serialize(),
        "alias_tvars": self.alias_tvars,
        "no_args": self.no_args,
        "normalized": self.normalized,
        "line": self.line,
        "column": self.column,
    }
    return data

</t>
<t tx="ekr.20220525082934.1227">def accept(self, visitor: NodeVisitor[T]) -&gt; T:
    return visitor.visit_type_alias(self)

</t>
<t tx="ekr.20220525082934.1228">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'TypeAlias':
    assert data['.class'] == 'TypeAlias'
    fullname = data['fullname']
    alias_tvars = data['alias_tvars']
    target = mypy.types.deserialize_type(data['target'])
    no_args = data['no_args']
    normalized = data['normalized']
    line = data['line']
    column = data['column']
    return cls(target, fullname, line, column, alias_tvars=alias_tvars,
               no_args=no_args, normalized=normalized)


</t>
<t tx="ekr.20220525082934.1229">class PlaceholderNode(SymbolNode):
    """Temporary symbol node that will later become a real SymbolNode.

    These are only present during semantic analysis when using the new
    semantic analyzer. These are created if some essential dependencies
    of a definition are not yet complete.

    A typical use is for names imported from a module which is still
    incomplete (within an import cycle):

      from m import f  # Initially may create PlaceholderNode

    This is particularly important if the imported shadows a name from
    an enclosing scope or builtins:

      from m import int  # Placeholder avoids mixups with builtins.int

    Another case where this is useful is when there is another definition
    or assignment:

      from m import f
      def f() -&gt; None: ...

    In the above example, the presence of PlaceholderNode allows us to
    handle the second definition as a redefinition.

    They are also used to create PlaceholderType instances for types
    that refer to incomplete types. Example:

      class C(Sequence[C]): ...

    We create a PlaceholderNode (with becomes_typeinfo=True) for C so
    that the type C in Sequence[C] can be bound.

    Attributes:

      fullname: Full name of of the PlaceholderNode.
      node: AST node that contains the definition that caused this to
          be created. This is useful for tracking order of incomplete definitions
          and for debugging.
      becomes_typeinfo: If True, this refers something that could later
          become a TypeInfo. It can't be used with type variables, in
          particular, as this would cause issues with class type variable
          detection.

    The long-term purpose of placeholder nodes/types is to evolve into
    something that can support general recursive types.
    """

    __slots__ = ('_fullname', 'node', 'line', 'becomes_typeinfo')

    @others
</t>
<t tx="ekr.20220525082934.123"># Attribute(expr value, identifier attr, expr_context ctx)
def visit_Attribute(self, n: Attribute) -&gt; Union[MemberExpr, SuperExpr]:
    value = n.value
    member_expr = MemberExpr(self.visit(value), n.attr)
    obj = member_expr.expr
    if (isinstance(obj, CallExpr) and
            isinstance(obj.callee, NameExpr) and
            obj.callee.name == 'super'):
        e: Union[MemberExpr, SuperExpr] = SuperExpr(member_expr.name, obj)
    else:
        e = member_expr
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1230">def __init__(self, fullname: str, node: Node, line: int, *,
             becomes_typeinfo: bool = False) -&gt; None:
    self._fullname = fullname
    self.node = node
    self.becomes_typeinfo = becomes_typeinfo
    self.line = line

</t>
<t tx="ekr.20220525082934.1231">@property
def name(self) -&gt; str:
    return self._fullname.split('.')[-1]

</t>
<t tx="ekr.20220525082934.1232">@property
def fullname(self) -&gt; str:
    return self._fullname

</t>
<t tx="ekr.20220525082934.1233">def serialize(self) -&gt; JsonDict:
    assert False, "PlaceholderNode can't be serialized"

</t>
<t tx="ekr.20220525082934.1234">def accept(self, visitor: NodeVisitor[T]) -&gt; T:
    return visitor.visit_placeholder_node(self)


</t>
<t tx="ekr.20220525082934.1235">class SymbolTableNode:
    """Description of a name binding in a symbol table.

    These are only used as values in module (global), function (local)
    and class symbol tables (see SymbolTable). The name that is bound is
    the key in SymbolTable.

    Symbol tables don't contain direct references to AST nodes primarily
    because there can be multiple symbol table references to a single
    AST node (due to imports and aliases), and different references can
    behave differently. This class describes the unique properties of
    each reference.

    The most fundamental attribute is 'node', which is the AST node that
    the name refers to.

    The kind is usually one of LDEF, GDEF or MDEF, depending on the scope
    of the definition. These three kinds can usually be used
    interchangeably and the difference between local, global and class
    scopes is mostly descriptive, with no semantic significance.
    However, some tools that consume mypy ASTs may care about these so
    they should be correct.

    Attributes:
        node: AST node of definition. Among others, this can be one of
            FuncDef, Var, TypeInfo, TypeVarExpr or MypyFile -- or None
            for cross_ref that hasn't been fixed up yet.
        kind: Kind of node. Possible values:
               - LDEF: local definition
               - GDEF: global (module-level) definition
               - MDEF: class member definition
               - UNBOUND_IMPORTED: temporary kind for imported names (we
                 don't know the final kind yet)
        module_public: If False, this name won't be imported via
            'from &lt;module&gt; import *'. This has no effect on names within
            classes.
        module_hidden: If True, the name will be never exported (needed for
            stub files)
        cross_ref: For deserialized MypyFile nodes, the referenced module
            name; for other nodes, optionally the name of the referenced object.
        implicit: Was this defined by assignment to self attribute?
        plugin_generated: Was this symbol generated by a plugin?
            (And therefore needs to be removed in aststrip.)
        no_serialize: Do not serialize this node if True. This is used to prevent
            keys in the cache that refer to modules on which this file does not
            depend. Currently this can happen if there is a module not in build
            used e.g. like this:
                import a.b.c # type: ignore
            This will add a submodule symbol to parent module `a` symbol table,
            but `a.b` is _not_ added as its dependency. Therefore, we should
            not serialize these symbols as they may not be found during fixup
            phase, instead they will be re-added during subsequent patch parents
            phase.
            TODO: Refactor build.py to make dependency tracking more transparent
            and/or refactor look-up functions to not require parent patching.

    NOTE: No other attributes should be added to this class unless they
    are shared by all node kinds.
    """

    __slots__ = ('kind',
                 'node',
                 'module_public',
                 'module_hidden',
                 'cross_ref',
                 'implicit',
                 'plugin_generated',
                 'no_serialize',
                 )

    @others
</t>
<t tx="ekr.20220525082934.1236">def __init__(self,
             kind: int,
             node: Optional[SymbolNode],
             module_public: bool = True,
             implicit: bool = False,
             module_hidden: bool = False,
             *,
             plugin_generated: bool = False,
             no_serialize: bool = False) -&gt; None:
    self.kind = kind
    self.node = node
    self.module_public = module_public
    self.implicit = implicit
    self.module_hidden = module_hidden
    self.cross_ref: Optional[str] = None
    self.plugin_generated = plugin_generated
    self.no_serialize = no_serialize

</t>
<t tx="ekr.20220525082934.1237">@property
def fullname(self) -&gt; Optional[str]:
    if self.node is not None:
        return self.node.fullname
    else:
        return None

</t>
<t tx="ekr.20220525082934.1238">@property
def type(self) -&gt; 'Optional[mypy.types.Type]':
    node = self.node
    if isinstance(node, (Var, SYMBOL_FUNCBASE_TYPES)) and node.type is not None:
        return node.type
    elif isinstance(node, Decorator):
        return node.var.type
    else:
        return None

</t>
<t tx="ekr.20220525082934.1239">def copy(self) -&gt; 'SymbolTableNode':
    new = SymbolTableNode(self.kind,
                          self.node,
                          self.module_public,
                          self.implicit,
                          self.module_hidden)
    new.cross_ref = self.cross_ref
    return new

</t>
<t tx="ekr.20220525082934.124"># Subscript(expr value, slice slice, expr_context ctx)
def visit_Subscript(self, n: ast3.Subscript) -&gt; IndexExpr:
    e = IndexExpr(self.visit(n.value), self.visit(n.slice))
    self.set_line(e, n)
    # alias to please mypyc
    is_py38_or_earlier = sys.version_info &lt; (3, 9)
    if (
        isinstance(n.slice, ast3.Slice) or
        (is_py38_or_earlier and isinstance(n.slice, ast3.ExtSlice))
    ):
        # Before Python 3.9, Slice has no line/column in the raw ast. To avoid incompatibility
        # visit_Slice doesn't set_line, even in Python 3.9 on.
        # ExtSlice also has no line/column info. In Python 3.9 on, line/column is set for
        # e.index when visiting n.slice.
        e.index.line = e.line
        e.index.column = e.column
    return e

</t>
<t tx="ekr.20220525082934.1240">def __str__(self) -&gt; str:
    s = f'{node_kinds[self.kind]}/{short_type(self.node)}'
    if isinstance(self.node, SymbolNode):
        s += f' ({self.node.fullname})'
    # Include declared type of variables and functions.
    if self.type is not None:
        s += f' : {self.type}'
    return s

</t>
<t tx="ekr.20220525082934.1241">def serialize(self, prefix: str, name: str) -&gt; JsonDict:
    """Serialize a SymbolTableNode.

    Args:
      prefix: full name of the containing module or class; or None
      name: name of this object relative to the containing object
    """
    data: JsonDict = {
        ".class": "SymbolTableNode",
        "kind": node_kinds[self.kind],
    }
    if self.module_hidden:
        data['module_hidden'] = True
    if not self.module_public:
        data['module_public'] = False
    if self.implicit:
        data['implicit'] = True
    if self.plugin_generated:
        data['plugin_generated'] = True
    if isinstance(self.node, MypyFile):
        data['cross_ref'] = self.node.fullname
    else:
        assert self.node is not None, f'{prefix}:{name}'
        if prefix is not None:
            fullname = self.node.fullname
            if (fullname is not None and '.' in fullname
                    and fullname != prefix + '.' + name
                    and not (isinstance(self.node, Var)
                             and self.node.from_module_getattr)):
                assert not isinstance(self.node, PlaceholderNode), (
                    f'Definition of {fullname} is unexpectedly incomplete'
                )
                data['cross_ref'] = fullname
                return data
        data['node'] = self.node.serialize()
    return data

</t>
<t tx="ekr.20220525082934.1242">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'SymbolTableNode':
    assert data['.class'] == 'SymbolTableNode'
    kind = inverse_node_kinds[data['kind']]
    if 'cross_ref' in data:
        # This will be fixed up later.
        stnode = SymbolTableNode(kind, None)
        stnode.cross_ref = data['cross_ref']
    else:
        assert 'node' in data, data
        node = SymbolNode.deserialize(data['node'])
        stnode = SymbolTableNode(kind, node)
    if 'module_hidden' in data:
        stnode.module_hidden = data['module_hidden']
    if 'module_public' in data:
        stnode.module_public = data['module_public']
    if 'implicit' in data:
        stnode.implicit = data['implicit']
    if 'plugin_generated' in data:
        stnode.plugin_generated = data['plugin_generated']
    return stnode


</t>
<t tx="ekr.20220525082934.1243">class SymbolTable(Dict[str, SymbolTableNode]):
    """Static representation of a namespace dictionary.

    This is used for module, class and function namespaces.
    """

    __slots__ = ()

    @others
</t>
<t tx="ekr.20220525082934.1244">def __str__(self) -&gt; str:
    a: List[str] = []
    for key, value in self.items():
        # Filter out the implicit import of builtins.
        if isinstance(value, SymbolTableNode):
            if (value.fullname != 'builtins' and
                    (value.fullname or '').split('.')[-1] not in
                    implicit_module_attrs):
                a.append('  ' + str(key) + ' : ' + str(value))
        else:
            a.append('  &lt;invalid item&gt;')
    a = sorted(a)
    a.insert(0, 'SymbolTable(')
    a[-1] += ')'
    return '\n'.join(a)

</t>
<t tx="ekr.20220525082934.1245">def copy(self) -&gt; 'SymbolTable':
    return SymbolTable([(key, node.copy())
                        for key, node in self.items()])

</t>
<t tx="ekr.20220525082934.1246">def serialize(self, fullname: str) -&gt; JsonDict:
    data: JsonDict = {".class": "SymbolTable"}
    for key, value in self.items():
        # Skip __builtins__: it's a reference to the builtins
        # module that gets added to every module by
        # SemanticAnalyzerPass2.visit_file(), but it shouldn't be
        # accessed by users of the module.
        if key == '__builtins__' or value.no_serialize:
            continue
        data[key] = value.serialize(fullname, key)
    return data

</t>
<t tx="ekr.20220525082934.1247">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'SymbolTable':
    assert data['.class'] == 'SymbolTable'
    st = SymbolTable()
    for key, value in data.items():
        if key != '.class':
            st[key] = SymbolTableNode.deserialize(value)
    return st


</t>
<t tx="ekr.20220525082934.1248">def get_flags(node: Node, names: List[str]) -&gt; List[str]:
    return [name for name in names if getattr(node, name)]


</t>
<t tx="ekr.20220525082934.1249">def set_flags(node: Node, flags: List[str]) -&gt; None:
    for name in flags:
        setattr(node, name, True)


</t>
<t tx="ekr.20220525082934.125"># Starred(expr value, expr_context ctx)
def visit_Starred(self, n: Starred) -&gt; StarExpr:
    e = StarExpr(self.visit(n.value))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1250">def get_member_expr_fullname(expr: MemberExpr) -&gt; Optional[str]:
    """Return the qualified name representation of a member expression.

    Return a string of form foo.bar, foo.bar.baz, or similar, or None if the
    argument cannot be represented in this form.
    """
    initial: Optional[str] = None
    if isinstance(expr.expr, NameExpr):
        initial = expr.expr.name
    elif isinstance(expr.expr, MemberExpr):
        initial = get_member_expr_fullname(expr.expr)
    else:
        return None
    return f'{initial}.{expr.name}'


</t>
<t tx="ekr.20220525082934.1251">deserialize_map: Final = {
    key: obj.deserialize
    for key, obj in globals().items()
    if type(obj) is not FakeInfo
    and isinstance(obj, type) and issubclass(obj, SymbolNode) and obj is not SymbolNode
}


</t>
<t tx="ekr.20220525082934.1252">def check_arg_kinds(
        arg_kinds: List[ArgKind], nodes: List[T], fail: Callable[[str, T], None]) -&gt; None:
    is_var_arg = False
    is_kw_arg = False
    seen_named = False
    seen_opt = False
    for kind, node in zip(arg_kinds, nodes):
        if kind == ARG_POS:
            if is_var_arg or is_kw_arg or seen_named or seen_opt:
                fail("Required positional args may not appear "
                     "after default, named or var args",
                     node)
                break
        elif kind == ARG_OPT:
            if is_var_arg or is_kw_arg or seen_named:
                fail("Positional default args may not appear after named or var args", node)
                break
            seen_opt = True
        elif kind == ARG_STAR:
            if is_var_arg or is_kw_arg or seen_named:
                fail("Var args may not appear after named or var args", node)
                break
            is_var_arg = True
        elif kind == ARG_NAMED or kind == ARG_NAMED_OPT:
            seen_named = True
            if is_kw_arg:
                fail("A **kwargs argument must be the last argument", node)
                break
        elif kind == ARG_STAR2:
            if is_kw_arg:
                fail("You may only have one **kwargs argument", node)
                break
            is_kw_arg = True


</t>
<t tx="ekr.20220525082934.1253">def check_arg_names(names: Sequence[Optional[str]], nodes: List[T], fail: Callable[[str, T], None],
                    description: str = 'function definition') -&gt; None:
    seen_names: Set[Optional[str]] = set()
    for name, node in zip(names, nodes):
        if name is not None and name in seen_names:
            fail(f'Duplicate argument "{name}" in {description}', node)
            break
        seen_names.add(name)


</t>
<t tx="ekr.20220525082934.1254">def is_class_var(expr: NameExpr) -&gt; bool:
    """Return whether the expression is ClassVar[...]"""
    if isinstance(expr.node, Var):
        return expr.node.is_classvar
    return False


</t>
<t tx="ekr.20220525082934.1255">def is_final_node(node: Optional[SymbolNode]) -&gt; bool:
    """Check whether `node` corresponds to a final attribute."""
    return isinstance(node, (Var, FuncDef, OverloadedFuncDef, Decorator)) and node.is_final


</t>
<t tx="ekr.20220525082934.1256">def local_definitions(names: SymbolTable,
                      name_prefix: str,
                      info: Optional[TypeInfo] = None) -&gt; Iterator[Definition]:
    """Iterate over local definitions (not imported) in a symbol table.

    Recursively iterate over class members and nested classes.
    """
    # TODO: What should the name be? Or maybe remove it?
    for name, symnode in names.items():
        shortname = name
        if '-redef' in name:
            # Restore original name from mangled name of multiply defined function
            shortname = name.split('-redef')[0]
        fullname = name_prefix + '.' + shortname
        node = symnode.node
        if node and node.fullname == fullname:
            yield fullname, symnode, info
            if isinstance(node, TypeInfo):
                yield from local_definitions(node.names, fullname, node)
</t>
<t tx="ekr.20220525082934.1257">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Information about Python operators"""

from typing_extensions import Final


# Map from binary operator id to related method name (in Python 3).
op_methods: Final = {
    '+': '__add__',
    '-': '__sub__',
    '*': '__mul__',
    '/': '__truediv__',
    '%': '__mod__',
    'divmod': '__divmod__',
    '//': '__floordiv__',
    '**': '__pow__',
    '@': '__matmul__',
    '&amp;': '__and__',
    '|': '__or__',
    '^': '__xor__',
    '&lt;&lt;': '__lshift__',
    '&gt;&gt;': '__rshift__',
    '==': '__eq__',
    '!=': '__ne__',
    '&lt;': '__lt__',
    '&gt;=': '__ge__',
    '&gt;': '__gt__',
    '&lt;=': '__le__',
    'in': '__contains__',
}

op_methods_to_symbols: Final = {v: k for (k, v) in op_methods.items()}
op_methods_to_symbols['__div__'] = '/'

comparison_fallback_method: Final = "__cmp__"
ops_falling_back_to_cmp: Final = {"__ne__", "__eq__", "__lt__", "__le__", "__gt__", "__ge__"}


ops_with_inplace_method: Final = {
    "+",
    "-",
    "*",
    "/",
    "%",
    "//",
    "**",
    "@",
    "&amp;",
    "|",
    "^",
    "&lt;&lt;",
    "&gt;&gt;",
}

inplace_operator_methods: Final = {"__i" + op_methods[op][2:] for op in ops_with_inplace_method}

reverse_op_methods: Final = {
    '__add__': '__radd__',
    '__sub__': '__rsub__',
    '__mul__': '__rmul__',
    '__truediv__': '__rtruediv__',
    '__mod__': '__rmod__',
    '__divmod__': '__rdivmod__',
    '__floordiv__': '__rfloordiv__',
    '__pow__': '__rpow__',
    '__matmul__': '__rmatmul__',
    '__and__': '__rand__',
    '__or__': '__ror__',
    '__xor__': '__rxor__',
    '__lshift__': '__rlshift__',
    '__rshift__': '__rrshift__',
    '__eq__': '__eq__',
    '__ne__': '__ne__',
    '__lt__': '__gt__',
    '__ge__': '__le__',
    '__gt__': '__lt__',
    '__le__': '__ge__',
}

reverse_op_method_names: Final = set(reverse_op_methods.values())

# Suppose we have some class A. When we do A() + A(), Python will only check
# the output of A().__add__(A()) and skip calling the __radd__ method entirely.
# This shortcut is used only for the following methods:
op_methods_that_shortcut: Final = {
    '__add__',
    '__sub__',
    '__mul__',
    '__div__',
    '__truediv__',
    '__mod__',
    '__divmod__',
    '__floordiv__',
    '__pow__',
    '__matmul__',
    '__and__',
    '__or__',
    '__xor__',
    '__lshift__',
    '__rshift__',
}

normal_from_reverse_op: Final = {m: n for n, m in reverse_op_methods.items()}
reverse_op_method_set: Final = set(reverse_op_methods.values())

unary_op_methods: Final = {
    '-': '__neg__',
    '+': '__pos__',
    '~': '__invert__',
}
</t>
<t tx="ekr.20220525082934.1258">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from mypy.backports import OrderedDict
import re
import pprint
import sys

from typing_extensions import Final, TYPE_CHECKING
from typing import Dict, List, Mapping, Optional, Pattern, Set, Tuple, Callable, Any

from mypy import defaults
from mypy.util import get_class_descriptors, replace_object_state

if TYPE_CHECKING:
    from mypy.errors import ErrorCode


@others
</t>
<t tx="ekr.20220525082934.1259">class BuildType:
    STANDARD: Final = 0
    MODULE: Final = 1
    PROGRAM_TEXT: Final = 2


</t>
<t tx="ekr.20220525082934.126"># Name(identifier id, expr_context ctx)
def visit_Name(self, n: Name) -&gt; NameExpr:
    e = NameExpr(n.id)
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1260">PER_MODULE_OPTIONS: Final = {
    # Please keep this list sorted
    "allow_redefinition",
    "allow_untyped_globals",
    "always_false",
    "always_true",
    "check_untyped_defs",
    "debug_cache",
    "disallow_any_decorated",
    "disallow_any_explicit",
    "disallow_any_expr",
    "disallow_any_generics",
    "disallow_any_unimported",
    "disallow_incomplete_defs",
    "disallow_subclassing_any",
    "disallow_untyped_calls",
    "disallow_untyped_decorators",
    "disallow_untyped_defs",
    "follow_imports",
    "follow_imports_for_stubs",
    "ignore_errors",
    "ignore_missing_imports",
    "implicit_reexport",
    "local_partial_types",
    "mypyc",
    "no_implicit_optional",
    "show_none_errors",
    "strict_concatenate",
    "strict_equality",
    "strict_optional",
    "strict_optional_whitelist",
    "warn_no_return",
    "warn_return_any",
    "warn_unreachable",
    "warn_unused_ignores",
}

OPTIONS_AFFECTING_CACHE: Final = (PER_MODULE_OPTIONS | {"platform", "bazel", "plugins"}) - {
    "debug_cache"
}


</t>
<t tx="ekr.20220525082934.1261">class Options:
    """Options collected from flags."""

    @others
</t>
<t tx="ekr.20220525082934.1262">def __init__(self) -&gt; None:
    # Cache for clone_for_module()
    self._per_module_cache: Optional[Dict[str, Options]] = None

    # -- build options --
    self.build_type = BuildType.STANDARD
    self.python_version: Tuple[int, int] = sys.version_info[:2]
    # The executable used to search for PEP 561 packages. If this is None,
    # then mypy does not search for PEP 561 packages.
    self.python_executable: Optional[str] = sys.executable
    self.platform = sys.platform
    self.custom_typing_module: Optional[str] = None
    self.custom_typeshed_dir: Optional[str] = None
    self.mypy_path: List[str] = []
    self.report_dirs: Dict[str, str] = {}
    # Show errors in PEP 561 packages/site-packages modules
    self.no_silence_site_packages = False
    self.no_site_packages = False
    self.ignore_missing_imports = False
    # Is ignore_missing_imports set in a per-module section
    self.ignore_missing_imports_per_module = False
    self.follow_imports = 'normal'  # normal|silent|skip|error
    # Whether to respect the follow_imports setting even for stub files.
    # Intended to be used for disabling specific stubs.
    self.follow_imports_for_stubs = False
    # PEP 420 namespace packages
    # This allows definitions of packages without __init__.py and allows packages to span
    # multiple directories. This flag affects both import discovery and the association of
    # input files/modules/packages to the relevant file and fully qualified module name.
    self.namespace_packages = False
    # Use current directory and MYPYPATH to determine fully qualified module names of files
    # passed by automatically considering their subdirectories as packages. This is only
    # relevant if namespace packages are enabled, since otherwise examining __init__.py's is
    # sufficient to determine module names for files. As a possible alternative, add a single
    # top-level __init__.py to your packages.
    self.explicit_package_bases = False
    # File names, directory names or subpaths to avoid checking
    self.exclude: List[str] = []

    # disallow_any options
    self.disallow_any_generics = False
    self.disallow_any_unimported = False
    self.disallow_any_expr = False
    self.disallow_any_decorated = False
    self.disallow_any_explicit = False

    # Disallow calling untyped functions from typed ones
    self.disallow_untyped_calls = False

    # Disallow defining untyped (or incompletely typed) functions
    self.disallow_untyped_defs = False

    # Disallow defining incompletely typed functions
    self.disallow_incomplete_defs = False

    # Type check unannotated functions
    self.check_untyped_defs = False

    # Disallow decorating typed functions with untyped decorators
    self.disallow_untyped_decorators = False

    # Disallow subclassing values of type 'Any'
    self.disallow_subclassing_any = False

    # Also check typeshed for missing annotations
    self.warn_incomplete_stub = False

    # Warn about casting an expression to its inferred type
    self.warn_redundant_casts = False

    # Warn about falling off the end of a function returning non-None
    self.warn_no_return = True

    # Warn about returning objects of type Any when the function is
    # declared with a precise type
    self.warn_return_any = False

    # Warn about unused '# type: ignore' comments
    self.warn_unused_ignores = False

    # Warn about unused '[mypy-&lt;pattern&gt;]'  or '[[tool.mypy.overrides]]' config sections
    self.warn_unused_configs = False

    # Files in which to ignore all non-fatal errors
    self.ignore_errors = False

    # Apply strict None checking
    self.strict_optional = True

    # Show "note: In function "foo":" messages.
    self.show_error_context = False

    # Use nicer output (when possible).
    self.color_output = True
    self.error_summary = True

    # Files in which to allow strict-Optional related errors
    # TODO: Kill this in favor of show_none_errors
    self.strict_optional_whitelist: Optional[List[str]] = None

    # Alternate way to show/hide strict-None-checking related errors
    self.show_none_errors = True

    # Don't assume arguments with default values of None are Optional
    self.no_implicit_optional = False

    # Don't re-export names unless they are imported with `from ... as ...`
    self.implicit_reexport = True

    # Suppress toplevel errors caused by missing annotations
    self.allow_untyped_globals = False

    # Allow variable to be redefined with an arbitrary type in the same block
    # and the same nesting level as the initialization
    self.allow_redefinition = False

    # Prohibit equality, identity, and container checks for non-overlapping types.
    # This makes 1 == '1', 1 in ['1'], and 1 is '1' errors.
    self.strict_equality = False

    # Make arguments prepended via Concatenate be truly positional-only.
    self.strict_concatenate = False

    # Report an error for any branches inferred to be unreachable as a result of
    # type analysis.
    self.warn_unreachable = False

    # Variable names considered True
    self.always_true: List[str] = []

    # Variable names considered False
    self.always_false: List[str] = []

    # Error codes to disable
    self.disable_error_code: List[str] = []
    self.disabled_error_codes: Set[ErrorCode] = set()

    # Error codes to enable
    self.enable_error_code: List[str] = []
    self.enabled_error_codes: Set[ErrorCode] = set()

    # Use script name instead of __main__
    self.scripts_are_modules = False

    # Config file name
    self.config_file: Optional[str] = None

    # A filename containing a JSON mapping from filenames to
    # mtime/size/hash arrays, used to avoid having to recalculate
    # source hashes as often.
    self.quickstart_file: Optional[str] = None

    # A comma-separated list of files/directories for mypy to type check;
    # supports globbing
    self.files: Optional[List[str]] = None

    # Write junit.xml to given file
    self.junit_xml: Optional[str] = None

    # Caching and incremental checking options
    self.incremental = True
    self.cache_dir = defaults.CACHE_DIR
    self.sqlite_cache = False
    self.debug_cache = False
    self.skip_version_check = False
    self.skip_cache_mtime_checks = False
    self.fine_grained_incremental = False
    # Include fine-grained dependencies in written cache files
    self.cache_fine_grained = False
    # Read cache files in fine-grained incremental mode (cache must include dependencies)
    self.use_fine_grained_cache = False

    # Tune certain behaviors when being used as a front-end to mypyc. Set per-module
    # in modules being compiled. Not in the config file or command line.
    self.mypyc = False

    # Disable the memory optimization of freeing ASTs when
    # possible. This isn't exposed as a command line option
    # because it is intended for software integrating with
    # mypy. (Like mypyc.)
    self.preserve_asts = False

    # Paths of user plugins
    self.plugins: List[str] = []

    # Per-module options (raw)
    self.per_module_options: OrderedDict[str, Dict[str, object]] = OrderedDict()
    self._glob_options: List[Tuple[str, Pattern[str]]] = []
    self.unused_configs: Set[str] = set()

    # -- development options --
    self.verbosity = 0  # More verbose messages (for troubleshooting)
    self.pdb = False
    self.show_traceback = False
    self.raise_exceptions = False
    self.dump_type_stats = False
    self.dump_inference_stats = False
    self.dump_build_stats = False
    self.enable_incomplete_features = False
    self.timing_stats: Optional[str] = None

    # -- test options --
    # Stop after the semantic analysis phase
    self.semantic_analysis_only = False

    # Use stub builtins fixtures to speed up tests
    self.use_builtins_fixtures = False

    # -- experimental options --
    self.shadow_file: Optional[List[List[str]]] = None
    self.show_column_numbers: bool = False
    self.show_error_codes = False
    # Use soft word wrap and show trimmed source snippets with error location markers.
    self.pretty = False
    self.dump_graph = False
    self.dump_deps = False
    self.logical_deps = False
    # If True, partial types can't span a module top level and a function
    self.local_partial_types = False
    # Some behaviors are changed when using Bazel (https://bazel.build).
    self.bazel = False
    # If True, export inferred types for all expressions as BuildResult.types
    self.export_types = False
    # List of package roots -- directories under these are packages even
    # if they don't have __init__.py.
    self.package_root: List[str] = []
    self.cache_map: Dict[str, Tuple[str, str]] = {}
    # Don't properly free objects on exit, just kill the current process.
    self.fast_exit = True
    # fast path for finding modules from source set
    self.fast_module_lookup = False
    # Used to transform source code before parsing if not None
    # TODO: Make the type precise (AnyStr -&gt; AnyStr)
    self.transform_source: Optional[Callable[[Any], Any]] = None
    # Print full path to each file in the report.
    self.show_absolute_path: bool = False
    # Install missing stub packages if True
    self.install_types = False
    # Install missing stub packages in non-interactive mode (don't prompt for
    # confirmation, and don't show any errors)
    self.non_interactive = False
    # When we encounter errors that may cause many additional errors,
    # skip most errors after this many messages have been reported.
    # -1 means unlimited.
    self.many_errors_threshold = defaults.MANY_ERRORS_THRESHOLD

</t>
<t tx="ekr.20220525082934.1263"># To avoid breaking plugin compatibility, keep providing new_semantic_analyzer
@property
def new_semantic_analyzer(self) -&gt; bool:
    return True

</t>
<t tx="ekr.20220525082934.1264">def snapshot(self) -&gt; object:
    """Produce a comparable snapshot of this Option"""
    # Under mypyc, we don't have a __dict__, so we need to do worse things.
    d = dict(getattr(self, '__dict__', ()))
    for k in get_class_descriptors(Options):
        if hasattr(self, k) and k != "new_semantic_analyzer":
            d[k] = getattr(self, k)
    # Remove private attributes from snapshot
    d = {k: v for k, v in d.items() if not k.startswith('_')}
    return d

</t>
<t tx="ekr.20220525082934.1265">def __repr__(self) -&gt; str:
    return f'Options({pprint.pformat(self.snapshot())})'

</t>
<t tx="ekr.20220525082934.1266">def apply_changes(self, changes: Dict[str, object]) -&gt; 'Options':
    new_options = Options()
    # Under mypyc, we don't have a __dict__, so we need to do worse things.
    replace_object_state(new_options, self, copy_dict=True)
    for key, value in changes.items():
        setattr(new_options, key, value)
    if changes.get("ignore_missing_imports"):
        # This is the only option for which a per-module and a global
        # option sometimes beheave differently.
        new_options.ignore_missing_imports_per_module = True
    return new_options

</t>
<t tx="ekr.20220525082934.1267">def build_per_module_cache(self) -&gt; None:
    self._per_module_cache = {}

    # Config precedence is as follows:
    #  1. Concrete section names: foo.bar.baz
    #  2. "Unstructured" glob patterns: foo.*.baz, in the order
    #     they appear in the file (last wins)
    #  3. "Well-structured" wildcard patterns: foo.bar.*, in specificity order.

    # Since structured configs inherit from structured configs above them in the hierarchy,
    # we need to process per-module configs in a careful order.
    # We have to process foo.* before foo.bar.* before foo.bar,
    # and we need to apply *.bar to foo.bar but not to foo.bar.*.
    # To do this, process all well-structured glob configs before non-glob configs and
    # exploit the fact that foo.* sorts earlier ASCIIbetically (unicodebetically?)
    # than foo.bar.*.
    # (A section being "processed last" results in its config "winning".)
    # Unstructured glob configs are stored and are all checked for each module.
    unstructured_glob_keys = [k for k in self.per_module_options.keys()
                              if '*' in k[:-1]]
    structured_keys = [k for k in self.per_module_options.keys()
                       if '*' not in k[:-1]]
    wildcards = sorted(k for k in structured_keys if k.endswith('.*'))
    concrete = [k for k in structured_keys if not k.endswith('.*')]

    for glob in unstructured_glob_keys:
        self._glob_options.append((glob, self.compile_glob(glob)))

    # We (for ease of implementation) treat unstructured glob
    # sections as used if any real modules use them or if any
    # concrete config sections use them. This means we need to
    # track which get used while constructing.
    self.unused_configs = set(unstructured_glob_keys)

    for key in wildcards + concrete:
        # Find what the options for this key would be, just based
        # on inheriting from parent configs.
        options = self.clone_for_module(key)
        # And then update it with its per-module options.
        self._per_module_cache[key] = options.apply_changes(self.per_module_options[key])

    # Add the more structured sections into unused configs, since
    # they only count as used if actually used by a real module.
    self.unused_configs.update(structured_keys)

</t>
<t tx="ekr.20220525082934.1268">def clone_for_module(self, module: str) -&gt; 'Options':
    """Create an Options object that incorporates per-module options.

    NOTE: Once this method is called all Options objects should be
    considered read-only, else the caching might be incorrect.
    """
    if self._per_module_cache is None:
        self.build_per_module_cache()
    assert self._per_module_cache is not None

    # If the module just directly has a config entry, use it.
    if module in self._per_module_cache:
        self.unused_configs.discard(module)
        return self._per_module_cache[module]

    # If not, search for glob paths at all the parents. So if we are looking for
    # options for foo.bar.baz, we search foo.bar.baz.*, foo.bar.*, foo.*,
    # in that order, looking for an entry.
    # This is technically quadratic in the length of the path, but module paths
    # don't actually get all that long.
    options = self
    path = module.split('.')
    for i in range(len(path), 0, -1):
        key = '.'.join(path[:i] + ['*'])
        if key in self._per_module_cache:
            self.unused_configs.discard(key)
            options = self._per_module_cache[key]
            break

    # OK and *now* we need to look for unstructured glob matches.
    # We only do this for concrete modules, not structured wildcards.
    if not module.endswith('.*'):
        for key, pattern in self._glob_options:
            if pattern.match(module):
                self.unused_configs.discard(key)
                options = options.apply_changes(self.per_module_options[key])

    # We could update the cache to directly point to modules once
    # they have been looked up, but in testing this made things
    # slower and not faster, so we don't bother.

    return options

</t>
<t tx="ekr.20220525082934.1269">def compile_glob(self, s: str) -&gt; Pattern[str]:
    # Compile one of the glob patterns to a regex so that '.*' can
    # match *zero or more* module sections. This means we compile
    # '.*' into '(\..*)?'.
    parts = s.split('.')
    expr = re.escape(parts[0]) if parts[0] != '*' else '.*'
    for part in parts[1:]:
        expr += re.escape('.' + part) if part != '*' else r'(\..*)?'
    return re.compile(expr + '\\Z')

</t>
<t tx="ekr.20220525082934.127"># List(expr* elts, expr_context ctx)
def visit_List(self, n: ast3.List) -&gt; Union[ListExpr, TupleExpr]:
    expr_list: List[Expression] = [self.visit(e) for e in n.elts]
    if isinstance(n.ctx, ast3.Store):
        # [x, y] = z and (x, y) = z means exactly the same thing
        e: Union[ListExpr, TupleExpr] = TupleExpr(expr_list)
    else:
        e = ListExpr(expr_list)
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1270">def select_options_affecting_cache(self) -&gt; Mapping[str, object]:
    return {opt: getattr(self, opt) for opt in OPTIONS_AFFECTING_CACHE}
</t>
<t tx="ekr.20220525082934.1271">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from typing import Union, Optional

from mypy.errors import Errors
from mypy.options import Options
from mypy.nodes import MypyFile


@others
</t>
<t tx="ekr.20220525082934.1272">def parse(source: Union[str, bytes],
          fnam: str,
          module: Optional[str],
          errors: Optional[Errors],
          options: Options) -&gt; MypyFile:
    """Parse a source file, without doing any semantic analysis.

    Return the parse tree. If errors is not provided, raise ParseError
    on failure. Otherwise, use the errors object to report parse errors.

    The python_version (major, minor) option determines the Python syntax variant.
    """
    is_stub_file = fnam.endswith('.pyi')
    if options.transform_source is not None:
        source = options.transform_source(source)
    if options.python_version[0] &gt;= 3 or is_stub_file:
        import mypy.fastparse
        return mypy.fastparse.parse(source,
                                    fnam=fnam,
                                    module=module,
                                    errors=errors,
                                    options=options)
    else:
        import mypy.fastparse2
        return mypy.fastparse2.parse(source,
                                     fnam=fnam,
                                     module=module,
                                     errors=errors,
                                     options=options)
</t>
<t tx="ekr.20220525082934.1273">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Classes for representing match statement patterns."""
from typing import TypeVar, List, Optional, Union

from mypy_extensions import trait

from mypy.nodes import Node, RefExpr, NameExpr, Expression
from mypy.visitor import PatternVisitor


T = TypeVar('T')


@others
</t>
<t tx="ekr.20220525082934.1274">@trait
class Pattern(Node):
    """A pattern node."""

    __slots__ = ()

    @others
</t>
<t tx="ekr.20220525082934.1275">def accept(self, visitor: PatternVisitor[T]) -&gt; T:
    raise RuntimeError('Not implemented')


</t>
<t tx="ekr.20220525082934.1276">class AsPattern(Pattern):
    """The pattern &lt;pattern&gt; as &lt;name&gt;"""
    # The python ast, and therefore also our ast merges capture, wildcard and as patterns into one
    # for easier handling.
    # If pattern is None this is a capture pattern. If name and pattern are both none this is a
    # wildcard pattern.
    # Only name being None should not happen but also won't break anything.
    pattern: Optional[Pattern]
    name: Optional[NameExpr]

    @others
</t>
<t tx="ekr.20220525082934.1277">def __init__(self, pattern: Optional[Pattern], name: Optional[NameExpr]) -&gt; None:
    super().__init__()
    self.pattern = pattern
    self.name = name

</t>
<t tx="ekr.20220525082934.1278">def accept(self, visitor: PatternVisitor[T]) -&gt; T:
    return visitor.visit_as_pattern(self)


</t>
<t tx="ekr.20220525082934.1279">class OrPattern(Pattern):
    """The pattern &lt;pattern&gt; | &lt;pattern&gt; | ..."""
    patterns: List[Pattern]

    @others
</t>
<t tx="ekr.20220525082934.128"># Tuple(expr* elts, expr_context ctx)
def visit_Tuple(self, n: ast3.Tuple) -&gt; TupleExpr:
    e = TupleExpr(self.translate_expr_list(n.elts))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.1280">def __init__(self, patterns: List[Pattern]) -&gt; None:
    super().__init__()
    self.patterns = patterns

</t>
<t tx="ekr.20220525082934.1281">def accept(self, visitor: PatternVisitor[T]) -&gt; T:
    return visitor.visit_or_pattern(self)


</t>
<t tx="ekr.20220525082934.1282">class ValuePattern(Pattern):
    """The pattern x.y (or x.y.z, ...)"""
    expr: Expression

    @others
</t>
<t tx="ekr.20220525082934.1283">def __init__(self, expr: Expression):
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20220525082934.1284">def accept(self, visitor: PatternVisitor[T]) -&gt; T:
    return visitor.visit_value_pattern(self)


</t>
<t tx="ekr.20220525082934.1285">class SingletonPattern(Pattern):
    # This can be exactly True, False or None
    value: Union[bool, None]

    @others
</t>
<t tx="ekr.20220525082934.1286">def __init__(self, value: Union[bool, None]):
    super().__init__()
    self.value = value

</t>
<t tx="ekr.20220525082934.1287">def accept(self, visitor: PatternVisitor[T]) -&gt; T:
    return visitor.visit_singleton_pattern(self)


</t>
<t tx="ekr.20220525082934.1288">class SequencePattern(Pattern):
    """The pattern [&lt;pattern&gt;, ...]"""
    patterns: List[Pattern]

    @others
</t>
<t tx="ekr.20220525082934.1289">def __init__(self, patterns: List[Pattern]):
    super().__init__()
    self.patterns = patterns

</t>
<t tx="ekr.20220525082934.129"># --- slice ---

</t>
<t tx="ekr.20220525082934.1290">def accept(self, visitor: PatternVisitor[T]) -&gt; T:
    return visitor.visit_sequence_pattern(self)


</t>
<t tx="ekr.20220525082934.1291">class StarredPattern(Pattern):
    # None corresponds to *_ in a list pattern. It will match multiple items but won't bind them to
    # a name.
    capture: Optional[NameExpr]

    @others
</t>
<t tx="ekr.20220525082934.1292">def __init__(self, capture: Optional[NameExpr]):
    super().__init__()
    self.capture = capture

</t>
<t tx="ekr.20220525082934.1293">def accept(self, visitor: PatternVisitor[T]) -&gt; T:
    return visitor.visit_starred_pattern(self)


</t>
<t tx="ekr.20220525082934.1294">class MappingPattern(Pattern):
    keys: List[Expression]
    values: List[Pattern]
    rest: Optional[NameExpr]

    @others
</t>
<t tx="ekr.20220525082934.1295">def __init__(self, keys: List[Expression], values: List[Pattern],
             rest: Optional[NameExpr]):
    super().__init__()
    assert len(keys) == len(values)
    self.keys = keys
    self.values = values
    self.rest = rest

</t>
<t tx="ekr.20220525082934.1296">def accept(self, visitor: PatternVisitor[T]) -&gt; T:
    return visitor.visit_mapping_pattern(self)


</t>
<t tx="ekr.20220525082934.1297">class ClassPattern(Pattern):
    """The pattern Cls(...)"""
    class_ref: RefExpr
    positionals: List[Pattern]
    keyword_keys: List[str]
    keyword_values: List[Pattern]

    @others
</t>
<t tx="ekr.20220525082934.1298">def __init__(self, class_ref: RefExpr, positionals: List[Pattern], keyword_keys: List[str],
             keyword_values: List[Pattern]):
    super().__init__()
    assert len(keyword_keys) == len(keyword_values)
    self.class_ref = class_ref
    self.positionals = positionals
    self.keyword_keys = keyword_keys
    self.keyword_values = keyword_values

</t>
<t tx="ekr.20220525082934.1299">def accept(self, visitor: PatternVisitor[T]) -&gt; T:
    return visitor.visit_class_pattern(self)
</t>
<t tx="ekr.20220525082934.13">def visit_instance(self, t: Instance) -&gt; Type:
    args = self.expand_types(t.args)
    return Instance(t.type, args, t.line, t.column)

</t>
<t tx="ekr.20220525082934.130"># Slice(expr? lower, expr? upper, expr? step)
def visit_Slice(self, n: ast3.Slice) -&gt; SliceExpr:
    return SliceExpr(self.visit(n.lower),
                     self.visit(n.upper),
                     self.visit(n.step))

</t>
<t tx="ekr.20220525082934.1300">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Plugin system for extending mypy.

At large scale the plugin system works as following:

* Plugins are collected from the corresponding mypy config file option
  (either via paths to Python files, or installed Python modules)
  and imported using importlib.

* Every module should get an entry point function (called 'plugin' by default,
  but may be overridden in the config file) that should accept a single string
  argument that is a full mypy version (includes git commit hash for dev
  versions) and return a subclass of mypy.plugins.Plugin.

* All plugin class constructors should match the signature of mypy.plugin.Plugin
  (i.e. should accept an mypy.options.Options object), and *must* call
  super().__init__().

* At several steps during semantic analysis and type checking mypy calls
  special `get_xxx` methods on user plugins with a single string argument that
  is a fully qualified name (full name) of a relevant definition
  (see mypy.plugin.Plugin method docstrings for details).

* The plugins are called in the order they are passed in the config option.
  Every plugin must decide whether to act on a given full name. The first
  plugin that returns non-None object will be used.

* The above decision should be made using the limited common API specified by
  mypy.plugin.CommonPluginApi.

* The callback returned by the plugin will be called with a larger context that
  includes relevant current state (e.g. a default return type, or a default
  attribute type) and a wider relevant API provider (e.g.
  SemanticAnalyzerPluginInterface or CheckerPluginInterface).

* The result of this is used for further processing. See various `XxxContext`
  named tuples for details about which information is given to each hook.

Plugin developers should ensure that their plugins work well in incremental and
daemon modes. In particular, plugins should not hold global state, and should
always call add_plugin_dependency() in plugin hooks called during semantic
analysis. See the method docstring for more details.

There is no dedicated cache storage for plugins, but plugins can store
per-TypeInfo data in a special .metadata attribute that is serialized to the
mypy caches between incremental runs. To avoid collisions between plugins, they
are encouraged to store their state under a dedicated key coinciding with
plugin name in the metadata dictionary. Every value stored there must be
JSON-serializable.

## Notes about the semantic analyzer

Mypy 0.710 introduced a new semantic analyzer that changed how plugins are
expected to work in several notable ways (from mypy 0.730 the old semantic
analyzer is no longer available):

1. The order of processing AST nodes in modules is different. The old semantic
   analyzer processed modules in textual order, one module at a time. The new
   semantic analyzer first processes the module top levels, including bodies of
   any top-level classes and classes nested within classes. ("Top-level" here
   means "not nested within a function/method".) Functions and methods are
   processed only after module top levels have been finished. If there is an
   import cycle, all module top levels in the cycle are processed before
   processing any functions or methods. Each unit of processing (a module top
   level or a function/method) is called a *target*.

   This also means that function signatures in the same module have not been
   analyzed yet when analyzing the module top level. If you need access to
   a function signature, you'll need to explicitly analyze the signature first
   using `anal_type()`.

2. Each target can be processed multiple times. This may happen if some forward
   references are not ready yet, for example. This means that semantic analyzer
   related plugin hooks can be called multiple times for the same full name.
   These plugin methods must thus be idempotent.

3. The `anal_type` API function returns None if some part of the type is not
   available yet. If this happens, the current target being analyzed will be
   *deferred*, which means that it will be processed again soon, in the hope
   that additional dependencies will be available. This may happen if there are
   forward references to types or inter-module references to types within an
   import cycle.

   Note that if there is a circular definition, mypy may decide to stop
   processing to avoid an infinite number of iterations. When this happens,
   `anal_type` will generate an error and return an `AnyType` type object
   during the final iteration (instead of None).

4. There is a new API method `defer()`. This can be used to explicitly request
   the current target to be reprocessed one more time. You don't need this
   to call this if `anal_type` returns None, however.

5. There is a new API property `final_iteration`, which is true once mypy
   detected no progress during the previous iteration or if the maximum
   semantic analysis iteration count has been reached. You must never
   defer during the final iteration, as it will cause a crash.

6. The `node` attribute of SymbolTableNode objects may contain a reference to
   a PlaceholderNode object. This object means that this definition has not
   been fully processed yet. If you encounter a PlaceholderNode, you should
   defer unless it's the final iteration. If it's the final iteration, you
   should generate an error message. It usually means that there's a cyclic
   definition that cannot be resolved by mypy. PlaceholderNodes can only refer
   to references inside an import cycle. If you are looking up things from
   another module, such as the builtins, that is outside the current module or
   import cycle, you can safely assume that you won't receive a placeholder.

When testing your plugin, you should have a test case that forces a module top
level to be processed multiple times. The easiest way to do this is to include
a forward reference to a class in a top-level annotation. Example:

    c: C  # Forward reference causes second analysis pass
    class C: pass

Note that a forward reference in a function signature won't trigger another
pass, since all functions are processed only after the top level has been fully
analyzed.

You can use `api.options.new_semantic_analyzer` to check whether the new
semantic analyzer is enabled (it's always true in mypy 0.730 and later).
"""

from abc import abstractmethod
from typing import Any, Callable, List, Tuple, Optional, NamedTuple, TypeVar, Dict, Union
from mypy_extensions import trait, mypyc_attr

from mypy.nodes import (
    Expression, Context, ClassDef, SymbolTableNode, MypyFile, CallExpr, ArgKind, TypeInfo
)
from mypy.tvar_scope import TypeVarLikeScope
from mypy.types import (
    Type, Instance, CallableType, TypeList, UnboundType, ProperType, FunctionLike
)
from mypy.messages import MessageBuilder
from mypy.options import Options
from mypy.lookup import lookup_fully_qualified
from mypy.errorcodes import ErrorCode
from mypy.message_registry import ErrorMessage


@others
</t>
<t tx="ekr.20220525082934.1301">@trait
class TypeAnalyzerPluginInterface:
    """Interface for accessing semantic analyzer functionality in plugins.

    Methods docstrings contain only basic info. Look for corresponding implementation
    docstrings in typeanal.py for more details.
    """

    # An options object. Note: these are the cloned options for the current file.
    # This might be different from Plugin.options (that contains default/global options)
    # if there are per-file options in the config. This applies to all other interfaces
    # in this file.
    options: Options

    @others
</t>
<t tx="ekr.20220525082934.1302">@abstractmethod
def fail(self, msg: str, ctx: Context, *, code: Optional[ErrorCode] = None) -&gt; None:
    """Emit an error message at given location."""
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.1303">@abstractmethod
def named_type(self, name: str, args: List[Type]) -&gt; Instance:
    """Construct an instance of a builtin type with given name."""
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.1304">@abstractmethod
def analyze_type(self, typ: Type) -&gt; Type:
    """Analyze an unbound type using the default mypy logic."""
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.1305">@abstractmethod
def analyze_callable_args(self, arglist: TypeList) -&gt; Optional[Tuple[List[Type],
                                                                     List[ArgKind],
                                                                     List[Optional[str]]]]:
    """Find types, kinds, and names of arguments from extended callable syntax."""
    raise NotImplementedError


</t>
<t tx="ekr.20220525082934.1306"># A context for a hook that semantically analyzes an unbound type.
class AnalyzeTypeContext(NamedTuple):
    type: UnboundType  # Type to analyze
    context: Context   # Relevant location context (e.g. for error messages)
    api: TypeAnalyzerPluginInterface


</t>
<t tx="ekr.20220525082934.1307">@mypyc_attr(allow_interpreted_subclasses=True)
class CommonPluginApi:
    """
    A common plugin API (shared between semantic analysis and type checking phases)
    that all plugin hooks get independently of the context.
    """

    # Global mypy options.
    # Per-file options can be only accessed on various
    # XxxPluginInterface classes.
    options: Options

    @others
</t>
<t tx="ekr.20220525082934.1308">@abstractmethod
def lookup_fully_qualified(self, fullname: str) -&gt; Optional[SymbolTableNode]:
    """Lookup a symbol by its full name (including module).

    This lookup function available for all plugins. Return None if a name
    is not found. This function doesn't support lookup from current scope.
    Use SemanticAnalyzerPluginInterface.lookup_qualified() for this."""
    raise NotImplementedError


</t>
<t tx="ekr.20220525082934.1309">@trait
class CheckerPluginInterface:
    """Interface for accessing type checker functionality in plugins.

    Methods docstrings contain only basic info. Look for corresponding implementation
    docstrings in checker.py for more details.
    """

    msg: MessageBuilder
    options: Options
    path: str

    @others
</t>
<t tx="ekr.20220525082934.131"># ExtSlice(slice* dims)
def visit_ExtSlice(self, n: ast3.ExtSlice) -&gt; TupleExpr:
    # cast for mypyc's benefit on Python 3.9
    return TupleExpr(self.translate_expr_list(cast(Any, n).dims))

</t>
<t tx="ekr.20220525082934.1310"># Type context for type inference
@property
@abstractmethod
def type_context(self) -&gt; List[Optional[Type]]:
    """Return the type context of the plugin"""
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.1311">@abstractmethod
def fail(self, msg: Union[str, ErrorMessage], ctx: Context, *,
         code: Optional[ErrorCode] = None) -&gt; None:
    """Emit an error message at given location."""
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.1312">@abstractmethod
def named_generic_type(self, name: str, args: List[Type]) -&gt; Instance:
    """Construct an instance of a builtin type with given type arguments."""
    raise NotImplementedError


</t>
<t tx="ekr.20220525082934.1313">@trait
class SemanticAnalyzerPluginInterface:
    """Interface for accessing semantic analyzer functionality in plugins.

    Methods docstrings contain only basic info. Look for corresponding implementation
    docstrings in semanal.py for more details.

    # TODO: clean-up lookup functions.
    """

    modules: Dict[str, MypyFile]
    # Options for current file.
    options: Options
    cur_mod_id: str
    msg: MessageBuilder

    @others
</t>
<t tx="ekr.20220525082934.1314">@abstractmethod
def named_type(self, fullname: str,
               args: Optional[List[Type]] = None) -&gt; Instance:
    """Construct an instance of a builtin type with given type arguments."""
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.1315">@abstractmethod
def builtin_type(self, fully_qualified_name: str) -&gt; Instance:
    """Legacy function -- use named_type() instead."""
    # NOTE: Do not delete this since many plugins may still use it.
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.1316">@abstractmethod
def named_type_or_none(self, fullname: str,
                       args: Optional[List[Type]] = None) -&gt; Optional[Instance]:
    """Construct an instance of a type with given type arguments.

    Return None if a type could not be constructed for the qualified
    type name. This is possible when the qualified name includes a
    module name and the module has not been imported.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.1317">@abstractmethod
def basic_new_typeinfo(self, name: str, basetype_or_fallback: Instance, line: int) -&gt; TypeInfo:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.1318">@abstractmethod
def parse_bool(self, expr: Expression) -&gt; Optional[bool]:
    """Parse True/False literals."""
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.1319">@abstractmethod
def fail(self, msg: str, ctx: Context, serious: bool = False, *,
         blocker: bool = False, code: Optional[ErrorCode] = None) -&gt; None:
    """Emit an error message at given location."""
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.132"># Index(expr value)
def visit_Index(self, n: Index) -&gt; Node:
    # cast for mypyc's benefit on Python 3.9
    return self.visit(cast(Any, n).value)

</t>
<t tx="ekr.20220525082934.1320">@abstractmethod
def anal_type(self, t: Type, *,
              tvar_scope: Optional[TypeVarLikeScope] = None,
              allow_tuple_literal: bool = False,
              allow_unbound_tvars: bool = False,
              report_invalid_types: bool = True,
              third_pass: bool = False) -&gt; Optional[Type]:
    """Analyze an unbound type.

    Return None if some part of the type is not ready yet. In this
    case the current target being analyzed will be deferred and
    analyzed again.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.1321">@abstractmethod
def class_type(self, self_type: Type) -&gt; Type:
    """Generate type of first argument of class methods from type of self."""
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.1322">@abstractmethod
def lookup_fully_qualified(self, name: str) -&gt; SymbolTableNode:
    """Lookup a symbol by its fully qualified name.

    Raise an error if not found.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.1323">@abstractmethod
def lookup_fully_qualified_or_none(self, name: str) -&gt; Optional[SymbolTableNode]:
    """Lookup a symbol by its fully qualified name.

    Return None if not found.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.1324">@abstractmethod
def lookup_qualified(self, name: str, ctx: Context,
                     suppress_errors: bool = False) -&gt; Optional[SymbolTableNode]:
    """Lookup symbol using a name in current scope.

    This follows Python local-&gt;non-local-&gt;global-&gt;builtins rules.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.1325">@abstractmethod
def add_plugin_dependency(self, trigger: str, target: Optional[str] = None) -&gt; None:
    """Specify semantic dependencies for generated methods/variables.

    If the symbol with full name given by trigger is found to be stale by mypy,
    then the body of node with full name given by target will be re-checked.
    By default, this is the node that is currently analyzed.

    For example, the dataclass plugin adds a generated __init__ method with
    a signature that depends on types of attributes in ancestor classes. If any
    attribute in an ancestor class gets stale (modified), we need to reprocess
    the subclasses (and thus regenerate __init__ methods).

    This is used by fine-grained incremental mode (mypy daemon). See mypy/server/deps.py
    for more details.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.1326">@abstractmethod
def add_symbol_table_node(self, name: str, stnode: SymbolTableNode) -&gt; Any:
    """Add node to global symbol table (or to nearest class if there is one)."""
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.1327">@abstractmethod
def qualified_name(self, n: str) -&gt; str:
    """Make qualified name using current module and enclosing class (if any)."""
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.1328">@abstractmethod
def defer(self) -&gt; None:
    """Call this to defer the processing of the current node.

    This will request an additional iteration of semantic analysis.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.1329">@property
@abstractmethod
def final_iteration(self) -&gt; bool:
    """Is this the final iteration of semantic analysis?"""
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.133"># Match(expr subject, match_case* cases) # python 3.10 and later
def visit_Match(self, n: Match) -&gt; MatchStmt:
    node = MatchStmt(self.visit(n.subject),
                     [self.visit(c.pattern) for c in n.cases],
                     [self.visit(c.guard) for c in n.cases],
                     [self.as_required_block(c.body, n.lineno) for c in n.cases])
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.1330">@property
@abstractmethod
def is_stub_file(self) -&gt; bool:
    raise NotImplementedError


</t>
<t tx="ekr.20220525082934.1331"># A context for querying for configuration data about a module for
# cache invalidation purposes.
class ReportConfigContext(NamedTuple):
    id: str  # Module name
    path: str  # Module file path
    is_check: bool  # Is this invocation for checking whether the config matches


</t>
<t tx="ekr.20220525082934.1332"># A context for a function signature hook that infers a better signature for a
# function.  Note that argument types aren't available yet.  If you need them,
# you have to use a method hook instead.
class FunctionSigContext(NamedTuple):
    args: List[List[Expression]]  # Actual expressions for each formal argument
    default_signature: CallableType  # Original signature of the method
    context: Context  # Relevant location context (e.g. for error messages)
    api: CheckerPluginInterface


</t>
<t tx="ekr.20220525082934.1333"># A context for a function hook that infers the return type of a function with
# a special signature.
#
# A no-op callback would just return the inferred return type, but a useful
# callback at least sometimes can infer a more precise type.
class FunctionContext(NamedTuple):
    arg_types: List[List[Type]]  # List of actual caller types for each formal argument
    arg_kinds: List[List[ArgKind]]  # Ditto for argument kinds, see nodes.ARG_* constants
    # Names of formal parameters from the callee definition,
    # these will be sufficient in most cases.
    callee_arg_names: List[Optional[str]]
    # Names of actual arguments in the call expression. For example,
    # in a situation like this:
    #     def func(**kwargs) -&gt; None:
    #         pass
    #     func(kw1=1, kw2=2)
    # callee_arg_names will be ['kwargs'] and arg_names will be [['kw1', 'kw2']].
    arg_names: List[List[Optional[str]]]
    default_return_type: Type  # Return type inferred from signature
    args: List[List[Expression]]  # Actual expressions for each formal argument
    context: Context  # Relevant location context (e.g. for error messages)
    api: CheckerPluginInterface


</t>
<t tx="ekr.20220525082934.1334"># A context for a method signature hook that infers a better signature for a
# method.  Note that argument types aren't available yet.  If you need them,
# you have to use a method hook instead.
# TODO: document ProperType in the plugin changelog/update issue.
class MethodSigContext(NamedTuple):
    type: ProperType  # Base object type for method call
    args: List[List[Expression]]  # Actual expressions for each formal argument
    default_signature: CallableType  # Original signature of the method
    context: Context  # Relevant location context (e.g. for error messages)
    api: CheckerPluginInterface


</t>
<t tx="ekr.20220525082934.1335"># A context for a method hook that infers the return type of a method with a
# special signature.
#
# This is very similar to FunctionContext (only differences are documented).
class MethodContext(NamedTuple):
    type: ProperType  # Base object type for method call
    arg_types: List[List[Type]]  # List of actual caller types for each formal argument
    # see FunctionContext for details about names and kinds
    arg_kinds: List[List[ArgKind]]
    callee_arg_names: List[Optional[str]]
    arg_names: List[List[Optional[str]]]
    default_return_type: Type  # Return type inferred by mypy
    args: List[List[Expression]]  # Lists of actual expressions for every formal argument
    context: Context
    api: CheckerPluginInterface


</t>
<t tx="ekr.20220525082934.1336"># A context for an attribute type hook that infers the type of an attribute.
class AttributeContext(NamedTuple):
    type: ProperType  # Type of object with attribute
    default_attr_type: Type  # Original attribute type
    context: Context  # Relevant location context (e.g. for error messages)
    api: CheckerPluginInterface


</t>
<t tx="ekr.20220525082934.1337"># A context for a class hook that modifies the class definition.
class ClassDefContext(NamedTuple):
    cls: ClassDef  # The class definition
    reason: Expression   # The expression being applied (decorator, metaclass, base class)
    api: SemanticAnalyzerPluginInterface


</t>
<t tx="ekr.20220525082934.1338"># A context for dynamic class definitions like
# Base = declarative_base()
class DynamicClassDefContext(NamedTuple):
    call: CallExpr  # The r.h.s. of dynamic class definition
    name: str  # The name this class is being assigned to
    api: SemanticAnalyzerPluginInterface


</t>
<t tx="ekr.20220525082934.1339">@mypyc_attr(allow_interpreted_subclasses=True)
class Plugin(CommonPluginApi):
    """Base class of all type checker plugins.

    This defines a no-op plugin.  Subclasses can override some methods to
    provide some actual functionality.

    All get_ methods are treated as pure functions (you should assume that
    results might be cached). A plugin should return None from a get_ method
    to give way to other plugins.

    Look at the comments of various *Context objects for additional information on
    various hooks.
    """

    @others
</t>
<t tx="ekr.20220525082934.134">def visit_MatchValue(self, n: MatchValue) -&gt; ValuePattern:
    node = ValuePattern(self.visit(n.value))
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.1340">def __init__(self, options: Options) -&gt; None:
    self.options = options
    self.python_version = options.python_version
    # This can't be set in __init__ because it is executed too soon in build.py.
    # Therefore, build.py *must* set it later before graph processing starts
    # by calling set_modules().
    self._modules: Optional[Dict[str, MypyFile]] = None

</t>
<t tx="ekr.20220525082934.1341">def set_modules(self, modules: Dict[str, MypyFile]) -&gt; None:
    self._modules = modules

</t>
<t tx="ekr.20220525082934.1342">def lookup_fully_qualified(self, fullname: str) -&gt; Optional[SymbolTableNode]:
    assert self._modules is not None
    return lookup_fully_qualified(fullname, self._modules)

</t>
<t tx="ekr.20220525082934.1343">def report_config_data(self, ctx: ReportConfigContext) -&gt; Any:
    """Get representation of configuration data for a module.

    The data must be encodable as JSON and will be stored in the
    cache metadata for the module. A mismatch between the cached
    values and the returned will result in that module's cache
    being invalidated and the module being rechecked.

    This can be called twice for each module, once after loading
    the cache to check if it is valid and once while writing new
    cache information.

    If is_check in the context is true, then the return of this
    call will be checked against the cached version. Otherwise the
    call is being made to determine what to put in the cache. This
    can be used to allow consulting extra cache files in certain
    complex situations.

    This can be used to incorporate external configuration information
    that might require changes to typechecking.
    """
    return None

</t>
<t tx="ekr.20220525082934.1344">def get_additional_deps(self, file: MypyFile) -&gt; List[Tuple[int, str, int]]:
    """Customize dependencies for a module.

    This hook allows adding in new dependencies for a module. It
    is called after parsing a file but before analysis. This can
    be useful if a library has dependencies that are dynamic based
    on configuration information, for example.

    Returns a list of (priority, module name, line number) tuples.

    The line number can be -1 when there is not a known real line number.

    Priorities are defined in mypy.build (but maybe shouldn't be).
    10 is a good choice for priority.
    """
    return []

</t>
<t tx="ekr.20220525082934.1345">def get_type_analyze_hook(self, fullname: str
                          ) -&gt; Optional[Callable[[AnalyzeTypeContext], Type]]:
    """Customize behaviour of the type analyzer for given full names.

    This method is called during the semantic analysis pass whenever mypy sees an
    unbound type. For example, while analysing this code:

        from lib import Special, Other

        var: Special
        def func(x: Other[int]) -&gt; None:
            ...

    this method will be called with 'lib.Special', and then with 'lib.Other'.
    The callback returned by plugin must return an analyzed type,
    i.e. an instance of `mypy.types.Type`.
    """
    return None

</t>
<t tx="ekr.20220525082934.1346">def get_function_signature_hook(self, fullname: str
                                ) -&gt; Optional[Callable[[FunctionSigContext], FunctionLike]]:
    """Adjust the signature of a function.

    This method is called before type checking a function call. Plugin
    may infer a better type for the function.

        from lib import Class, do_stuff

        do_stuff(42)
        Class()

    This method will be called with 'lib.do_stuff' and then with 'lib.Class'.
    """
    return None

</t>
<t tx="ekr.20220525082934.1347">def get_function_hook(self, fullname: str
                      ) -&gt; Optional[Callable[[FunctionContext], Type]]:
    """Adjust the return type of a function call.

    This method is called after type checking a call. Plugin may adjust the return
    type inferred by mypy, and/or emit some error messages. Note, this hook is also
    called for class instantiation calls, so that in this example:

        from lib import Class, do_stuff

        do_stuff(42)
        Class()

    This method will be called with 'lib.do_stuff' and then with 'lib.Class'.
    """
    return None

</t>
<t tx="ekr.20220525082934.1348">def get_method_signature_hook(self, fullname: str
                              ) -&gt; Optional[Callable[[MethodSigContext], FunctionLike]]:
    """Adjust the signature of a method.

    This method is called before type checking a method call. Plugin
    may infer a better type for the method. The hook is also called for special
    Python dunder methods except __init__ and __new__ (use get_function_hook to customize
    class instantiation). This function is called with the method full name using
    the class where it was _defined_. For example, in this code:

        from lib import Special

        class Base:
            def method(self, arg: Any) -&gt; Any:
                ...
        class Derived(Base):
            ...

        var: Derived
        var.method(42)

        x: Special
        y = x[0]

    this method is called with '__main__.Base.method', and then with
    'lib.Special.__getitem__'.
    """
    return None

</t>
<t tx="ekr.20220525082934.1349">def get_method_hook(self, fullname: str
                    ) -&gt; Optional[Callable[[MethodContext], Type]]:
    """Adjust return type of a method call.

    This is the same as get_function_hook(), but is called with the
    method full name (again, using the class where the method is defined).
    """
    return None

</t>
<t tx="ekr.20220525082934.135">def visit_MatchSingleton(self, n: MatchSingleton) -&gt; SingletonPattern:
    node = SingletonPattern(n.value)
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.1350">def get_attribute_hook(self, fullname: str
                       ) -&gt; Optional[Callable[[AttributeContext], Type]]:
    """Adjust type of an instance attribute.

    This method is called with attribute full name using the class of the instance where
    the attribute was defined (or Var.info.fullname for generated attributes).

    For classes without __getattr__ or __getattribute__, this hook is only called for
    names of fields/properties (but not methods) that exist in the instance MRO.

    For classes that implement __getattr__ or __getattribute__, this hook is called
    for all fields/properties, including nonexistent ones (but still not methods).

    For example:

        class Base:
            x: Any
            def __getattr__(self, attr: str) -&gt; Any: ...

        class Derived(Base):
            ...

        var: Derived
        var.x
        var.y

    get_attribute_hook is called with '__main__.Base.x' and '__main__.Base.y'.
    However, if we had not implemented __getattr__ on Base, you would only get
    the callback for 'var.x'; 'var.y' would produce an error without calling the hook.
    """
    return None

</t>
<t tx="ekr.20220525082934.1351">def get_class_attribute_hook(self, fullname: str
                             ) -&gt; Optional[Callable[[AttributeContext], Type]]:
    """
    Adjust type of a class attribute.

    This method is called with attribute full name using the class where the attribute was
    defined (or Var.info.fullname for generated attributes).

    For example:

        class Cls:
            x: Any

        Cls.x

    get_class_attribute_hook is called with '__main__.Cls.x' as fullname.
    """
    return None

</t>
<t tx="ekr.20220525082934.1352">def get_class_decorator_hook(self, fullname: str
                             ) -&gt; Optional[Callable[[ClassDefContext], None]]:
    """Update class definition for given class decorators.

    The plugin can modify a TypeInfo _in place_ (for example add some generated
    methods to the symbol table). This hook is called after the class body was
    semantically analyzed, but *there may still be placeholders* (typically
    caused by forward references).

    NOTE: Usually get_class_decorator_hook_2 is the better option, since it
          guarantees that there are no placeholders.

    The hook is called with full names of all class decorators.

    The hook can be called multiple times per class, so it must be
    idempotent.
    """
    return None

</t>
<t tx="ekr.20220525082934.1353">def get_class_decorator_hook_2(self, fullname: str
                               ) -&gt; Optional[Callable[[ClassDefContext], bool]]:
    """Update class definition for given class decorators.

    Similar to get_class_decorator_hook, but this runs in a later pass when
    placeholders have been resolved.

    The hook can return False if some base class hasn't been
    processed yet using class hooks. It causes all class hooks
    (that are run in this same pass) to be invoked another time for
    the file(s) currently being processed.

    The hook can be called multiple times per class, so it must be
    idempotent.
    """
    return None

</t>
<t tx="ekr.20220525082934.1354">def get_metaclass_hook(self, fullname: str
                       ) -&gt; Optional[Callable[[ClassDefContext], None]]:
    """Update class definition for given declared metaclasses.

    Same as get_class_decorator_hook() but for metaclasses. Note:
    this hook will be only called for explicit metaclasses, not for
    inherited ones.

    TODO: probably it should also be called on inherited metaclasses.
    """
    return None

</t>
<t tx="ekr.20220525082934.1355">def get_base_class_hook(self, fullname: str
                        ) -&gt; Optional[Callable[[ClassDefContext], None]]:
    """Update class definition for given base classes.

    Same as get_class_decorator_hook() but for base classes. Base classes
    don't need to refer to TypeInfos, if a base class refers to a variable with
    Any type, this hook will still be called.
    """
    return None

</t>
<t tx="ekr.20220525082934.1356">def get_customize_class_mro_hook(self, fullname: str
                                 ) -&gt; Optional[Callable[[ClassDefContext], None]]:
    """Customize MRO for given classes.

    The plugin can modify the class MRO _in place_. This method is called
    with the class full name before its body was semantically analyzed.
    """
    return None

</t>
<t tx="ekr.20220525082934.1357">def get_dynamic_class_hook(self, fullname: str
                           ) -&gt; Optional[Callable[[DynamicClassDefContext], None]]:
    """Semantically analyze a dynamic class definition.

    This plugin hook allows one to semantically analyze dynamic class definitions like:

        from lib import dynamic_class

        X = dynamic_class('X', [])

    For such definition, this hook will be called with 'lib.dynamic_class'.
    The plugin should create the corresponding TypeInfo, and place it into a relevant
    symbol table, e.g. using ctx.api.add_symbol_table_node().
    """
    return None


</t>
<t tx="ekr.20220525082934.1358">T = TypeVar('T')


</t>
<t tx="ekr.20220525082934.1359">class ChainedPlugin(Plugin):
    """A plugin that represents a sequence of chained plugins.

    Each lookup method returns the hook for the first plugin that
    reports a match.

    This class should not be subclassed -- use Plugin as the base class
    for all plugins.
    """

    # TODO: Support caching of lookup results (through a LRU cache, for example).

    @others
</t>
<t tx="ekr.20220525082934.136">def visit_MatchSequence(self, n: MatchSequence) -&gt; SequencePattern:
    patterns = [self.visit(p) for p in n.patterns]
    stars = [p for p in patterns if isinstance(p, StarredPattern)]
    assert len(stars) &lt; 2

    node = SequencePattern(patterns)
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.1360">def __init__(self, options: Options, plugins: List[Plugin]) -&gt; None:
    """Initialize chained plugin.

    Assume that the child plugins aren't mutated (results may be cached).
    """
    super().__init__(options)
    self._plugins = plugins

</t>
<t tx="ekr.20220525082934.1361">def set_modules(self, modules: Dict[str, MypyFile]) -&gt; None:
    for plugin in self._plugins:
        plugin.set_modules(modules)

</t>
<t tx="ekr.20220525082934.1362">def report_config_data(self, ctx: ReportConfigContext) -&gt; Any:
    config_data = [plugin.report_config_data(ctx) for plugin in self._plugins]
    return config_data if any(x is not None for x in config_data) else None

</t>
<t tx="ekr.20220525082934.1363">def get_additional_deps(self, file: MypyFile) -&gt; List[Tuple[int, str, int]]:
    deps = []
    for plugin in self._plugins:
        deps.extend(plugin.get_additional_deps(file))
    return deps

</t>
<t tx="ekr.20220525082934.1364">def get_type_analyze_hook(self, fullname: str
                          ) -&gt; Optional[Callable[[AnalyzeTypeContext], Type]]:
    return self._find_hook(lambda plugin: plugin.get_type_analyze_hook(fullname))

</t>
<t tx="ekr.20220525082934.1365">def get_function_signature_hook(self, fullname: str
                                ) -&gt; Optional[Callable[[FunctionSigContext], FunctionLike]]:
    return self._find_hook(lambda plugin: plugin.get_function_signature_hook(fullname))

</t>
<t tx="ekr.20220525082934.1366">def get_function_hook(self, fullname: str
                      ) -&gt; Optional[Callable[[FunctionContext], Type]]:
    return self._find_hook(lambda plugin: plugin.get_function_hook(fullname))

</t>
<t tx="ekr.20220525082934.1367">def get_method_signature_hook(self, fullname: str
                              ) -&gt; Optional[Callable[[MethodSigContext], FunctionLike]]:
    return self._find_hook(lambda plugin: plugin.get_method_signature_hook(fullname))

</t>
<t tx="ekr.20220525082934.1368">def get_method_hook(self, fullname: str
                    ) -&gt; Optional[Callable[[MethodContext], Type]]:
    return self._find_hook(lambda plugin: plugin.get_method_hook(fullname))

</t>
<t tx="ekr.20220525082934.1369">def get_attribute_hook(self, fullname: str
                       ) -&gt; Optional[Callable[[AttributeContext], Type]]:
    return self._find_hook(lambda plugin: plugin.get_attribute_hook(fullname))

</t>
<t tx="ekr.20220525082934.137">def visit_MatchStar(self, n: MatchStar) -&gt; StarredPattern:
    if n.name is None:
        node = StarredPattern(None)
    else:
        node = StarredPattern(NameExpr(n.name))

    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.1370">def get_class_attribute_hook(self, fullname: str
                             ) -&gt; Optional[Callable[[AttributeContext], Type]]:
    return self._find_hook(lambda plugin: plugin.get_class_attribute_hook(fullname))

</t>
<t tx="ekr.20220525082934.1371">def get_class_decorator_hook(self, fullname: str
                             ) -&gt; Optional[Callable[[ClassDefContext], None]]:
    return self._find_hook(lambda plugin: plugin.get_class_decorator_hook(fullname))

</t>
<t tx="ekr.20220525082934.1372">def get_class_decorator_hook_2(self, fullname: str
                               ) -&gt; Optional[Callable[[ClassDefContext], bool]]:
    return self._find_hook(lambda plugin: plugin.get_class_decorator_hook_2(fullname))

</t>
<t tx="ekr.20220525082934.1373">def get_metaclass_hook(self, fullname: str
                       ) -&gt; Optional[Callable[[ClassDefContext], None]]:
    return self._find_hook(lambda plugin: plugin.get_metaclass_hook(fullname))

</t>
<t tx="ekr.20220525082934.1374">def get_base_class_hook(self, fullname: str
                        ) -&gt; Optional[Callable[[ClassDefContext], None]]:
    return self._find_hook(lambda plugin: plugin.get_base_class_hook(fullname))

</t>
<t tx="ekr.20220525082934.1375">def get_customize_class_mro_hook(self, fullname: str
                                 ) -&gt; Optional[Callable[[ClassDefContext], None]]:
    return self._find_hook(lambda plugin: plugin.get_customize_class_mro_hook(fullname))

</t>
<t tx="ekr.20220525082934.1376">def get_dynamic_class_hook(self, fullname: str
                           ) -&gt; Optional[Callable[[DynamicClassDefContext], None]]:
    return self._find_hook(lambda plugin: plugin.get_dynamic_class_hook(fullname))

</t>
<t tx="ekr.20220525082934.1377">def _find_hook(self, lookup: Callable[[Plugin], T]) -&gt; Optional[T]:
    for plugin in self._plugins:
        hook = lookup(plugin)
        if hook:
            return hook
    return None
</t>
<t tx="ekr.20220525082934.1378">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from __future__ import print_function
"""Utilities to find the site and prefix information of a Python executable, which may be Python 2.

This file MUST remain compatible with Python 2. Since we cannot make any assumptions about the
Python being executed, this module should not use *any* dependencies outside of the standard
library found in Python 2. This file is run each mypy run, so it should be kept as fast as
possible.
"""
import site
import sys

if __name__ == '__main__':
    sys.path = sys.path[1:]  # we don't want to pick up mypy.types

MYPY = False
if MYPY:
    from typing import List, Tuple


@others
if __name__ == '__main__':
    if sys.argv[-1] == 'getsitepackages':
        print(repr(getsitepackages()))
    elif sys.argv[-1] == 'getprefixes':
        print(repr(getprefixes()))
    else:
        print("ERROR: incorrect argument to pyinfo.py.", file=sys.stderr)
        sys.exit(1)
</t>
<t tx="ekr.20220525082934.1379">def getprefixes():
    # type: () -&gt; Tuple[str, str]
    return getattr(sys, "base_prefix", sys.prefix), sys.prefix


</t>
<t tx="ekr.20220525082934.138">def visit_MatchMapping(self, n: MatchMapping) -&gt; MappingPattern:
    keys = [self.visit(k) for k in n.keys]
    values = [self.visit(v) for v in n.patterns]

    if n.rest is None:
        rest = None
    else:
        rest = NameExpr(n.rest)

    node = MappingPattern(keys, values, rest)
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.1380">def getsitepackages():
    # type: () -&gt; List[str]
    res = []
    if hasattr(site, 'getsitepackages'):
        res.extend(site.getsitepackages())

        if hasattr(site, 'getusersitepackages') and site.ENABLE_USER_SITE:
            res.insert(0, site.getusersitepackages())
    else:
        from distutils.sysconfig import get_python_lib
        res = [get_python_lib()]
    return res


</t>
<t tx="ekr.20220525082934.1381">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Utilities related to determining the reachability of code (in semantic analysis)."""

from typing import Tuple, TypeVar, Union, Optional
from typing_extensions import Final

from mypy.nodes import (
    Expression, IfStmt, Block, AssertStmt, MatchStmt, NameExpr, UnaryExpr, MemberExpr, OpExpr,
    ComparisonExpr, StrExpr, UnicodeExpr, CallExpr, IntExpr, TupleExpr, IndexExpr, SliceExpr,
    Import, ImportFrom, ImportAll, LITERAL_YES
)
from mypy.options import Options
from mypy.patterns import Pattern, AsPattern, OrPattern
from mypy.traverser import TraverserVisitor
from mypy.literals import literal

# Inferred truth value of an expression.
ALWAYS_TRUE: Final = 1
MYPY_TRUE: Final = 2  # True in mypy, False at runtime
ALWAYS_FALSE: Final = 3
MYPY_FALSE: Final = 4  # False in mypy, True at runtime
TRUTH_VALUE_UNKNOWN: Final = 5

inverted_truth_mapping: Final = {
    ALWAYS_TRUE: ALWAYS_FALSE,
    ALWAYS_FALSE: ALWAYS_TRUE,
    TRUTH_VALUE_UNKNOWN: TRUTH_VALUE_UNKNOWN,
    MYPY_TRUE: MYPY_FALSE,
    MYPY_FALSE: MYPY_TRUE,
}

reverse_op: Final = {
    "==": "==",
    "!=": "!=",
    "&lt;": "&gt;",
    "&gt;": "&lt;",
    "&lt;=": "&gt;=",
    "&gt;=": "&lt;=",
}


@others
</t>
<t tx="ekr.20220525082934.1382">def infer_reachability_of_if_statement(s: IfStmt, options: Options) -&gt; None:
    for i in range(len(s.expr)):
        result = infer_condition_value(s.expr[i], options)
        if result in (ALWAYS_FALSE, MYPY_FALSE):
            # The condition is considered always false, so we skip the if/elif body.
            mark_block_unreachable(s.body[i])
        elif result in (ALWAYS_TRUE, MYPY_TRUE):
            # This condition is considered always true, so all of the remaining
            # elif/else bodies should not be checked.
            if result == MYPY_TRUE:
                # This condition is false at runtime; this will affect
                # import priorities.
                mark_block_mypy_only(s.body[i])
            for body in s.body[i + 1:]:
                mark_block_unreachable(body)

            # Make sure else body always exists and is marked as
            # unreachable so the type checker always knows that
            # all control flow paths will flow through the if
            # statement body.
            if not s.else_body:
                s.else_body = Block([])
            mark_block_unreachable(s.else_body)
            break


</t>
<t tx="ekr.20220525082934.1383">def infer_reachability_of_match_statement(s: MatchStmt, options: Options) -&gt; None:
    for i, guard in enumerate(s.guards):
        pattern_value = infer_pattern_value(s.patterns[i])

        if guard is not None:
            guard_value = infer_condition_value(guard, options)
        else:
            guard_value = ALWAYS_TRUE

        if pattern_value in (ALWAYS_FALSE, MYPY_FALSE) \
                or guard_value in (ALWAYS_FALSE, MYPY_FALSE):
            # The case is considered always false, so we skip the case body.
            mark_block_unreachable(s.bodies[i])
        elif pattern_value in (ALWAYS_FALSE, MYPY_TRUE) \
                and guard_value in (ALWAYS_TRUE, MYPY_TRUE):
            for body in s.bodies[i + 1:]:
                mark_block_unreachable(body)

        if guard_value == MYPY_TRUE:
            # This condition is false at runtime; this will affect
            # import priorities.
            mark_block_mypy_only(s.bodies[i])


</t>
<t tx="ekr.20220525082934.1384">def assert_will_always_fail(s: AssertStmt, options: Options) -&gt; bool:
    return infer_condition_value(s.expr, options) in (ALWAYS_FALSE, MYPY_FALSE)


</t>
<t tx="ekr.20220525082934.1385">def infer_condition_value(expr: Expression, options: Options) -&gt; int:
    """Infer whether the given condition is always true/false.

    Return ALWAYS_TRUE if always true, ALWAYS_FALSE if always false,
    MYPY_TRUE if true under mypy and false at runtime, MYPY_FALSE if
    false under mypy and true at runtime, else TRUTH_VALUE_UNKNOWN.
    """
    pyversion = options.python_version
    name = ''
    negated = False
    alias = expr
    if isinstance(alias, UnaryExpr):
        if alias.op == 'not':
            expr = alias.expr
            negated = True
    result = TRUTH_VALUE_UNKNOWN
    if isinstance(expr, NameExpr):
        name = expr.name
    elif isinstance(expr, MemberExpr):
        name = expr.name
    elif isinstance(expr, OpExpr) and expr.op in ('and', 'or'):
        left = infer_condition_value(expr.left, options)
        if ((left in (ALWAYS_TRUE, MYPY_TRUE) and expr.op == 'and') or
                (left in (ALWAYS_FALSE, MYPY_FALSE) and expr.op == 'or')):
            # Either `True and &lt;other&gt;` or `False or &lt;other&gt;`: the result will
            # always be the right-hand-side.
            return infer_condition_value(expr.right, options)
        else:
            # The result will always be the left-hand-side (e.g. ALWAYS_* or
            # TRUTH_VALUE_UNKNOWN).
            return left
    else:
        result = consider_sys_version_info(expr, pyversion)
        if result == TRUTH_VALUE_UNKNOWN:
            result = consider_sys_platform(expr, options.platform)
    if result == TRUTH_VALUE_UNKNOWN:
        if name == 'PY2':
            result = ALWAYS_TRUE if pyversion[0] == 2 else ALWAYS_FALSE
        elif name == 'PY3':
            result = ALWAYS_TRUE if pyversion[0] == 3 else ALWAYS_FALSE
        elif name == 'MYPY' or name == 'TYPE_CHECKING':
            result = MYPY_TRUE
        elif name in options.always_true:
            result = ALWAYS_TRUE
        elif name in options.always_false:
            result = ALWAYS_FALSE
    if negated:
        result = inverted_truth_mapping[result]
    return result


</t>
<t tx="ekr.20220525082934.1386">def infer_pattern_value(pattern: Pattern) -&gt; int:
    if isinstance(pattern, AsPattern) and pattern.pattern is None:
        return ALWAYS_TRUE
    elif isinstance(pattern, OrPattern) and \
            any(infer_pattern_value(p) == ALWAYS_TRUE for p in pattern.patterns):
        return ALWAYS_TRUE
    else:
        return TRUTH_VALUE_UNKNOWN


</t>
<t tx="ekr.20220525082934.1387">def consider_sys_version_info(expr: Expression, pyversion: Tuple[int, ...]) -&gt; int:
    """Consider whether expr is a comparison involving sys.version_info.

    Return ALWAYS_TRUE, ALWAYS_FALSE, or TRUTH_VALUE_UNKNOWN.
    """
    # Cases supported:
    # - sys.version_info[&lt;int&gt;] &lt;compare_op&gt; &lt;int&gt;
    # - sys.version_info[:&lt;int&gt;] &lt;compare_op&gt; &lt;tuple_of_n_ints&gt;
    # - sys.version_info &lt;compare_op&gt; &lt;tuple_of_1_or_2_ints&gt;
    #   (in this case &lt;compare_op&gt; must be &gt;, &gt;=, &lt;, &lt;=, but cannot be ==, !=)
    if not isinstance(expr, ComparisonExpr):
        return TRUTH_VALUE_UNKNOWN
    # Let's not yet support chained comparisons.
    if len(expr.operators) &gt; 1:
        return TRUTH_VALUE_UNKNOWN
    op = expr.operators[0]
    if op not in ('==', '!=', '&lt;=', '&gt;=', '&lt;', '&gt;'):
        return TRUTH_VALUE_UNKNOWN

    index = contains_sys_version_info(expr.operands[0])
    thing = contains_int_or_tuple_of_ints(expr.operands[1])
    if index is None or thing is None:
        index = contains_sys_version_info(expr.operands[1])
        thing = contains_int_or_tuple_of_ints(expr.operands[0])
        op = reverse_op[op]
    if isinstance(index, int) and isinstance(thing, int):
        # sys.version_info[i] &lt;compare_op&gt; k
        if 0 &lt;= index &lt;= 1:
            return fixed_comparison(pyversion[index], op, thing)
        else:
            return TRUTH_VALUE_UNKNOWN
    elif isinstance(index, tuple) and isinstance(thing, tuple):
        lo, hi = index
        if lo is None:
            lo = 0
        if hi is None:
            hi = 2
        if 0 &lt;= lo &lt; hi &lt;= 2:
            val = pyversion[lo:hi]
            if len(val) == len(thing) or len(val) &gt; len(thing) and op not in ('==', '!='):
                return fixed_comparison(val, op, thing)
    return TRUTH_VALUE_UNKNOWN


</t>
<t tx="ekr.20220525082934.1388">def consider_sys_platform(expr: Expression, platform: str) -&gt; int:
    """Consider whether expr is a comparison involving sys.platform.

    Return ALWAYS_TRUE, ALWAYS_FALSE, or TRUTH_VALUE_UNKNOWN.
    """
    # Cases supported:
    # - sys.platform == 'posix'
    # - sys.platform != 'win32'
    # - sys.platform.startswith('win')
    if isinstance(expr, ComparisonExpr):
        # Let's not yet support chained comparisons.
        if len(expr.operators) &gt; 1:
            return TRUTH_VALUE_UNKNOWN
        op = expr.operators[0]
        if op not in ('==', '!='):
            return TRUTH_VALUE_UNKNOWN
        if not is_sys_attr(expr.operands[0], 'platform'):
            return TRUTH_VALUE_UNKNOWN
        right = expr.operands[1]
        if not isinstance(right, (StrExpr, UnicodeExpr)):
            return TRUTH_VALUE_UNKNOWN
        return fixed_comparison(platform, op, right.value)
    elif isinstance(expr, CallExpr):
        if not isinstance(expr.callee, MemberExpr):
            return TRUTH_VALUE_UNKNOWN
        if len(expr.args) != 1 or not isinstance(expr.args[0], (StrExpr, UnicodeExpr)):
            return TRUTH_VALUE_UNKNOWN
        if not is_sys_attr(expr.callee.expr, 'platform'):
            return TRUTH_VALUE_UNKNOWN
        if expr.callee.name != 'startswith':
            return TRUTH_VALUE_UNKNOWN
        if platform.startswith(expr.args[0].value):
            return ALWAYS_TRUE
        else:
            return ALWAYS_FALSE
    else:
        return TRUTH_VALUE_UNKNOWN


</t>
<t tx="ekr.20220525082934.1389">Targ = TypeVar('Targ', int, str, Tuple[int, ...])


</t>
<t tx="ekr.20220525082934.139">def visit_MatchClass(self, n: MatchClass) -&gt; ClassPattern:
    class_ref = self.visit(n.cls)
    assert isinstance(class_ref, RefExpr)
    positionals = [self.visit(p) for p in n.patterns]
    keyword_keys = n.kwd_attrs
    keyword_values = [self.visit(p) for p in n.kwd_patterns]

    node = ClassPattern(class_ref, positionals, keyword_keys, keyword_values)
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.1390">def fixed_comparison(left: Targ, op: str, right: Targ) -&gt; int:
    rmap = {False: ALWAYS_FALSE, True: ALWAYS_TRUE}
    if op == '==':
        return rmap[left == right]
    if op == '!=':
        return rmap[left != right]
    if op == '&lt;=':
        return rmap[left &lt;= right]
    if op == '&gt;=':
        return rmap[left &gt;= right]
    if op == '&lt;':
        return rmap[left &lt; right]
    if op == '&gt;':
        return rmap[left &gt; right]
    return TRUTH_VALUE_UNKNOWN


</t>
<t tx="ekr.20220525082934.1391">def contains_int_or_tuple_of_ints(expr: Expression
                                  ) -&gt; Union[None, int, Tuple[int], Tuple[int, ...]]:
    if isinstance(expr, IntExpr):
        return expr.value
    if isinstance(expr, TupleExpr):
        if literal(expr) == LITERAL_YES:
            thing = []
            for x in expr.items:
                if not isinstance(x, IntExpr):
                    return None
                thing.append(x.value)
            return tuple(thing)
    return None


</t>
<t tx="ekr.20220525082934.1392">def contains_sys_version_info(expr: Expression
                              ) -&gt; Union[None, int, Tuple[Optional[int], Optional[int]]]:
    if is_sys_attr(expr, 'version_info'):
        return (None, None)  # Same as sys.version_info[:]
    if isinstance(expr, IndexExpr) and is_sys_attr(expr.base, 'version_info'):
        index = expr.index
        if isinstance(index, IntExpr):
            return index.value
        if isinstance(index, SliceExpr):
            if index.stride is not None:
                if not isinstance(index.stride, IntExpr) or index.stride.value != 1:
                    return None
            begin = end = None
            if index.begin_index is not None:
                if not isinstance(index.begin_index, IntExpr):
                    return None
                begin = index.begin_index.value
            if index.end_index is not None:
                if not isinstance(index.end_index, IntExpr):
                    return None
                end = index.end_index.value
            return (begin, end)
    return None


</t>
<t tx="ekr.20220525082934.1393">def is_sys_attr(expr: Expression, name: str) -&gt; bool:
    # TODO: This currently doesn't work with code like this:
    # - import sys as _sys
    # - from sys import version_info
    if isinstance(expr, MemberExpr) and expr.name == name:
        if isinstance(expr.expr, NameExpr) and expr.expr.name == 'sys':
            # TODO: Guard against a local named sys, etc.
            # (Though later passes will still do most checking.)
            return True
    return False


</t>
<t tx="ekr.20220525082934.1394">def mark_block_unreachable(block: Block) -&gt; None:
    block.is_unreachable = True
    block.accept(MarkImportsUnreachableVisitor())


</t>
<t tx="ekr.20220525082934.1395">class MarkImportsUnreachableVisitor(TraverserVisitor):
    """Visitor that flags all imports nested within a node as unreachable."""

    @others
</t>
<t tx="ekr.20220525082934.1396">def visit_import(self, node: Import) -&gt; None:
    node.is_unreachable = True

</t>
<t tx="ekr.20220525082934.1397">def visit_import_from(self, node: ImportFrom) -&gt; None:
    node.is_unreachable = True

</t>
<t tx="ekr.20220525082934.1398">def visit_import_all(self, node: ImportAll) -&gt; None:
    node.is_unreachable = True


</t>
<t tx="ekr.20220525082934.1399">def mark_block_mypy_only(block: Block) -&gt; None:
    block.accept(MarkImportsMypyOnlyVisitor())


</t>
<t tx="ekr.20220525082934.14">def visit_type_var(self, t: TypeVarType) -&gt; Type:
    repl = get_proper_type(self.variables.get(t.id, t))
    if isinstance(repl, Instance):
        inst = repl
        return Instance(inst.type, inst.args, line=inst.line, column=inst.column)
    else:
        return repl

</t>
<t tx="ekr.20220525082934.140"># MatchAs(expr pattern, identifier name)
def visit_MatchAs(self, n: MatchAs) -&gt; AsPattern:
    if n.name is None:
        name = None
    else:
        name = NameExpr(n.name)
        name = self.set_line(name, n)
    node = AsPattern(self.visit(n.pattern), name)
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.1400">class MarkImportsMypyOnlyVisitor(TraverserVisitor):
    """Visitor that sets is_mypy_only (which affects priority)."""

    @others
</t>
<t tx="ekr.20220525082934.1401">def visit_import(self, node: Import) -&gt; None:
    node.is_mypy_only = True

</t>
<t tx="ekr.20220525082934.1402">def visit_import_from(self, node: ImportFrom) -&gt; None:
    node.is_mypy_only = True

</t>
<t tx="ekr.20220525082934.1403">def visit_import_all(self, node: ImportAll) -&gt; None:
    node.is_mypy_only = True
</t>
<t tx="ekr.20220525082934.1404">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from contextlib import contextmanager
from typing import Dict, Iterator, List, Set
from typing_extensions import Final

from mypy.nodes import (
    Block, AssignmentStmt, NameExpr, MypyFile, FuncDef, Lvalue, ListExpr, TupleExpr,
    WhileStmt, ForStmt, BreakStmt, ContinueStmt, TryStmt, WithStmt, MatchStmt, StarExpr,
    ImportFrom, MemberExpr, IndexExpr, Import, ImportAll, ClassDef
)
from mypy.patterns import AsPattern
from mypy.traverser import TraverserVisitor

# Scope kinds
FILE: Final = 0
FUNCTION: Final = 1
CLASS: Final = 2


@others
</t>
<t tx="ekr.20220525082934.1405">class VariableRenameVisitor(TraverserVisitor):
    """Rename variables to allow redefinition of variables.

    For example, consider this code:

      x = 0
      f(x)

      x = "a"
      g(x)

    It will be transformed like this:

      x' = 0
      f(x')

      x = "a"
      g(x)

    There will be two independent variables (x' and x) that will have separate
    inferred types. The publicly exposed variant will get the non-suffixed name.
    This is the last definition at module top level and the first definition
    (argument) within a function.

    Renaming only happens for assignments within the same block. Renaming is
    performed before semantic analysis, immediately after parsing.

    The implementation performs a rudimentary static analysis. The analysis is
    overly conservative to keep things simple.
    """

    @others
</t>
<t tx="ekr.20220525082934.1406">def __init__(self) -&gt; None:
    # Counter for labeling new blocks
    self.block_id = 0
    # Number of surrounding try statements that disallow variable redefinition
    self.disallow_redef_depth = 0
    # Number of surrounding loop statements
    self.loop_depth = 0
    # Map block id to loop depth.
    self.block_loop_depth: Dict[int, int] = {}
    # Stack of block ids being processed.
    self.blocks: List[int] = []
    # List of scopes; each scope maps short (unqualified) name to block id.
    self.var_blocks: List[Dict[str, int]] = []

    # References to variables that we may need to rename. List of
    # scopes; each scope is a mapping from name to list of collections
    # of names that refer to the same logical variable.
    self.refs: List[Dict[str, List[List[NameExpr]]]] = []
    # Number of reads of the most recent definition of a variable (per scope)
    self.num_reads: List[Dict[str, int]] = []
    # Kinds of nested scopes (FILE, FUNCTION or CLASS)
    self.scope_kinds: List[int] = []

</t>
<t tx="ekr.20220525082934.1407">def visit_mypy_file(self, file_node: MypyFile) -&gt; None:
    """Rename variables within a file.

    This is the main entry point to this class.
    """
    self.clear()
    with self.enter_scope(FILE), self.enter_block():
        for d in file_node.defs:
            d.accept(self)

</t>
<t tx="ekr.20220525082934.1408">def visit_func_def(self, fdef: FuncDef) -&gt; None:
    # Conservatively do not allow variable defined before a function to
    # be redefined later, since function could refer to either definition.
    self.reject_redefinition_of_vars_in_scope()

    with self.enter_scope(FUNCTION), self.enter_block():
        for arg in fdef.arguments:
            name = arg.variable.name
            # 'self' can't be redefined since it's special as it allows definition of
            # attributes. 'cls' can't be used to define attributes so we can ignore it.
            can_be_redefined = name != 'self'  # TODO: Proper check
            self.record_assignment(arg.variable.name, can_be_redefined)
            self.handle_arg(name)

        for stmt in fdef.body.body:
            stmt.accept(self)

</t>
<t tx="ekr.20220525082934.1409">def visit_class_def(self, cdef: ClassDef) -&gt; None:
    self.reject_redefinition_of_vars_in_scope()
    with self.enter_scope(CLASS):
        super().visit_class_def(cdef)

</t>
<t tx="ekr.20220525082934.141"># MatchOr(expr* pattern)
def visit_MatchOr(self, n: MatchOr) -&gt; OrPattern:
    node = OrPattern([self.visit(pattern) for pattern in n.patterns])
    return self.set_line(node, n)


</t>
<t tx="ekr.20220525082934.1410">def visit_block(self, block: Block) -&gt; None:
    with self.enter_block():
        super().visit_block(block)

</t>
<t tx="ekr.20220525082934.1411">def visit_while_stmt(self, stmt: WhileStmt) -&gt; None:
    with self.enter_loop():
        super().visit_while_stmt(stmt)

</t>
<t tx="ekr.20220525082934.1412">def visit_for_stmt(self, stmt: ForStmt) -&gt; None:
    stmt.expr.accept(self)
    self.analyze_lvalue(stmt.index, True)
    # Also analyze as non-lvalue so that every for loop index variable is assumed to be read.
    stmt.index.accept(self)
    with self.enter_loop():
        stmt.body.accept(self)
    if stmt.else_body:
        stmt.else_body.accept(self)

</t>
<t tx="ekr.20220525082934.1413">def visit_break_stmt(self, stmt: BreakStmt) -&gt; None:
    self.reject_redefinition_of_vars_in_loop()

</t>
<t tx="ekr.20220525082934.1414">def visit_continue_stmt(self, stmt: ContinueStmt) -&gt; None:
    self.reject_redefinition_of_vars_in_loop()

</t>
<t tx="ekr.20220525082934.1415">def visit_try_stmt(self, stmt: TryStmt) -&gt; None:
    # Variables defined by a try statement get special treatment in the
    # type checker which allows them to be always redefined, so no need to
    # do renaming here.
    with self.enter_try():
        super().visit_try_stmt(stmt)

</t>
<t tx="ekr.20220525082934.1416">def visit_with_stmt(self, stmt: WithStmt) -&gt; None:
    for expr in stmt.expr:
        expr.accept(self)
    for target in stmt.target:
        if target is not None:
            self.analyze_lvalue(target)
    # We allow redefinitions in the body of a with statement for
    # convenience.  This is unsafe since with statements can affect control
    # flow by catching exceptions, but this is rare except for
    # assertRaises() and other similar functions, where the exception is
    # raised by the last statement in the body, which usually isn't a
    # problem.
    stmt.body.accept(self)

</t>
<t tx="ekr.20220525082934.1417">def visit_import(self, imp: Import) -&gt; None:
    for id, as_id in imp.ids:
        self.record_assignment(as_id or id, False)

</t>
<t tx="ekr.20220525082934.1418">def visit_import_from(self, imp: ImportFrom) -&gt; None:
    for id, as_id in imp.names:
        self.record_assignment(as_id or id, False)

</t>
<t tx="ekr.20220525082934.1419">def visit_assignment_stmt(self, s: AssignmentStmt) -&gt; None:
    s.rvalue.accept(self)
    for lvalue in s.lvalues:
        self.analyze_lvalue(lvalue)

</t>
<t tx="ekr.20220525082934.142">class TypeConverter:
    @others
</t>
<t tx="ekr.20220525082934.1420">def visit_match_stmt(self, s: MatchStmt) -&gt; None:
    for i in range(len(s.patterns)):
        with self.enter_block():
            s.patterns[i].accept(self)
            guard = s.guards[i]
            if guard is not None:
                guard.accept(self)
            # We already entered a block, so visit this block's statements directly
            for stmt in s.bodies[i].body:
                stmt.accept(self)

</t>
<t tx="ekr.20220525082934.1421">def visit_capture_pattern(self, p: AsPattern) -&gt; None:
    if p.name is not None:
        self.analyze_lvalue(p.name)

</t>
<t tx="ekr.20220525082934.1422">def analyze_lvalue(self, lvalue: Lvalue, is_nested: bool = False) -&gt; None:
    """Process assignment; in particular, keep track of (re)defined names.

    Args:
        is_nested: True for non-outermost Lvalue in a multiple assignment such as
            "x, y = ..."
    """
    if isinstance(lvalue, NameExpr):
        name = lvalue.name
        is_new = self.record_assignment(name, True)
        if is_new:
            self.handle_def(lvalue)
        else:
            self.handle_refine(lvalue)
        if is_nested:
            # This allows these to be redefined freely even if never read. Multiple
            # assignment like "x, _ _ = y" defines dummy variables that are never read.
            self.handle_ref(lvalue)
    elif isinstance(lvalue, (ListExpr, TupleExpr)):
        for item in lvalue.items:
            self.analyze_lvalue(item, is_nested=True)
    elif isinstance(lvalue, MemberExpr):
        lvalue.expr.accept(self)
    elif isinstance(lvalue, IndexExpr):
        lvalue.base.accept(self)
        lvalue.index.accept(self)
    elif isinstance(lvalue, StarExpr):
        # Propagate is_nested since in a typical use case like "x, *rest = ..." 'rest' may
        # be freely reused.
        self.analyze_lvalue(lvalue.expr, is_nested=is_nested)

</t>
<t tx="ekr.20220525082934.1423">def visit_name_expr(self, expr: NameExpr) -&gt; None:
    self.handle_ref(expr)

</t>
<t tx="ekr.20220525082934.1424"># Helpers for renaming references

</t>
<t tx="ekr.20220525082934.1425">def handle_arg(self, name: str) -&gt; None:
    """Store function argument."""
    self.refs[-1][name] = [[]]
    self.num_reads[-1][name] = 0

</t>
<t tx="ekr.20220525082934.1426">def handle_def(self, expr: NameExpr) -&gt; None:
    """Store new name definition."""
    name = expr.name
    names = self.refs[-1].setdefault(name, [])
    names.append([expr])
    self.num_reads[-1][name] = 0

</t>
<t tx="ekr.20220525082934.1427">def handle_refine(self, expr: NameExpr) -&gt; None:
    """Store assignment to an existing name (that replaces previous value, if any)."""
    name = expr.name
    if name in self.refs[-1]:
        names = self.refs[-1][name]
        if not names:
            names.append([])
        names[-1].append(expr)

</t>
<t tx="ekr.20220525082934.1428">def handle_ref(self, expr: NameExpr) -&gt; None:
    """Store reference to defined name."""
    name = expr.name
    if name in self.refs[-1]:
        names = self.refs[-1][name]
        if not names:
            names.append([])
        names[-1].append(expr)
    num_reads = self.num_reads[-1]
    num_reads[name] = num_reads.get(name, 0) + 1

</t>
<t tx="ekr.20220525082934.1429">def flush_refs(self) -&gt; None:
    """Rename all references within the current scope.

    This will be called at the end of a scope.
    """
    is_func = self.scope_kinds[-1] == FUNCTION
    for name, refs in self.refs[-1].items():
        if len(refs) == 1:
            # Only one definition -- no renaming needed.
            continue
        if is_func:
            # In a function, don't rename the first definition, as it
            # may be an argument that must preserve the name.
            to_rename = refs[1:]
        else:
            # At module top level, don't rename the final definition,
            # as it will be publicly visible outside the module.
            to_rename = refs[:-1]
        for i, item in enumerate(to_rename):
            rename_refs(item, i)
    self.refs.pop()

</t>
<t tx="ekr.20220525082934.143">def __init__(self,
             errors: Optional[Errors],
             line: int = -1,
             override_column: int = -1,
             assume_str_is_unicode: bool = True,
             is_evaluated: bool = True,
             ) -&gt; None:
    self.errors = errors
    self.line = line
    self.override_column = override_column
    self.node_stack: List[AST] = []
    self.assume_str_is_unicode = assume_str_is_unicode
    self.is_evaluated = is_evaluated

</t>
<t tx="ekr.20220525082934.1430"># Helpers for determining which assignments define new variables

</t>
<t tx="ekr.20220525082934.1431">def clear(self) -&gt; None:
    self.blocks = []
    self.var_blocks = []

</t>
<t tx="ekr.20220525082934.1432">@contextmanager
def enter_block(self) -&gt; Iterator[None]:
    self.block_id += 1
    self.blocks.append(self.block_id)
    self.block_loop_depth[self.block_id] = self.loop_depth
    try:
        yield
    finally:
        self.blocks.pop()

</t>
<t tx="ekr.20220525082934.1433">@contextmanager
def enter_try(self) -&gt; Iterator[None]:
    self.disallow_redef_depth += 1
    try:
        yield
    finally:
        self.disallow_redef_depth -= 1

</t>
<t tx="ekr.20220525082934.1434">@contextmanager
def enter_loop(self) -&gt; Iterator[None]:
    self.loop_depth += 1
    try:
        yield
    finally:
        self.loop_depth -= 1

</t>
<t tx="ekr.20220525082934.1435">def current_block(self) -&gt; int:
    return self.blocks[-1]

</t>
<t tx="ekr.20220525082934.1436">@contextmanager
def enter_scope(self, kind: int) -&gt; Iterator[None]:
    self.var_blocks.append({})
    self.refs.append({})
    self.num_reads.append({})
    self.scope_kinds.append(kind)
    try:
        yield
    finally:
        self.flush_refs()
        self.var_blocks.pop()
        self.num_reads.pop()
        self.scope_kinds.pop()

</t>
<t tx="ekr.20220525082934.1437">def is_nested(self) -&gt; int:
    return len(self.var_blocks) &gt; 1

</t>
<t tx="ekr.20220525082934.1438">def reject_redefinition_of_vars_in_scope(self) -&gt; None:
    """Make it impossible to redefine defined variables in the current scope.

    This is used if we encounter a function definition that
    can make it ambiguous which definition is live. Example:

      x = 0

      def f() -&gt; int:
          return x

      x = ''  # Error -- cannot redefine x across function definition
    """
    var_blocks = self.var_blocks[-1]
    for key in var_blocks:
        var_blocks[key] = -1

</t>
<t tx="ekr.20220525082934.1439">def reject_redefinition_of_vars_in_loop(self) -&gt; None:
    """Reject redefinition of variables in the innermost loop.

    If there is an early exit from a loop, there may be ambiguity about which
    value may escape the loop. Example where this matters:

      while f():
          x = 0
          if g():
              break
          x = ''  # Error -- not a redefinition
      reveal_type(x)  # int

    This method ensures that the second assignment to 'x' doesn't introduce a new
    variable.
    """
    var_blocks = self.var_blocks[-1]
    for key, block in var_blocks.items():
        if self.block_loop_depth.get(block) == self.loop_depth:
            var_blocks[key] = -1

</t>
<t tx="ekr.20220525082934.144">def convert_column(self, column: int) -&gt; int:
    """Apply column override if defined; otherwise return column.

    Column numbers are sometimes incorrect in the AST and the column
    override can be used to work around that.
    """
    if self.override_column &lt; 0:
        return column
    else:
        return self.override_column

</t>
<t tx="ekr.20220525082934.1440">def record_assignment(self, name: str, can_be_redefined: bool) -&gt; bool:
    """Record assignment to given name and return True if it defines a new variable.

    Args:
        can_be_redefined: If True, allows assignment in the same block to redefine
            this name (if this is a new definition)
    """
    if self.num_reads[-1].get(name, -1) == 0:
        # Only set, not read, so no reason to redefine
        return False
    if self.disallow_redef_depth &gt; 0:
        # Can't redefine within try/with a block.
        can_be_redefined = False
    block = self.current_block()
    var_blocks = self.var_blocks[-1]
    if name not in var_blocks:
        # New definition in this scope.
        if can_be_redefined:
            # Store the block where this was defined to allow redefinition in
            # the same block only.
            var_blocks[name] = block
        else:
            # This doesn't support arbitrary redefinition.
            var_blocks[name] = -1
        return True
    elif var_blocks[name] == block:
        # Redefinition -- defines a new variable with the same name.
        return True
    else:
        # Assigns to an existing variable.
        return False


</t>
<t tx="ekr.20220525082934.1441">class LimitedVariableRenameVisitor(TraverserVisitor):
    """Perform some limited variable renaming in with statements.

    This allows reusing a variable in multiple with statements with
    different types. For example, the two instances of 'x' can have
    incompatible types:

       with C() as x:
           f(x)
       with D() as x:
           g(x)

    The above code gets renamed conceptually into this (not valid Python!):

       with C() as x':
           f(x')
       with D() as x:
           g(x)

    If there's a reference to a variable defined in 'with' outside the
    statement, or if there's any trickiness around variable visibility
    (e.g. function definitions), we give up and won't perform renaming.

    The main use case is to allow binding both readable and writable
    binary files into the same variable. These have different types:

        with open(fnam, 'rb') as f: ...
        with open(fnam, 'wb') as f: ...
    """

    @others
</t>
<t tx="ekr.20220525082934.1442">def __init__(self) -&gt; None:
    # Short names of variables bound in with statements using "as"
    # in a surrounding scope
    self.bound_vars: List[str] = []
    # Stack of names that can't be safely renamed, per scope ('*' means that
    # no names can be renamed)
    self.skipped: List[Set[str]] = []
    # References to variables that we may need to rename. Stack of
    # scopes; each scope is a mapping from name to list of collections
    # of names that refer to the same logical variable.
    self.refs: List[Dict[str, List[List[NameExpr]]]] = []

</t>
<t tx="ekr.20220525082934.1443">def visit_mypy_file(self, file_node: MypyFile) -&gt; None:
    """Rename variables within a file.

    This is the main entry point to this class.
    """
    with self.enter_scope():
        for d in file_node.defs:
            d.accept(self)

</t>
<t tx="ekr.20220525082934.1444">def visit_func_def(self, fdef: FuncDef) -&gt; None:
    self.reject_redefinition_of_vars_in_scope()
    with self.enter_scope():
        for arg in fdef.arguments:
            self.record_skipped(arg.variable.name)
        super().visit_func_def(fdef)

</t>
<t tx="ekr.20220525082934.1445">def visit_class_def(self, cdef: ClassDef) -&gt; None:
    self.reject_redefinition_of_vars_in_scope()
    with self.enter_scope():
        super().visit_class_def(cdef)

</t>
<t tx="ekr.20220525082934.1446">def visit_with_stmt(self, stmt: WithStmt) -&gt; None:
    for expr in stmt.expr:
        expr.accept(self)
    old_len = len(self.bound_vars)
    for target in stmt.target:
        if target is not None:
            self.analyze_lvalue(target)
    for target in stmt.target:
        if target:
            target.accept(self)
    stmt.body.accept(self)

    while len(self.bound_vars) &gt; old_len:
        self.bound_vars.pop()

</t>
<t tx="ekr.20220525082934.1447">def analyze_lvalue(self, lvalue: Lvalue) -&gt; None:
    if isinstance(lvalue, NameExpr):
        name = lvalue.name
        if name in self.bound_vars:
            # Name bound in a surrounding with statement, so it can be renamed
            self.visit_name_expr(lvalue)
        else:
            var_info = self.refs[-1]
            if name not in var_info:
                var_info[name] = []
            var_info[name].append([])
            self.bound_vars.append(name)
    elif isinstance(lvalue, (ListExpr, TupleExpr)):
        for item in lvalue.items:
            self.analyze_lvalue(item)
    elif isinstance(lvalue, MemberExpr):
        lvalue.expr.accept(self)
    elif isinstance(lvalue, IndexExpr):
        lvalue.base.accept(self)
        lvalue.index.accept(self)
    elif isinstance(lvalue, StarExpr):
        self.analyze_lvalue(lvalue.expr)

</t>
<t tx="ekr.20220525082934.1448">def visit_import(self, imp: Import) -&gt; None:
    # We don't support renaming imports
    for id, as_id in imp.ids:
        self.record_skipped(as_id or id)

</t>
<t tx="ekr.20220525082934.1449">def visit_import_from(self, imp: ImportFrom) -&gt; None:
    # We don't support renaming imports
    for id, as_id in imp.names:
        self.record_skipped(as_id or id)

</t>
<t tx="ekr.20220525082934.145">def invalid_type(self, node: AST, note: Optional[str] = None) -&gt; RawExpressionType:
    """Constructs a type representing some expression that normally forms an invalid type.
    For example, if we see a type hint that says "3 + 4", we would transform that
    expression into a RawExpressionType.

    The semantic analysis layer will report an "Invalid type" error when it
    encounters this type, along with the given note if one is provided.

    See RawExpressionType's docstring for more details on how it's used.
    """
    return RawExpressionType(
        None,
        'typing.Any',
        line=self.line,
        column=getattr(node, 'col_offset', -1),
        note=note,
    )

</t>
<t tx="ekr.20220525082934.1450">def visit_import_all(self, imp: ImportAll) -&gt; None:
    # Give up, since we don't know all imported names yet
    self.reject_redefinition_of_vars_in_scope()

</t>
<t tx="ekr.20220525082934.1451">def visit_name_expr(self, expr: NameExpr) -&gt; None:
    name = expr.name
    if name in self.bound_vars:
        # Record reference so that it can be renamed later
        for scope in reversed(self.refs):
            if name in scope:
                scope[name][-1].append(expr)
    else:
        self.record_skipped(name)

</t>
<t tx="ekr.20220525082934.1452">@contextmanager
def enter_scope(self) -&gt; Iterator[None]:
    self.skipped.append(set())
    self.refs.append({})
    yield None
    self.flush_refs()

</t>
<t tx="ekr.20220525082934.1453">def reject_redefinition_of_vars_in_scope(self) -&gt; None:
    self.record_skipped('*')

</t>
<t tx="ekr.20220525082934.1454">def record_skipped(self, name: str) -&gt; None:
    self.skipped[-1].add(name)

</t>
<t tx="ekr.20220525082934.1455">def flush_refs(self) -&gt; None:
    ref_dict = self.refs.pop()
    skipped = self.skipped.pop()
    if '*' not in skipped:
        for name, refs in ref_dict.items():
            if len(refs) &lt;= 1 or name in skipped:
                continue
            # At module top level we must not rename the final definition,
            # as it may be publicly visible
            to_rename = refs[:-1]
            for i, item in enumerate(to_rename):
                rename_refs(item, i)


</t>
<t tx="ekr.20220525082934.1456">def rename_refs(names: List[NameExpr], index: int) -&gt; None:
    name = names[0].name
    new_name = name + "'" * (index + 1)
    for expr in names:
        expr.name = new_name
</t>
<t tx="ekr.20220525082934.1457">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Classes for producing HTML reports about imprecision."""

from abc import ABCMeta, abstractmethod
import collections
import json
import os
import shutil
import tokenize
import time
import sys
import itertools
from operator import attrgetter
from urllib.request import pathname2url

import typing
from typing import Any, Callable, Dict, List, Optional, Tuple, cast, Iterator
from typing_extensions import Final, TypeAlias as _TypeAlias

from mypy.nodes import MypyFile, Expression, FuncDef
from mypy import stats
from mypy.options import Options
from mypy.traverser import TraverserVisitor
from mypy.types import Type, TypeOfAny
from mypy.version import __version__
from mypy.defaults import REPORTER_NAMES

try:
    from lxml import etree  # type: ignore
    LXML_INSTALLED = True
except ImportError:
    LXML_INSTALLED = False

type_of_any_name_map: Final["collections.OrderedDict[int, str]"] = collections.OrderedDict(
    [
        (TypeOfAny.unannotated, "Unannotated"),
        (TypeOfAny.explicit, "Explicit"),
        (TypeOfAny.from_unimported_type, "Unimported"),
        (TypeOfAny.from_omitted_generics, "Omitted Generics"),
        (TypeOfAny.from_error, "Error"),
        (TypeOfAny.special_form, "Special Form"),
        (TypeOfAny.implementation_artifact, "Implementation Artifact"),
    ]
)

ReporterClasses: _TypeAlias = Dict[
    str,
    Tuple[Callable[['Reports', str], 'AbstractReporter'], bool],
]

reporter_classes: Final[ReporterClasses] = {}


@others
register_reporter('lineprecision', LinePrecisionReporter)


# Reporter class names are defined twice to speed up mypy startup, as this
# module is slow to import. Ensure that the two definitions match.
assert set(reporter_classes) == set(REPORTER_NAMES)
</t>
<t tx="ekr.20220525082934.1458">class Reports:
    @others
</t>
<t tx="ekr.20220525082934.1459">def __init__(self, data_dir: str, report_dirs: Dict[str, str]) -&gt; None:
    self.data_dir = data_dir
    self.reporters: List[AbstractReporter] = []
    self.named_reporters: Dict[str, AbstractReporter] = {}

    for report_type, report_dir in sorted(report_dirs.items()):
        self.add_report(report_type, report_dir)

</t>
<t tx="ekr.20220525082934.146">@overload
def visit(self, node: ast3.expr) -&gt; ProperType: ...

</t>
<t tx="ekr.20220525082934.1460">def add_report(self, report_type: str, report_dir: str) -&gt; 'AbstractReporter':
    try:
        return self.named_reporters[report_type]
    except KeyError:
        pass
    reporter_cls, needs_lxml = reporter_classes[report_type]
    if needs_lxml and not LXML_INSTALLED:
        print(('You must install the lxml package before you can run mypy'
               ' with `--{}-report`.\n'
               'You can do this with `python3 -m pip install lxml`.').format(report_type),
              file=sys.stderr)
        raise ImportError
    reporter = reporter_cls(self, report_dir)
    self.reporters.append(reporter)
    self.named_reporters[report_type] = reporter
    return reporter

</t>
<t tx="ekr.20220525082934.1461">def file(self,
         tree: MypyFile,
         modules: Dict[str, MypyFile],
         type_map: Dict[Expression, Type],
         options: Options) -&gt; None:
    for reporter in self.reporters:
        reporter.on_file(tree, modules, type_map, options)

</t>
<t tx="ekr.20220525082934.1462">def finish(self) -&gt; None:
    for reporter in self.reporters:
        reporter.on_finish()


</t>
<t tx="ekr.20220525082934.1463">class AbstractReporter(metaclass=ABCMeta):
    @others
</t>
<t tx="ekr.20220525082934.1464">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    self.output_dir = output_dir
    if output_dir != '&lt;memory&gt;':
        stats.ensure_dir_exists(output_dir)

</t>
<t tx="ekr.20220525082934.1465">@abstractmethod
def on_file(self,
            tree: MypyFile,
            modules: Dict[str, MypyFile],
            type_map: Dict[Expression, Type],
            options: Options) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082934.1466">@abstractmethod
def on_finish(self) -&gt; None:
    pass


</t>
<t tx="ekr.20220525082934.1467">def register_reporter(report_name: str,
                      reporter: Callable[[Reports, str], AbstractReporter],
                      needs_lxml: bool = False) -&gt; None:
    reporter_classes[report_name] = (reporter, needs_lxml)


</t>
<t tx="ekr.20220525082934.1468">def alias_reporter(source_reporter: str, target_reporter: str) -&gt; None:
    reporter_classes[target_reporter] = reporter_classes[source_reporter]


</t>
<t tx="ekr.20220525082934.1469">def should_skip_path(path: str) -&gt; bool:
    if stats.is_special_module(path):
        return True
    if path.startswith('..'):
        return True
    if 'stubs' in path.split('/') or 'stubs' in path.split(os.sep):
        return True
    return False


</t>
<t tx="ekr.20220525082934.147">@overload
def visit(self, node: Optional[AST]) -&gt; Optional[ProperType]: ...

</t>
<t tx="ekr.20220525082934.1470">def iterate_python_lines(path: str) -&gt; Iterator[Tuple[int, str]]:
    """Return an iterator over (line number, line text) from a Python file."""
    with tokenize.open(path) as input_file:
        yield from enumerate(input_file, 1)


</t>
<t tx="ekr.20220525082934.1471">class FuncCounterVisitor(TraverserVisitor):
    def __init__(self) -&gt; None:
        super().__init__()
        self.counts = [0, 0]

    def visit_func_def(self, defn: FuncDef) -&gt; None:
        self.counts[defn.type is not None] += 1


</t>
<t tx="ekr.20220525082934.1472">class LineCountReporter(AbstractReporter):
    @others
</t>
<t tx="ekr.20220525082934.1473">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)
    self.counts: Dict[str, Tuple[int, int, int, int]] = {}

</t>
<t tx="ekr.20220525082934.1474">def on_file(self,
            tree: MypyFile,
            modules: Dict[str, MypyFile],
            type_map: Dict[Expression, Type],
            options: Options) -&gt; None:
    # Count physical lines.  This assumes the file's encoding is a
    # superset of ASCII (or at least uses \n in its line endings).
    with open(tree.path, 'rb') as f:
        physical_lines = len(f.readlines())

    func_counter = FuncCounterVisitor()
    tree.accept(func_counter)
    unannotated_funcs, annotated_funcs = func_counter.counts
    total_funcs = annotated_funcs + unannotated_funcs

    # Don't count lines or functions as annotated if they have their errors ignored.
    if options.ignore_errors:
        annotated_funcs = 0

    imputed_annotated_lines = (physical_lines * annotated_funcs // total_funcs
                               if total_funcs else physical_lines)

    self.counts[tree._fullname] = (imputed_annotated_lines, physical_lines,
                                   annotated_funcs, total_funcs)

</t>
<t tx="ekr.20220525082934.1475">def on_finish(self) -&gt; None:
    counts: List[Tuple[Tuple[int, int, int, int], str]] = sorted(
        ((c, p) for p, c in self.counts.items()), reverse=True
    )
    total_counts = tuple(sum(c[i] for c, p in counts) for i in range(4))
    with open(os.path.join(self.output_dir, "linecount.txt"), "w") as f:
        f.write("{:7} {:7} {:6} {:6} total\n".format(*total_counts))
        for c, p in counts:
            f.write(f'{c[0]:7} {c[1]:7} {c[2]:6} {c[3]:6} {p}\n')


</t>
<t tx="ekr.20220525082934.1476">register_reporter('linecount', LineCountReporter)


</t>
<t tx="ekr.20220525082934.1477">class AnyExpressionsReporter(AbstractReporter):
    """Report frequencies of different kinds of Any types."""

    @others
</t>
<t tx="ekr.20220525082934.1478">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)
    self.counts: Dict[str, Tuple[int, int]] = {}
    self.any_types_counter: Dict[str, typing.Counter[int]] = {}

</t>
<t tx="ekr.20220525082934.1479">def on_file(self,
            tree: MypyFile,
            modules: Dict[str, MypyFile],
            type_map: Dict[Expression, Type],
            options: Options) -&gt; None:
    visitor = stats.StatisticsVisitor(inferred=True,
                                      filename=tree.fullname,
                                      modules=modules,
                                      typemap=type_map,
                                      all_nodes=True,
                                      visit_untyped_defs=False)
    tree.accept(visitor)
    self.any_types_counter[tree.fullname] = visitor.type_of_any_counter
    num_unanalyzed_lines = list(visitor.line_map.values()).count(stats.TYPE_UNANALYZED)
    # count each line of dead code as one expression of type "Any"
    num_any = visitor.num_any_exprs + num_unanalyzed_lines
    num_total = visitor.num_imprecise_exprs + visitor.num_precise_exprs + num_any
    if num_total &gt; 0:
        self.counts[tree.fullname] = (num_any, num_total)

</t>
<t tx="ekr.20220525082934.148">def visit(self, node: Optional[AST]) -&gt; Optional[ProperType]:
    """Modified visit -- keep track of the stack of nodes"""
    if node is None:
        return None
    self.node_stack.append(node)
    try:
        method = 'visit_' + node.__class__.__name__
        visitor = getattr(self, method, None)
        if visitor is not None:
            return visitor(node)
        else:
            return self.invalid_type(node)
    finally:
        self.node_stack.pop()

</t>
<t tx="ekr.20220525082934.1480">def on_finish(self) -&gt; None:
    self._report_any_exprs()
    self._report_types_of_anys()

</t>
<t tx="ekr.20220525082934.1481">def _write_out_report(self,
                      filename: str,
                      header: List[str],
                      rows: List[List[str]],
                      footer: List[str],
                      ) -&gt; None:
    row_len = len(header)
    assert all(len(row) == row_len for row in rows + [header, footer])
    min_column_distance = 3  # minimum distance between numbers in two columns
    widths = [-1] * row_len
    for row in rows + [header, footer]:
        for i, value in enumerate(row):
            widths[i] = max(widths[i], len(value))
    for i, w in enumerate(widths):
        # Do not add min_column_distance to the first column.
        if i &gt; 0:
            widths[i] = w + min_column_distance
    with open(os.path.join(self.output_dir, filename), 'w') as f:
        header_str = ("{:&gt;{}}" * len(widths)).format(*itertools.chain(*zip(header, widths)))
        separator = '-' * len(header_str)
        f.write(header_str + '\n')
        f.write(separator + '\n')
        for row_values in rows:
            r = ("{:&gt;{}}" * len(widths)).format(*itertools.chain(*zip(row_values, widths)))
            f.write(r + '\n')
        f.write(separator + '\n')
        footer_str = ("{:&gt;{}}" * len(widths)).format(*itertools.chain(*zip(footer, widths)))
        f.write(footer_str + '\n')

</t>
<t tx="ekr.20220525082934.1482">def _report_any_exprs(self) -&gt; None:
    total_any = sum(num_any for num_any, _ in self.counts.values())
    total_expr = sum(total for _, total in self.counts.values())
    total_coverage = 100.0
    if total_expr &gt; 0:
        total_coverage = (float(total_expr - total_any) / float(total_expr)) * 100

    column_names = ["Name", "Anys", "Exprs", "Coverage"]
    rows: List[List[str]] = []
    for filename in sorted(self.counts):
        (num_any, num_total) = self.counts[filename]
        coverage = (float(num_total - num_any) / float(num_total)) * 100
        coverage_str = f'{coverage:.2f}%'
        rows.append([filename, str(num_any), str(num_total), coverage_str])
    rows.sort(key=lambda x: x[0])
    total_row = ["Total", str(total_any), str(total_expr), f'{total_coverage:.2f}%']
    self._write_out_report('any-exprs.txt', column_names, rows, total_row)

</t>
<t tx="ekr.20220525082934.1483">def _report_types_of_anys(self) -&gt; None:
    total_counter: typing.Counter[int] = collections.Counter()
    for counter in self.any_types_counter.values():
        for any_type, value in counter.items():
            total_counter[any_type] += value
    file_column_name = "Name"
    total_row_name = "Total"
    column_names = [file_column_name] + list(type_of_any_name_map.values())
    rows: List[List[str]] = []
    for filename, counter in self.any_types_counter.items():
        rows.append([filename] + [str(counter[typ]) for typ in type_of_any_name_map])
    rows.sort(key=lambda x: x[0])
    total_row = [total_row_name] + [str(total_counter[typ])
                                    for typ in type_of_any_name_map]
    self._write_out_report('types-of-anys.txt', column_names, rows, total_row)


</t>
<t tx="ekr.20220525082934.1484">register_reporter('any-exprs', AnyExpressionsReporter)


</t>
<t tx="ekr.20220525082934.1485">class LineCoverageVisitor(TraverserVisitor):
    @others
</t>
<t tx="ekr.20220525082934.1486">def __init__(self, source: List[str]) -&gt; None:
    self.source = source

    # For each line of source, we maintain a pair of
    #  * the indentation level of the surrounding function
    #    (-1 if not inside a function), and
    #  * whether the surrounding function is typed.
    # Initially, everything is covered at indentation level -1.
    self.lines_covered = [(-1, True) for l in source]

</t>
<t tx="ekr.20220525082934.1487"># The Python AST has position information for the starts of
# elements, but not for their ends. Fortunately the
# indentation-based syntax makes it pretty easy to find where a
# block ends without doing any real parsing.

# TODO: Handle line continuations (explicit and implicit) and
# multi-line string literals. (But at least line continuations
# are normally more indented than their surrounding block anyways,
# by PEP 8.)

</t>
<t tx="ekr.20220525082934.1488">def indentation_level(self, line_number: int) -&gt; Optional[int]:
    """Return the indentation of a line of the source (specified by
    zero-indexed line number). Returns None for blank lines or comments."""
    line = self.source[line_number]
    indent = 0
    for char in list(line):
        if char == ' ':
            indent += 1
        elif char == '\t':
            indent = 8 * ((indent + 8) // 8)
        elif char == '#':
            # Line is a comment; ignore it
            return None
        elif char == '\n':
            # Line is entirely whitespace; ignore it
            return None
        # TODO line continuation (\)
        else:
            # Found a non-whitespace character
            return indent
    # Line is entirely whitespace, and at end of file
    # with no trailing newline; ignore it
    return None

</t>
<t tx="ekr.20220525082934.1489">def visit_func_def(self, defn: FuncDef) -&gt; None:
    start_line = defn.get_line() - 1
    start_indent = None
    # When a function is decorated, sometimes the start line will point to
    # whitespace or comments between the decorator and the function, so
    # we have to look for the start.
    while start_line &lt; len(self.source):
        start_indent = self.indentation_level(start_line)
        if start_indent is not None:
            break
        start_line += 1
    # If we can't find the function give up and don't annotate anything.
    # Our line numbers are not reliable enough to be asserting on.
    if start_indent is None:
        return

    cur_line = start_line + 1
    end_line = cur_line
    # After this loop, function body will be lines [start_line, end_line)
    while cur_line &lt; len(self.source):
        cur_indent = self.indentation_level(cur_line)
        if cur_indent is None:
            # Consume the line, but don't mark it as belonging to the function yet.
            cur_line += 1
        elif start_indent is not None and cur_indent &gt; start_indent:
            # A non-blank line that belongs to the function.
            cur_line += 1
            end_line = cur_line
        else:
            # We reached a line outside the function definition.
            break

    is_typed = defn.type is not None
    for line in range(start_line, end_line):
        old_indent, _ = self.lines_covered[line]
        # If there was an old indent level for this line, and the new
        # level isn't increasing the indentation, ignore it.
        # This is to be defensive against funniness in our line numbers,
        # which are not always reliable.
        if old_indent &lt;= start_indent:
            self.lines_covered[line] = (start_indent, is_typed)

    # Visit the body, in case there are nested functions
    super().visit_func_def(defn)


</t>
<t tx="ekr.20220525082934.149">def parent(self) -&gt; Optional[AST]:
    """Return the AST node above the one we are processing"""
    if len(self.node_stack) &lt; 2:
        return None
    return self.node_stack[-2]

</t>
<t tx="ekr.20220525082934.1490">class LineCoverageReporter(AbstractReporter):
    """Exact line coverage reporter.

    This reporter writes a JSON dictionary with one field 'lines' to
    the file 'coverage.json' in the specified report directory. The
    value of that field is a dictionary which associates to each
    source file's absolute pathname the list of line numbers that
    belong to typed functions in that file.
    """

    @others
</t>
<t tx="ekr.20220525082934.1491">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)
    self.lines_covered: Dict[str, List[int]] = {}

</t>
<t tx="ekr.20220525082934.1492">def on_file(self,
            tree: MypyFile,
            modules: Dict[str, MypyFile],
            type_map: Dict[Expression, Type],
            options: Options) -&gt; None:
    with open(tree.path) as f:
        tree_source = f.readlines()

    coverage_visitor = LineCoverageVisitor(tree_source)
    tree.accept(coverage_visitor)

    covered_lines = []
    for line_number, (_, typed) in enumerate(coverage_visitor.lines_covered):
        if typed:
            covered_lines.append(line_number + 1)

    self.lines_covered[os.path.abspath(tree.path)] = covered_lines

</t>
<t tx="ekr.20220525082934.1493">def on_finish(self) -&gt; None:
    with open(os.path.join(self.output_dir, 'coverage.json'), 'w') as f:
        json.dump({'lines': self.lines_covered}, f)


</t>
<t tx="ekr.20220525082934.1494">register_reporter('linecoverage', LineCoverageReporter)


</t>
<t tx="ekr.20220525082934.1495">class FileInfo:
    @others
</t>
<t tx="ekr.20220525082934.1496">def __init__(self, name: str, module: str) -&gt; None:
    self.name = name
    self.module = module
    self.counts = [0] * len(stats.precision_names)

</t>
<t tx="ekr.20220525082934.1497">def total(self) -&gt; int:
    return sum(self.counts)

</t>
<t tx="ekr.20220525082934.1498">def attrib(self) -&gt; Dict[str, str]:
    return {name: str(val) for name, val in sorted(zip(stats.precision_names, self.counts))}


</t>
<t tx="ekr.20220525082934.1499">class MemoryXmlReporter(AbstractReporter):
    """Internal reporter that generates XML in memory.

    This is used by all other XML-based reporters to avoid duplication.
    """

    @others
</t>
<t tx="ekr.20220525082934.15">def visit_param_spec(self, t: ParamSpecType) -&gt; Type:
    repl = get_proper_type(self.variables.get(t.id, t))
    if isinstance(repl, Instance):
        inst = repl
        # Return copy of instance with type erasure flag on.
        # TODO: what does prefix mean in this case?
        # TODO: why does this case even happen? Instances aren't plural.
        return Instance(inst.type, inst.args, line=inst.line, column=inst.column)
    elif isinstance(repl, ParamSpecType):
        return repl.copy_modified(flavor=t.flavor, prefix=t.prefix.copy_modified(
            arg_types=t.prefix.arg_types + repl.prefix.arg_types,
            arg_kinds=t.prefix.arg_kinds + repl.prefix.arg_kinds,
            arg_names=t.prefix.arg_names + repl.prefix.arg_names,
        ))
    elif isinstance(repl, Parameters) or isinstance(repl, CallableType):
        # if the paramspec is *P.args or **P.kwargs:
        if t.flavor != ParamSpecFlavor.BARE:
            assert isinstance(repl, CallableType), "Should not be able to get here."
            # Is this always the right thing to do?
            param_spec = repl.param_spec()
            if param_spec:
                return param_spec.with_flavor(t.flavor)
            else:
                return repl
        else:
            return Parameters(t.prefix.arg_types + repl.arg_types,
                              t.prefix.arg_kinds + repl.arg_kinds,
                              t.prefix.arg_names + repl.arg_names,
                              variables=[*t.prefix.variables, *repl.variables])
    else:
        # TODO: should this branch be removed? better not to fail silently
        return repl

</t>
<t tx="ekr.20220525082934.150">def fail(self, msg: str, line: int, column: int) -&gt; None:
    if self.errors:
        self.errors.report(line, column, msg, blocker=True, code=codes.SYNTAX)

</t>
<t tx="ekr.20220525082934.1500">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)

    self.xslt_html_path = os.path.join(reports.data_dir, 'xml', 'mypy-html.xslt')
    self.xslt_txt_path = os.path.join(reports.data_dir, 'xml', 'mypy-txt.xslt')
    self.css_html_path = os.path.join(reports.data_dir, 'xml', 'mypy-html.css')
    xsd_path = os.path.join(reports.data_dir, 'xml', 'mypy.xsd')
    self.schema = etree.XMLSchema(etree.parse(xsd_path))
    self.last_xml: Optional[Any] = None
    self.files: List[FileInfo] = []

</t>
<t tx="ekr.20220525082934.1501"># XML doesn't like control characters, but they are sometimes
# legal in source code (e.g. comments, string literals).
# Tabs (#x09) are allowed in XML content.
control_fixer: Final = str.maketrans("".join(chr(i) for i in range(32) if i != 9), "?" * 31)

</t>
<t tx="ekr.20220525082934.1502">def on_file(self,
            tree: MypyFile,
            modules: Dict[str, MypyFile],
            type_map: Dict[Expression, Type],
            options: Options) -&gt; None:
    self.last_xml = None

    try:
        path = os.path.relpath(tree.path)
    except ValueError:
        return

    if should_skip_path(path) or os.path.isdir(path):
        return  # `path` can sometimes be a directory, see #11334

    visitor = stats.StatisticsVisitor(inferred=True,
                                      filename=tree.fullname,
                                      modules=modules,
                                      typemap=type_map,
                                      all_nodes=True)
    tree.accept(visitor)

    root = etree.Element('mypy-report-file', name=path, module=tree._fullname)
    doc = etree.ElementTree(root)
    file_info = FileInfo(path, tree._fullname)

    for lineno, line_text in iterate_python_lines(path):
        status = visitor.line_map.get(lineno, stats.TYPE_EMPTY)
        file_info.counts[status] += 1
        etree.SubElement(root, 'line',
                         any_info=self._get_any_info_for_line(visitor, lineno),
                         content=line_text.rstrip('\n').translate(self.control_fixer),
                         number=str(lineno),
                         precision=stats.precision_names[status])
    # Assumes a layout similar to what XmlReporter uses.
    xslt_path = os.path.relpath('mypy-html.xslt', path)
    transform_pi = etree.ProcessingInstruction('xml-stylesheet',
            f'type="text/xsl" href="{pathname2url(xslt_path)}"')
    root.addprevious(transform_pi)
    self.schema.assertValid(doc)

    self.last_xml = doc
    self.files.append(file_info)

</t>
<t tx="ekr.20220525082934.1503">@staticmethod
def _get_any_info_for_line(visitor: stats.StatisticsVisitor, lineno: int) -&gt; str:
    if lineno in visitor.any_line_map:
        result = "Any Types on this line: "
        counter: typing.Counter[int] = collections.Counter()
        for typ in visitor.any_line_map[lineno]:
            counter[typ.type_of_any] += 1
        for any_type, occurrences in counter.items():
            result += f"\n{type_of_any_name_map[any_type]} (x{occurrences})"
        return result
    else:
        return "No Anys on this line!"

</t>
<t tx="ekr.20220525082934.1504">def on_finish(self) -&gt; None:
    self.last_xml = None
    # index_path = os.path.join(self.output_dir, 'index.xml')
    output_files = sorted(self.files, key=lambda x: x.module)

    root = etree.Element('mypy-report-index', name='index')
    doc = etree.ElementTree(root)

    for file_info in output_files:
        etree.SubElement(root, 'file',
                         file_info.attrib(),
                         module=file_info.module,
                         name=pathname2url(file_info.name),
                         total=str(file_info.total()))
    xslt_path = os.path.relpath('mypy-html.xslt', '.')
    transform_pi = etree.ProcessingInstruction('xml-stylesheet',
            f'type="text/xsl" href="{pathname2url(xslt_path)}"')
    root.addprevious(transform_pi)
    self.schema.assertValid(doc)

    self.last_xml = doc


</t>
<t tx="ekr.20220525082934.1505">register_reporter('memory-xml', MemoryXmlReporter, needs_lxml=True)


</t>
<t tx="ekr.20220525082934.1506">def get_line_rate(covered_lines: int, total_lines: int) -&gt; str:
    if total_lines == 0:
        return str(1.0)
    else:
        return f'{covered_lines / total_lines:.4f}'


</t>
<t tx="ekr.20220525082934.1507">class CoberturaPackage:
    """Container for XML and statistics mapping python modules to Cobertura package."""

    @others
</t>
<t tx="ekr.20220525082934.1508">def __init__(self, name: str) -&gt; None:
    self.name = name
    self.classes: Dict[str, Any] = {}
    self.packages: Dict[str, CoberturaPackage] = {}
    self.total_lines = 0
    self.covered_lines = 0

</t>
<t tx="ekr.20220525082934.1509">def as_xml(self) -&gt; Any:
    package_element = etree.Element('package',
                                    complexity='1.0',
                                    name=self.name)
    package_element.attrib['branch-rate'] = '0'
    package_element.attrib['line-rate'] = get_line_rate(self.covered_lines, self.total_lines)
    classes_element = etree.SubElement(package_element, 'classes')
    for class_name in sorted(self.classes):
        classes_element.append(self.classes[class_name])
    self.add_packages(package_element)
    return package_element

</t>
<t tx="ekr.20220525082934.151">def note(self, msg: str, line: int, column: int) -&gt; None:
    if self.errors:
        self.errors.report(line, column, msg, severity='note', code=codes.SYNTAX)

</t>
<t tx="ekr.20220525082934.1510">def add_packages(self, parent_element: Any) -&gt; None:
    if self.packages:
        packages_element = etree.SubElement(parent_element, 'packages')
        for package in sorted(self.packages.values(), key=attrgetter('name')):
            packages_element.append(package.as_xml())


</t>
<t tx="ekr.20220525082934.1511">class CoberturaXmlReporter(AbstractReporter):
    """Reporter for generating Cobertura compliant XML."""

    @others
</t>
<t tx="ekr.20220525082934.1512">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)

    self.root = etree.Element('coverage',
                              timestamp=str(int(time.time())),
                              version=__version__)
    self.doc = etree.ElementTree(self.root)
    self.root_package = CoberturaPackage('.')

</t>
<t tx="ekr.20220525082934.1513">def on_file(self,
            tree: MypyFile,
            modules: Dict[str, MypyFile],
            type_map: Dict[Expression, Type],
            options: Options) -&gt; None:
    path = os.path.relpath(tree.path)
    visitor = stats.StatisticsVisitor(inferred=True,
                                      filename=tree.fullname,
                                      modules=modules,
                                      typemap=type_map,
                                      all_nodes=True)
    tree.accept(visitor)

    class_name = os.path.basename(path)
    file_info = FileInfo(path, tree._fullname)
    class_element = etree.Element('class',
                                  complexity='1.0',
                                  filename=path,
                                  name=class_name)
    etree.SubElement(class_element, 'methods')
    lines_element = etree.SubElement(class_element, 'lines')

    with tokenize.open(path) as input_file:
        class_lines_covered = 0
        class_total_lines = 0
        for lineno, _ in enumerate(input_file, 1):
            status = visitor.line_map.get(lineno, stats.TYPE_EMPTY)
            hits = 0
            branch = False
            if status == stats.TYPE_EMPTY:
                continue
            class_total_lines += 1
            if status != stats.TYPE_ANY:
                class_lines_covered += 1
                hits = 1
            if status == stats.TYPE_IMPRECISE:
                branch = True
            file_info.counts[status] += 1
            line_element = etree.SubElement(lines_element, 'line',
                                            branch=str(branch).lower(),
                                            hits=str(hits),
                                            number=str(lineno),
                                            precision=stats.precision_names[status])
            if branch:
                line_element.attrib['condition-coverage'] = '50% (1/2)'
        class_element.attrib['branch-rate'] = '0'
        class_element.attrib['line-rate'] = get_line_rate(class_lines_covered,
                                                          class_total_lines)
        # parent_module is set to whichever module contains this file.  For most files, we want
        # to simply strip the last element off of the module.  But for __init__.py files,
        # the module == the parent module.
        parent_module = file_info.module.rsplit('.', 1)[0]
        if file_info.name.endswith('__init__.py'):
            parent_module = file_info.module

        if parent_module not in self.root_package.packages:
            self.root_package.packages[parent_module] = CoberturaPackage(parent_module)
        current_package = self.root_package.packages[parent_module]
        packages_to_update = [self.root_package, current_package]
        for package in packages_to_update:
            package.total_lines += class_total_lines
            package.covered_lines += class_lines_covered
        current_package.classes[class_name] = class_element

</t>
<t tx="ekr.20220525082934.1514">def on_finish(self) -&gt; None:
    self.root.attrib['line-rate'] = get_line_rate(self.root_package.covered_lines,
                                                  self.root_package.total_lines)
    self.root.attrib['branch-rate'] = '0'
    sources = etree.SubElement(self.root, 'sources')
    source_element = etree.SubElement(sources, 'source')
    source_element.text = os.getcwd()
    self.root_package.add_packages(self.root)
    out_path = os.path.join(self.output_dir, 'cobertura.xml')
    self.doc.write(out_path, encoding='utf-8', pretty_print=True)
    print('Generated Cobertura report:', os.path.abspath(out_path))


</t>
<t tx="ekr.20220525082934.1515">register_reporter('cobertura-xml', CoberturaXmlReporter, needs_lxml=True)


</t>
<t tx="ekr.20220525082934.1516">class AbstractXmlReporter(AbstractReporter):
    """Internal abstract class for reporters that work via XML."""

    @others
</t>
<t tx="ekr.20220525082934.1517">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)

    memory_reporter = reports.add_report('memory-xml', '&lt;memory&gt;')
    # The dependency will be called first.
    self.memory_xml = cast(MemoryXmlReporter, memory_reporter)


</t>
<t tx="ekr.20220525082934.1518">class XmlReporter(AbstractXmlReporter):
    """Public reporter that exports XML.

    The produced XML files contain a reference to the absolute path
    of the html transform, so they will be locally viewable in a browser.

    However, there is a bug in Chrome and all other WebKit-based browsers
    that makes it fail from file:// URLs but work on http:// URLs.
    """

    @others
</t>
<t tx="ekr.20220525082934.1519">def on_file(self,
            tree: MypyFile,
            modules: Dict[str, MypyFile],
            type_map: Dict[Expression, Type],
            options: Options) -&gt; None:
    last_xml = self.memory_xml.last_xml
    if last_xml is None:
        return
    path = os.path.relpath(tree.path)
    if path.startswith('..'):
        return
    out_path = os.path.join(self.output_dir, 'xml', path + '.xml')
    stats.ensure_dir_exists(os.path.dirname(out_path))
    last_xml.write(out_path, encoding='utf-8')

</t>
<t tx="ekr.20220525082934.152">def translate_expr_list(self, l: Sequence[ast3.expr]) -&gt; List[Type]:
    return [self.visit(e) for e in l]

</t>
<t tx="ekr.20220525082934.1520">def on_finish(self) -&gt; None:
    last_xml = self.memory_xml.last_xml
    assert last_xml is not None
    out_path = os.path.join(self.output_dir, 'index.xml')
    out_xslt = os.path.join(self.output_dir, 'mypy-html.xslt')
    out_css = os.path.join(self.output_dir, 'mypy-html.css')
    last_xml.write(out_path, encoding='utf-8')
    shutil.copyfile(self.memory_xml.xslt_html_path, out_xslt)
    shutil.copyfile(self.memory_xml.css_html_path, out_css)
    print('Generated XML report:', os.path.abspath(out_path))


</t>
<t tx="ekr.20220525082934.1521">register_reporter('xml', XmlReporter, needs_lxml=True)


</t>
<t tx="ekr.20220525082934.1522">class XsltHtmlReporter(AbstractXmlReporter):
    """Public reporter that exports HTML via XSLT.

    This is slightly different than running `xsltproc` on the .xml files,
    because it passes a parameter to rewrite the links.
    """

    @others
</t>
<t tx="ekr.20220525082934.1523">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)

    self.xslt_html = etree.XSLT(etree.parse(self.memory_xml.xslt_html_path))
    self.param_html = etree.XSLT.strparam('html')

</t>
<t tx="ekr.20220525082934.1524">def on_file(self,
            tree: MypyFile,
            modules: Dict[str, MypyFile],
            type_map: Dict[Expression, Type],
            options: Options) -&gt; None:
    last_xml = self.memory_xml.last_xml
    if last_xml is None:
        return
    path = os.path.relpath(tree.path)
    if path.startswith('..'):
        return
    out_path = os.path.join(self.output_dir, 'html', path + '.html')
    stats.ensure_dir_exists(os.path.dirname(out_path))
    transformed_html = bytes(self.xslt_html(last_xml, ext=self.param_html))
    with open(out_path, 'wb') as out_file:
        out_file.write(transformed_html)

</t>
<t tx="ekr.20220525082934.1525">def on_finish(self) -&gt; None:
    last_xml = self.memory_xml.last_xml
    assert last_xml is not None
    out_path = os.path.join(self.output_dir, 'index.html')
    out_css = os.path.join(self.output_dir, 'mypy-html.css')
    transformed_html = bytes(self.xslt_html(last_xml, ext=self.param_html))
    with open(out_path, 'wb') as out_file:
        out_file.write(transformed_html)
    shutil.copyfile(self.memory_xml.css_html_path, out_css)
    print('Generated HTML report (via XSLT):', os.path.abspath(out_path))


</t>
<t tx="ekr.20220525082934.1526">register_reporter('xslt-html', XsltHtmlReporter, needs_lxml=True)


</t>
<t tx="ekr.20220525082934.1527">class XsltTxtReporter(AbstractXmlReporter):
    """Public reporter that exports TXT via XSLT.

    Currently this only does the summary, not the individual reports.
    """

    @others
</t>
<t tx="ekr.20220525082934.1528">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)

    self.xslt_txt = etree.XSLT(etree.parse(self.memory_xml.xslt_txt_path))

</t>
<t tx="ekr.20220525082934.1529">def on_file(self,
            tree: MypyFile,
            modules: Dict[str, MypyFile],
            type_map: Dict[Expression, Type],
            options: Options) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082934.153">def visit_raw_str(self, s: str) -&gt; Type:
    # An escape hatch that allows the AST walker in fastparse2 to
    # directly hook into the Python 3 type converter in some cases
    # without needing to create an intermediary `Str` object.
    _, typ = parse_type_comment(s.strip(),
                                self.line,
                                -1,
                                self.errors,
                                self.assume_str_is_unicode)
    return typ or AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20220525082934.1530">def on_finish(self) -&gt; None:
    last_xml = self.memory_xml.last_xml
    assert last_xml is not None
    out_path = os.path.join(self.output_dir, 'index.txt')
    transformed_txt = bytes(self.xslt_txt(last_xml))
    with open(out_path, 'wb') as out_file:
        out_file.write(transformed_txt)
    print('Generated TXT report (via XSLT):', os.path.abspath(out_path))


</t>
<t tx="ekr.20220525082934.1531">register_reporter('xslt-txt', XsltTxtReporter, needs_lxml=True)

alias_reporter('xslt-html', 'html')
alias_reporter('xslt-txt', 'txt')


</t>
<t tx="ekr.20220525082934.1532">class LinePrecisionReporter(AbstractReporter):
    """Report per-module line counts for typing precision.

    Each line is classified into one of these categories:

    * precise (fully type checked)
    * imprecise (Any types in a type component, such as List[Any])
    * any (something with an Any type, implicit or explicit)
    * empty (empty line, comment or docstring)
    * unanalyzed (mypy considers line unreachable)

    The meaning of these categories varies slightly depending on
    context.
    """

    @others
</t>
<t tx="ekr.20220525082934.1533">def __init__(self, reports: Reports, output_dir: str) -&gt; None:
    super().__init__(reports, output_dir)
    self.files: List[FileInfo] = []

</t>
<t tx="ekr.20220525082934.1534">def on_file(self,
            tree: MypyFile,
            modules: Dict[str, MypyFile],
            type_map: Dict[Expression, Type],
            options: Options) -&gt; None:

    try:
        path = os.path.relpath(tree.path)
    except ValueError:
        return

    if should_skip_path(path):
        return

    visitor = stats.StatisticsVisitor(inferred=True,
                                      filename=tree.fullname,
                                      modules=modules,
                                      typemap=type_map,
                                      all_nodes=True)
    tree.accept(visitor)

    file_info = FileInfo(path, tree._fullname)
    for lineno, _ in iterate_python_lines(path):
        status = visitor.line_map.get(lineno, stats.TYPE_EMPTY)
        file_info.counts[status] += 1

    self.files.append(file_info)

</t>
<t tx="ekr.20220525082934.1535">def on_finish(self) -&gt; None:
    if not self.files:
        # Nothing to do.
        return
    output_files = sorted(self.files, key=lambda x: x.module)
    report_file = os.path.join(self.output_dir, 'lineprecision.txt')
    width = max(4, max(len(info.module) for info in output_files))
    titles = ('Lines', 'Precise', 'Imprecise', 'Any', 'Empty', 'Unanalyzed')
    widths = (width,) + tuple(len(t) for t in titles)
    fmt = '{:%d}  {:%d}  {:%d}  {:%d}  {:%d}  {:%d}  {:%d}\n' % widths
    with open(report_file, 'w') as f:
        f.write(
            fmt.format('Name', *titles))
        f.write('-' * (width + 51) + '\n')
        for file_info in output_files:
            counts = file_info.counts
            f.write(fmt.format(file_info.module.ljust(width),
                               file_info.total(),
                               counts[stats.TYPE_PRECISE],
                               counts[stats.TYPE_IMPRECISE],
                               counts[stats.TYPE_ANY],
                               counts[stats.TYPE_EMPTY],
                               counts[stats.TYPE_UNANALYZED]))


</t>
<t tx="ekr.20220525082934.1536">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from typing import Sequence, Tuple, Set, List

from mypy.types import (
    Type, UnboundType, AnyType, NoneType, TupleType, TypedDictType,
    UnionType, CallableType, TypeVarType, Instance, TypeVisitor, ErasedType,
    Overloaded, PartialType, DeletedType, UninhabitedType, TypeType, LiteralType,
    ProperType, get_proper_type, TypeAliasType, ParamSpecType, Parameters,
    UnpackType, TypeVarTupleType,
)
from mypy.typeops import tuple_fallback, make_simplified_union, is_simple_literal


@others
</t>
<t tx="ekr.20220525082934.1537">def is_same_type(left: Type, right: Type) -&gt; bool:
    """Is 'left' the same type as 'right'?"""

    left = get_proper_type(left)
    right = get_proper_type(right)

    if isinstance(right, UnboundType):
        # Make unbound types same as anything else to reduce the number of
        # generated spurious error messages.
        return True
    else:
        # Simplify types to canonical forms.
        #
        # There are multiple possible union types that represent the same type,
        # such as Union[int, bool, str] and Union[int, str]. Also, some union
        # types can be simplified to non-union types such as Union[int, bool]
        # -&gt; int. It would be nice if we always had simplified union types but
        # this is currently not the case, though it often is.
        left = simplify_union(left)
        right = simplify_union(right)

        return left.accept(SameTypeVisitor(right))


</t>
<t tx="ekr.20220525082934.1538">def simplify_union(t: Type) -&gt; ProperType:
    t = get_proper_type(t)
    if isinstance(t, UnionType):
        return make_simplified_union(t.items)
    return t


</t>
<t tx="ekr.20220525082934.1539">def is_same_types(a1: Sequence[Type], a2: Sequence[Type]) -&gt; bool:
    if len(a1) != len(a2):
        return False
    for i in range(len(a1)):
        if not is_same_type(a1[i], a2[i]):
            return False
    return True


</t>
<t tx="ekr.20220525082934.154">def visit_Call(self, e: Call) -&gt; Type:
    # Parse the arg constructor
    f = e.func
    constructor = stringify_name(f)

    if not isinstance(self.parent(), ast3.List):
        note = None
        if constructor:
            note = "Suggestion: use {0}[...] instead of {0}(...)".format(constructor)
        return self.invalid_type(e, note=note)
    if not constructor:
        self.fail("Expected arg constructor name", e.lineno, e.col_offset)

    name: Optional[str] = None
    default_type = AnyType(TypeOfAny.special_form)
    typ: Type = default_type
    for i, arg in enumerate(e.args):
        if i == 0:
            converted = self.visit(arg)
            assert converted is not None
            typ = converted
        elif i == 1:
            name = self._extract_argument_name(arg)
        else:
            self.fail("Too many arguments for argument constructor",
                      f.lineno, f.col_offset)
    for k in e.keywords:
        value = k.value
        if k.arg == "name":
            if name is not None:
                self.fail('"{}" gets multiple values for keyword argument "name"'.format(
                    constructor), f.lineno, f.col_offset)
            name = self._extract_argument_name(value)
        elif k.arg == "type":
            if typ is not default_type:
                self.fail('"{}" gets multiple values for keyword argument "type"'.format(
                    constructor), f.lineno, f.col_offset)
            converted = self.visit(value)
            assert converted is not None
            typ = converted
        else:
            self.fail(
                f'Unexpected argument "{k.arg}" for argument constructor',
                value.lineno, value.col_offset)
    return CallableArgument(typ, name, constructor, e.lineno, e.col_offset)

</t>
<t tx="ekr.20220525082934.1540">def _extract_literals(u: UnionType) -&gt; Tuple[Set[Type], List[Type]]:
    """Given a UnionType, separate out its items into a set of simple literals and a remainder list
    This is a useful helper to avoid O(n**2) behavior when comparing large unions, which can often
    result from large enums in contexts where type narrowing removes a small subset of entries.
    """
    lit: Set[Type] = set()
    rem: List[Type] = []
    for i in u.relevant_items():
        i = get_proper_type(i)
        if is_simple_literal(i):
            lit.add(i)
        else:
            rem.append(i)
    return lit, rem


</t>
<t tx="ekr.20220525082934.1541">class SameTypeVisitor(TypeVisitor[bool]):
    """Visitor for checking whether two types are the 'same' type."""

    @others
</t>
<t tx="ekr.20220525082934.1542">def __init__(self, right: ProperType) -&gt; None:
    self.right = right

</t>
<t tx="ekr.20220525082934.1543"># visit_x(left) means: is left (which is an instance of X) the same type as
# right?

</t>
<t tx="ekr.20220525082934.1544">def visit_unbound_type(self, left: UnboundType) -&gt; bool:
    return True

</t>
<t tx="ekr.20220525082934.1545">def visit_any(self, left: AnyType) -&gt; bool:
    return isinstance(self.right, AnyType)

</t>
<t tx="ekr.20220525082934.1546">def visit_none_type(self, left: NoneType) -&gt; bool:
    return isinstance(self.right, NoneType)

</t>
<t tx="ekr.20220525082934.1547">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; bool:
    return isinstance(self.right, UninhabitedType)

</t>
<t tx="ekr.20220525082934.1548">def visit_erased_type(self, left: ErasedType) -&gt; bool:
    # We can get here when isinstance is used inside a lambda
    # whose type is being inferred. In any event, we have no reason
    # to think that an ErasedType will end up being the same as
    # any other type, except another ErasedType (for protocols).
    return isinstance(self.right, ErasedType)

</t>
<t tx="ekr.20220525082934.1549">def visit_deleted_type(self, left: DeletedType) -&gt; bool:
    return isinstance(self.right, DeletedType)

</t>
<t tx="ekr.20220525082934.155">def translate_argument_list(self, l: Sequence[ast3.expr]) -&gt; TypeList:
    return TypeList([self.visit(e) for e in l], line=self.line)

</t>
<t tx="ekr.20220525082934.1550">def visit_instance(self, left: Instance) -&gt; bool:
    return (isinstance(self.right, Instance) and
            left.type == self.right.type and
            is_same_types(left.args, self.right.args) and
            left.last_known_value == self.right.last_known_value)

</t>
<t tx="ekr.20220525082934.1551">def visit_type_alias_type(self, left: TypeAliasType) -&gt; bool:
    # Similar to protocols, two aliases with the same targets return False here,
    # but both is_subtype(t, s) and is_subtype(s, t) return True.
    return (isinstance(self.right, TypeAliasType) and
            left.alias == self.right.alias and
            is_same_types(left.args, self.right.args))

</t>
<t tx="ekr.20220525082934.1552">def visit_type_var(self, left: TypeVarType) -&gt; bool:
    return (isinstance(self.right, TypeVarType) and
            left.id == self.right.id)

</t>
<t tx="ekr.20220525082934.1553">def visit_param_spec(self, left: ParamSpecType) -&gt; bool:
    # Ignore upper bound since it's derived from flavor.
    return (isinstance(self.right, ParamSpecType) and
            left.id == self.right.id and left.flavor == self.right.flavor)

</t>
<t tx="ekr.20220525082934.1554">def visit_type_var_tuple(self, left: TypeVarTupleType) -&gt; bool:
    return (isinstance(self.right, TypeVarTupleType) and
            left.id == self.right.id)

</t>
<t tx="ekr.20220525082934.1555">def visit_unpack_type(self, left: UnpackType) -&gt; bool:
    return (isinstance(self.right, UnpackType) and
            is_same_type(left.type, self.right.type))

</t>
<t tx="ekr.20220525082934.1556">def visit_parameters(self, left: Parameters) -&gt; bool:
    return (isinstance(self.right, Parameters) and
            left.arg_names == self.right.arg_names and
            is_same_types(left.arg_types, self.right.arg_types) and
            left.arg_kinds == self.right.arg_kinds)

</t>
<t tx="ekr.20220525082934.1557">def visit_callable_type(self, left: CallableType) -&gt; bool:
    # FIX generics
    if isinstance(self.right, CallableType):
        cright = self.right
        return (is_same_type(left.ret_type, cright.ret_type) and
                is_same_types(left.arg_types, cright.arg_types) and
                left.arg_names == cright.arg_names and
                left.arg_kinds == cright.arg_kinds and
                left.is_type_obj() == cright.is_type_obj() and
                left.is_ellipsis_args == cright.is_ellipsis_args)
    else:
        return False

</t>
<t tx="ekr.20220525082934.1558">def visit_tuple_type(self, left: TupleType) -&gt; bool:
    if isinstance(self.right, TupleType):
        return (is_same_type(tuple_fallback(left), tuple_fallback(self.right))
                and is_same_types(left.items, self.right.items))
    else:
        return False

</t>
<t tx="ekr.20220525082934.1559">def visit_typeddict_type(self, left: TypedDictType) -&gt; bool:
    if isinstance(self.right, TypedDictType):
        if left.items.keys() != self.right.items.keys():
            return False
        for (_, left_item_type, right_item_type) in left.zip(self.right):
            if not is_same_type(left_item_type, right_item_type):
                return False
        return True
    else:
        return False

</t>
<t tx="ekr.20220525082934.156">def _extract_argument_name(self, n: ast3.expr) -&gt; Optional[str]:
    if isinstance(n, Str):
        return n.s.strip()
    elif isinstance(n, NameConstant) and str(n.value) == 'None':
        return None
    self.fail('Expected string literal for argument name, got {}'.format(
        type(n).__name__), self.line, 0)
    return None

</t>
<t tx="ekr.20220525082934.1560">def visit_literal_type(self, left: LiteralType) -&gt; bool:
    if isinstance(self.right, LiteralType):
        if left.value != self.right.value:
            return False
        return is_same_type(left.fallback, self.right.fallback)
    else:
        return False

</t>
<t tx="ekr.20220525082934.1561">def visit_union_type(self, left: UnionType) -&gt; bool:
    if isinstance(self.right, UnionType):
        left_lit, left_rem = _extract_literals(left)
        right_lit, right_rem = _extract_literals(self.right)

        if left_lit != right_lit:
            return False

        # Check that everything in left is in right
        for left_item in left_rem:
            if not any(is_same_type(left_item, right_item) for right_item in right_rem):
                return False

        # Check that everything in right is in left
        for right_item in right_rem:
            if not any(is_same_type(right_item, left_item) for left_item in left_rem):
                return False

        return True
    else:
        return False

</t>
<t tx="ekr.20220525082934.1562">def visit_overloaded(self, left: Overloaded) -&gt; bool:
    if isinstance(self.right, Overloaded):
        return is_same_types(left.items, self.right.items)
    else:
        return False

</t>
<t tx="ekr.20220525082934.1563">def visit_partial_type(self, left: PartialType) -&gt; bool:
    # A partial type is not fully defined, so the result is indeterminate. We shouldn't
    # get here.
    raise RuntimeError

</t>
<t tx="ekr.20220525082934.1564">def visit_type_type(self, left: TypeType) -&gt; bool:
    if isinstance(self.right, TypeType):
        return is_same_type(left.item, self.right.item)
    else:
        return False
</t>
<t tx="ekr.20220525082934.1565">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Track current scope to easily calculate the corresponding fine-grained target.

TODO: Use everywhere where we track targets, including in mypy.errors.
"""

from contextlib import contextmanager
from typing import List, Optional, Iterator, Tuple
from typing_extensions import TypeAlias as _TypeAlias

from mypy.backports import nullcontext
from mypy.nodes import TypeInfo, FuncBase


SavedScope: _TypeAlias = Tuple[str, Optional[TypeInfo], Optional[FuncBase]]


@others
</t>
<t tx="ekr.20220525082934.1566">class Scope:
    """Track which target we are processing at any given time."""

    @others
</t>
<t tx="ekr.20220525082934.1567">def __init__(self) -&gt; None:
    self.module: Optional[str] = None
    self.classes: List[TypeInfo] = []
    self.function: Optional[FuncBase] = None
    # Number of nested scopes ignored (that don't get their own separate targets)
    self.ignored = 0

</t>
<t tx="ekr.20220525082934.1568">def current_module_id(self) -&gt; str:
    assert self.module
    return self.module

</t>
<t tx="ekr.20220525082934.1569">def current_target(self) -&gt; str:
    """Return the current target (non-class; for a class return enclosing module)."""
    assert self.module
    if self.function:
        fullname = self.function.fullname
        return fullname or ''
    return self.module

</t>
<t tx="ekr.20220525082934.157">def visit_Name(self, n: Name) -&gt; Type:
    return UnboundType(n.id, line=self.line, column=self.convert_column(n.col_offset))

</t>
<t tx="ekr.20220525082934.1570">def current_full_target(self) -&gt; str:
    """Return the current target (may be a class)."""
    assert self.module
    if self.function:
        return self.function.fullname
    if self.classes:
        return self.classes[-1].fullname
    return self.module

</t>
<t tx="ekr.20220525082934.1571">def current_type_name(self) -&gt; Optional[str]:
    """Return the current type's short name if it exists"""
    return self.classes[-1].name if self.classes else None

</t>
<t tx="ekr.20220525082934.1572">def current_function_name(self) -&gt; Optional[str]:
    """Return the current function's short name if it exists"""
    return self.function.name if self.function else None

</t>
<t tx="ekr.20220525082934.1573">@contextmanager
def module_scope(self, prefix: str) -&gt; Iterator[None]:
    self.module = prefix
    self.classes = []
    self.function = None
    self.ignored = 0
    yield
    assert self.module
    self.module = None

</t>
<t tx="ekr.20220525082934.1574">@contextmanager
def function_scope(self, fdef: FuncBase) -&gt; Iterator[None]:
    if not self.function:
        self.function = fdef
    else:
        # Nested functions are part of the topmost function target.
        self.ignored += 1
    yield
    if self.ignored:
        # Leave a scope that's included in the enclosing target.
        self.ignored -= 1
    else:
        assert self.function
        self.function = None

</t>
<t tx="ekr.20220525082934.1575">def enter_class(self, info: TypeInfo) -&gt; None:
    """Enter a class target scope."""
    if not self.function:
        self.classes.append(info)
    else:
        # Classes within functions are part of the enclosing function target.
        self.ignored += 1

</t>
<t tx="ekr.20220525082934.1576">def leave_class(self) -&gt; None:
    """Leave a class target scope."""
    if self.ignored:
        # Leave a scope that's included in the enclosing target.
        self.ignored -= 1
    else:
        assert self.classes
        # Leave the innermost class.
        self.classes.pop()

</t>
<t tx="ekr.20220525082934.1577">@contextmanager
def class_scope(self, info: TypeInfo) -&gt; Iterator[None]:
    self.enter_class(info)
    yield
    self.leave_class()

</t>
<t tx="ekr.20220525082934.1578">def save(self) -&gt; SavedScope:
    """Produce a saved scope that can be entered with saved_scope()"""
    assert self.module
    # We only save the innermost class, which is sufficient since
    # the rest are only needed for when classes are left.
    cls = self.classes[-1] if self.classes else None
    return self.module, cls, self.function

</t>
<t tx="ekr.20220525082934.1579">@contextmanager
def saved_scope(self, saved: SavedScope) -&gt; Iterator[None]:
    module, info, function = saved
    with self.module_scope(module):
        with self.class_scope(info) if info else nullcontext():
            with self.function_scope(function) if function else nullcontext():
                yield
</t>
<t tx="ekr.20220525082934.158">def visit_BinOp(self, n: ast3.BinOp) -&gt; Type:
    if not isinstance(n.op, ast3.BitOr):
        return self.invalid_type(n)

    left = self.visit(n.left)
    right = self.visit(n.right)
    return UnionType([left, right],
                     line=self.line,
                     column=self.convert_column(n.col_offset),
                     is_evaluated=self.is_evaluated,
                     uses_pep604_syntax=True)

</t>
<t tx="ekr.20220525082934.1580">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""The semantic analyzer.

Bind names to definitions and do various other simple consistency
checks.  Populate symbol tables.  The semantic analyzer also detects
special forms which reuse generic syntax such as NamedTuple and
cast().  Multiple analysis iterations may be needed to analyze forward
references and import cycles. Each iteration "fills in" additional
bindings and references until everything has been bound.

For example, consider this program:

  x = 1
  y = x

Here semantic analysis would detect that the assignment 'x = 1'
defines a new variable, the type of which is to be inferred (in a
later pass; type inference or type checking is not part of semantic
analysis).  Also, it would bind both references to 'x' to the same
module-level variable (Var) node.  The second assignment would also
be analyzed, and the type of 'y' marked as being inferred.

Semantic analysis of types is implemented in typeanal.py.

See semanal_main.py for the top-level logic.

Some important properties:

* After semantic analysis is complete, no PlaceholderNode and
  PlaceholderType instances should remain. During semantic analysis,
  if we encounter one of these, the current target should be deferred.

* A TypeInfo is only created once we know certain basic information about
  a type, such as the MRO, existence of a Tuple base class (e.g., for named
  tuples), and whether we have a TypedDict. We use a temporary
  PlaceholderNode node in the symbol table if some such information is
  missing.

* For assignments, we only add a non-placeholder symbol table entry once
  we know the sort of thing being defined (variable, NamedTuple, type alias,
  etc.).

* Every part of the analysis step must support multiple iterations over
  the same AST nodes, and each iteration must be able to fill in arbitrary
  things that were missing or incomplete in previous iterations.

* Changes performed by the analysis need to be reversible, since mypy
  daemon strips and reuses existing ASTs (to improve performance and/or
  reduce memory use).
"""

from contextlib import contextmanager

from typing import (
    Any, List, Dict, Set, Tuple, cast, TypeVar, Union, Optional, Callable, Iterator, Iterable
)
from typing_extensions import Final, TypeAlias as _TypeAlias

from mypy.nodes import (
    AssertTypeExpr, MypyFile, TypeInfo, Node, AssignmentStmt, FuncDef, OverloadedFuncDef,
    ClassDef, Var, GDEF, FuncItem, Import, Expression, Lvalue,
    ImportFrom, ImportAll, Block, LDEF, NameExpr, MemberExpr,
    IndexExpr, TupleExpr, ListExpr, ExpressionStmt, ReturnStmt,
    RaiseStmt, AssertStmt, OperatorAssignmentStmt, WhileStmt,
    ForStmt, BreakStmt, ContinueStmt, IfStmt, TryStmt, WithStmt, DelStmt,
    GlobalDecl, SuperExpr, DictExpr, CallExpr, RefExpr, OpExpr, UnaryExpr,
    SliceExpr, CastExpr, RevealExpr, TypeApplication, Context, SymbolTable,
    SymbolTableNode, ListComprehension, GeneratorExpr,
    LambdaExpr, MDEF, Decorator, SetExpr, TypeVarExpr,
    StrExpr, BytesExpr, PrintStmt, ConditionalExpr, PromoteExpr,
    ComparisonExpr, StarExpr, ArgKind, ARG_POS, ARG_NAMED, type_aliases,
    YieldFromExpr, NamedTupleExpr, NonlocalDecl, SymbolNode,
    SetComprehension, DictionaryComprehension, TypeAlias, TypeAliasExpr,
    YieldExpr, ExecStmt, BackquoteExpr, ImportBase, AwaitExpr,
    IntExpr, FloatExpr, UnicodeExpr, TempNode, OverloadPart,
    PlaceholderNode, COVARIANT, CONTRAVARIANT, INVARIANT,
    get_nongen_builtins, get_member_expr_fullname, REVEAL_TYPE,
    REVEAL_LOCALS, is_final_node, TypedDictExpr, type_aliases_source_versions,
    typing_extensions_aliases,
    EnumCallExpr, RUNTIME_PROTOCOL_DECOS, FakeExpression, Statement, AssignmentExpr,
    ParamSpecExpr, EllipsisExpr, TypeVarLikeExpr, implicit_module_attrs,
    MatchStmt, FuncBase, TypeVarTupleExpr
)
from mypy.patterns import (
    AsPattern, OrPattern, ValuePattern, SequencePattern,
    StarredPattern, MappingPattern, ClassPattern,
)
from mypy.tvar_scope import TypeVarLikeScope
from mypy.typevars import fill_typevars
from mypy.visitor import NodeVisitor
from mypy.errors import Errors, report_internal_error
from mypy.messages import (
    best_matches, MessageBuilder, pretty_seq, SUGGESTED_TEST_FIXTURES, TYPES_FOR_UNIMPORTED_HINTS
)
from mypy.errorcodes import ErrorCode
from mypy import message_registry, errorcodes as codes
from mypy.types import (
    NEVER_NAMES, FunctionLike, UnboundType, TypeVarType, TupleType, UnionType, StarType,
    CallableType, Overloaded, Instance, Type, AnyType, LiteralType, LiteralValue,
    TypeTranslator, TypeOfAny, TypeType, NoneType, PlaceholderType, TPDICT_NAMES, ProperType,
    get_proper_type, get_proper_types, TypeAliasType, TypeVarLikeType, Parameters, ParamSpecType,
    PROTOCOL_NAMES, TYPE_ALIAS_NAMES, FINAL_TYPE_NAMES, FINAL_DECORATOR_NAMES, REVEAL_TYPE_NAMES,
    ASSERT_TYPE_NAMES, OVERLOAD_NAMES, is_named_instance,
)
from mypy.typeops import function_type, get_type_vars
from mypy.type_visitor import TypeQuery
from mypy.typeanal import (
    TypeAnalyser, analyze_type_alias, no_subscript_builtin_alias,
    TypeVarLikeQuery, TypeVarLikeList, remove_dups, has_any_from_unimported_type,
    check_for_explicit_any, type_constructors, fix_instance_types
)
from mypy.exprtotype import expr_to_unanalyzed_type, TypeTranslationError
from mypy.options import Options
from mypy.plugin import (
    Plugin, ClassDefContext, SemanticAnalyzerPluginInterface,
    DynamicClassDefContext
)
from mypy.util import (
    correct_relative_import, unmangle, module_prefix, is_typeshed_file, unnamed_function,
    is_dunder,
)
from mypy.scope import Scope
from mypy.semanal_shared import (
    SemanticAnalyzerInterface, set_callable_name, calculate_tuple_fallback, PRIORITY_FALLBACKS
)
from mypy.semanal_namedtuple import NamedTupleAnalyzer
from mypy.semanal_typeddict import TypedDictAnalyzer
from mypy.semanal_enum import EnumCallAnalyzer
from mypy.semanal_newtype import NewTypeAnalyzer
from mypy.reachability import (
    infer_reachability_of_if_statement, infer_reachability_of_match_statement,
    infer_condition_value, ALWAYS_FALSE, ALWAYS_TRUE, MYPY_TRUE, MYPY_FALSE
)
from mypy.mro import calculate_mro, MroError

T = TypeVar('T')


FUTURE_IMPORTS: Final = {
    '__future__.nested_scopes': 'nested_scopes',
    '__future__.generators': 'generators',
    '__future__.division': 'division',
    '__future__.absolute_import': 'absolute_import',
    '__future__.with_statement': 'with_statement',
    '__future__.print_function': 'print_function',
    '__future__.unicode_literals': 'unicode_literals',
    '__future__.barry_as_FLUFL': 'barry_as_FLUFL',
    '__future__.generator_stop': 'generator_stop',
    '__future__.annotations': 'annotations',
}


# Special cased built-in classes that are needed for basic functionality and need to be
# available very early on.
CORE_BUILTIN_CLASSES: Final = ["object", "bool", "function"]

# Subclasses can override these Var attributes with incompatible types. This can also be
# set for individual attributes using 'allow_incompatible_override' of Var.
ALLOW_INCOMPATIBLE_OVERRIDE: Final = ('__slots__', '__deletable__', '__match_args__')


# Used for tracking incomplete references
Tag: _TypeAlias = int


@others
</t>
<t tx="ekr.20220525082934.159">def visit_NameConstant(self, n: NameConstant) -&gt; Type:
    if isinstance(n.value, bool):
        return RawExpressionType(n.value, 'builtins.bool', line=self.line)
    else:
        return UnboundType(str(n.value), line=self.line, column=n.col_offset)

</t>
<t tx="ekr.20220525082934.16">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; Type:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.160"># Only for 3.8 and newer
def visit_Constant(self, n: Constant) -&gt; Type:
    val = n.value
    if val is None:
        # None is a type.
        return UnboundType('None', line=self.line)
    if isinstance(val, str):
        # Parse forward reference.
        if (n.kind and 'u' in n.kind) or self.assume_str_is_unicode:
            return parse_type_string(n.s, 'builtins.unicode', self.line, n.col_offset,
                                     assume_str_is_unicode=self.assume_str_is_unicode)
        else:
            return parse_type_string(n.s, 'builtins.str', self.line, n.col_offset,
                                     assume_str_is_unicode=self.assume_str_is_unicode)
    if val is Ellipsis:
        # '...' is valid in some types.
        return EllipsisType(line=self.line)
    if isinstance(val, bool):
        # Special case for True/False.
        return RawExpressionType(val, 'builtins.bool', line=self.line)
    if isinstance(val, (int, float, complex)):
        return self.numeric_type(val, n)
    if isinstance(val, bytes):
        contents = bytes_to_human_readable_repr(val)
        return RawExpressionType(contents, 'builtins.bytes', self.line, column=n.col_offset)
    # Everything else is invalid.
    return self.invalid_type(n)

</t>
<t tx="ekr.20220525082934.161"># UnaryOp(op, operand)
def visit_UnaryOp(self, n: UnaryOp) -&gt; Type:
    # We support specifically Literal[-4] and nothing else.
    # For example, Literal[+4] or Literal[~6] is not supported.
    typ = self.visit(n.operand)
    if isinstance(typ, RawExpressionType) and isinstance(n.op, USub):
        if isinstance(typ.literal_value, int):
            typ.literal_value *= -1
            return typ
    return self.invalid_type(n)

</t>
<t tx="ekr.20220525082934.162">def numeric_type(self, value: object, n: AST) -&gt; Type:
    # The node's field has the type complex, but complex isn't *really*
    # a parent of int and float, and this causes isinstance below
    # to think that the complex branch is always picked. Avoid
    # this by throwing away the type.
    if isinstance(value, int):
        numeric_value: Optional[int] = value
        type_name = 'builtins.int'
    else:
        # Other kinds of numbers (floats, complex) are not valid parameters for
        # RawExpressionType so we just pass in 'None' for now. We'll report the
        # appropriate error at a later stage.
        numeric_value = None
        type_name = f'builtins.{type(value).__name__}'
    return RawExpressionType(
        numeric_value,
        type_name,
        line=self.line,
        column=getattr(n, 'col_offset', -1),
    )

</t>
<t tx="ekr.20220525082934.163"># These next three methods are only used if we are on python &lt;
# 3.8, using typed_ast.  They are defined unconditionally because
# mypyc can't handle conditional method definitions.

</t>
<t tx="ekr.20220525082934.164"># Num(number n)
def visit_Num(self, n: Num) -&gt; Type:
    return self.numeric_type(n.n, n)

</t>
<t tx="ekr.20220525082934.165"># Str(string s)
def visit_Str(self, n: Str) -&gt; Type:
    # Note: we transform these fallback types into the correct types in
    # 'typeanal.py' -- specifically in the named_type_with_normalized_str method.
    # If we're analyzing Python 3, that function will translate 'builtins.unicode'
    # into 'builtins.str'. In contrast, if we're analyzing Python 2 code, we'll
    # translate 'builtins.bytes' in the method below into 'builtins.str'.

    # Do a getattr because the field doesn't exist in 3.8 (where
    # this method doesn't actually ever run.) We can't just do
    # an attribute access with a `# type: ignore` because it would be
    # unused on &lt; 3.8.
    kind: str = getattr(n, "kind")  # noqa

    if 'u' in kind or self.assume_str_is_unicode:
        return parse_type_string(n.s, 'builtins.unicode', self.line, n.col_offset,
                                 assume_str_is_unicode=self.assume_str_is_unicode)
    else:
        return parse_type_string(n.s, 'builtins.str', self.line, n.col_offset,
                                 assume_str_is_unicode=self.assume_str_is_unicode)

</t>
<t tx="ekr.20220525082934.166"># Bytes(bytes s)
def visit_Bytes(self, n: Bytes) -&gt; Type:
    contents = bytes_to_human_readable_repr(n.s)
    return RawExpressionType(contents, 'builtins.bytes', self.line, column=n.col_offset)

</t>
<t tx="ekr.20220525082934.167">def visit_Index(self, n: ast3.Index) -&gt; Type:
    # cast for mypyc's benefit on Python 3.9
    return self.visit(cast(Any, n).value)

</t>
<t tx="ekr.20220525082934.168">def visit_Slice(self, n: ast3.Slice) -&gt; Type:
    return self.invalid_type(
        n, note="did you mean to use ',' instead of ':' ?"
    )

</t>
<t tx="ekr.20220525082934.169"># Subscript(expr value, slice slice, expr_context ctx)  # Python 3.8 and before
# Subscript(expr value, expr slice, expr_context ctx)  # Python 3.9 and later
def visit_Subscript(self, n: ast3.Subscript) -&gt; Type:
    if sys.version_info &gt;= (3, 9):  # Really 3.9a5 or later
        sliceval: Any = n.slice
    # Python 3.8 or earlier use a different AST structure for subscripts
    elif isinstance(n.slice, ast3.Index):
        sliceval: Any = n.slice.value
    elif isinstance(n.slice, ast3.Slice):
        sliceval = copy.deepcopy(n.slice)  # so we don't mutate passed AST
        if getattr(sliceval, "col_offset", None) is None:
            # Fix column information so that we get Python 3.9+ message order
            sliceval.col_offset = sliceval.lower.col_offset
    else:
        assert isinstance(n.slice, ast3.ExtSlice)
        dims = copy.deepcopy(n.slice.dims)
        for s in dims:
            if getattr(s, "col_offset", None) is None:
                if isinstance(s, ast3.Index):
                    s.col_offset = s.value.col_offset  # type: ignore
                elif isinstance(s, ast3.Slice):
                    s.col_offset = s.lower.col_offset  # type: ignore
        sliceval = ast3.Tuple(dims, n.ctx)

    empty_tuple_index = False
    if isinstance(sliceval, ast3.Tuple):
        params = self.translate_expr_list(sliceval.elts)
        if len(sliceval.elts) == 0:
            empty_tuple_index = True
    else:
        params = [self.visit(sliceval)]

    value = self.visit(n.value)
    if isinstance(value, UnboundType) and not value.args:
        return UnboundType(value.name, params, line=self.line, column=value.column,
                           empty_tuple_index=empty_tuple_index)
    else:
        return self.invalid_type(n)

</t>
<t tx="ekr.20220525082934.17">def visit_unpack_type(self, t: UnpackType) -&gt; Type:
    # It is impossible to reasonally implement visit_unpack_type, because
    # unpacking inherently expands to something more like a list of types.
    #
    # Relevant sections that can call unpack should call expand_unpack()
    # instead.
    assert False, "Mypy bug: unpacking must happen at a higher level"

</t>
<t tx="ekr.20220525082934.170">def visit_Tuple(self, n: ast3.Tuple) -&gt; Type:
    return TupleType(self.translate_expr_list(n.elts), _dummy_fallback,
                     implicit=True, line=self.line, column=self.convert_column(n.col_offset))

</t>
<t tx="ekr.20220525082934.171"># Attribute(expr value, identifier attr, expr_context ctx)
def visit_Attribute(self, n: Attribute) -&gt; Type:
    before_dot = self.visit(n.value)

    if isinstance(before_dot, UnboundType) and not before_dot.args:
        return UnboundType(f"{before_dot.name}.{n.attr}", line=self.line)
    else:
        return self.invalid_type(n)

</t>
<t tx="ekr.20220525082934.172"># Ellipsis
def visit_Ellipsis(self, n: ast3_Ellipsis) -&gt; Type:
    return EllipsisType(line=self.line)

</t>
<t tx="ekr.20220525082934.173"># List(expr* elts, expr_context ctx)
def visit_List(self, n: ast3.List) -&gt; Type:
    assert isinstance(n.ctx, ast3.Load)
    return self.translate_argument_list(n.elts)


</t>
<t tx="ekr.20220525082934.174">def stringify_name(n: AST) -&gt; Optional[str]:
    if isinstance(n, Name):
        return n.id
    elif isinstance(n, Attribute):
        sv = stringify_name(n.value)
        if sv is not None:
            return f"{sv}.{n.attr}"
    return None  # Can't do it.
</t>
<t tx="ekr.20220525082934.175">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""
This file is nearly identical to `fastparse.py`, except that it works with a Python 2
AST instead of a Python 3 AST.

Previously, how we handled Python 2 code was by first obtaining the Python 2 AST via
typed_ast, converting it into a Python 3 AST by using typed_ast.conversion, then
running it through mypy.fastparse.

While this worked, it did add some overhead, especially in larger Python 2 codebases.
This module allows us to skip the conversion step, saving us some time.

The reason why this file is not easily merged with mypy.fastparse despite the large amount
of redundancy is because the Python 2 AST and the Python 3 AST nodes belong to two completely
different class hierarchies, which made it difficult to write a shared visitor between the
two in a typesafe way.
"""
from mypy.util import unnamed_function
import sys
import warnings

import typing  # for typing.Type, which conflicts with types.Type
from typing import Tuple, Union, TypeVar, Callable, Sequence, Optional, Any, Dict, cast, List
from typing_extensions import Final, Literal

from mypy.sharedparse import (
    special_function_elide_names, argument_elide_name,
)
from mypy.nodes import (
    MypyFile, Node, ImportBase, Import, ImportAll, ImportFrom, FuncDef, OverloadedFuncDef,
    ClassDef, Decorator, Block, Var, OperatorAssignmentStmt,
    ExpressionStmt, AssignmentStmt, ReturnStmt, RaiseStmt, AssertStmt,
    DelStmt, BreakStmt, ContinueStmt, PassStmt, GlobalDecl,
    WhileStmt, ForStmt, IfStmt, TryStmt, WithStmt,
    TupleExpr, GeneratorExpr, ListComprehension, ListExpr, ConditionalExpr,
    DictExpr, SetExpr, NameExpr, IntExpr, StrExpr, UnicodeExpr,
    FloatExpr, CallExpr, SuperExpr, MemberExpr, IndexExpr, SliceExpr, OpExpr,
    UnaryExpr, LambdaExpr, ComparisonExpr, DictionaryComprehension,
    SetComprehension, ComplexExpr, EllipsisExpr, YieldExpr, Argument,
    Expression, Statement, BackquoteExpr, PrintStmt, ExecStmt,
    ArgKind, ARG_POS, ARG_OPT, ARG_STAR, ARG_NAMED, ARG_STAR2, OverloadPart, check_arg_names,
    FakeInfo,
)
from mypy.types import (
    Type, CallableType, AnyType, UnboundType, EllipsisType, TypeOfAny, Instance,
    ProperType
)
from mypy import message_registry, errorcodes as codes
from mypy.errors import Errors
from mypy.fastparse import (
    TypeConverter, parse_type_comment, parse_type_ignore_tag,
    TYPE_IGNORE_PATTERN, INVALID_TYPE_IGNORE
)
from mypy.options import Options
from mypy.util import bytes_to_human_readable_repr
from mypy.reachability import mark_block_unreachable

try:
    from typed_ast import ast27
    from typed_ast.ast27 import (
        AST,
        Call,
        Name,
        Attribute,
        Tuple as ast27_Tuple,
    )
    # Import ast3 from fastparse, which has special case for Python 3.8
    from mypy.fastparse import ast3, ast3_parse
except ImportError:
    try:
        from typed_ast import ast35  # type: ignore[attr-defined]  # noqa: F401
    except ImportError:
        print('The typed_ast package is not installed.\n'
              'For Python 2 support, install mypy using `python3 -m pip install "mypy[python2]"`'
              'Alternatively, you can install typed_ast with `python3 -m pip install typed-ast`.',
              file=sys.stderr)
    else:
        print('You need a more recent version of the typed_ast package.\n'
              'You can update to the latest version with '
              '`python3 -m pip install -U typed-ast`.',
              file=sys.stderr)
    sys.exit(1)

N = TypeVar('N', bound=Node)

# There is no way to create reasonable fallbacks at this stage,
# they must be patched later.
MISSING_FALLBACK: Final = FakeInfo("fallback can't be filled out until semanal")
_dummy_fallback: Final = Instance(MISSING_FALLBACK, [], -1)

TYPE_COMMENT_SYNTAX_ERROR: Final = "syntax error in type comment"
TYPE_COMMENT_AST_ERROR: Final = "invalid type comment"


@others
</t>
<t tx="ekr.20220525082934.176">def parse(source: Union[str, bytes],
          fnam: str,
          module: Optional[str],
          errors: Optional[Errors] = None,
          options: Optional[Options] = None) -&gt; MypyFile:
    """Parse a source file, without doing any semantic analysis.

    Return the parse tree. If errors is not provided, raise ParseError
    on failure. Otherwise, use the errors object to report parse errors.
    """
    raise_on_error = False
    if errors is None:
        errors = Errors()
        raise_on_error = True
    if options is None:
        options = Options()
    errors.set_file(fnam, module)
    is_stub_file = fnam.endswith('.pyi')
    try:
        assert options.python_version[0] &lt; 3 and not is_stub_file
        # Disable deprecation warnings about &lt;&gt;.
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=DeprecationWarning)
            ast = ast27.parse(source, fnam, 'exec')
        tree = ASTConverter(options=options,
                            errors=errors,
                            ).visit(ast)
        assert isinstance(tree, MypyFile)
        tree.path = fnam
        tree.is_stub = is_stub_file
    except SyntaxError as e:
        errors.report(e.lineno if e.lineno is not None else -1, e.offset, e.msg, blocker=True,
                      code=codes.SYNTAX)
        tree = MypyFile([], [], False, {})

    if raise_on_error and errors.is_errors():
        errors.raise_error()

    return tree


</t>
<t tx="ekr.20220525082934.177">def is_no_type_check_decorator(expr: ast27.expr) -&gt; bool:
    if isinstance(expr, Name):
        return expr.id == 'no_type_check'
    elif isinstance(expr, Attribute):
        if isinstance(expr.value, Name):
            return expr.value.id == 'typing' and expr.attr == 'no_type_check'
    return False


</t>
<t tx="ekr.20220525082934.178">class ASTConverter:
    @others
</t>
<t tx="ekr.20220525082934.179">def __init__(self,
             options: Options,
             errors: Errors) -&gt; None:
    # 'C' for class, 'F' for function
    self.class_and_function_stack: List[Literal["C", "F"]] = []
    self.imports: List[ImportBase] = []

    self.options = options
    self.errors = errors

    # Indicates whether this file is being parsed with unicode_literals enabled.
    # Note: typed_ast already naturally takes unicode_literals into account when
    # parsing so we don't have to worry when analyzing strings within this class.
    #
    # The only place where we use this field is when we call fastparse's TypeConverter
    # and any related methods. That class accepts a Python 3 AST instead of a Python 2
    # AST: as a result, it don't special-case the `unicode_literals` import and won't know
    # exactly whether to parse some string as bytes or unicode.
    #
    # This distinction is relevant mostly when handling Literal types -- Literal[u"foo"]
    # is not the same type as Literal[b"foo"], and Literal["foo"] could mean either the
    # former or the latter based on context.
    #
    # This field is set in the 'visit_ImportFrom' method: it's ok to delay computing it
    # because any `from __future__ import blah` import must be located at the top of the
    # file, with the exception of the docstring. This means we're guaranteed to correctly
    # set this field before we encounter any type hints.
    self.unicode_literals = False

    # Cache of visit_X methods keyed by type of visited object
    self.visitor_cache: Dict[type, Callable[[Optional[AST]], Any]] = {}

    self.type_ignores: Dict[int, List[str]] = {}

</t>
<t tx="ekr.20220525082934.18">def expand_unpack(self, t: UnpackType) -&gt; Optional[Union[List[Type], Instance, AnyType]]:
    """May return either a list of types to unpack to, any, or a single
    variable length tuple. The latter may not be valid in all contexts.
    """
    proper_typ = get_proper_type(t.type)
    if isinstance(proper_typ, TypeVarTupleType):
        repl = get_proper_type(self.variables.get(proper_typ.id, t))
        if isinstance(repl, TupleType):
            return repl.items
        elif isinstance(repl, Instance) and repl.type.fullname == "builtins.tuple":
            return repl
        elif isinstance(repl, AnyType):
            # tuple[Any, ...] would be better, but we don't have
            # the type info to construct that type here.
            return repl
        elif isinstance(repl, TypeVarTupleType):
            return [UnpackType(typ=repl)]
        elif isinstance(repl, UninhabitedType):
            return None
        else:
            raise NotImplementedError(f"Invalid type to expand: {repl}")
    else:
        raise NotImplementedError

</t>
<t tx="ekr.20220525082934.180">def fail(self, msg: str, line: int, column: int, blocker: bool = True) -&gt; None:
    if blocker or not self.options.ignore_errors:
        self.errors.report(line, column, msg, blocker=blocker, code=codes.SYNTAX)

</t>
<t tx="ekr.20220525082934.181">def visit(self, node: Optional[AST]) -&gt; Any:  # same as in typed_ast stub
    if node is None:
        return None
    typeobj = type(node)
    visitor = self.visitor_cache.get(typeobj)
    if visitor is None:
        method = 'visit_' + node.__class__.__name__
        visitor = getattr(self, method)
        self.visitor_cache[typeobj] = visitor
    return visitor(node)

</t>
<t tx="ekr.20220525082934.182">def set_line(self, node: N, n: Union[ast27.expr, ast27.stmt, ast27.ExceptHandler]) -&gt; N:
    node.line = n.lineno
    node.column = n.col_offset
    return node

</t>
<t tx="ekr.20220525082934.183">def translate_expr_list(self, l: Sequence[AST]) -&gt; List[Expression]:
    res: List[Expression] = []
    for e in l:
        exp = self.visit(e)
        assert isinstance(exp, Expression)
        res.append(exp)
    return res

</t>
<t tx="ekr.20220525082934.184">def get_lineno(self, node: Union[ast27.expr, ast27.stmt]) -&gt; int:
    if isinstance(node, (ast27.ClassDef, ast27.FunctionDef)) and node.decorator_list:
        return node.decorator_list[0].lineno
    return node.lineno

</t>
<t tx="ekr.20220525082934.185">def translate_stmt_list(self,
                        stmts: Sequence[ast27.stmt],
                        module: bool = False) -&gt; List[Statement]:
    # A "# type: ignore" comment before the first statement of a module
    # ignores the whole module:
    if (module and stmts and self.type_ignores
            and min(self.type_ignores) &lt; self.get_lineno(stmts[0])):
        self.errors.used_ignored_lines[self.errors.file][min(self.type_ignores)].append(
            codes.FILE.code)
        block = Block(self.fix_function_overloads(self.translate_stmt_list(stmts)))
        mark_block_unreachable(block)
        return [block]

    res: List[Statement] = []
    for stmt in stmts:
        node = self.visit(stmt)
        assert isinstance(node, Statement)
        res.append(node)
    return res

</t>
<t tx="ekr.20220525082934.186">def translate_type_comment(self, n: ast27.stmt,
                           type_comment: Optional[str]) -&gt; Optional[ProperType]:
    if type_comment is None:
        return None
    else:
        lineno = n.lineno
        extra_ignore, typ = parse_type_comment(type_comment,
                                               lineno,
                                               n.col_offset,
                                               self.errors,
                                               assume_str_is_unicode=self.unicode_literals)
        if extra_ignore is not None:
            self.type_ignores[lineno] = extra_ignore
        return typ

</t>
<t tx="ekr.20220525082934.187">op_map: Final[Dict[typing.Type[AST], str]] = {
    ast27.Add: '+',
    ast27.Sub: '-',
    ast27.Mult: '*',
    ast27.Div: '/',
    ast27.Mod: '%',
    ast27.Pow: '**',
    ast27.LShift: '&lt;&lt;',
    ast27.RShift: '&gt;&gt;',
    ast27.BitOr: '|',
    ast27.BitXor: '^',
    ast27.BitAnd: '&amp;',
    ast27.FloorDiv: '//'
}

</t>
<t tx="ekr.20220525082934.188">def from_operator(self, op: ast27.operator) -&gt; str:
    op_name = ASTConverter.op_map.get(type(op))
    if op_name is None:
        raise RuntimeError('Unknown operator ' + str(type(op)))
    elif op_name == '@':
        raise RuntimeError('mypy does not support the MatMult operator')
    else:
        return op_name

</t>
<t tx="ekr.20220525082934.189">comp_op_map: Final[Dict[typing.Type[AST], str]] = {
    ast27.Gt: '&gt;',
    ast27.Lt: '&lt;',
    ast27.Eq: '==',
    ast27.GtE: '&gt;=',
    ast27.LtE: '&lt;=',
    ast27.NotEq: '!=',
    ast27.Is: 'is',
    ast27.IsNot: 'is not',
    ast27.In: 'in',
    ast27.NotIn: 'not in'
}

</t>
<t tx="ekr.20220525082934.19">def visit_parameters(self, t: Parameters) -&gt; Type:
    return t.copy_modified(arg_types=self.expand_types(t.arg_types))

</t>
<t tx="ekr.20220525082934.190">def from_comp_operator(self, op: ast27.cmpop) -&gt; str:
    op_name = ASTConverter.comp_op_map.get(type(op))
    if op_name is None:
        raise RuntimeError('Unknown comparison operator ' + str(type(op)))
    else:
        return op_name

</t>
<t tx="ekr.20220525082934.191">def as_block(self, stmts: List[ast27.stmt], lineno: int) -&gt; Optional[Block]:
    b = None
    if stmts:
        b = Block(self.fix_function_overloads(self.translate_stmt_list(stmts)))
        b.set_line(lineno)
    return b

</t>
<t tx="ekr.20220525082934.192">def as_required_block(self, stmts: List[ast27.stmt], lineno: int) -&gt; Block:
    assert stmts  # must be non-empty
    b = Block(self.fix_function_overloads(self.translate_stmt_list(stmts)))
    b.set_line(lineno)
    return b

</t>
<t tx="ekr.20220525082934.193">def fix_function_overloads(self, stmts: List[Statement]) -&gt; List[Statement]:
    ret: List[Statement] = []
    current_overload: List[OverloadPart] = []
    current_overload_name: Optional[str] = None
    for stmt in stmts:
        if (current_overload_name is not None
                and isinstance(stmt, (Decorator, FuncDef))
                and stmt.name == current_overload_name):
            current_overload.append(stmt)
        else:
            if len(current_overload) == 1:
                ret.append(current_overload[0])
            elif len(current_overload) &gt; 1:
                ret.append(OverloadedFuncDef(current_overload))

            if isinstance(stmt, Decorator) and not unnamed_function(stmt.name):
                current_overload = [stmt]
                current_overload_name = stmt.name
            else:
                current_overload = []
                current_overload_name = None
                ret.append(stmt)

    if len(current_overload) == 1:
        ret.append(current_overload[0])
    elif len(current_overload) &gt; 1:
        ret.append(OverloadedFuncDef(current_overload))
    return ret

</t>
<t tx="ekr.20220525082934.194">def in_method_scope(self) -&gt; bool:
    return self.class_and_function_stack[-2:] == ['C', 'F']

</t>
<t tx="ekr.20220525082934.195">def translate_module_id(self, id: str) -&gt; str:
    """Return the actual, internal module id for a source text id.

    For example, translate '__builtin__' in Python 2 to 'builtins'.
    """
    if id == self.options.custom_typing_module:
        return 'typing'
    elif id == '__builtin__':
        # HACK: __builtin__ in Python 2 is aliases to builtins. However, the implementation
        #   is named __builtin__.py (there is another layer of translation elsewhere).
        return 'builtins'
    return id

</t>
<t tx="ekr.20220525082934.196">def visit_Module(self, mod: ast27.Module) -&gt; MypyFile:
    self.type_ignores = {}
    for ti in mod.type_ignores:
        parsed = parse_type_ignore_tag(ti.tag)  # type: ignore[attr-defined]
        if parsed is not None:
            self.type_ignores[ti.lineno] = parsed
        else:
            self.fail(INVALID_TYPE_IGNORE, ti.lineno, -1)
    body = self.fix_function_overloads(self.translate_stmt_list(mod.body, module=True))
    return MypyFile(body,
                    self.imports,
                    False,
                    self.type_ignores,
                    )

</t>
<t tx="ekr.20220525082934.197"># --- stmt ---
# FunctionDef(identifier name, arguments args,
#             stmt* body, expr*    decorator_list, expr? returns, string? type_comment)
# arguments = (arg* args, arg? vararg, arg* kwonlyargs, expr* kw_defaults,
#              arg? kwarg, expr* defaults)
def visit_FunctionDef(self, n: ast27.FunctionDef) -&gt; Statement:
    self.class_and_function_stack.append('F')
    lineno = n.lineno
    converter = TypeConverter(self.errors, line=lineno, override_column=n.col_offset,
                              assume_str_is_unicode=self.unicode_literals)
    args, decompose_stmts = self.transform_args(n.args, lineno)
    if special_function_elide_names(n.name):
        for arg in args:
            arg.pos_only = True

    arg_kinds = [arg.kind for arg in args]
    arg_names = [None if arg.pos_only else arg.variable.name for arg in args]

    arg_types: List[Optional[Type]] = []
    type_comment = n.type_comment
    if (n.decorator_list and any(is_no_type_check_decorator(d) for d in n.decorator_list)):
        arg_types = [None] * len(args)
        return_type = None
    elif type_comment is not None and len(type_comment) &gt; 0:
        try:
            func_type_ast = ast3_parse(type_comment, '&lt;func_type&gt;', 'func_type')
            assert isinstance(func_type_ast, ast3.FunctionType)
            # for ellipsis arg
            if (len(func_type_ast.argtypes) == 1 and
                    isinstance(func_type_ast.argtypes[0], ast3.Ellipsis)):
                arg_types = [a.type_annotation
                             if a.type_annotation is not None
                             else AnyType(TypeOfAny.unannotated)
                             for a in args]
            else:
                # PEP 484 disallows both type annotations and type comments
                if any(a.type_annotation is not None for a in args):
                    self.fail(message_registry.DUPLICATE_TYPE_SIGNATURES, lineno, n.col_offset)
                arg_types = [a if a is not None else AnyType(TypeOfAny.unannotated) for
                             a in converter.translate_expr_list(func_type_ast.argtypes)]
            return_type = converter.visit(func_type_ast.returns)

            # add implicit self type
            if self.in_method_scope() and len(arg_types) &lt; len(args):
                arg_types.insert(0, AnyType(TypeOfAny.special_form))
        except SyntaxError:
            stripped_type = type_comment.split("#", 2)[0].strip()
            err_msg = f'{TYPE_COMMENT_SYNTAX_ERROR} "{stripped_type}"'
            self.fail(err_msg, lineno, n.col_offset)
            arg_types = [AnyType(TypeOfAny.from_error)] * len(args)
            return_type = AnyType(TypeOfAny.from_error)
    else:
        arg_types = [a.type_annotation for a in args]
        return_type = converter.visit(None)

    for arg, arg_type in zip(args, arg_types):
        self.set_type_optional(arg_type, arg.initializer)

    func_type = None
    if any(arg_types) or return_type:
        if len(arg_types) != 1 and any(isinstance(t, EllipsisType)
                                       for t in arg_types):
            self.fail("Ellipses cannot accompany other argument types "
                      "in function type signature", lineno, n.col_offset)
        elif len(arg_types) &gt; len(arg_kinds):
            self.fail('Type signature has too many arguments', lineno, n.col_offset,
                      blocker=False)
        elif len(arg_types) &lt; len(arg_kinds):
            self.fail('Type signature has too few arguments', lineno, n.col_offset,
                      blocker=False)
        else:
            any_type = AnyType(TypeOfAny.unannotated)
            func_type = CallableType([a if a is not None else any_type for a in arg_types],
                                    arg_kinds,
                                    arg_names,
                                    return_type if return_type is not None else any_type,
                                    _dummy_fallback)

    body = self.as_required_block(n.body, lineno)
    if decompose_stmts:
        body.body = decompose_stmts + body.body
    func_def = FuncDef(n.name,
                   args,
                   body,
                   func_type)
    if isinstance(func_def.type, CallableType):
        # semanal.py does some in-place modifications we want to avoid
        func_def.unanalyzed_type = func_def.type.copy_modified()
    if func_type is not None:
        func_type.definition = func_def
        func_type.line = lineno

    if n.decorator_list:
        var = Var(func_def.name)
        var.is_ready = False
        var.set_line(n.decorator_list[0].lineno)

        func_def.is_decorated = True
        func_def.set_line(lineno + len(n.decorator_list))
        func_def.body.set_line(func_def.get_line())
        dec = Decorator(func_def, self.translate_expr_list(n.decorator_list), var)
        dec.set_line(lineno, n.col_offset)
        retval: Statement = dec
    else:
        # Overrides set_line -- can't use self.set_line
        func_def.set_line(lineno, n.col_offset)
        retval = func_def
    self.class_and_function_stack.pop()
    return retval

</t>
<t tx="ekr.20220525082934.198">def set_type_optional(self, type: Optional[Type], initializer: Optional[Expression]) -&gt; None:
    if self.options.no_implicit_optional:
        return
    # Indicate that type should be wrapped in an Optional if arg is initialized to None.
    optional = isinstance(initializer, NameExpr) and initializer.name == 'None'
    if isinstance(type, UnboundType):
        type.optional = optional

</t>
<t tx="ekr.20220525082934.199">def transform_args(self,
                   n: ast27.arguments,
                   line: int,
                   ) -&gt; Tuple[List[Argument], List[Statement]]:
    type_comments: Sequence[Optional[str]] = n.type_comments
    converter = TypeConverter(self.errors, line=line,
                              assume_str_is_unicode=self.unicode_literals)
    decompose_stmts: List[Statement] = []

    n_args = n.args
    args = [(self.convert_arg(i, arg, line, decompose_stmts),
             self.get_type(i, type_comments, converter))
            for i, arg in enumerate(n_args)]
    defaults = self.translate_expr_list(n.defaults)
    names: List[str] = [name for arg in n_args for name in self.extract_names(arg)]

    new_args: List[Argument] = []
    num_no_defaults = len(args) - len(defaults)
    # positional arguments without defaults
    for a, annotation in args[:num_no_defaults]:
        new_args.append(Argument(a, annotation, None, ARG_POS))

    # positional arguments with defaults
    for (a, annotation), d in zip(args[num_no_defaults:], defaults):
        new_args.append(Argument(a, annotation, d, ARG_OPT))

    # *arg
    if n.vararg is not None:
        new_args.append(Argument(Var(n.vararg),
                                 self.get_type(len(args), type_comments, converter),
                                 None,
                                 ARG_STAR))
        names.append(n.vararg)

    # **kwarg
    if n.kwarg is not None:
        typ = self.get_type(len(args) + (0 if n.vararg is None else 1),
                            type_comments,
                            converter)
        new_args.append(Argument(Var(n.kwarg), typ, None, ARG_STAR2))
        names.append(n.kwarg)

    for arg in new_args:
        if argument_elide_name(arg.variable.name):
            arg.pos_only = True

    # We don't have any context object to give, but we have closed around the line num
    def fail_arg(msg: str, arg: None) -&gt; None:
        self.fail(msg, line, 0)
    check_arg_names(names, [None] * len(names), fail_arg)

    return new_args, decompose_stmts

</t>
<t tx="ekr.20220525082934.2">def expand_type_by_instance(typ: Type, instance: Instance) -&gt; Type:
    """Substitute type variables in type using values from an Instance.
    Type variables are considered to be bound by the class declaration."""
    # TODO: use an overloaded signature? (ProperType stays proper after expansion.)
    if not instance.args:
        return typ
    else:
        variables: Dict[TypeVarId, Type] = {}
        for binder, arg in zip(instance.type.defn.type_vars, instance.args):
            variables[binder.id] = arg
        return expand_type(typ, variables)


</t>
<t tx="ekr.20220525082934.20">def visit_callable_type(self, t: CallableType) -&gt; Type:
    param_spec = t.param_spec()
    if param_spec is not None:
        repl = get_proper_type(self.variables.get(param_spec.id))
        # If a ParamSpec in a callable type is substituted with a
        # callable type, we can't use normal substitution logic,
        # since ParamSpec is actually split into two components
        # *P.args and **P.kwargs in the original type. Instead, we
        # must expand both of them with all the argument types,
        # kinds and names in the replacement. The return type in
        # the replacement is ignored.
        if isinstance(repl, CallableType) or isinstance(repl, Parameters):
            # Substitute *args: P.args, **kwargs: P.kwargs
            prefix = param_spec.prefix
            # we need to expand the types in the prefix, so might as well
            # not get them in the first place
            t = t.expand_param_spec(repl, no_prefix=True)
            return t.copy_modified(
                arg_types=self.expand_types(prefix.arg_types) + t.arg_types,
                arg_kinds=prefix.arg_kinds + t.arg_kinds,
                arg_names=prefix.arg_names + t.arg_names,
                ret_type=t.ret_type.accept(self),
                type_guard=(t.type_guard.accept(self) if t.type_guard is not None else None))

    return t.copy_modified(arg_types=self.expand_types(t.arg_types),
                           ret_type=t.ret_type.accept(self),
                           type_guard=(t.type_guard.accept(self)
                                       if t.type_guard is not None else None))

</t>
<t tx="ekr.20220525082934.200">def extract_names(self, arg: ast27.expr) -&gt; List[str]:
    if isinstance(arg, Name):
        return [arg.id]
    elif isinstance(arg, ast27_Tuple):
        return [name for elt in arg.elts for name in self.extract_names(elt)]
    else:
        return []

</t>
<t tx="ekr.20220525082934.201">def convert_arg(self, index: int, arg: ast27.expr, line: int,
                decompose_stmts: List[Statement]) -&gt; Var:
    if isinstance(arg, Name):
        v = arg.id
    elif isinstance(arg, ast27_Tuple):
        v = f'__tuple_arg_{index + 1}'
        rvalue = NameExpr(v)
        rvalue.set_line(line)
        assignment = AssignmentStmt([self.visit(arg)], rvalue)
        assignment.set_line(line)
        decompose_stmts.append(assignment)
    else:
        raise RuntimeError(f"'{ast27.dump(arg)}' is not a valid argument.")
    return Var(v)

</t>
<t tx="ekr.20220525082934.202">def get_type(self,
             i: int,
             type_comments: Sequence[Optional[str]],
             converter: TypeConverter) -&gt; Optional[Type]:
    if i &lt; len(type_comments):
        comment = type_comments[i]
        if comment is not None:
            typ = converter.visit_raw_str(comment)
            extra_ignore = TYPE_IGNORE_PATTERN.match(comment)
            if extra_ignore:
                tag: Optional[str] = cast(Any, extra_ignore).group(1)
                ignored = parse_type_ignore_tag(tag)
                if ignored is None:
                    self.fail(INVALID_TYPE_IGNORE, converter.line, -1)
                else:
                    self.type_ignores[converter.line] = ignored
            return typ
    return None

</t>
<t tx="ekr.20220525082934.203">def stringify_name(self, n: AST) -&gt; str:
    if isinstance(n, Name):
        return n.id
    elif isinstance(n, Attribute):
        return f"{self.stringify_name(n.value)}.{n.attr}"
    else:
        assert False, "can't stringify " + str(type(n))

</t>
<t tx="ekr.20220525082934.204"># ClassDef(identifier name,
#  expr* bases,
#  keyword* keywords,
#  stmt* body,
#  expr* decorator_list)
def visit_ClassDef(self, n: ast27.ClassDef) -&gt; ClassDef:
    self.class_and_function_stack.append('C')

    cdef = ClassDef(n.name,
                    self.as_required_block(n.body, n.lineno),
                    None,
                    self.translate_expr_list(n.bases),
                    metaclass=None)
    cdef.decorators = self.translate_expr_list(n.decorator_list)
    cdef.line = n.lineno + len(n.decorator_list)
    cdef.column = n.col_offset
    cdef.end_line = n.lineno
    self.class_and_function_stack.pop()
    return cdef

</t>
<t tx="ekr.20220525082934.205"># Return(expr? value)
def visit_Return(self, n: ast27.Return) -&gt; ReturnStmt:
    stmt = ReturnStmt(self.visit(n.value))
    return self.set_line(stmt, n)

</t>
<t tx="ekr.20220525082934.206"># Delete(expr* targets)
def visit_Delete(self, n: ast27.Delete) -&gt; DelStmt:
    if len(n.targets) &gt; 1:
        tup = TupleExpr(self.translate_expr_list(n.targets))
        tup.set_line(n.lineno)
        stmt = DelStmt(tup)
    else:
        stmt = DelStmt(self.visit(n.targets[0]))
    return self.set_line(stmt, n)

</t>
<t tx="ekr.20220525082934.207"># Assign(expr* targets, expr value, string? type_comment)
def visit_Assign(self, n: ast27.Assign) -&gt; AssignmentStmt:
    typ = self.translate_type_comment(n, n.type_comment)
    stmt = AssignmentStmt(self.translate_expr_list(n.targets),
                          self.visit(n.value),
                          type=typ)
    return self.set_line(stmt, n)

</t>
<t tx="ekr.20220525082934.208"># AugAssign(expr target, operator op, expr value)
def visit_AugAssign(self, n: ast27.AugAssign) -&gt; OperatorAssignmentStmt:
    stmt = OperatorAssignmentStmt(self.from_operator(n.op),
                                  self.visit(n.target),
                                  self.visit(n.value))
    return self.set_line(stmt, n)

</t>
<t tx="ekr.20220525082934.209"># For(expr target, expr iter, stmt* body, stmt* orelse, string? type_comment)
def visit_For(self, n: ast27.For) -&gt; ForStmt:
    typ = self.translate_type_comment(n, n.type_comment)
    stmt = ForStmt(self.visit(n.target),
                   self.visit(n.iter),
                   self.as_required_block(n.body, n.lineno),
                   self.as_block(n.orelse, n.lineno),
                   typ)
    return self.set_line(stmt, n)

</t>
<t tx="ekr.20220525082934.21">def visit_overloaded(self, t: Overloaded) -&gt; Type:
    items: List[CallableType] = []
    for item in t.items:
        new_item = item.accept(self)
        assert isinstance(new_item, ProperType)
        assert isinstance(new_item, CallableType)
        items.append(new_item)
    return Overloaded(items)

</t>
<t tx="ekr.20220525082934.210"># While(expr test, stmt* body, stmt* orelse)
def visit_While(self, n: ast27.While) -&gt; WhileStmt:
    stmt = WhileStmt(self.visit(n.test),
                     self.as_required_block(n.body, n.lineno),
                     self.as_block(n.orelse, n.lineno))
    return self.set_line(stmt, n)

</t>
<t tx="ekr.20220525082934.211"># If(expr test, stmt* body, stmt* orelse)
def visit_If(self, n: ast27.If) -&gt; IfStmt:
    stmt = IfStmt([self.visit(n.test)],
                  [self.as_required_block(n.body, n.lineno)],
                  self.as_block(n.orelse, n.lineno))
    return self.set_line(stmt, n)

</t>
<t tx="ekr.20220525082934.212"># With(withitem* items, stmt* body, string? type_comment)
def visit_With(self, n: ast27.With) -&gt; WithStmt:
    typ = self.translate_type_comment(n, n.type_comment)
    stmt = WithStmt([self.visit(n.context_expr)],
                    [self.visit(n.optional_vars)],
                    self.as_required_block(n.body, n.lineno),
                    typ)
    return self.set_line(stmt, n)

</t>
<t tx="ekr.20220525082934.213"># 'raise' [test [',' test [',' test]]]
def visit_Raise(self, n: ast27.Raise) -&gt; RaiseStmt:
    legacy_mode = False
    if n.type is None:
        e = None
    else:
        if n.inst is None:
            e = self.visit(n.type)
        else:
            legacy_mode = True
            if n.tback is None:
                e = TupleExpr([self.visit(n.type), self.visit(n.inst)])
            else:
                e = TupleExpr([self.visit(n.type), self.visit(n.inst), self.visit(n.tback)])

    stmt = RaiseStmt(e, None)
    stmt.legacy_mode = legacy_mode
    return self.set_line(stmt, n)

</t>
<t tx="ekr.20220525082934.214"># TryExcept(stmt* body, excepthandler* handlers, stmt* orelse)
def visit_TryExcept(self, n: ast27.TryExcept) -&gt; TryStmt:
    stmt = self.try_handler(n.body, n.handlers, n.orelse, [], n.lineno)
    return self.set_line(stmt, n)

</t>
<t tx="ekr.20220525082934.215">def visit_TryFinally(self, n: ast27.TryFinally) -&gt; TryStmt:
    if len(n.body) == 1 and isinstance(n.body[0], ast27.TryExcept):
        stmt = self.try_handler([n.body[0]], [], [], n.finalbody, n.lineno)
    else:
        stmt = self.try_handler(n.body, [], [], n.finalbody, n.lineno)
    return self.set_line(stmt, n)

</t>
<t tx="ekr.20220525082934.216">def try_handler(self,
                body: List[ast27.stmt],
                handlers: List[ast27.ExceptHandler],
                orelse: List[ast27.stmt],
                finalbody: List[ast27.stmt],
                lineno: int) -&gt; TryStmt:
    vs: List[Optional[NameExpr]] = []
    for item in handlers:
        if item.name is None:
            vs.append(None)
        elif isinstance(item.name, Name):
            vs.append(self.set_line(NameExpr(item.name.id), item))
        else:
            self.fail('Sorry, "except &lt;expr&gt;, &lt;anything but a name&gt;" is not supported',
                      item.lineno, item.col_offset)
            vs.append(None)
    types = [self.visit(h.type) for h in handlers]
    handlers_ = [self.as_required_block(h.body, h.lineno) for h in handlers]

    return TryStmt(self.as_required_block(body, lineno),
                   vs,
                   types,
                   handlers_,
                   self.as_block(orelse, lineno),
                   self.as_block(finalbody, lineno))

</t>
<t tx="ekr.20220525082934.217">def visit_Print(self, n: ast27.Print) -&gt; PrintStmt:
    stmt = PrintStmt(self.translate_expr_list(n.values), n.nl, self.visit(n.dest))
    return self.set_line(stmt, n)

</t>
<t tx="ekr.20220525082934.218">def visit_Exec(self, n: ast27.Exec) -&gt; ExecStmt:
    stmt = ExecStmt(self.visit(n.body),
                 self.visit(n.globals),
                 self.visit(n.locals))
    return self.set_line(stmt, n)

</t>
<t tx="ekr.20220525082934.219">def visit_Repr(self, n: ast27.Repr) -&gt; BackquoteExpr:
    stmt = BackquoteExpr(self.visit(n.value))
    return self.set_line(stmt, n)

</t>
<t tx="ekr.20220525082934.22">def visit_tuple_type(self, t: TupleType) -&gt; Type:
    items = []
    for item in t.items:
        proper_item = get_proper_type(item)
        if isinstance(proper_item, UnpackType):
            unpacked_items = self.expand_unpack(proper_item)
            if unpacked_items is None:
                # TODO: better error, something like tuple of unknown?
                return UninhabitedType()
            elif isinstance(unpacked_items, Instance):
                if len(t.items) == 1:
                    return unpacked_items
                else:
                    assert False, "Invalid unpack of variable length tuple"
            elif isinstance(unpacked_items, AnyType):
                return unpacked_items
            else:
                items.extend(unpacked_items)
        else:
            items.append(proper_item.accept(self))

    return t.copy_modified(items=items)

</t>
<t tx="ekr.20220525082934.220"># Assert(expr test, expr? msg)
def visit_Assert(self, n: ast27.Assert) -&gt; AssertStmt:
    stmt = AssertStmt(self.visit(n.test), self.visit(n.msg))
    return self.set_line(stmt, n)

</t>
<t tx="ekr.20220525082934.221"># Import(alias* names)
def visit_Import(self, n: ast27.Import) -&gt; Import:
    names: List[Tuple[str, Optional[str]]] = []
    for alias in n.names:
        name = self.translate_module_id(alias.name)
        asname = alias.asname
        if asname is None and name != alias.name:
            # if the module name has been translated (and it's not already
            # an explicit import-as), make it an implicit import-as the
            # original name
            asname = alias.name
        names.append((name, asname))
    i = Import(names)
    self.imports.append(i)
    return self.set_line(i, n)

</t>
<t tx="ekr.20220525082934.222"># ImportFrom(identifier? module, alias* names, int? level)
def visit_ImportFrom(self, n: ast27.ImportFrom) -&gt; ImportBase:
    assert n.level is not None
    if len(n.names) == 1 and n.names[0].name == '*':
        mod = n.module if n.module is not None else ''
        i: ImportBase = ImportAll(mod, n.level)
    else:
        module_id = self.translate_module_id(n.module) if n.module is not None else ''
        i = ImportFrom(module_id, n.level, [(a.name, a.asname) for a in n.names])

        # See comments in the constructor for more information about this field.
        if module_id == '__future__' and any(a.name == 'unicode_literals' for a in n.names):
            self.unicode_literals = True
    self.imports.append(i)
    return self.set_line(i, n)

</t>
<t tx="ekr.20220525082934.223"># Global(identifier* names)
def visit_Global(self, n: ast27.Global) -&gt; GlobalDecl:
    stmt = GlobalDecl(n.names)
    return self.set_line(stmt, n)

</t>
<t tx="ekr.20220525082934.224"># Expr(expr value)
def visit_Expr(self, n: ast27.Expr) -&gt; ExpressionStmt:
    value = self.visit(n.value)
    stmt = ExpressionStmt(value)
    return self.set_line(stmt, n)

</t>
<t tx="ekr.20220525082934.225"># Pass
def visit_Pass(self, n: ast27.Pass) -&gt; PassStmt:
    stmt = PassStmt()
    return self.set_line(stmt, n)

</t>
<t tx="ekr.20220525082934.226"># Break
def visit_Break(self, n: ast27.Break) -&gt; BreakStmt:
    stmt = BreakStmt()
    return self.set_line(stmt, n)

</t>
<t tx="ekr.20220525082934.227"># Continue
def visit_Continue(self, n: ast27.Continue) -&gt; ContinueStmt:
    stmt = ContinueStmt()
    return self.set_line(stmt, n)

</t>
<t tx="ekr.20220525082934.228"># --- expr ---

</t>
<t tx="ekr.20220525082934.229"># BoolOp(boolop op, expr* values)
def visit_BoolOp(self, n: ast27.BoolOp) -&gt; OpExpr:
    # mypy translates (1 and 2 and 3) as (1 and (2 and 3))
    assert len(n.values) &gt;= 2
    if isinstance(n.op, ast27.And):
        op = 'and'
    elif isinstance(n.op, ast27.Or):
        op = 'or'
    else:
        raise RuntimeError('unknown BoolOp ' + str(type(n)))

    # potentially inefficient!
    e = self.group(self.translate_expr_list(n.values), op)
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.23">def visit_typeddict_type(self, t: TypedDictType) -&gt; Type:
    return t.copy_modified(item_types=self.expand_types(t.items.values()))

</t>
<t tx="ekr.20220525082934.230">def group(self, vals: List[Expression], op: str) -&gt; OpExpr:
    if len(vals) == 2:
        return OpExpr(op, vals[0], vals[1])
    else:
        return OpExpr(op, vals[0], self.group(vals[1:], op))

</t>
<t tx="ekr.20220525082934.231"># BinOp(expr left, operator op, expr right)
def visit_BinOp(self, n: ast27.BinOp) -&gt; OpExpr:
    op = self.from_operator(n.op)

    if op is None:
        raise RuntimeError('cannot translate BinOp ' + str(type(n.op)))

    e = OpExpr(op, self.visit(n.left), self.visit(n.right))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.232"># UnaryOp(unaryop op, expr operand)
def visit_UnaryOp(self, n: ast27.UnaryOp) -&gt; UnaryExpr:
    op = None
    if isinstance(n.op, ast27.Invert):
        op = '~'
    elif isinstance(n.op, ast27.Not):
        op = 'not'
    elif isinstance(n.op, ast27.UAdd):
        op = '+'
    elif isinstance(n.op, ast27.USub):
        op = '-'

    if op is None:
        raise RuntimeError('cannot translate UnaryOp ' + str(type(n.op)))

    e = UnaryExpr(op, self.visit(n.operand))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.233"># Lambda(arguments args, expr body)
def visit_Lambda(self, n: ast27.Lambda) -&gt; LambdaExpr:
    args, decompose_stmts = self.transform_args(n.args, n.lineno)

    n_body = ast27.Return(n.body)
    n_body.lineno = n.body.lineno
    n_body.col_offset = n.body.col_offset
    body = self.as_required_block([n_body], n.lineno)
    if decompose_stmts:
        body.body = decompose_stmts + body.body

    e = LambdaExpr(args, body)
    e.set_line(n.lineno, n.col_offset)  # Overrides set_line -- can't use self.set_line
    return e

</t>
<t tx="ekr.20220525082934.234"># IfExp(expr test, expr body, expr orelse)
def visit_IfExp(self, n: ast27.IfExp) -&gt; ConditionalExpr:
    e = ConditionalExpr(self.visit(n.test),
                        self.visit(n.body),
                        self.visit(n.orelse))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.235"># Dict(expr* keys, expr* values)
def visit_Dict(self, n: ast27.Dict) -&gt; DictExpr:
    e = DictExpr(list(zip(self.translate_expr_list(n.keys),
                          self.translate_expr_list(n.values))))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.236"># Set(expr* elts)
def visit_Set(self, n: ast27.Set) -&gt; SetExpr:
    e = SetExpr(self.translate_expr_list(n.elts))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.237"># ListComp(expr elt, comprehension* generators)
def visit_ListComp(self, n: ast27.ListComp) -&gt; ListComprehension:
    e = ListComprehension(self.visit_GeneratorExp(cast(ast27.GeneratorExp, n)))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.238"># SetComp(expr elt, comprehension* generators)
def visit_SetComp(self, n: ast27.SetComp) -&gt; SetComprehension:
    e = SetComprehension(self.visit_GeneratorExp(cast(ast27.GeneratorExp, n)))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.239"># DictComp(expr key, expr value, comprehension* generators)
def visit_DictComp(self, n: ast27.DictComp) -&gt; DictionaryComprehension:
    targets = [self.visit(c.target) for c in n.generators]
    iters = [self.visit(c.iter) for c in n.generators]
    ifs_list = [self.translate_expr_list(c.ifs) for c in n.generators]
    e = DictionaryComprehension(self.visit(n.key),
                                self.visit(n.value),
                                targets,
                                iters,
                                ifs_list,
                                [False for _ in n.generators])
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.24">def visit_literal_type(self, t: LiteralType) -&gt; Type:
    # TODO: Verify this implementation is correct
    return t

</t>
<t tx="ekr.20220525082934.240"># GeneratorExp(expr elt, comprehension* generators)
def visit_GeneratorExp(self, n: ast27.GeneratorExp) -&gt; GeneratorExpr:
    targets = [self.visit(c.target) for c in n.generators]
    iters = [self.visit(c.iter) for c in n.generators]
    ifs_list = [self.translate_expr_list(c.ifs) for c in n.generators]
    e = GeneratorExpr(self.visit(n.elt),
                      targets,
                      iters,
                      ifs_list,
                      [False for _ in n.generators])
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.241"># Yield(expr? value)
def visit_Yield(self, n: ast27.Yield) -&gt; YieldExpr:
    e = YieldExpr(self.visit(n.value))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.242"># Compare(expr left, cmpop* ops, expr* comparators)
def visit_Compare(self, n: ast27.Compare) -&gt; ComparisonExpr:
    operators = [self.from_comp_operator(o) for o in n.ops]
    operands = self.translate_expr_list([n.left] + n.comparators)
    e = ComparisonExpr(operators, operands)
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.243"># Call(expr func, expr* args, keyword* keywords)
# keyword = (identifier? arg, expr value)
def visit_Call(self, n: Call) -&gt; CallExpr:
    arg_types: List[ast27.expr] = []
    arg_kinds: List[ArgKind] = []
    signature: List[Optional[str]] = []

    args = n.args
    arg_types.extend(args)
    arg_kinds.extend(ARG_POS for a in args)
    signature.extend(None for a in args)

    if n.starargs is not None:
        arg_types.append(n.starargs)
        arg_kinds.append(ARG_STAR)
        signature.append(None)

    keywords = n.keywords
    arg_types.extend(k.value for k in keywords)
    arg_kinds.extend(ARG_NAMED for k in keywords)
    signature.extend(k.arg for k in keywords)

    if n.kwargs is not None:
        arg_types.append(n.kwargs)
        arg_kinds.append(ARG_STAR2)
        signature.append(None)

    e = CallExpr(self.visit(n.func),
                 self.translate_expr_list(arg_types),
                 arg_kinds,
                 signature)
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.244"># Num(object n) -- a number as a PyObject.
def visit_Num(self, n: ast27.Num) -&gt; Expression:
    # The n field has the type complex, but complex isn't *really*
    # a parent of int and float, and this causes isinstance below
    # to think that the complex branch is always picked. Avoid
    # this by throwing away the type.
    value: object = n.n
    is_inverse = False
    if str(n.n).startswith('-'):  # Hackish because of complex.
        value = -n.n
        is_inverse = True

    if isinstance(value, int):
        expr: Expression = IntExpr(value)
    elif isinstance(value, float):
        expr = FloatExpr(value)
    elif isinstance(value, complex):
        expr = ComplexExpr(value)
    else:
        raise RuntimeError('num not implemented for ' + str(type(n.n)))

    if is_inverse:
        expr = UnaryExpr('-', expr)

    return self.set_line(expr, n)

</t>
<t tx="ekr.20220525082934.245"># Str(string s)
def visit_Str(self, n: ast27.Str) -&gt; Expression:
    # Note: typed_ast.ast27 will handled unicode_literals for us. If
    # n.s is of type 'bytes', we know unicode_literals was not enabled;
    # otherwise we know it was.
    #
    # Note that the following code is NOT run when parsing Python 2.7 stubs:
    # we always parse stub files (no matter what version) using the Python 3
    # parser. This is also why string literals in Python 2.7 stubs are assumed
    # to be unicode.
    if isinstance(n.s, bytes):
        contents = bytes_to_human_readable_repr(n.s)
        e: Union[StrExpr, UnicodeExpr] = StrExpr(contents, from_python_3=False)
        return self.set_line(e, n)
    else:
        e = UnicodeExpr(n.s)
        return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.246"># Ellipsis
def visit_Ellipsis(self, n: ast27.Ellipsis) -&gt; EllipsisExpr:
    return EllipsisExpr()

</t>
<t tx="ekr.20220525082934.247"># Attribute(expr value, identifier attr, expr_context ctx)
def visit_Attribute(self, n: Attribute) -&gt; Expression:
    # First create MemberExpr and then potentially replace with a SuperExpr
    # to improve performance when compiled. The check for "super()" will be
    # faster with native AST nodes. Note also that super expressions are
    # less common than normal member expressions.
    member_expr = MemberExpr(self.visit(n.value), n.attr)
    obj = member_expr.expr
    if (isinstance(obj, CallExpr) and
            isinstance(obj.callee, NameExpr) and
            obj.callee.name == 'super'):
        e: Expression = SuperExpr(member_expr.name, obj)
    else:
        e = member_expr
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.248"># Subscript(expr value, slice slice, expr_context ctx)
def visit_Subscript(self, n: ast27.Subscript) -&gt; IndexExpr:
    e = IndexExpr(self.visit(n.value), self.visit(n.slice))
    self.set_line(e, n)
    if isinstance(e.index, SliceExpr):
        # Slice has no line/column in the raw ast.
        e.index.line = e.line
        e.index.column = e.column
    return e

</t>
<t tx="ekr.20220525082934.249"># Name(identifier id, expr_context ctx)
def visit_Name(self, n: Name) -&gt; NameExpr:
    e = NameExpr(n.id)
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.25">def visit_union_type(self, t: UnionType) -&gt; Type:
    # After substituting for type variables in t.items,
    # some of the resulting types might be subtypes of others.
    from mypy.typeops import make_simplified_union  # asdf
    return make_simplified_union(self.expand_types(t.items), t.line, t.column)

</t>
<t tx="ekr.20220525082934.250"># List(expr* elts, expr_context ctx)
def visit_List(self, n: ast27.List) -&gt; Union[ListExpr, TupleExpr]:
    expr_list: List[Expression] = [self.visit(e) for e in n.elts]
    if isinstance(n.ctx, ast27.Store):
        # [x, y] = z and (x, y) = z means exactly the same thing
        e: Union[ListExpr, TupleExpr] = TupleExpr(expr_list)
    else:
        e = ListExpr(expr_list)
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.251"># Tuple(expr* elts, expr_context ctx)
def visit_Tuple(self, n: ast27_Tuple) -&gt; TupleExpr:
    e = TupleExpr([self.visit(e) for e in n.elts])
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.252"># --- slice ---

</t>
<t tx="ekr.20220525082934.253"># Slice(expr? lower, expr? upper, expr? step)
def visit_Slice(self, n: ast27.Slice) -&gt; SliceExpr:
    return SliceExpr(self.visit(n.lower),
                     self.visit(n.upper),
                     self.visit(n.step))

</t>
<t tx="ekr.20220525082934.254"># ExtSlice(slice* dims)
def visit_ExtSlice(self, n: ast27.ExtSlice) -&gt; TupleExpr:
    return TupleExpr(self.translate_expr_list(n.dims))

</t>
<t tx="ekr.20220525082934.255"># Index(expr value)
def visit_Index(self, n: ast27.Index) -&gt; Expression:
    return self.visit(n.value)
</t>
<t tx="ekr.20220525082934.256">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Routines for finding the sources that mypy will check"""

import functools
import os

from typing import List, Sequence, Set, Tuple, Optional
from typing_extensions import Final

from mypy.modulefinder import BuildSource, PYTHON_EXTENSIONS, mypy_path, matches_exclude
from mypy.fscache import FileSystemCache
from mypy.options import Options

PY_EXTENSIONS: Final = tuple(PYTHON_EXTENSIONS)


@others
</t>
<t tx="ekr.20220525082934.257">class InvalidSourceList(Exception):
    """Exception indicating a problem in the list of sources given to mypy."""


</t>
<t tx="ekr.20220525082934.258">def create_source_list(paths: Sequence[str], options: Options,
                       fscache: Optional[FileSystemCache] = None,
                       allow_empty_dir: bool = False) -&gt; List[BuildSource]:
    """From a list of source files/directories, makes a list of BuildSources.

    Raises InvalidSourceList on errors.
    """
    fscache = fscache or FileSystemCache()
    finder = SourceFinder(fscache, options)

    sources = []
    for path in paths:
        path = os.path.normpath(path)
        if path.endswith(PY_EXTENSIONS):
            # Can raise InvalidSourceList if a directory doesn't have a valid module name.
            name, base_dir = finder.crawl_up(path)
            sources.append(BuildSource(path, name, None, base_dir))
        elif fscache.isdir(path):
            sub_sources = finder.find_sources_in_dir(path)
            if not sub_sources and not allow_empty_dir:
                raise InvalidSourceList(
                    f"There are no .py[i] files in directory '{path}'"
                )
            sources.extend(sub_sources)
        else:
            mod = os.path.basename(path) if options.scripts_are_modules else None
            sources.append(BuildSource(path, mod, None))
    return sources


</t>
<t tx="ekr.20220525082934.259">def keyfunc(name: str) -&gt; Tuple[bool, int, str]:
    """Determines sort order for directory listing.

    The desirable properties are:
    1) foo &lt; foo.pyi &lt; foo.py
    2) __init__.py[i] &lt; foo
    """
    base, suffix = os.path.splitext(name)
    for i, ext in enumerate(PY_EXTENSIONS):
        if suffix == ext:
            return (base != "__init__", i, base)
    return (base != "__init__", -1, name)


</t>
<t tx="ekr.20220525082934.26">def visit_partial_type(self, t: PartialType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082934.260">def normalise_package_base(root: str) -&gt; str:
    if not root:
        root = os.curdir
    root = os.path.abspath(root)
    if root.endswith(os.sep):
        root = root[:-1]
    return root


</t>
<t tx="ekr.20220525082934.261">def get_explicit_package_bases(options: Options) -&gt; Optional[List[str]]:
    """Returns explicit package bases to use if the option is enabled, or None if disabled.

    We currently use MYPYPATH and the current directory as the package bases. In the future,
    when --namespace-packages is the default could also use the values passed with the
    --package-root flag, see #9632.

    Values returned are normalised so we can use simple string comparisons in
    SourceFinder.is_explicit_package_base
    """
    if not options.explicit_package_bases:
        return None
    roots = mypy_path() + options.mypy_path + [os.getcwd()]
    return [normalise_package_base(root) for root in roots]


</t>
<t tx="ekr.20220525082934.262">class SourceFinder:
    @others
</t>
<t tx="ekr.20220525082934.263">def __init__(self, fscache: FileSystemCache, options: Options) -&gt; None:
    self.fscache = fscache
    self.explicit_package_bases = get_explicit_package_bases(options)
    self.namespace_packages = options.namespace_packages
    self.exclude = options.exclude
    self.verbosity = options.verbosity

</t>
<t tx="ekr.20220525082934.264">def is_explicit_package_base(self, path: str) -&gt; bool:
    assert self.explicit_package_bases
    return normalise_package_base(path) in self.explicit_package_bases

</t>
<t tx="ekr.20220525082934.265">def find_sources_in_dir(self, path: str) -&gt; List[BuildSource]:
    sources = []

    seen: Set[str] = set()
    names = sorted(self.fscache.listdir(path), key=keyfunc)
    for name in names:
        # Skip certain names altogether
        if name in ("__pycache__", "site-packages", "node_modules") or name.startswith("."):
            continue
        subpath = os.path.join(path, name)

        if matches_exclude(
            subpath, self.exclude, self.fscache, self.verbosity &gt;= 2
        ):
            continue

        if self.fscache.isdir(subpath):
            sub_sources = self.find_sources_in_dir(subpath)
            if sub_sources:
                seen.add(name)
                sources.extend(sub_sources)
        else:
            stem, suffix = os.path.splitext(name)
            if stem not in seen and suffix in PY_EXTENSIONS:
                seen.add(stem)
                module, base_dir = self.crawl_up(subpath)
                sources.append(BuildSource(subpath, module, None, base_dir))

    return sources

</t>
<t tx="ekr.20220525082934.266">def crawl_up(self, path: str) -&gt; Tuple[str, str]:
    """Given a .py[i] filename, return module and base directory.

    For example, given "xxx/yyy/foo/bar.py", we might return something like:
    ("foo.bar", "xxx/yyy")

    If namespace packages is off, we crawl upwards until we find a directory without
    an __init__.py

    If namespace packages is on, we crawl upwards until the nearest explicit base directory.
    Failing that, we return one past the highest directory containing an __init__.py

    We won't crawl past directories with invalid package names.
    The base directory returned is an absolute path.
    """
    path = os.path.abspath(path)
    parent, filename = os.path.split(path)

    module_name = strip_py(filename) or filename

    parent_module, base_dir = self.crawl_up_dir(parent)
    if module_name == "__init__":
        return parent_module, base_dir

    # Note that module_name might not actually be a valid identifier, but that's okay
    # Ignoring this possibility sidesteps some search path confusion
    module = module_join(parent_module, module_name)
    return module, base_dir

</t>
<t tx="ekr.20220525082934.267">def crawl_up_dir(self, dir: str) -&gt; Tuple[str, str]:
    return self._crawl_up_helper(dir) or ("", dir)

</t>
<t tx="ekr.20220525082934.268">@functools.lru_cache()  # noqa: B019
def _crawl_up_helper(self, dir: str) -&gt; Optional[Tuple[str, str]]:
    """Given a directory, maybe returns module and base directory.

    We return a non-None value if we were able to find something clearly intended as a base
    directory (as adjudicated by being an explicit base directory or by containing a package
    with __init__.py).

    This distinction is necessary for namespace packages, so that we know when to treat
    ourselves as a subpackage.
    """
    # stop crawling if we're an explicit base directory
    if self.explicit_package_bases is not None and self.is_explicit_package_base(dir):
        return "", dir

    parent, name = os.path.split(dir)
    if name.endswith('-stubs'):
        name = name[:-6]  # PEP-561 stub-only directory

    # recurse if there's an __init__.py
    init_file = self.get_init_file(dir)
    if init_file is not None:
        if not name.isidentifier():
            # in most cases the directory name is invalid, we'll just stop crawling upwards
            # but if there's an __init__.py in the directory, something is messed up
            raise InvalidSourceList(f"{name} is not a valid Python package name")
        # we're definitely a package, so we always return a non-None value
        mod_prefix, base_dir = self.crawl_up_dir(parent)
        return module_join(mod_prefix, name), base_dir

    # stop crawling if we're out of path components or our name is an invalid identifier
    if not name or not parent or not name.isidentifier():
        return None

    # stop crawling if namespace packages is off (since we don't have an __init__.py)
    if not self.namespace_packages:
        return None

    # at this point: namespace packages is on, we don't have an __init__.py and we're not an
    # explicit base directory
    result = self._crawl_up_helper(parent)
    if result is None:
        # we're not an explicit base directory and we don't have an __init__.py
        # and none of our parents are either, so return
        return None
    # one of our parents was an explicit base directory or had an __init__.py, so we're
    # definitely a subpackage! chain our name to the module.
    mod_prefix, base_dir = result
    return module_join(mod_prefix, name), base_dir

</t>
<t tx="ekr.20220525082934.269">def get_init_file(self, dir: str) -&gt; Optional[str]:
    """Check whether a directory contains a file named __init__.py[i].

    If so, return the file's name (with dir prefixed).  If not, return None.

    This prefers .pyi over .py (because of the ordering of PY_EXTENSIONS).
    """
    for ext in PY_EXTENSIONS:
        f = os.path.join(dir, '__init__' + ext)
        if self.fscache.isfile(f):
            return f
        if ext == '.py' and self.fscache.init_under_package_root(f):
            return f
    return None


</t>
<t tx="ekr.20220525082934.27">def visit_type_type(self, t: TypeType) -&gt; Type:
    # TODO: Verify that the new item type is valid (instance or
    # union of instances or Any).  Sadly we can't report errors
    # here yet.
    item = t.item.accept(self)
    return TypeType.make_normalized(item)

</t>
<t tx="ekr.20220525082934.270">def module_join(parent: str, child: str) -&gt; str:
    """Join module ids, accounting for a possibly empty parent."""
    if parent:
        return parent + '.' + child
    return child


</t>
<t tx="ekr.20220525082934.271">def strip_py(arg: str) -&gt; Optional[str]:
    """Strip a trailing .py or .pyi suffix.

    Return None if no such suffix is found.
    """
    for ext in PY_EXTENSIONS:
        if arg.endswith(ext):
            return arg[:-len(ext)]
    return None
</t>
<t tx="ekr.20220525082934.272">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Fix up various things after deserialization."""

from typing import Any, Dict, Optional
from typing_extensions import Final

from mypy.nodes import (
    MypyFile, SymbolTable, TypeInfo, FuncDef, OverloadedFuncDef,
    Decorator, Var, TypeVarExpr, ClassDef, Block, TypeAlias,
)
from mypy.types import (
    CallableType, Instance, Overloaded, TupleType, TypedDictType,
    TypeVarType, UnboundType, UnionType, TypeVisitor, LiteralType,
    TypeType, NOT_READY, TypeAliasType, AnyType, TypeOfAny, ParamSpecType,
    Parameters, UnpackType, TypeVarTupleType
)
from mypy.visitor import NodeVisitor
from mypy.lookup import lookup_fully_qualified


@others
</t>
<t tx="ekr.20220525082934.273"># N.B: we do a allow_missing fixup when fixing up a fine-grained
# incremental cache load (since there may be cross-refs into deleted
# modules)
def fixup_module(tree: MypyFile, modules: Dict[str, MypyFile],
                 allow_missing: bool) -&gt; None:
    node_fixer = NodeFixer(modules, allow_missing)
    node_fixer.visit_symbol_table(tree.names, tree.fullname)


</t>
<t tx="ekr.20220525082934.274"># TODO: Fix up .info when deserializing, i.e. much earlier.
class NodeFixer(NodeVisitor[None]):
    current_info: Optional[TypeInfo] = None

    @others
</t>
<t tx="ekr.20220525082934.275">def __init__(self, modules: Dict[str, MypyFile], allow_missing: bool) -&gt; None:
    self.modules = modules
    self.allow_missing = allow_missing
    self.type_fixer = TypeFixer(self.modules, allow_missing)

</t>
<t tx="ekr.20220525082934.276"># NOTE: This method isn't (yet) part of the NodeVisitor API.
def visit_type_info(self, info: TypeInfo) -&gt; None:
    save_info = self.current_info
    try:
        self.current_info = info
        if info.defn:
            info.defn.accept(self)
        if info.names:
            self.visit_symbol_table(info.names, info.fullname)
        if info.bases:
            for base in info.bases:
                base.accept(self.type_fixer)
        if info._promote:
            info._promote.accept(self.type_fixer)
        if info.tuple_type:
            info.tuple_type.accept(self.type_fixer)
        if info.typeddict_type:
            info.typeddict_type.accept(self.type_fixer)
        if info.declared_metaclass:
            info.declared_metaclass.accept(self.type_fixer)
        if info.metaclass_type:
            info.metaclass_type.accept(self.type_fixer)
        if info._mro_refs:
            info.mro = [lookup_fully_qualified_typeinfo(self.modules, name,
                                                        allow_missing=self.allow_missing)
                        for name in info._mro_refs]
            info._mro_refs = None
    finally:
        self.current_info = save_info

</t>
<t tx="ekr.20220525082934.277"># NOTE: This method *definitely* isn't part of the NodeVisitor API.
def visit_symbol_table(self, symtab: SymbolTable, table_fullname: str) -&gt; None:
    # Copy the items because we may mutate symtab.
    for key, value in list(symtab.items()):
        cross_ref = value.cross_ref
        if cross_ref is not None:  # Fix up cross-reference.
            value.cross_ref = None
            if cross_ref in self.modules:
                value.node = self.modules[cross_ref]
            else:
                stnode = lookup_fully_qualified(cross_ref, self.modules,
                                                raise_on_missing=not self.allow_missing)
                if stnode is not None:
                    assert stnode.node is not None, (table_fullname + "." + key, cross_ref)
                    value.node = stnode.node
                elif not self.allow_missing:
                    assert False, f"Could not find cross-ref {cross_ref}"
                else:
                    # We have a missing crossref in allow missing mode, need to put something
                    value.node = missing_info(self.modules)
        else:
            if isinstance(value.node, TypeInfo):
                # TypeInfo has no accept().  TODO: Add it?
                self.visit_type_info(value.node)
            elif value.node is not None:
                value.node.accept(self)
            else:
                assert False, f'Unexpected empty node {key!r}: {value}'

</t>
<t tx="ekr.20220525082934.278">def visit_func_def(self, func: FuncDef) -&gt; None:
    if self.current_info is not None:
        func.info = self.current_info
    if func.type is not None:
        func.type.accept(self.type_fixer)

</t>
<t tx="ekr.20220525082934.279">def visit_overloaded_func_def(self, o: OverloadedFuncDef) -&gt; None:
    if self.current_info is not None:
        o.info = self.current_info
    if o.type:
        o.type.accept(self.type_fixer)
    for item in o.items:
        item.accept(self)
    if o.impl:
        o.impl.accept(self)

</t>
<t tx="ekr.20220525082934.28">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # Target of the type alias cannot contain type variables,
    # so we just expand the arguments.
    return t.copy_modified(args=self.expand_types(t.args))

</t>
<t tx="ekr.20220525082934.280">def visit_decorator(self, d: Decorator) -&gt; None:
    if self.current_info is not None:
        d.var.info = self.current_info
    if d.func:
        d.func.accept(self)
    if d.var:
        d.var.accept(self)
    for node in d.decorators:
        node.accept(self)

</t>
<t tx="ekr.20220525082934.281">def visit_class_def(self, c: ClassDef) -&gt; None:
    for v in c.type_vars:
        if isinstance(v, TypeVarType):
            for value in v.values:
                value.accept(self.type_fixer)
            v.upper_bound.accept(self.type_fixer)

</t>
<t tx="ekr.20220525082934.282">def visit_type_var_expr(self, tv: TypeVarExpr) -&gt; None:
    for value in tv.values:
        value.accept(self.type_fixer)
    tv.upper_bound.accept(self.type_fixer)

</t>
<t tx="ekr.20220525082934.283">def visit_var(self, v: Var) -&gt; None:
    if self.current_info is not None:
        v.info = self.current_info
    if v.type is not None:
        v.type.accept(self.type_fixer)

</t>
<t tx="ekr.20220525082934.284">def visit_type_alias(self, a: TypeAlias) -&gt; None:
    a.target.accept(self.type_fixer)


</t>
<t tx="ekr.20220525082934.285">class TypeFixer(TypeVisitor[None]):
    @others
</t>
<t tx="ekr.20220525082934.286">def __init__(self, modules: Dict[str, MypyFile], allow_missing: bool) -&gt; None:
    self.modules = modules
    self.allow_missing = allow_missing

</t>
<t tx="ekr.20220525082934.287">def visit_instance(self, inst: Instance) -&gt; None:
    # TODO: Combine Instances that are exactly the same?
    type_ref = inst.type_ref
    if type_ref is None:
        return  # We've already been here.
    inst.type_ref = None
    inst.type = lookup_fully_qualified_typeinfo(self.modules, type_ref,
                                                allow_missing=self.allow_missing)
    # TODO: Is this needed or redundant?
    # Also fix up the bases, just in case.
    for base in inst.type.bases:
        if base.type is NOT_READY:
            base.accept(self)
    for a in inst.args:
        a.accept(self)
    if inst.last_known_value is not None:
        inst.last_known_value.accept(self)

</t>
<t tx="ekr.20220525082934.288">def visit_type_alias_type(self, t: TypeAliasType) -&gt; None:
    type_ref = t.type_ref
    if type_ref is None:
        return  # We've already been here.
    t.type_ref = None
    t.alias = lookup_fully_qualified_alias(self.modules, type_ref,
                                           allow_missing=self.allow_missing)
    for a in t.args:
        a.accept(self)

</t>
<t tx="ekr.20220525082934.289">def visit_any(self, o: Any) -&gt; None:
    pass  # Nothing to descend into.

</t>
<t tx="ekr.20220525082934.29">def expand_types(self, types: Iterable[Type]) -&gt; List[Type]:
    a: List[Type] = []
    for t in types:
        a.append(t.accept(self))
    return a
</t>
<t tx="ekr.20220525082934.290">def visit_callable_type(self, ct: CallableType) -&gt; None:
    if ct.fallback:
        ct.fallback.accept(self)
    for argt in ct.arg_types:
        # argt may be None, e.g. for __self in NamedTuple constructors.
        if argt is not None:
            argt.accept(self)
    if ct.ret_type is not None:
        ct.ret_type.accept(self)
    for v in ct.variables:
        v.accept(self)
    for arg in ct.bound_args:
        if arg:
            arg.accept(self)
    if ct.type_guard is not None:
        ct.type_guard.accept(self)

</t>
<t tx="ekr.20220525082934.291">def visit_overloaded(self, t: Overloaded) -&gt; None:
    for ct in t.items:
        ct.accept(self)

</t>
<t tx="ekr.20220525082934.292">def visit_erased_type(self, o: Any) -&gt; None:
    # This type should exist only temporarily during type inference
    raise RuntimeError("Shouldn't get here", o)

</t>
<t tx="ekr.20220525082934.293">def visit_deleted_type(self, o: Any) -&gt; None:
    pass  # Nothing to descend into.

</t>
<t tx="ekr.20220525082934.294">def visit_none_type(self, o: Any) -&gt; None:
    pass  # Nothing to descend into.

</t>
<t tx="ekr.20220525082934.295">def visit_uninhabited_type(self, o: Any) -&gt; None:
    pass  # Nothing to descend into.

</t>
<t tx="ekr.20220525082934.296">def visit_partial_type(self, o: Any) -&gt; None:
    raise RuntimeError("Shouldn't get here", o)

</t>
<t tx="ekr.20220525082934.297">def visit_tuple_type(self, tt: TupleType) -&gt; None:
    if tt.items:
        for it in tt.items:
            it.accept(self)
    if tt.partial_fallback is not None:
        tt.partial_fallback.accept(self)

</t>
<t tx="ekr.20220525082934.298">def visit_typeddict_type(self, tdt: TypedDictType) -&gt; None:
    if tdt.items:
        for it in tdt.items.values():
            it.accept(self)
    if tdt.fallback is not None:
        if tdt.fallback.type_ref is not None:
            if lookup_fully_qualified(tdt.fallback.type_ref, self.modules,
                                      raise_on_missing=not self.allow_missing) is None:
                # We reject fake TypeInfos for TypedDict fallbacks because
                # the latter are used in type checking and must be valid.
                tdt.fallback.type_ref = 'typing._TypedDict'
        tdt.fallback.accept(self)

</t>
<t tx="ekr.20220525082934.299">def visit_literal_type(self, lt: LiteralType) -&gt; None:
    lt.fallback.accept(self)

</t>
<t tx="ekr.20220525082934.3">F = TypeVar('F', bound=FunctionLike)


</t>
<t tx="ekr.20220525082934.30">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Translate an Expression to a Type value."""

from typing import Optional

from mypy.nodes import (
    Expression, NameExpr, MemberExpr, IndexExpr, RefExpr, TupleExpr, IntExpr, FloatExpr, UnaryExpr,
    ComplexExpr, ListExpr, StrExpr, BytesExpr, UnicodeExpr, EllipsisExpr, CallExpr, OpExpr,
    get_member_expr_fullname
)
from mypy.fastparse import parse_type_string
from mypy.types import (
    Type, UnboundType, TypeList, EllipsisType, AnyType, CallableArgument, TypeOfAny,
    RawExpressionType, ProperType, UnionType, ANNOTATED_TYPE_NAMES,
)
from mypy.options import Options


@others
</t>
<t tx="ekr.20220525082934.300">def visit_type_var(self, tvt: TypeVarType) -&gt; None:
    if tvt.values:
        for vt in tvt.values:
            vt.accept(self)
    if tvt.upper_bound is not None:
        tvt.upper_bound.accept(self)

</t>
<t tx="ekr.20220525082934.301">def visit_param_spec(self, p: ParamSpecType) -&gt; None:
    p.upper_bound.accept(self)

</t>
<t tx="ekr.20220525082934.302">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; None:
    t.upper_bound.accept(self)

</t>
<t tx="ekr.20220525082934.303">def visit_unpack_type(self, u: UnpackType) -&gt; None:
    u.type.accept(self)

</t>
<t tx="ekr.20220525082934.304">def visit_parameters(self, p: Parameters) -&gt; None:
    for argt in p.arg_types:
        if argt is not None:
            argt.accept(self)
    for var in p.variables:
        var.accept(self)

</t>
<t tx="ekr.20220525082934.305">def visit_unbound_type(self, o: UnboundType) -&gt; None:
    for a in o.args:
        a.accept(self)

</t>
<t tx="ekr.20220525082934.306">def visit_union_type(self, ut: UnionType) -&gt; None:
    if ut.items:
        for it in ut.items:
            it.accept(self)

</t>
<t tx="ekr.20220525082934.307">def visit_void(self, o: Any) -&gt; None:
    pass  # Nothing to descend into.

</t>
<t tx="ekr.20220525082934.308">def visit_type_type(self, t: TypeType) -&gt; None:
    t.item.accept(self)


</t>
<t tx="ekr.20220525082934.309">def lookup_fully_qualified_typeinfo(modules: Dict[str, MypyFile], name: str, *,
                                    allow_missing: bool) -&gt; TypeInfo:
    stnode = lookup_fully_qualified(name, modules, raise_on_missing=not allow_missing)
    node = stnode.node if stnode else None
    if isinstance(node, TypeInfo):
        return node
    else:
        # Looks like a missing TypeInfo during an initial daemon load, put something there
        assert allow_missing, "Should never get here in normal mode," \
                              " got {}:{} instead of TypeInfo".format(type(node).__name__,
                                                                      node.fullname if node
                                                                      else '')
        return missing_info(modules)


</t>
<t tx="ekr.20220525082934.31">class TypeTranslationError(Exception):
    """Exception raised when an expression is not valid as a type."""


</t>
<t tx="ekr.20220525082934.310">def lookup_fully_qualified_alias(modules: Dict[str, MypyFile], name: str, *,
                                 allow_missing: bool) -&gt; TypeAlias:
    stnode = lookup_fully_qualified(name, modules, raise_on_missing=not allow_missing)
    node = stnode.node if stnode else None
    if isinstance(node, TypeAlias):
        return node
    else:
        # Looks like a missing TypeAlias during an initial daemon load, put something there
        assert allow_missing, "Should never get here in normal mode," \
                              " got {}:{} instead of TypeAlias".format(type(node).__name__,
                                                                       node.fullname if node
                                                                       else '')
        return missing_alias()


</t>
<t tx="ekr.20220525082934.311">_SUGGESTION: Final = "&lt;missing {}: *should* have gone away during fine-grained update&gt;"


</t>
<t tx="ekr.20220525082934.312">def missing_info(modules: Dict[str, MypyFile]) -&gt; TypeInfo:
    suggestion = _SUGGESTION.format('info')
    dummy_def = ClassDef(suggestion, Block([]))
    dummy_def.fullname = suggestion

    info = TypeInfo(SymbolTable(), dummy_def, "&lt;missing&gt;")
    obj_type = lookup_fully_qualified_typeinfo(modules, 'builtins.object', allow_missing=False)
    info.bases = [Instance(obj_type, [])]
    info.mro = [info, obj_type]
    return info


</t>
<t tx="ekr.20220525082934.313">def missing_alias() -&gt; TypeAlias:
    suggestion = _SUGGESTION.format('alias')
    return TypeAlias(AnyType(TypeOfAny.special_form), suggestion,
                     line=-1, column=-1)
</t>
<t tx="ekr.20220525082934.314">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Generic node traverser visitor"""

from mypy.traverser import TraverserVisitor
from mypy.nodes import Block, MypyFile


@others
</t>
<t tx="ekr.20220525082934.315">class TreeFreer(TraverserVisitor):
    def visit_block(self, block: Block) -&gt; None:
        super().visit_block(block)
        block.body.clear()


</t>
<t tx="ekr.20220525082934.316">def free_tree(tree: MypyFile) -&gt; None:
    """Free all the ASTs associated with a module.

    This needs to be done recursively, since symbol tables contain
    references to definitions, so those won't be freed but we want their
    contents to be.
    """
    tree.accept(TreeFreer())
    tree.defs.clear()
</t>
<t tx="ekr.20220525082934.317">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Interface for accessing the file system with automatic caching.

The idea is to cache the results of any file system state reads during
a single transaction. This has two main benefits:

* This avoids redundant syscalls, as we won't perform the same OS
  operations multiple times.

* This makes it easier to reason about concurrent FS updates, as different
  operations targeting the same paths can't report different state during
  a transaction.

Note that this only deals with reading state, not writing.

Properties maintained by the API:

* The contents of the file are always from the same or later time compared
  to the reported mtime of the file, even if mtime is queried after reading
  a file.

* Repeating an operation produces the same result as the first one during
  a transaction.

* Call flush() to start a new transaction (flush the caches).

The API is a bit limited. It's easy to add new cached operations, however.
You should perform all file system reads through the API to actually take
advantage of the benefits.
"""

import os
import stat
from typing import Dict, List, Set
from mypy.util import hash_digest
from mypy_extensions import mypyc_attr


@others
</t>
<t tx="ekr.20220525082934.318">@mypyc_attr(allow_interpreted_subclasses=True)  # for tests
class FileSystemCache:
    @others
</t>
<t tx="ekr.20220525082934.319">def __init__(self) -&gt; None:
    # The package root is not flushed with the caches.
    # It is set by set_package_root() below.
    self.package_root: List[str] = []
    self.flush()

</t>
<t tx="ekr.20220525082934.32">def _extract_argument_name(expr: Expression) -&gt; Optional[str]:
    if isinstance(expr, NameExpr) and expr.name == 'None':
        return None
    elif isinstance(expr, StrExpr):
        return expr.value
    elif isinstance(expr, UnicodeExpr):
        return expr.value
    else:
        raise TypeTranslationError()


</t>
<t tx="ekr.20220525082934.320">def set_package_root(self, package_root: List[str]) -&gt; None:
    self.package_root = package_root

</t>
<t tx="ekr.20220525082934.321">def flush(self) -&gt; None:
    """Start another transaction and empty all caches."""
    self.stat_cache: Dict[str, os.stat_result] = {}
    self.stat_error_cache: Dict[str, OSError] = {}
    self.listdir_cache: Dict[str, List[str]] = {}
    self.listdir_error_cache: Dict[str, OSError] = {}
    self.isfile_case_cache: Dict[str, bool] = {}
    self.exists_case_cache: Dict[str, bool] = {}
    self.read_cache: Dict[str, bytes] = {}
    self.read_error_cache: Dict[str, Exception] = {}
    self.hash_cache: Dict[str, str] = {}
    self.fake_package_cache: Set[str] = set()

</t>
<t tx="ekr.20220525082934.322">def stat(self, path: str) -&gt; os.stat_result:
    if path in self.stat_cache:
        return self.stat_cache[path]
    if path in self.stat_error_cache:
        raise copy_os_error(self.stat_error_cache[path])
    try:
        st = os.stat(path)
    except OSError as err:
        if self.init_under_package_root(path):
            try:
                return self._fake_init(path)
            except OSError:
                pass
        # Take a copy to get rid of associated traceback and frame objects.
        # Just assigning to __traceback__ doesn't free them.
        self.stat_error_cache[path] = copy_os_error(err)
        raise err
    self.stat_cache[path] = st
    return st

</t>
<t tx="ekr.20220525082934.323">def init_under_package_root(self, path: str) -&gt; bool:
    """Is this path an __init__.py under a package root?

    This is used to detect packages that don't contain __init__.py
    files, which is needed to support Bazel.  The function should
    only be called for non-existing files.

    It will return True if it refers to a __init__.py file that
    Bazel would create, so that at runtime Python would think the
    directory containing it is a package.  For this to work you
    must pass one or more package roots using the --package-root
    flag.

    As an exceptional case, any directory that is a package root
    itself will not be considered to contain a __init__.py file.
    This is different from the rules Bazel itself applies, but is
    necessary for mypy to properly distinguish packages from other
    directories.

    See https://docs.bazel.build/versions/master/be/python.html,
    where this behavior is described under legacy_create_init.
    """
    if not self.package_root:
        return False
    dirname, basename = os.path.split(path)
    if basename != '__init__.py':
        return False
    if not os.path.basename(dirname).isidentifier():
        # Can't put an __init__.py in a place that's not an identifier
        return False
    try:
        st = self.stat(dirname)
    except OSError:
        return False
    else:
        if not stat.S_ISDIR(st.st_mode):
            return False
    ok = False
    drive, path = os.path.splitdrive(path)  # Ignore Windows drive name
    if os.path.isabs(path):
        path = os.path.relpath(path)
    path = os.path.normpath(path)
    for root in self.package_root:
        if path.startswith(root):
            if path == root + basename:
                # A package root itself is never a package.
                ok = False
                break
            else:
                ok = True
    return ok

</t>
<t tx="ekr.20220525082934.324">def _fake_init(self, path: str) -&gt; os.stat_result:
    """Prime the cache with a fake __init__.py file.

    This makes code that looks for path believe an empty file by
    that name exists.  Should only be called after
    init_under_package_root() returns True.
    """
    dirname, basename = os.path.split(path)
    assert basename == '__init__.py', path
    assert not os.path.exists(path), path  # Not cached!
    dirname = os.path.normpath(dirname)
    st = self.stat(dirname)  # May raise OSError
    # Get stat result as a list so we can modify it.
    seq: List[float] = list(st)
    seq[stat.ST_MODE] = stat.S_IFREG | 0o444
    seq[stat.ST_INO] = 1
    seq[stat.ST_NLINK] = 1
    seq[stat.ST_SIZE] = 0
    st = os.stat_result(seq)
    self.stat_cache[path] = st
    # Make listdir() and read() also pretend this file exists.
    self.fake_package_cache.add(dirname)
    return st

</t>
<t tx="ekr.20220525082934.325">def listdir(self, path: str) -&gt; List[str]:
    path = os.path.normpath(path)
    if path in self.listdir_cache:
        res = self.listdir_cache[path]
        # Check the fake cache.
        if path in self.fake_package_cache and '__init__.py' not in res:
            res.append('__init__.py')  # Updates the result as well as the cache
        return res
    if path in self.listdir_error_cache:
        raise copy_os_error(self.listdir_error_cache[path])
    try:
        results = os.listdir(path)
    except OSError as err:
        # Like above, take a copy to reduce memory use.
        self.listdir_error_cache[path] = copy_os_error(err)
        raise err
    self.listdir_cache[path] = results
    # Check the fake cache.
    if path in self.fake_package_cache and '__init__.py' not in results:
        results.append('__init__.py')
    return results

</t>
<t tx="ekr.20220525082934.326">def isfile(self, path: str) -&gt; bool:
    try:
        st = self.stat(path)
    except OSError:
        return False
    return stat.S_ISREG(st.st_mode)

</t>
<t tx="ekr.20220525082934.327">def isfile_case(self, path: str, prefix: str) -&gt; bool:
    """Return whether path exists and is a file.

    On case-insensitive filesystems (like Mac or Windows) this returns
    False if the case of path's last component does not exactly match
    the case found in the filesystem.

    We check also the case of other path components up to prefix.
    For example, if path is 'user-stubs/pack/mod.pyi' and prefix is 'user-stubs',
    we check that the case of 'pack' and 'mod.py' matches exactly, 'user-stubs' will be
    case insensitive on case insensitive filesystems.

    The caller must ensure that prefix is a valid file system prefix of path.
    """
    if not self.isfile(path):
        # Fast path
        return False
    if path in self.isfile_case_cache:
        return self.isfile_case_cache[path]
    head, tail = os.path.split(path)
    if not tail:
        self.isfile_case_cache[path] = False
        return False
    try:
        names = self.listdir(head)
        # This allows one to check file name case sensitively in
        # case-insensitive filesystems.
        res = tail in names
    except OSError:
        res = False
    if res:
        # Also recursively check the other path components in case sensitive way.
        res = self.exists_case(head, prefix)
    self.isfile_case_cache[path] = res
    return res

</t>
<t tx="ekr.20220525082934.328">def exists_case(self, path: str, prefix: str) -&gt; bool:
    """Return whether path exists - checking path components in case sensitive
    fashion, up to prefix.
    """
    if path in self.exists_case_cache:
        return self.exists_case_cache[path]
    head, tail = os.path.split(path)
    if not head.startswith(prefix) or not tail:
        # Only perform the check for paths under prefix.
        self.exists_case_cache[path] = True
        return True
    try:
        names = self.listdir(head)
        # This allows one to check file name case sensitively in
        # case-insensitive filesystems.
        res = tail in names
    except OSError:
        res = False
    if res:
        # Also recursively check other path components.
        res = self.exists_case(head, prefix)
    self.exists_case_cache[path] = res
    return res

</t>
<t tx="ekr.20220525082934.329">def isdir(self, path: str) -&gt; bool:
    try:
        st = self.stat(path)
    except OSError:
        return False
    return stat.S_ISDIR(st.st_mode)

</t>
<t tx="ekr.20220525082934.33">def expr_to_unanalyzed_type(expr: Expression,
                            options: Optional[Options] = None,
                            allow_new_syntax: bool = False,
                            _parent: Optional[Expression] = None) -&gt; ProperType:
    """Translate an expression to the corresponding type.

    The result is not semantically analyzed. It can be UnboundType or TypeList.
    Raise TypeTranslationError if the expression cannot represent a type.

    If allow_new_syntax is True, allow all type syntax independent of the target
    Python version (used in stubs).
    """
    # The `parent` parameter is used in recursive calls to provide context for
    # understanding whether an CallableArgument is ok.
    name: Optional[str] = None
    if isinstance(expr, NameExpr):
        name = expr.name
        if name == 'True':
            return RawExpressionType(True, 'builtins.bool', line=expr.line, column=expr.column)
        elif name == 'False':
            return RawExpressionType(False, 'builtins.bool', line=expr.line, column=expr.column)
        else:
            return UnboundType(name, line=expr.line, column=expr.column)
    elif isinstance(expr, MemberExpr):
        fullname = get_member_expr_fullname(expr)
        if fullname:
            return UnboundType(fullname, line=expr.line, column=expr.column)
        else:
            raise TypeTranslationError()
    elif isinstance(expr, IndexExpr):
        base = expr_to_unanalyzed_type(expr.base, options, allow_new_syntax, expr)
        if isinstance(base, UnboundType):
            if base.args:
                raise TypeTranslationError()
            if isinstance(expr.index, TupleExpr):
                args = expr.index.items
            else:
                args = [expr.index]

            if isinstance(expr.base, RefExpr) and expr.base.fullname in ANNOTATED_TYPE_NAMES:
                # TODO: this is not the optimal solution as we are basically getting rid
                # of the Annotation definition and only returning the type information,
                # losing all the annotations.

                return expr_to_unanalyzed_type(args[0], options, allow_new_syntax, expr)
            else:
                base.args = tuple(expr_to_unanalyzed_type(arg, options, allow_new_syntax, expr)
                                  for arg in args)
            if not base.args:
                base.empty_tuple_index = True
            return base
        else:
            raise TypeTranslationError()
    elif (isinstance(expr, OpExpr)
          and expr.op == '|'
          and ((options and options.python_version &gt;= (3, 10)) or allow_new_syntax)):
        return UnionType([expr_to_unanalyzed_type(expr.left, options, allow_new_syntax),
                          expr_to_unanalyzed_type(expr.right, options, allow_new_syntax)])
    elif isinstance(expr, CallExpr) and isinstance(_parent, ListExpr):
        c = expr.callee
        names = []
        # Go through the dotted member expr chain to get the full arg
        # constructor name to look up
        while True:
            if isinstance(c, NameExpr):
                names.append(c.name)
                break
            elif isinstance(c, MemberExpr):
                names.append(c.name)
                c = c.expr
            else:
                raise TypeTranslationError()
        arg_const = '.'.join(reversed(names))

        # Go through the constructor args to get its name and type.
        name = None
        default_type = AnyType(TypeOfAny.unannotated)
        typ: Type = default_type
        for i, arg in enumerate(expr.args):
            if expr.arg_names[i] is not None:
                if expr.arg_names[i] == "name":
                    if name is not None:
                        # Two names
                        raise TypeTranslationError()
                    name = _extract_argument_name(arg)
                    continue
                elif expr.arg_names[i] == "type":
                    if typ is not default_type:
                        # Two types
                        raise TypeTranslationError()
                    typ = expr_to_unanalyzed_type(arg, options, allow_new_syntax, expr)
                    continue
                else:
                    raise TypeTranslationError()
            elif i == 0:
                typ = expr_to_unanalyzed_type(arg, options, allow_new_syntax, expr)
            elif i == 1:
                name = _extract_argument_name(arg)
            else:
                raise TypeTranslationError()
        return CallableArgument(typ, name, arg_const, expr.line, expr.column)
    elif isinstance(expr, ListExpr):
        return TypeList([expr_to_unanalyzed_type(t, options, allow_new_syntax, expr)
                         for t in expr.items],
                        line=expr.line, column=expr.column)
    elif isinstance(expr, StrExpr):
        return parse_type_string(expr.value, 'builtins.str', expr.line, expr.column,
                                 assume_str_is_unicode=expr.from_python_3)
    elif isinstance(expr, BytesExpr):
        return parse_type_string(expr.value, 'builtins.bytes', expr.line, expr.column,
                                 assume_str_is_unicode=False)
    elif isinstance(expr, UnicodeExpr):
        return parse_type_string(expr.value, 'builtins.unicode', expr.line, expr.column,
                                 assume_str_is_unicode=True)
    elif isinstance(expr, UnaryExpr):
        typ = expr_to_unanalyzed_type(expr.expr, options, allow_new_syntax)
        if isinstance(typ, RawExpressionType):
            if isinstance(typ.literal_value, int) and expr.op == '-':
                typ.literal_value *= -1
                return typ
        raise TypeTranslationError()
    elif isinstance(expr, IntExpr):
        return RawExpressionType(expr.value, 'builtins.int', line=expr.line, column=expr.column)
    elif isinstance(expr, FloatExpr):
        # Floats are not valid parameters for RawExpressionType , so we just
        # pass in 'None' for now. We'll report the appropriate error at a later stage.
        return RawExpressionType(None, 'builtins.float', line=expr.line, column=expr.column)
    elif isinstance(expr, ComplexExpr):
        # Same thing as above with complex numbers.
        return RawExpressionType(None, 'builtins.complex', line=expr.line, column=expr.column)
    elif isinstance(expr, EllipsisExpr):
        return EllipsisType(expr.line)
    else:
        raise TypeTranslationError()
</t>
<t tx="ekr.20220525082934.330">def exists(self, path: str) -&gt; bool:
    try:
        self.stat(path)
    except FileNotFoundError:
        return False
    return True

</t>
<t tx="ekr.20220525082934.331">def read(self, path: str) -&gt; bytes:
    if path in self.read_cache:
        return self.read_cache[path]
    if path in self.read_error_cache:
        raise self.read_error_cache[path]

    # Need to stat first so that the contents of file are from no
    # earlier instant than the mtime reported by self.stat().
    self.stat(path)

    dirname, basename = os.path.split(path)
    dirname = os.path.normpath(dirname)
    # Check the fake cache.
    if basename == '__init__.py' and dirname in self.fake_package_cache:
        data = b''
    else:
        try:
            with open(path, 'rb') as f:
                data = f.read()
        except OSError as err:
            self.read_error_cache[path] = err
            raise

    self.read_cache[path] = data
    self.hash_cache[path] = hash_digest(data)
    return data

</t>
<t tx="ekr.20220525082934.332">def hash_digest(self, path: str) -&gt; str:
    if path not in self.hash_cache:
        self.read(path)
    return self.hash_cache[path]

</t>
<t tx="ekr.20220525082934.333">def samefile(self, f1: str, f2: str) -&gt; bool:
    s1 = self.stat(f1)
    s2 = self.stat(f2)
    return os.path.samestat(s1, s2)


</t>
<t tx="ekr.20220525082934.334">def copy_os_error(e: OSError) -&gt; OSError:
    new = OSError(*e.args)
    new.errno = e.errno
    new.strerror = e.strerror
    new.filename = e.filename
    if e.filename2:
        new.filename2 = e.filename2
    return new
</t>
<t tx="ekr.20220525082934.335">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Watch parts of the file system for changes."""

from mypy.fscache import FileSystemCache
from typing import AbstractSet, Dict, Iterable, List, NamedTuple, Optional, Set, Tuple


FileData = NamedTuple('FileData', [('st_mtime', float),
                                   ('st_size', int),
                                   ('hash', str)])


@others
</t>
<t tx="ekr.20220525082934.336">class FileSystemWatcher:
    """Watcher for file system changes among specific paths.

    All file system access is performed using FileSystemCache. We
    detect changed files by stat()ing them all and comparing hashes
    of potentially changed files. If a file has both size and mtime
    unmodified, the file is assumed to be unchanged.

    An important goal of this class is to make it easier to eventually
    use file system events to detect file changes.

    Note: This class doesn't flush the file system cache. If you don't
    manually flush it, changes won't be seen.
    """

    # TODO: Watching directories?
    # TODO: Handle non-files

    @others
</t>
<t tx="ekr.20220525082934.337">def __init__(self, fs: FileSystemCache) -&gt; None:
    self.fs = fs
    self._paths: Set[str] = set()
    self._file_data: Dict[str, Optional[FileData]] = {}

</t>
<t tx="ekr.20220525082934.338">def dump_file_data(self) -&gt; Dict[str, Tuple[float, int, str]]:
    return {k: v for k, v in self._file_data.items() if v is not None}

</t>
<t tx="ekr.20220525082934.339">def set_file_data(self, path: str, data: FileData) -&gt; None:
    self._file_data[path] = data

</t>
<t tx="ekr.20220525082934.34">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from mypy.util import unnamed_function
import copy
import re
import sys
import warnings

import typing  # for typing.Type, which conflicts with types.Type
from typing import (
    Tuple, Union, TypeVar, Callable, Sequence, Optional, Any, Dict, cast, List
)

from typing_extensions import Final, Literal, overload

from mypy.sharedparse import (
    special_function_elide_names, argument_elide_name,
)
from mypy.nodes import (
    MypyFile, Node, ImportBase, Import, ImportAll, ImportFrom, FuncDef,
    OverloadedFuncDef, OverloadPart,
    ClassDef, Decorator, Block, Var, OperatorAssignmentStmt,
    ExpressionStmt, AssignmentStmt, ReturnStmt, RaiseStmt, AssertStmt,
    DelStmt, BreakStmt, ContinueStmt, PassStmt, GlobalDecl,
    WhileStmt, ForStmt, IfStmt, TryStmt, WithStmt, MatchStmt,
    TupleExpr, GeneratorExpr, ListComprehension, ListExpr, ConditionalExpr,
    DictExpr, SetExpr, NameExpr, IntExpr, StrExpr, BytesExpr, UnicodeExpr,
    FloatExpr, CallExpr, SuperExpr, MemberExpr, IndexExpr, SliceExpr, OpExpr,
    UnaryExpr, LambdaExpr, ComparisonExpr, AssignmentExpr,
    StarExpr, YieldFromExpr, NonlocalDecl, DictionaryComprehension,
    SetComprehension, ComplexExpr, EllipsisExpr, YieldExpr, Argument,
    AwaitExpr, TempNode, RefExpr, Expression, Statement,
    ArgKind, ARG_POS, ARG_OPT, ARG_STAR, ARG_NAMED, ARG_NAMED_OPT, ARG_STAR2,
    check_arg_names,
    FakeInfo,
)
from mypy.patterns import (
    AsPattern, OrPattern, ValuePattern, SequencePattern, StarredPattern, MappingPattern,
    ClassPattern, SingletonPattern
)
from mypy.types import (
    Type, CallableType, AnyType, UnboundType, TupleType, TypeList, EllipsisType, CallableArgument,
    TypeOfAny, Instance, RawExpressionType, ProperType, UnionType,
)
from mypy import defaults
from mypy import message_registry, errorcodes as codes
from mypy.errors import Errors
from mypy.options import Options
from mypy.reachability import infer_reachability_of_if_statement, mark_block_unreachable
from mypy.util import bytes_to_human_readable_repr

try:
    # pull this into a final variable to make mypyc be quiet about the
    # the default argument warning
    PY_MINOR_VERSION: Final = sys.version_info[1]

    # Check if we can use the stdlib ast module instead of typed_ast.
    if sys.version_info &gt;= (3, 8):
        import ast as ast3
        assert 'kind' in ast3.Constant._fields, \
               f"This 3.8.0 alpha ({sys.version.split()[0]}) is too old; 3.8.0a3 required"
        # TODO: Num, Str, Bytes, NameConstant, Ellipsis are deprecated in 3.8.
        # TODO: Index, ExtSlice are deprecated in 3.9.
        from ast import (
            AST,
            Call,
            FunctionType,
            Name,
            Attribute,
            Ellipsis as ast3_Ellipsis,
            Starred,
            NameConstant,
            Expression as ast3_Expression,
            Str,
            Bytes,
            Index,
            Num,
            UnaryOp,
            USub,
        )

        def ast3_parse(source: Union[str, bytes], filename: str, mode: str,
                       feature_version: int = PY_MINOR_VERSION) -&gt; AST:
            return ast3.parse(source, filename, mode,
                              type_comments=True,  # This works the magic
                              feature_version=feature_version)

        NamedExpr = ast3.NamedExpr
        Constant = ast3.Constant
    else:
        from typed_ast import ast3
        from typed_ast.ast3 import (
            AST,
            Call,
            FunctionType,
            Name,
            Attribute,
            Ellipsis as ast3_Ellipsis,
            Starred,
            NameConstant,
            Expression as ast3_Expression,
            Str,
            Bytes,
            Index,
            Num,
            UnaryOp,
            USub,
        )

        def ast3_parse(source: Union[str, bytes], filename: str, mode: str,
                       feature_version: int = PY_MINOR_VERSION) -&gt; AST:
            return ast3.parse(source, filename, mode, feature_version=feature_version)

        # These don't exist before 3.8
        NamedExpr = Any
        Constant = Any

    if sys.version_info &gt;= (3, 10):
        Match = ast3.Match
        MatchValue = ast3.MatchValue
        MatchSingleton = ast3.MatchSingleton
        MatchSequence = ast3.MatchSequence
        MatchStar = ast3.MatchStar
        MatchMapping = ast3.MatchMapping
        MatchClass = ast3.MatchClass
        MatchAs = ast3.MatchAs
        MatchOr = ast3.MatchOr
        AstNode = Union[ast3.expr, ast3.stmt, ast3.pattern, ast3.ExceptHandler]
    else:
        Match = Any
        MatchValue = Any
        MatchSingleton = Any
        MatchSequence = Any
        MatchStar = Any
        MatchMapping = Any
        MatchClass = Any
        MatchAs = Any
        MatchOr = Any
        AstNode = Union[ast3.expr, ast3.stmt, ast3.ExceptHandler]
except ImportError:
    try:
        from typed_ast import ast35  # type: ignore[attr-defined]  # noqa: F401
    except ImportError:
        print('The typed_ast package is not installed.\n'
              'You can install it with `python3 -m pip install typed-ast`.',
              file=sys.stderr)
    else:
        print('You need a more recent version of the typed_ast package.\n'
              'You can update to the latest version with '
              '`python3 -m pip install -U typed-ast`.',
              file=sys.stderr)
    sys.exit(1)

N = TypeVar('N', bound=Node)

# There is no way to create reasonable fallbacks at this stage,
# they must be patched later.
MISSING_FALLBACK: Final = FakeInfo("fallback can't be filled out until semanal")
_dummy_fallback: Final = Instance(MISSING_FALLBACK, [], -1)

TYPE_COMMENT_SYNTAX_ERROR: Final = "syntax error in type comment"

INVALID_TYPE_IGNORE: Final = 'Invalid "type: ignore" comment'

TYPE_IGNORE_PATTERN: Final = re.compile(r'[^#]*#\s*type:\s*ignore\s*(.*)')


@others
</t>
<t tx="ekr.20220525082934.340">def add_watched_paths(self, paths: Iterable[str]) -&gt; None:
    for path in paths:
        if path not in self._paths:
            # By storing None this path will get reported as changed by
            # find_changed if it exists.
            self._file_data[path] = None
    self._paths |= set(paths)

</t>
<t tx="ekr.20220525082934.341">def remove_watched_paths(self, paths: Iterable[str]) -&gt; None:
    for path in paths:
        if path in self._file_data:
            del self._file_data[path]
    self._paths -= set(paths)

</t>
<t tx="ekr.20220525082934.342">def _update(self, path: str) -&gt; None:
    st = self.fs.stat(path)
    hash_digest = self.fs.hash_digest(path)
    self._file_data[path] = FileData(st.st_mtime, st.st_size, hash_digest)

</t>
<t tx="ekr.20220525082934.343">def _find_changed(self, paths: Iterable[str]) -&gt; AbstractSet[str]:
    changed = set()
    for path in paths:
        old = self._file_data[path]
        try:
            st = self.fs.stat(path)
        except FileNotFoundError:
            if old is not None:
                # File was deleted.
                changed.add(path)
                self._file_data[path] = None
        else:
            if old is None:
                # File is new.
                changed.add(path)
                self._update(path)
            # Round mtimes down, to match the mtimes we write to meta files
            elif st.st_size != old.st_size or int(st.st_mtime) != int(old.st_mtime):
                # Only look for changes if size or mtime has changed as an
                # optimization, since calculating hash is expensive.
                new_hash = self.fs.hash_digest(path)
                self._update(path)
                if st.st_size != old.st_size or new_hash != old.hash:
                    # Changed file.
                    changed.add(path)
    return changed

</t>
<t tx="ekr.20220525082934.344">def find_changed(self) -&gt; AbstractSet[str]:
    """Return paths that have changes since the last call, in the watched set."""
    return self._find_changed(self._paths)

</t>
<t tx="ekr.20220525082934.345">def update_changed(self,
                   remove: List[str],
                   update: List[str],
                   ) -&gt; AbstractSet[str]:
    """Alternative to find_changed() given explicit changes.

    This only calls self.fs.stat() on added or updated files, not
    on all files.  It believes all other files are unchanged!

    Implies add_watched_paths() for add and update, and
    remove_watched_paths() for remove.
    """
    self.remove_watched_paths(remove)
    self.add_watched_paths(update)
    return self._find_changed(update)
</t>
<t tx="ekr.20220525082934.346">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
import gc
import time

from typing import Mapping, Optional


@others
</t>
<t tx="ekr.20220525082934.347">class GcLogger:
    """Context manager to log GC stats and overall time."""

    @others
</t>
<t tx="ekr.20220525082934.348">def __enter__(self) -&gt; 'GcLogger':
    self.gc_start_time: Optional[float] = None
    self.gc_time = 0.0
    self.gc_calls = 0
    self.gc_collected = 0
    self.gc_uncollectable = 0
    gc.callbacks.append(self.gc_callback)
    self.start_time = time.time()
    return self

</t>
<t tx="ekr.20220525082934.349">def gc_callback(self, phase: str, info: Mapping[str, int]) -&gt; None:
    if phase == 'start':
        assert self.gc_start_time is None, "Start phase out of sequence"
        self.gc_start_time = time.time()
    elif phase == 'stop':
        assert self.gc_start_time is not None, "Stop phase out of sequence"
        self.gc_calls += 1
        self.gc_time += time.time() - self.gc_start_time
        self.gc_start_time = None
        self.gc_collected += info['collected']
        self.gc_uncollectable += info['uncollectable']
    else:
        assert False, f"Unrecognized gc phase ({phase!r})"

</t>
<t tx="ekr.20220525082934.35">def parse(source: Union[str, bytes],
          fnam: str,
          module: Optional[str],
          errors: Optional[Errors] = None,
          options: Optional[Options] = None) -&gt; MypyFile:

    """Parse a source file, without doing any semantic analysis.

    Return the parse tree. If errors is not provided, raise ParseError
    on failure. Otherwise, use the errors object to report parse errors.
    """
    raise_on_error = False
    if errors is None:
        errors = Errors()
        raise_on_error = True
    if options is None:
        options = Options()
    errors.set_file(fnam, module)
    is_stub_file = fnam.endswith('.pyi')
    try:
        if is_stub_file:
            feature_version = defaults.PYTHON3_VERSION[1]
        else:
            assert options.python_version[0] &gt;= 3
            feature_version = options.python_version[1]
        # Disable deprecation warnings about \u
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=DeprecationWarning)
            ast = ast3_parse(source, fnam, 'exec', feature_version=feature_version)

        tree = ASTConverter(options=options,
                            is_stub=is_stub_file,
                            errors=errors,
                            ).visit(ast)
        tree.path = fnam
        tree.is_stub = is_stub_file
    except SyntaxError as e:
        # alias to please mypyc
        is_py38_or_earlier = sys.version_info &lt; (3, 9)
        if is_py38_or_earlier and e.filename == "&lt;fstring&gt;":
            # In Python 3.8 and earlier, syntax errors in f-strings have lineno relative to the
            # start of the f-string. This would be misleading, as mypy will report the error as the
            # lineno within the file.
            e.lineno = None
        errors.report(e.lineno if e.lineno is not None else -1, e.offset, e.msg, blocker=True,
                      code=codes.SYNTAX)
        tree = MypyFile([], [], False, {})

    if raise_on_error and errors.is_errors():
        errors.raise_error()

    return tree


</t>
<t tx="ekr.20220525082934.350">def __exit__(self, *args: object) -&gt; None:
    while self.gc_callback in gc.callbacks:
        gc.callbacks.remove(self.gc_callback)

</t>
<t tx="ekr.20220525082934.351">def get_stats(self) -&gt; Mapping[str, float]:
    end_time = time.time()
    result = {}
    result['gc_time'] = self.gc_time
    result['gc_calls'] = self.gc_calls
    result['gc_collected'] = self.gc_collected
    result['gc_uncollectable'] = self.gc_uncollectable
    result['build_time'] = end_time - self.start_time
    return result
</t>
<t tx="ekr.20220525082934.352">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Git utilities."""

# Used also from setup.py, so don't pull in anything additional here (like mypy or typing):
import os
import subprocess


@others
</t>
<t tx="ekr.20220525082934.353">def is_git_repo(dir: str) -&gt; bool:
    """Is the given directory version-controlled with git?"""
    return os.path.exists(os.path.join(dir, ".git"))


</t>
<t tx="ekr.20220525082934.354">def have_git() -&gt; bool:
    """Can we run the git executable?"""
    try:
        subprocess.check_output(["git", "--help"])
        return True
    except subprocess.CalledProcessError:
        return False
    except OSError:
        return False


</t>
<t tx="ekr.20220525082934.355">def git_revision(dir: str) -&gt; bytes:
    """Get the SHA-1 of the HEAD of a git repository."""
    return subprocess.check_output(["git", "rev-parse", "HEAD"], cwd=dir).strip()


</t>
<t tx="ekr.20220525082934.356">def is_dirty(dir: str) -&gt; bool:
    """Check whether a git repository has uncommitted changes."""
    output = subprocess.check_output(["git", "status", "-uno", "--porcelain"], cwd=dir)
    return output.strip() != b""
</t>
<t tx="ekr.20220525082934.357">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from typing import Dict, Iterable, List, Optional, Set, Union

from mypy.types import TypeVisitor
import mypy.types as types
from mypy.util import split_module_names


@others
</t>
<t tx="ekr.20220525082934.358">def extract_module_names(type_name: Optional[str]) -&gt; List[str]:
    """Returns the module names of a fully qualified type name."""
    if type_name is not None:
        # Discard the first one, which is just the qualified name of the type
        possible_module_names = split_module_names(type_name)
        return possible_module_names[1:]
    else:
        return []


</t>
<t tx="ekr.20220525082934.359">class TypeIndirectionVisitor(TypeVisitor[Set[str]]):
    """Returns all module references within a particular type."""

    @others
</t>
<t tx="ekr.20220525082934.36">def parse_type_ignore_tag(tag: Optional[str]) -&gt; Optional[List[str]]:
    """Parse optional "[code, ...]" tag after "# type: ignore".

    Return:
     * [] if no tag was found (ignore all errors)
     * list of ignored error codes if a tag was found
     * None if the tag was invalid.
    """
    if not tag or tag.strip() == '' or tag.strip().startswith('#'):
        # No tag -- ignore all errors.
        return []
    m = re.match(r'\s*\[([^]#]*)\]\s*(#.*)?$', tag)
    if m is None:
        # Invalid "# type: ignore" comment.
        return None
    return [code.strip() for code in m.group(1).split(',')]


</t>
<t tx="ekr.20220525082934.360">def __init__(self) -&gt; None:
    self.cache: Dict[types.Type, Set[str]] = {}
    self.seen_aliases: Set[types.TypeAliasType] = set()

</t>
<t tx="ekr.20220525082934.361">def find_modules(self, typs: Iterable[types.Type]) -&gt; Set[str]:
    self.seen_aliases.clear()
    return self._visit(typs)

</t>
<t tx="ekr.20220525082934.362">def _visit(self, typ_or_typs: Union[types.Type, Iterable[types.Type]]) -&gt; Set[str]:
    typs = [typ_or_typs] if isinstance(typ_or_typs, types.Type) else typ_or_typs
    output: Set[str] = set()
    for typ in typs:
        if isinstance(typ, types.TypeAliasType):
            # Avoid infinite recursion for recursive type aliases.
            if typ in self.seen_aliases:
                continue
            self.seen_aliases.add(typ)
        if typ in self.cache:
            modules = self.cache[typ]
        else:
            modules = typ.accept(self)
            self.cache[typ] = set(modules)
        output.update(modules)
    return output

</t>
<t tx="ekr.20220525082934.363">def visit_unbound_type(self, t: types.UnboundType) -&gt; Set[str]:
    return self._visit(t.args)

</t>
<t tx="ekr.20220525082934.364">def visit_any(self, t: types.AnyType) -&gt; Set[str]:
    return set()

</t>
<t tx="ekr.20220525082934.365">def visit_none_type(self, t: types.NoneType) -&gt; Set[str]:
    return set()

</t>
<t tx="ekr.20220525082934.366">def visit_uninhabited_type(self, t: types.UninhabitedType) -&gt; Set[str]:
    return set()

</t>
<t tx="ekr.20220525082934.367">def visit_erased_type(self, t: types.ErasedType) -&gt; Set[str]:
    return set()

</t>
<t tx="ekr.20220525082934.368">def visit_deleted_type(self, t: types.DeletedType) -&gt; Set[str]:
    return set()

</t>
<t tx="ekr.20220525082934.369">def visit_type_var(self, t: types.TypeVarType) -&gt; Set[str]:
    return self._visit(t.values) | self._visit(t.upper_bound)

</t>
<t tx="ekr.20220525082934.37">def parse_type_comment(type_comment: str,
                       line: int,
                       column: int,
                       errors: Optional[Errors],
                       assume_str_is_unicode: bool = True,
                       ) -&gt; Tuple[Optional[List[str]], Optional[ProperType]]:
    """Parse type portion of a type comment (+ optional type ignore).

    Return (ignore info, parsed type).
    """
    try:
        typ = ast3_parse(type_comment, '&lt;type_comment&gt;', 'eval')
    except SyntaxError:
        if errors is not None:
            stripped_type = type_comment.split("#", 2)[0].strip()
            err_msg = f'{TYPE_COMMENT_SYNTAX_ERROR} "{stripped_type}"'
            errors.report(line, column, err_msg, blocker=True, code=codes.SYNTAX)
            return None, None
        else:
            raise
    else:
        extra_ignore = TYPE_IGNORE_PATTERN.match(type_comment)
        if extra_ignore:
            # Typeshed has a non-optional return type for group!
            tag: Optional[str] = cast(Any, extra_ignore).group(1)
            ignored: Optional[List[str]] = parse_type_ignore_tag(tag)
            if ignored is None:
                if errors is not None:
                    errors.report(line, column, INVALID_TYPE_IGNORE, code=codes.SYNTAX)
                else:
                    raise SyntaxError
        else:
            ignored = None
        assert isinstance(typ, ast3_Expression)
        converted = TypeConverter(errors,
                                  line=line,
                                  override_column=column,
                                  assume_str_is_unicode=assume_str_is_unicode,
                                  is_evaluated=False).visit(typ.body)
        return ignored, converted


</t>
<t tx="ekr.20220525082934.370">def visit_param_spec(self, t: types.ParamSpecType) -&gt; Set[str]:
    return set()

</t>
<t tx="ekr.20220525082934.371">def visit_type_var_tuple(self, t: types.TypeVarTupleType) -&gt; Set[str]:
    return self._visit(t.upper_bound)

</t>
<t tx="ekr.20220525082934.372">def visit_unpack_type(self, t: types.UnpackType) -&gt; Set[str]:
    return t.type.accept(self)

</t>
<t tx="ekr.20220525082934.373">def visit_parameters(self, t: types.Parameters) -&gt; Set[str]:
    return self._visit(t.arg_types)

</t>
<t tx="ekr.20220525082934.374">def visit_instance(self, t: types.Instance) -&gt; Set[str]:
    out = self._visit(t.args)
    if t.type:
        # Uses of a class depend on everything in the MRO,
        # as changes to classes in the MRO can add types to methods,
        # change property types, change the MRO itself, etc.
        for s in t.type.mro:
            out.update(split_module_names(s.module_name))
        if t.type.metaclass_type is not None:
            out.update(split_module_names(t.type.metaclass_type.type.module_name))
    return out

</t>
<t tx="ekr.20220525082934.375">def visit_callable_type(self, t: types.CallableType) -&gt; Set[str]:
    out = self._visit(t.arg_types) | self._visit(t.ret_type)
    if t.definition is not None:
        out.update(extract_module_names(t.definition.fullname))
    return out

</t>
<t tx="ekr.20220525082934.376">def visit_overloaded(self, t: types.Overloaded) -&gt; Set[str]:
    return self._visit(t.items) | self._visit(t.fallback)

</t>
<t tx="ekr.20220525082934.377">def visit_tuple_type(self, t: types.TupleType) -&gt; Set[str]:
    return self._visit(t.items) | self._visit(t.partial_fallback)

</t>
<t tx="ekr.20220525082934.378">def visit_typeddict_type(self, t: types.TypedDictType) -&gt; Set[str]:
    return self._visit(t.items.values()) | self._visit(t.fallback)

</t>
<t tx="ekr.20220525082934.379">def visit_literal_type(self, t: types.LiteralType) -&gt; Set[str]:
    return self._visit(t.fallback)

</t>
<t tx="ekr.20220525082934.38">def parse_type_string(expr_string: str, expr_fallback_name: str,
                      line: int, column: int, assume_str_is_unicode: bool = True) -&gt; ProperType:
    """Parses a type that was originally present inside of an explicit string,
    byte string, or unicode string.

    For example, suppose we have the type `Foo["blah"]`. We should parse the
    string expression "blah" using this function.

    If `assume_str_is_unicode` is set to true, this function will assume that
    `Foo["blah"]` is equivalent to `Foo[u"blah"]`. Otherwise, it assumes it's
    equivalent to `Foo[b"blah"]`.

    The caller is responsible for keeping track of the context in which the
    type string was encountered (e.g. in Python 3 code, Python 2 code, Python 2
    code with unicode_literals...) and setting `assume_str_is_unicode` accordingly.
    """
    try:
        _, node = parse_type_comment(expr_string.strip(), line=line, column=column, errors=None,
                                     assume_str_is_unicode=assume_str_is_unicode)
        if isinstance(node, UnboundType) and node.original_str_expr is None:
            node.original_str_expr = expr_string
            node.original_str_fallback = expr_fallback_name
            return node
        elif isinstance(node, UnionType):
            return node
        else:
            return RawExpressionType(expr_string, expr_fallback_name, line, column)
    except (SyntaxError, ValueError):
        # Note: the parser will raise a `ValueError` instead of a SyntaxError if
        # the string happens to contain things like \x00.
        return RawExpressionType(expr_string, expr_fallback_name, line, column)


</t>
<t tx="ekr.20220525082934.380">def visit_union_type(self, t: types.UnionType) -&gt; Set[str]:
    return self._visit(t.items)

</t>
<t tx="ekr.20220525082934.381">def visit_partial_type(self, t: types.PartialType) -&gt; Set[str]:
    return set()

</t>
<t tx="ekr.20220525082934.382">def visit_type_type(self, t: types.TypeType) -&gt; Set[str]:
    return self._visit(t.item)

</t>
<t tx="ekr.20220525082934.383">def visit_type_alias_type(self, t: types.TypeAliasType) -&gt; Set[str]:
    return self._visit(types.get_proper_type(t))
</t>
<t tx="ekr.20220525082934.384">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Utilities for type argument inference."""

from typing import List, Optional, Sequence, NamedTuple

from mypy.constraints import (
    infer_constraints, infer_constraints_for_callable, SUBTYPE_OF, SUPERTYPE_OF
)
from mypy.types import Type, TypeVarId, CallableType, Instance
from mypy.nodes import ArgKind
from mypy.solve import solve_constraints


@others
</t>
<t tx="ekr.20220525082934.385">class ArgumentInferContext(NamedTuple):
    """Type argument inference context.

    We need this because we pass around ``Mapping`` and ``Iterable`` types.
    These types are only known by ``TypeChecker`` itself.
    It is required for ``*`` and ``**`` argument inference.

    https://github.com/python/mypy/issues/11144
    """

    mapping_type: Instance
    iterable_type: Instance


</t>
<t tx="ekr.20220525082934.386">def infer_function_type_arguments(callee_type: CallableType,
                                  arg_types: Sequence[Optional[Type]],
                                  arg_kinds: List[ArgKind],
                                  formal_to_actual: List[List[int]],
                                  context: ArgumentInferContext,
                                  strict: bool = True) -&gt; List[Optional[Type]]:
    """Infer the type arguments of a generic function.

    Return an array of lower bound types for the type variables -1 (at
    index 0), -2 (at index 1), etc. A lower bound is None if a value
    could not be inferred.

    Arguments:
      callee_type: the target generic function
      arg_types: argument types at the call site (each optional; if None,
                 we are not considering this argument in the current pass)
      arg_kinds: nodes.ARG_* values for arg_types
      formal_to_actual: mapping from formal to actual variable indices
    """
    # Infer constraints.
    constraints = infer_constraints_for_callable(
        callee_type, arg_types, arg_kinds, formal_to_actual, context)

    # Solve constraints.
    type_vars = callee_type.type_var_ids()
    return solve_constraints(type_vars, constraints, strict)


</t>
<t tx="ekr.20220525082934.387">def infer_type_arguments(type_var_ids: List[TypeVarId],
                         template: Type, actual: Type,
                         is_supertype: bool = False) -&gt; List[Optional[Type]]:
    # Like infer_function_type_arguments, but only match a single type
    # against a generic type.
    constraints = infer_constraints(template, actual,
                                    SUPERTYPE_OF if is_supertype else SUBTYPE_OF)
    return solve_constraints(type_var_ids, constraints)
</t>
<t tx="ekr.20220525082934.388">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Cross platform abstractions for inter-process communication

On Unix, this uses AF_UNIX sockets.
On Windows, this uses NamedPipes.
"""

import base64
import os
import shutil
import sys
import tempfile

from typing import Optional, Callable
from typing_extensions import Final, Type

from types import TracebackType

if sys.platform == 'win32':
    # This may be private, but it is needed for IPC on Windows, and is basically stable
    import _winapi
    import ctypes

    _IPCHandle = int

    kernel32 = ctypes.windll.kernel32
    DisconnectNamedPipe: Callable[[_IPCHandle], int] = kernel32.DisconnectNamedPipe
    FlushFileBuffers: Callable[[_IPCHandle], int] = kernel32.FlushFileBuffers
else:
    import socket
    _IPCHandle = socket.socket


@others
</t>
<t tx="ekr.20220525082934.389">class IPCException(Exception):
    """Exception for IPC issues."""
    pass


</t>
<t tx="ekr.20220525082934.39">def is_no_type_check_decorator(expr: ast3.expr) -&gt; bool:
    if isinstance(expr, Name):
        return expr.id == 'no_type_check'
    elif isinstance(expr, Attribute):
        if isinstance(expr.value, Name):
            return expr.value.id == 'typing' and expr.attr == 'no_type_check'
    return False


</t>
<t tx="ekr.20220525082934.390">class IPCBase:
    """Base class for communication between the dmypy client and server.

    This contains logic shared between the client and server, such as reading
    and writing.
    """

    connection: _IPCHandle

    @others
</t>
<t tx="ekr.20220525082934.391">def __init__(self, name: str, timeout: Optional[float]) -&gt; None:
    self.name = name
    self.timeout = timeout

</t>
<t tx="ekr.20220525082934.392">def read(self, size: int = 100000) -&gt; bytes:
    """Read bytes from an IPC connection until its empty."""
    bdata = bytearray()
    if sys.platform == 'win32':
        while True:
            ov, err = _winapi.ReadFile(self.connection, size, overlapped=True)
            try:
                if err == _winapi.ERROR_IO_PENDING:
                    timeout = int(self.timeout * 1000) if self.timeout else _winapi.INFINITE
                    res = _winapi.WaitForSingleObject(ov.event, timeout)
                    if res != _winapi.WAIT_OBJECT_0:
                        raise IPCException(f"Bad result from I/O wait: {res}")
            except BaseException:
                ov.cancel()
                raise
            _, err = ov.GetOverlappedResult(True)
            more = ov.getbuffer()
            if more:
                bdata.extend(more)
            if err == 0:
                # we are done!
                break
            elif err == _winapi.ERROR_MORE_DATA:
                # read again
                continue
            elif err == _winapi.ERROR_OPERATION_ABORTED:
                raise IPCException("ReadFile operation aborted.")
    else:
        while True:
            more = self.connection.recv(size)
            if not more:
                break
            bdata.extend(more)
    return bytes(bdata)

</t>
<t tx="ekr.20220525082934.393">def write(self, data: bytes) -&gt; None:
    """Write bytes to an IPC connection."""
    if sys.platform == 'win32':
        try:
            ov, err = _winapi.WriteFile(self.connection, data, overlapped=True)
            # TODO: remove once typeshed supports Literal types
            assert isinstance(ov, _winapi.Overlapped)
            assert isinstance(err, int)
            try:
                if err == _winapi.ERROR_IO_PENDING:
                    timeout = int(self.timeout * 1000) if self.timeout else _winapi.INFINITE
                    res = _winapi.WaitForSingleObject(ov.event, timeout)
                    if res != _winapi.WAIT_OBJECT_0:
                        raise IPCException(f"Bad result from I/O wait: {res}")
                elif err != 0:
                    raise IPCException(f"Failed writing to pipe with error: {err}")
            except BaseException:
                ov.cancel()
                raise
            bytes_written, err = ov.GetOverlappedResult(True)
            assert err == 0, err
            assert bytes_written == len(data)
        except OSError as e:
            raise IPCException(f"Failed to write with error: {e.winerror}") from e
    else:
        self.connection.sendall(data)
        self.connection.shutdown(socket.SHUT_WR)

</t>
<t tx="ekr.20220525082934.394">def close(self) -&gt; None:
    if sys.platform == 'win32':
        if self.connection != _winapi.NULL:
            _winapi.CloseHandle(self.connection)
    else:
        self.connection.close()


</t>
<t tx="ekr.20220525082934.395">class IPCClient(IPCBase):
    """The client side of an IPC connection."""

    @others
</t>
<t tx="ekr.20220525082934.396">def __init__(self, name: str, timeout: Optional[float]) -&gt; None:
    super().__init__(name, timeout)
    if sys.platform == 'win32':
        timeout = int(self.timeout * 1000) if self.timeout else _winapi.NMPWAIT_WAIT_FOREVER
        try:
            _winapi.WaitNamedPipe(self.name, timeout)
        except FileNotFoundError as e:
            raise IPCException(f"The NamedPipe at {self.name} was not found.") from e
        except OSError as e:
            if e.winerror == _winapi.ERROR_SEM_TIMEOUT:
                raise IPCException("Timed out waiting for connection.") from e
            else:
                raise
        try:
            self.connection = _winapi.CreateFile(
                self.name,
                _winapi.GENERIC_READ | _winapi.GENERIC_WRITE,
                0,
                _winapi.NULL,
                _winapi.OPEN_EXISTING,
                _winapi.FILE_FLAG_OVERLAPPED,
                _winapi.NULL,
            )
        except OSError as e:
            if e.winerror == _winapi.ERROR_PIPE_BUSY:
                raise IPCException("The connection is busy.") from e
            else:
                raise
        _winapi.SetNamedPipeHandleState(self.connection,
                                        _winapi.PIPE_READMODE_MESSAGE,
                                        None,
                                        None)
    else:
        self.connection = socket.socket(socket.AF_UNIX)
        self.connection.settimeout(timeout)
        self.connection.connect(name)

</t>
<t tx="ekr.20220525082934.397">def __enter__(self) -&gt; 'IPCClient':
    return self

</t>
<t tx="ekr.20220525082934.398">def __exit__(self,
             exc_ty: 'Optional[Type[BaseException]]' = None,
             exc_val: Optional[BaseException] = None,
             exc_tb: Optional[TracebackType] = None,
             ) -&gt; None:
    self.close()


</t>
<t tx="ekr.20220525082934.399">class IPCServer(IPCBase):

    BUFFER_SIZE: Final = 2 ** 16

    @others
</t>
<t tx="ekr.20220525082934.4">def freshen_function_type_vars(callee: F) -&gt; F:
    """Substitute fresh type variables for generic function type variables."""
    if isinstance(callee, CallableType):
        if not callee.is_generic():
            return cast(F, callee)
        tvs = []
        tvmap: Dict[TypeVarId, Type] = {}
        for v in callee.variables:
            # TODO(PEP612): fix for ParamSpecType
            if isinstance(v, TypeVarType):
                tv: TypeVarLikeType = TypeVarType.new_unification_variable(v)
            elif isinstance(v, TypeVarTupleType):
                tv = TypeVarTupleType.new_unification_variable(v)
            else:
                assert isinstance(v, ParamSpecType)
                tv = ParamSpecType.new_unification_variable(v)
            tvs.append(tv)
            tvmap[v.id] = tv
        fresh = cast(CallableType, expand_type(callee, tvmap)).copy_modified(variables=tvs)
        return cast(F, fresh)
    else:
        assert isinstance(callee, Overloaded)
        fresh_overload = Overloaded([freshen_function_type_vars(item)
                                     for item in callee.items])
        return cast(F, fresh_overload)


</t>
<t tx="ekr.20220525082934.40">class ASTConverter:
    @others
</t>
<t tx="ekr.20220525082934.400">def __init__(self, name: str, timeout: Optional[float] = None) -&gt; None:
    if sys.platform == 'win32':
        name = r'\\.\pipe\{}-{}.pipe'.format(
            name, base64.urlsafe_b64encode(os.urandom(6)).decode())
    else:
        name = f'{name}.sock'
    super().__init__(name, timeout)
    if sys.platform == 'win32':
        self.connection = _winapi.CreateNamedPipe(self.name,
            _winapi.PIPE_ACCESS_DUPLEX
            | _winapi.FILE_FLAG_FIRST_PIPE_INSTANCE
            | _winapi.FILE_FLAG_OVERLAPPED,
            _winapi.PIPE_READMODE_MESSAGE
            | _winapi.PIPE_TYPE_MESSAGE
            | _winapi.PIPE_WAIT
            | 0x8,  # PIPE_REJECT_REMOTE_CLIENTS
            1,  # one instance
            self.BUFFER_SIZE,
            self.BUFFER_SIZE,
            _winapi.NMPWAIT_WAIT_FOREVER,
            0,  # Use default security descriptor
                                                  )
        if self.connection == -1:  # INVALID_HANDLE_VALUE
            err = _winapi.GetLastError()
            raise IPCException(f'Invalid handle to pipe: {err}')
    else:
        self.sock_directory = tempfile.mkdtemp()
        sockfile = os.path.join(self.sock_directory, self.name)
        self.sock = socket.socket(socket.AF_UNIX)
        self.sock.bind(sockfile)
        self.sock.listen(1)
        if timeout is not None:
            self.sock.settimeout(timeout)

</t>
<t tx="ekr.20220525082934.401">def __enter__(self) -&gt; 'IPCServer':
    if sys.platform == 'win32':
        # NOTE: It is theoretically possible that this will hang forever if the
        # client never connects, though this can be "solved" by killing the server
        try:
            ov = _winapi.ConnectNamedPipe(self.connection, overlapped=True)
            # TODO: remove once typeshed supports Literal types
            assert isinstance(ov, _winapi.Overlapped)
        except OSError as e:
            # Don't raise if the client already exists, or the client already connected
            if e.winerror not in (_winapi.ERROR_PIPE_CONNECTED, _winapi.ERROR_NO_DATA):
                raise
        else:
            try:
                timeout = int(self.timeout * 1000) if self.timeout else _winapi.INFINITE
                res = _winapi.WaitForSingleObject(ov.event, timeout)
                assert res == _winapi.WAIT_OBJECT_0
            except BaseException:
                ov.cancel()
                _winapi.CloseHandle(self.connection)
                raise
            _, err = ov.GetOverlappedResult(True)
            assert err == 0
    else:
        try:
            self.connection, _ = self.sock.accept()
        except socket.timeout as e:
            raise IPCException('The socket timed out') from e
    return self

</t>
<t tx="ekr.20220525082934.402">def __exit__(self,
             exc_ty: 'Optional[Type[BaseException]]' = None,
             exc_val: Optional[BaseException] = None,
             exc_tb: Optional[TracebackType] = None,
             ) -&gt; None:
    if sys.platform == 'win32':
        try:
            # Wait for the client to finish reading the last write before disconnecting
            if not FlushFileBuffers(self.connection):
                raise IPCException("Failed to flush NamedPipe buffer,"
                                   "maybe the client hung up?")
        finally:
            DisconnectNamedPipe(self.connection)
    else:
        self.close()

</t>
<t tx="ekr.20220525082934.403">def cleanup(self) -&gt; None:
    if sys.platform == 'win32':
        self.close()
    else:
        shutil.rmtree(self.sock_directory)

</t>
<t tx="ekr.20220525082934.404">@property
def connection_name(self) -&gt; str:
    if sys.platform == 'win32':
        return self.name
    else:
        return self.sock.getsockname()
</t>
<t tx="ekr.20220525082934.405">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Calculation of the least upper bound types (joins)."""

from mypy.backports import OrderedDict
from typing import List, Optional, Tuple

from mypy.types import (
    Type, AnyType, NoneType, TypeVisitor, Instance, UnboundType, TypeVarType, CallableType,
    TupleType, TypedDictType, ErasedType, UnionType, FunctionLike, Overloaded, LiteralType,
    PartialType, DeletedType, UninhabitedType, TypeType, TypeOfAny, get_proper_type,
    ProperType, get_proper_types, TypeAliasType, PlaceholderType, ParamSpecType, Parameters,
    UnpackType, TypeVarTupleType,
)
from mypy.maptype import map_instance_to_supertype
from mypy.subtypes import (
    is_subtype, is_equivalent, is_proper_subtype,
    is_protocol_implementation, find_member
)
from mypy.nodes import INVARIANT, COVARIANT, CONTRAVARIANT
import mypy.typeops
from mypy.state import state


@others
</t>
<t tx="ekr.20220525082934.406">class InstanceJoiner:
    @others
</t>
<t tx="ekr.20220525082934.407">def __init__(self) -&gt; None:
    self.seen_instances: List[Tuple[Instance, Instance]] = []

</t>
<t tx="ekr.20220525082934.408">def join_instances(self, t: Instance, s: Instance) -&gt; ProperType:
    if (t, s) in self.seen_instances or (s, t) in self.seen_instances:
        return object_from_instance(t)

    self.seen_instances.append((t, s))

    # Calculate the join of two instance types
    if t.type == s.type:
        # Simplest case: join two types with the same base type (but
        # potentially different arguments).

        # Combine type arguments.
        args: List[Type] = []
        # N.B: We use zip instead of indexing because the lengths might have
        # mismatches during daemon reprocessing.
        for ta, sa, type_var in zip(t.args, s.args, t.type.defn.type_vars):
            ta_proper = get_proper_type(ta)
            sa_proper = get_proper_type(sa)
            new_type: Optional[Type] = None
            if isinstance(ta_proper, AnyType):
                new_type = AnyType(TypeOfAny.from_another_any, ta_proper)
            elif isinstance(sa_proper, AnyType):
                new_type = AnyType(TypeOfAny.from_another_any, sa_proper)
            elif isinstance(type_var, TypeVarType):
                if type_var.variance == COVARIANT:
                    new_type = join_types(ta, sa, self)
                    if len(type_var.values) != 0 and new_type not in type_var.values:
                        self.seen_instances.pop()
                        return object_from_instance(t)
                    if not is_subtype(new_type, type_var.upper_bound):
                        self.seen_instances.pop()
                        return object_from_instance(t)
                # TODO: contravariant case should use meet but pass seen instances as
                # an argument to keep track of recursive checks.
                elif type_var.variance in (INVARIANT, CONTRAVARIANT):
                    if not is_equivalent(ta, sa):
                        self.seen_instances.pop()
                        return object_from_instance(t)
                    # If the types are different but equivalent, then an Any is involved
                    # so using a join in the contravariant case is also OK.
                    new_type = join_types(ta, sa, self)
            else:
                # ParamSpec type variables behave the same, independent of variance
                if not is_equivalent(ta, sa):
                    return get_proper_type(type_var.upper_bound)
                new_type = join_types(ta, sa, self)
            assert new_type is not None
            args.append(new_type)
        result: ProperType = Instance(t.type, args)
    elif t.type.bases and is_subtype(t, s, ignore_type_params=True):
        result = self.join_instances_via_supertype(t, s)
    else:
        # Now t is not a subtype of s, and t != s. Now s could be a subtype
        # of t; alternatively, we need to find a common supertype. This works
        # in of the both cases.
        result = self.join_instances_via_supertype(s, t)

    self.seen_instances.pop()
    return result

</t>
<t tx="ekr.20220525082934.409">def join_instances_via_supertype(self, t: Instance, s: Instance) -&gt; ProperType:
    # Give preference to joins via duck typing relationship, so that
    # join(int, float) == float, for example.
    if t.type._promote and is_subtype(t.type._promote, s):
        return join_types(t.type._promote, s, self)
    elif s.type._promote and is_subtype(s.type._promote, t):
        return join_types(t, s.type._promote, self)
    # Compute the "best" supertype of t when joined with s.
    # The definition of "best" may evolve; for now it is the one with
    # the longest MRO.  Ties are broken by using the earlier base.
    best: Optional[ProperType] = None
    for base in t.type.bases:
        mapped = map_instance_to_supertype(t, base.type)
        res = self.join_instances(mapped, s)
        if best is None or is_better(res, best):
            best = res
    assert best is not None
    promote = get_proper_type(t.type._promote)
    if isinstance(promote, Instance):
        res = self.join_instances(promote, s)
        if is_better(res, best):
            best = res
    return best


</t>
<t tx="ekr.20220525082934.41">def __init__(self,
             options: Options,
             is_stub: bool,
             errors: Errors) -&gt; None:
    # 'C' for class, 'F' for function
    self.class_and_function_stack: List[Literal["C", "F"]] = []
    self.imports: List[ImportBase] = []

    self.options = options
    self.is_stub = is_stub
    self.errors = errors

    self.type_ignores: Dict[int, List[str]] = {}

    # Cache of visit_X methods keyed by type of visited object
    self.visitor_cache: Dict[type, Callable[[Optional[AST]], Any]] = {}

</t>
<t tx="ekr.20220525082934.410">def join_simple(declaration: Optional[Type], s: Type, t: Type) -&gt; ProperType:
    """Return a simple least upper bound given the declared type."""
    # TODO: check infinite recursion for aliases here.
    declaration = get_proper_type(declaration)
    s = get_proper_type(s)
    t = get_proper_type(t)

    if (s.can_be_true, s.can_be_false) != (t.can_be_true, t.can_be_false):
        # if types are restricted in different ways, use the more general versions
        s = mypy.typeops.true_or_false(s)
        t = mypy.typeops.true_or_false(t)

    if isinstance(s, AnyType):
        return s

    if isinstance(s, ErasedType):
        return t

    if is_proper_subtype(s, t):
        return t

    if is_proper_subtype(t, s):
        return s

    if isinstance(declaration, UnionType):
        return mypy.typeops.make_simplified_union([s, t])

    if isinstance(s, NoneType) and not isinstance(t, NoneType):
        s, t = t, s

    if isinstance(s, UninhabitedType) and not isinstance(t, UninhabitedType):
        s, t = t, s

    value = t.accept(TypeJoinVisitor(s))
    if declaration is None or is_subtype(value, declaration):
        return value

    return declaration


</t>
<t tx="ekr.20220525082934.411">def trivial_join(s: Type, t: Type) -&gt; ProperType:
    """Return one of types (expanded) if it is a supertype of other, otherwise top type."""
    if is_subtype(s, t):
        return get_proper_type(t)
    elif is_subtype(t, s):
        return get_proper_type(s)
    else:
        return object_or_any_from_type(get_proper_type(t))


</t>
<t tx="ekr.20220525082934.412">def join_types(s: Type, t: Type, instance_joiner: Optional[InstanceJoiner] = None) -&gt; ProperType:
    """Return the least upper bound of s and t.

    For example, the join of 'int' and 'object' is 'object'.
    """
    if mypy.typeops.is_recursive_pair(s, t):
        # This case can trigger an infinite recursion, general support for this will be
        # tricky so we use a trivial join (like for protocols).
        return trivial_join(s, t)
    s = get_proper_type(s)
    t = get_proper_type(t)

    if (s.can_be_true, s.can_be_false) != (t.can_be_true, t.can_be_false):
        # if types are restricted in different ways, use the more general versions
        s = mypy.typeops.true_or_false(s)
        t = mypy.typeops.true_or_false(t)

    if isinstance(s, UnionType) and not isinstance(t, UnionType):
        s, t = t, s

    if isinstance(s, AnyType):
        return s

    if isinstance(s, ErasedType):
        return t

    if isinstance(s, NoneType) and not isinstance(t, NoneType):
        s, t = t, s

    if isinstance(s, UninhabitedType) and not isinstance(t, UninhabitedType):
        s, t = t, s

    # We shouldn't run into PlaceholderTypes here, but in practice we can encounter them
    # here in the presence of undefined names
    if isinstance(t, PlaceholderType) and not isinstance(s, PlaceholderType):
        # mypyc does not allow switching the values like above.
        return s.accept(TypeJoinVisitor(t))
    elif isinstance(t, PlaceholderType):
        return AnyType(TypeOfAny.from_error)

    # Use a visitor to handle non-trivial cases.
    return t.accept(TypeJoinVisitor(s, instance_joiner))


</t>
<t tx="ekr.20220525082934.413">class TypeJoinVisitor(TypeVisitor[ProperType]):
    """Implementation of the least upper bound algorithm.

    Attributes:
      s: The other (left) type operand.
    """

    @others
</t>
<t tx="ekr.20220525082934.414">def __init__(self, s: ProperType, instance_joiner: Optional[InstanceJoiner] = None) -&gt; None:
    self.s = s
    self.instance_joiner = instance_joiner

</t>
<t tx="ekr.20220525082934.415">def visit_unbound_type(self, t: UnboundType) -&gt; ProperType:
    return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082934.416">def visit_union_type(self, t: UnionType) -&gt; ProperType:
    if is_proper_subtype(self.s, t):
        return t
    else:
        return mypy.typeops.make_simplified_union([self.s, t])

</t>
<t tx="ekr.20220525082934.417">def visit_any(self, t: AnyType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20220525082934.418">def visit_none_type(self, t: NoneType) -&gt; ProperType:
    if state.strict_optional:
        if isinstance(self.s, (NoneType, UninhabitedType)):
            return t
        elif isinstance(self.s, UnboundType):
            return AnyType(TypeOfAny.special_form)
        else:
            return mypy.typeops.make_simplified_union([self.s, t])
    else:
        return self.s

</t>
<t tx="ekr.20220525082934.419">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; ProperType:
    return self.s

</t>
<t tx="ekr.20220525082934.42">def note(self, msg: str, line: int, column: int) -&gt; None:
    self.errors.report(line, column, msg, severity='note', code=codes.SYNTAX)

</t>
<t tx="ekr.20220525082934.420">def visit_deleted_type(self, t: DeletedType) -&gt; ProperType:
    return self.s

</t>
<t tx="ekr.20220525082934.421">def visit_erased_type(self, t: ErasedType) -&gt; ProperType:
    return self.s

</t>
<t tx="ekr.20220525082934.422">def visit_type_var(self, t: TypeVarType) -&gt; ProperType:
    if isinstance(self.s, TypeVarType) and self.s.id == t.id:
        return self.s
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20220525082934.423">def visit_param_spec(self, t: ParamSpecType) -&gt; ProperType:
    if self.s == t:
        return t
    return self.default(self.s)

</t>
<t tx="ekr.20220525082934.424">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; ProperType:
    if self.s == t:
        return t
    return self.default(self.s)

</t>
<t tx="ekr.20220525082934.425">def visit_unpack_type(self, t: UnpackType) -&gt; UnpackType:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.426">def visit_parameters(self, t: Parameters) -&gt; ProperType:
    if self.s == t:
        return t
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20220525082934.427">def visit_instance(self, t: Instance) -&gt; ProperType:
    if isinstance(self.s, Instance):
        if self.instance_joiner is None:
            self.instance_joiner = InstanceJoiner()
        nominal = self.instance_joiner.join_instances(t, self.s)
        structural: Optional[Instance] = None
        if t.type.is_protocol and is_protocol_implementation(self.s, t):
            structural = t
        elif self.s.type.is_protocol and is_protocol_implementation(t, self.s):
            structural = self.s
        # Structural join is preferred in the case where we have found both
        # structural and nominal and they have same MRO length (see two comments
        # in join_instances_via_supertype). Otherwise, just return the nominal join.
        if not structural or is_better(nominal, structural):
            return nominal
        return structural
    elif isinstance(self.s, FunctionLike):
        if t.type.is_protocol:
            call = unpack_callback_protocol(t)
            if call:
                return join_types(call, self.s)
        return join_types(t, self.s.fallback)
    elif isinstance(self.s, TypeType):
        return join_types(t, self.s)
    elif isinstance(self.s, TypedDictType):
        return join_types(t, self.s)
    elif isinstance(self.s, TupleType):
        return join_types(t, self.s)
    elif isinstance(self.s, LiteralType):
        return join_types(t, self.s)
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20220525082934.428">def visit_callable_type(self, t: CallableType) -&gt; ProperType:
    if isinstance(self.s, CallableType) and is_similar_callables(t, self.s):
        if is_equivalent(t, self.s):
            return combine_similar_callables(t, self.s)
        result = join_similar_callables(t, self.s)
        # We set the from_type_type flag to suppress error when a collection of
        # concrete class objects gets inferred as their common abstract superclass.
        if not ((t.is_type_obj() and t.type_object().is_abstract) or
                (self.s.is_type_obj() and self.s.type_object().is_abstract)):
            result.from_type_type = True
        if any(isinstance(tp, (NoneType, UninhabitedType))
               for tp in get_proper_types(result.arg_types)):
            # We don't want to return unusable Callable, attempt fallback instead.
            return join_types(t.fallback, self.s)
        return result
    elif isinstance(self.s, Overloaded):
        # Switch the order of arguments to that we'll get to visit_overloaded.
        return join_types(t, self.s)
    elif isinstance(self.s, Instance) and self.s.type.is_protocol:
        call = unpack_callback_protocol(self.s)
        if call:
            return join_types(t, call)
    return join_types(t.fallback, self.s)

</t>
<t tx="ekr.20220525082934.429">def visit_overloaded(self, t: Overloaded) -&gt; ProperType:
    # This is more complex than most other cases. Here are some
    # examples that illustrate how this works.
    #
    # First let's define a concise notation:
    #  - Cn are callable types (for n in 1, 2, ...)
    #  - Ov(C1, C2, ...) is an overloaded type with items C1, C2, ...
    #  - Callable[[T, ...], S] is written as [T, ...] -&gt; S.
    #
    # We want some basic properties to hold (assume Cn are all
    # unrelated via Any-similarity):
    #
    #   join(Ov(C1, C2), C1) == C1
    #   join(Ov(C1, C2), Ov(C1, C2)) == Ov(C1, C2)
    #   join(Ov(C1, C2), Ov(C1, C3)) == C1
    #   join(Ov(C2, C2), C3) == join of fallback types
    #
    # The presence of Any types makes things more interesting. The join is the
    # most general type we can get with respect to Any:
    #
    #   join(Ov([int] -&gt; int, [str] -&gt; str), [Any] -&gt; str) == Any -&gt; str
    #
    # We could use a simplification step that removes redundancies, but that's not
    # implemented right now. Consider this example, where we get a redundancy:
    #
    #   join(Ov([int, Any] -&gt; Any, [str, Any] -&gt; Any), [Any, int] -&gt; Any) ==
    #       Ov([Any, int] -&gt; Any, [Any, int] -&gt; Any)
    #
    # TODO: Consider more cases of callable subtyping.
    result: List[CallableType] = []
    s = self.s
    if isinstance(s, FunctionLike):
        # The interesting case where both types are function types.
        for t_item in t.items:
            for s_item in s.items:
                if is_similar_callables(t_item, s_item):
                    if is_equivalent(t_item, s_item):
                        result.append(combine_similar_callables(t_item, s_item))
                    elif is_subtype(t_item, s_item):
                        result.append(s_item)
        if result:
            # TODO: Simplify redundancies from the result.
            if len(result) == 1:
                return result[0]
            else:
                return Overloaded(result)
        return join_types(t.fallback, s.fallback)
    elif isinstance(s, Instance) and s.type.is_protocol:
        call = unpack_callback_protocol(s)
        if call:
            return join_types(t, call)
    return join_types(t.fallback, s)

</t>
<t tx="ekr.20220525082934.43">def fail(self,
         msg: str,
         line: int,
         column: int,
         blocker: bool = True,
         code: codes.ErrorCode = codes.SYNTAX) -&gt; None:
    if blocker or not self.options.ignore_errors:
        self.errors.report(line, column, msg, blocker=blocker, code=code)

</t>
<t tx="ekr.20220525082934.430">def visit_tuple_type(self, t: TupleType) -&gt; ProperType:
    # When given two fixed-length tuples:
    # * If they have the same length, join their subtypes item-wise:
    #   Tuple[int, bool] + Tuple[bool, bool] becomes Tuple[int, bool]
    # * If lengths do not match, return a variadic tuple:
    #   Tuple[bool, int] + Tuple[bool] becomes Tuple[int, ...]
    #
    # Otherwise, `t` is a fixed-length tuple but `self.s` is NOT:
    # * Joining with a variadic tuple returns variadic tuple:
    #   Tuple[int, bool] + Tuple[bool, ...] becomes Tuple[int, ...]
    # * Joining with any Sequence also returns a Sequence:
    #   Tuple[int, bool] + List[bool] becomes Sequence[int]
    if isinstance(self.s, TupleType) and self.s.length() == t.length():
        if self.instance_joiner is None:
            self.instance_joiner = InstanceJoiner()
        fallback = self.instance_joiner.join_instances(mypy.typeops.tuple_fallback(self.s),
                                                   mypy.typeops.tuple_fallback(t))
        assert isinstance(fallback, Instance)
        if self.s.length() == t.length():
            items: List[Type] = []
            for i in range(t.length()):
                items.append(self.join(t.items[i], self.s.items[i]))
            return TupleType(items, fallback)
        else:
            return fallback
    else:
        return join_types(self.s, mypy.typeops.tuple_fallback(t))

</t>
<t tx="ekr.20220525082934.431">def visit_typeddict_type(self, t: TypedDictType) -&gt; ProperType:
    if isinstance(self.s, TypedDictType):
        items = OrderedDict([
            (item_name, s_item_type)
            for (item_name, s_item_type, t_item_type) in self.s.zip(t)
            if (is_equivalent(s_item_type, t_item_type) and
                (item_name in t.required_keys) == (item_name in self.s.required_keys))
        ])
        fallback = self.s.create_anonymous_fallback()
        # We need to filter by items.keys() since some required keys present in both t and
        # self.s might be missing from the join if the types are incompatible.
        required_keys = set(items.keys()) &amp; t.required_keys &amp; self.s.required_keys
        return TypedDictType(items, required_keys, fallback)
    elif isinstance(self.s, Instance):
        return join_types(self.s, t.fallback)
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20220525082934.432">def visit_literal_type(self, t: LiteralType) -&gt; ProperType:
    if isinstance(self.s, LiteralType):
        if t == self.s:
            return t
        if self.s.fallback.type.is_enum and t.fallback.type.is_enum:
            return mypy.typeops.make_simplified_union([self.s, t])
        return join_types(self.s.fallback, t.fallback)
    else:
        return join_types(self.s, t.fallback)

</t>
<t tx="ekr.20220525082934.433">def visit_partial_type(self, t: PartialType) -&gt; ProperType:
    # We only have partial information so we can't decide the join result. We should
    # never get here.
    assert False, "Internal error"

</t>
<t tx="ekr.20220525082934.434">def visit_type_type(self, t: TypeType) -&gt; ProperType:
    if isinstance(self.s, TypeType):
        return TypeType.make_normalized(self.join(t.item, self.s.item), line=t.line)
    elif isinstance(self.s, Instance) and self.s.type.fullname == 'builtins.type':
        return self.s
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20220525082934.435">def visit_type_alias_type(self, t: TypeAliasType) -&gt; ProperType:
    assert False, f"This should be never called, got {t}"

</t>
<t tx="ekr.20220525082934.436">def join(self, s: Type, t: Type) -&gt; ProperType:
    return join_types(s, t)

</t>
<t tx="ekr.20220525082934.437">def default(self, typ: Type) -&gt; ProperType:
    typ = get_proper_type(typ)
    if isinstance(typ, Instance):
        return object_from_instance(typ)
    elif isinstance(typ, UnboundType):
        return AnyType(TypeOfAny.special_form)
    elif isinstance(typ, TupleType):
        return self.default(mypy.typeops.tuple_fallback(typ))
    elif isinstance(typ, TypedDictType):
        return self.default(typ.fallback)
    elif isinstance(typ, FunctionLike):
        return self.default(typ.fallback)
    elif isinstance(typ, TypeVarType):
        return self.default(typ.upper_bound)
    elif isinstance(typ, ParamSpecType):
        return self.default(typ.upper_bound)
    else:
        return AnyType(TypeOfAny.special_form)


</t>
<t tx="ekr.20220525082934.438">def is_better(t: Type, s: Type) -&gt; bool:
    # Given two possible results from join_instances_via_supertype(),
    # indicate whether t is the better one.
    t = get_proper_type(t)
    s = get_proper_type(s)

    if isinstance(t, Instance):
        if not isinstance(s, Instance):
            return True
        # Use len(mro) as a proxy for the better choice.
        if len(t.type.mro) &gt; len(s.type.mro):
            return True
    return False


</t>
<t tx="ekr.20220525082934.439">def is_similar_callables(t: CallableType, s: CallableType) -&gt; bool:
    """Return True if t and s have identical numbers of
    arguments, default arguments and varargs.
    """
    return (len(t.arg_types) == len(s.arg_types) and t.min_args == s.min_args and
            t.is_var_arg == s.is_var_arg)


</t>
<t tx="ekr.20220525082934.44">def fail_merge_overload(self, node: IfStmt) -&gt; None:
    self.fail(
        "Condition can't be inferred, unable to merge overloads",
        line=node.line,
        column=node.column,
        blocker=False,
        code=codes.MISC,
    )

</t>
<t tx="ekr.20220525082934.440">def join_similar_callables(t: CallableType, s: CallableType) -&gt; CallableType:
    from mypy.meet import meet_types

    arg_types: List[Type] = []
    for i in range(len(t.arg_types)):
        arg_types.append(meet_types(t.arg_types[i], s.arg_types[i]))
    # TODO in combine_similar_callables also applies here (names and kinds)
    # The fallback type can be either 'function' or 'type'. The result should have 'type' as
    # fallback only if both operands have it as 'type'.
    if t.fallback.type.fullname != 'builtins.type':
        fallback = t.fallback
    else:
        fallback = s.fallback
    return t.copy_modified(arg_types=arg_types,
                           arg_names=combine_arg_names(t, s),
                           ret_type=join_types(t.ret_type, s.ret_type),
                           fallback=fallback,
                           name=None)


</t>
<t tx="ekr.20220525082934.441">def combine_similar_callables(t: CallableType, s: CallableType) -&gt; CallableType:
    arg_types: List[Type] = []
    for i in range(len(t.arg_types)):
        arg_types.append(join_types(t.arg_types[i], s.arg_types[i]))
    # TODO kinds and argument names
    # The fallback type can be either 'function' or 'type'. The result should have 'type' as
    # fallback only if both operands have it as 'type'.
    if t.fallback.type.fullname != 'builtins.type':
        fallback = t.fallback
    else:
        fallback = s.fallback
    return t.copy_modified(arg_types=arg_types,
                           arg_names=combine_arg_names(t, s),
                           ret_type=join_types(t.ret_type, s.ret_type),
                           fallback=fallback,
                           name=None)


</t>
<t tx="ekr.20220525082934.442">def combine_arg_names(t: CallableType, s: CallableType) -&gt; List[Optional[str]]:
    """Produces a list of argument names compatible with both callables.

    For example, suppose 't' and 's' have the following signatures:

    - t: (a: int, b: str, X: str) -&gt; None
    - s: (a: int, b: str, Y: str) -&gt; None

    This function would return ["a", "b", None]. This information
    is then used above to compute the join of t and s, which results
    in a signature of (a: int, b: str, str) -&gt; None.

    Note that the third argument's name is omitted and 't' and 's'
    are both valid subtypes of this inferred signature.

    Precondition: is_similar_types(t, s) is true.
    """
    num_args = len(t.arg_types)
    new_names = []
    for i in range(num_args):
        t_name = t.arg_names[i]
        s_name = s.arg_names[i]
        if t_name == s_name or t.arg_kinds[i].is_named() or s.arg_kinds[i].is_named():
            new_names.append(t_name)
        else:
            new_names.append(None)
    return new_names


</t>
<t tx="ekr.20220525082934.443">def object_from_instance(instance: Instance) -&gt; Instance:
    """Construct the type 'builtins.object' from an instance type."""
    # Use the fact that 'object' is always the last class in the mro.
    res = Instance(instance.type.mro[-1], [])
    return res


</t>
<t tx="ekr.20220525082934.444">def object_or_any_from_type(typ: ProperType) -&gt; ProperType:
    # Similar to object_from_instance() but tries hard for all types.
    # TODO: find a better way to get object, or make this more reliable.
    if isinstance(typ, Instance):
        return object_from_instance(typ)
    elif isinstance(typ, (CallableType, TypedDictType, LiteralType)):
        return object_from_instance(typ.fallback)
    elif isinstance(typ, TupleType):
        return object_from_instance(typ.partial_fallback)
    elif isinstance(typ, TypeType):
        return object_or_any_from_type(typ.item)
    elif isinstance(typ, TypeVarType) and isinstance(typ.upper_bound, ProperType):
        return object_or_any_from_type(typ.upper_bound)
    elif isinstance(typ, UnionType):
        for item in typ.items:
            if isinstance(item, ProperType):
                candidate = object_or_any_from_type(item)
                if isinstance(candidate, Instance):
                    return candidate
    return AnyType(TypeOfAny.implementation_artifact)


</t>
<t tx="ekr.20220525082934.445">def join_type_list(types: List[Type]) -&gt; ProperType:
    if not types:
        # This is a little arbitrary but reasonable. Any empty tuple should be compatible
        # with all variable length tuples, and this makes it possible.
        return UninhabitedType()
    joined = get_proper_type(types[0])
    for t in types[1:]:
        joined = join_types(joined, t)
    return joined


</t>
<t tx="ekr.20220525082934.446">def unpack_callback_protocol(t: Instance) -&gt; Optional[Type]:
    assert t.type.is_protocol
    if t.type.protocol_members == ['__call__']:
        return find_member('__call__', t, t, is_operator=True)
    return None
</t>
<t tx="ekr.20220525082934.447">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from typing import Optional, Union, Any, Tuple, Iterable
from typing_extensions import Final

from mypy.nodes import (
    Expression, ComparisonExpr, OpExpr, MemberExpr, UnaryExpr, StarExpr, IndexExpr, LITERAL_YES,
    LITERAL_NO, NameExpr, LITERAL_TYPE, IntExpr, FloatExpr, ComplexExpr, StrExpr, BytesExpr,
    UnicodeExpr, ListExpr, TupleExpr, SetExpr, DictExpr, CallExpr, SliceExpr, CastExpr,
    ConditionalExpr, EllipsisExpr, YieldFromExpr, YieldExpr, RevealExpr, SuperExpr,
    TypeApplication, LambdaExpr, ListComprehension, SetComprehension, DictionaryComprehension,
    GeneratorExpr, BackquoteExpr, TypeVarExpr, TypeAliasExpr, NamedTupleExpr, EnumCallExpr,
    TypedDictExpr, NewTypeExpr, PromoteExpr, AwaitExpr, TempNode, AssignmentExpr, ParamSpecExpr,
    AssertTypeExpr, TypeVarTupleExpr,
)
from mypy.visitor import ExpressionVisitor

# [Note Literals and literal_hash]
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#
# Mypy uses the term "literal" to refer to any expression built out of
# the following:
#
# * Plain literal expressions, like `1` (integer, float, string, etc.)
#
# * Compound literal expressions, like `(lit1, lit2)` (list, dict,
#   set, or tuple)
#
# * Operator expressions, like `lit1 + lit2`
#
# * Variable references, like `x`
#
# * Member references, like `lit.m`
#
# * Index expressions, like `lit[0]`
#
# A typical "literal" looks like `x[(i,j+1)].m`.
#
# An expression that is a literal has a `literal_hash`, with the
# following properties.
#
# * `literal_hash` is a Key: a tuple containing basic data types and
#   possibly other Keys. So it can be used as a key in a dictionary
#   that will be compared by value (as opposed to the Node itself,
#   which is compared by identity).
#
# * Two expressions have equal `literal_hash`es if and only if they
#   are syntactically equal expressions. (NB: Actually, we also
#   identify as equal expressions like `3` and `3.0`; is this a good
#   idea?)
#
# * The elements of `literal_hash` that are tuples are exactly the
#   subexpressions of the original expression (e.g. the base and index
#   of an index expression, or the operands of an operator expression).


@others
_hasher: Final = _Hasher()
</t>
<t tx="ekr.20220525082934.448">def literal(e: Expression) -&gt; int:
    if isinstance(e, ComparisonExpr):
        return min(literal(o) for o in e.operands)

    elif isinstance(e, OpExpr):
        return min(literal(e.left), literal(e.right))

    elif isinstance(e, (MemberExpr, UnaryExpr, StarExpr)):
        return literal(e.expr)

    elif isinstance(e, AssignmentExpr):
        return literal(e.target)

    elif isinstance(e, IndexExpr):
        if literal(e.index) == LITERAL_YES:
            return literal(e.base)
        else:
            return LITERAL_NO

    elif isinstance(e, NameExpr):
        return LITERAL_TYPE

    if isinstance(e, (IntExpr, FloatExpr, ComplexExpr, StrExpr, BytesExpr, UnicodeExpr)):
        return LITERAL_YES

    if literal_hash(e):
        return LITERAL_YES

    return LITERAL_NO


</t>
<t tx="ekr.20220525082934.449">Key = Tuple[Any, ...]


</t>
<t tx="ekr.20220525082934.45">def visit(self, node: Optional[AST]) -&gt; Any:
    if node is None:
        return None
    typeobj = type(node)
    visitor = self.visitor_cache.get(typeobj)
    if visitor is None:
        method = 'visit_' + node.__class__.__name__
        visitor = getattr(self, method)
        self.visitor_cache[typeobj] = visitor
    return visitor(node)

</t>
<t tx="ekr.20220525082934.450">def subkeys(key: Key) -&gt; Iterable[Key]:
    return [elt for elt in key if isinstance(elt, tuple)]


</t>
<t tx="ekr.20220525082934.451">def literal_hash(e: Expression) -&gt; Optional[Key]:
    return e.accept(_hasher)


</t>
<t tx="ekr.20220525082934.452">class _Hasher(ExpressionVisitor[Optional[Key]]):
    @others
</t>
<t tx="ekr.20220525082934.453">def visit_int_expr(self, e: IntExpr) -&gt; Key:
    return ('Literal', e.value)

</t>
<t tx="ekr.20220525082934.454">def visit_str_expr(self, e: StrExpr) -&gt; Key:
    return ('Literal', e.value, e.from_python_3)

</t>
<t tx="ekr.20220525082934.455">def visit_bytes_expr(self, e: BytesExpr) -&gt; Key:
    return ('Literal', e.value)

</t>
<t tx="ekr.20220525082934.456">def visit_unicode_expr(self, e: UnicodeExpr) -&gt; Key:
    return ('Literal', e.value)

</t>
<t tx="ekr.20220525082934.457">def visit_float_expr(self, e: FloatExpr) -&gt; Key:
    return ('Literal', e.value)

</t>
<t tx="ekr.20220525082934.458">def visit_complex_expr(self, e: ComplexExpr) -&gt; Key:
    return ('Literal', e.value)

</t>
<t tx="ekr.20220525082934.459">def visit_star_expr(self, e: StarExpr) -&gt; Key:
    return ('Star', literal_hash(e.expr))

</t>
<t tx="ekr.20220525082934.46">def set_line(self, node: N, n: AstNode) -&gt; N:
    node.line = n.lineno
    node.column = n.col_offset
    node.end_line = getattr(n, "end_lineno", None) if isinstance(n, ast3.expr) else None
    return node

</t>
<t tx="ekr.20220525082934.460">def visit_name_expr(self, e: NameExpr) -&gt; Key:
    # N.B: We use the node itself as the key, and not the name,
    # because using the name causes issues when there is shadowing
    # (for example, in list comprehensions).
    return ('Var', e.node)

</t>
<t tx="ekr.20220525082934.461">def visit_member_expr(self, e: MemberExpr) -&gt; Key:
    return ('Member', literal_hash(e.expr), e.name)

</t>
<t tx="ekr.20220525082934.462">def visit_op_expr(self, e: OpExpr) -&gt; Key:
    return ('Binary', e.op, literal_hash(e.left), literal_hash(e.right))

</t>
<t tx="ekr.20220525082934.463">def visit_comparison_expr(self, e: ComparisonExpr) -&gt; Key:
    rest: Any = tuple(e.operators)
    rest += tuple(literal_hash(o) for o in e.operands)
    return ('Comparison',) + rest

</t>
<t tx="ekr.20220525082934.464">def visit_unary_expr(self, e: UnaryExpr) -&gt; Key:
    return ('Unary', e.op, literal_hash(e.expr))

</t>
<t tx="ekr.20220525082934.465">def seq_expr(self, e: Union[ListExpr, TupleExpr, SetExpr], name: str) -&gt; Optional[Key]:
    if all(literal(x) == LITERAL_YES for x in e.items):
        rest: Any = tuple(literal_hash(x) for x in e.items)
        return (name,) + rest
    return None

</t>
<t tx="ekr.20220525082934.466">def visit_list_expr(self, e: ListExpr) -&gt; Optional[Key]:
    return self.seq_expr(e, 'List')

</t>
<t tx="ekr.20220525082934.467">def visit_dict_expr(self, e: DictExpr) -&gt; Optional[Key]:
    if all(a and literal(a) == literal(b) == LITERAL_YES for a, b in e.items):
        rest: Any = tuple(
            (literal_hash(a) if a else None, literal_hash(b)) for a, b in e.items
        )
        return ("Dict",) + rest
    return None

</t>
<t tx="ekr.20220525082934.468">def visit_tuple_expr(self, e: TupleExpr) -&gt; Optional[Key]:
    return self.seq_expr(e, 'Tuple')

</t>
<t tx="ekr.20220525082934.469">def visit_set_expr(self, e: SetExpr) -&gt; Optional[Key]:
    return self.seq_expr(e, 'Set')

</t>
<t tx="ekr.20220525082934.47">def translate_opt_expr_list(self, l: Sequence[Optional[AST]]) -&gt; List[Optional[Expression]]:
    res: List[Optional[Expression]] = []
    for e in l:
        exp = self.visit(e)
        res.append(exp)
    return res

</t>
<t tx="ekr.20220525082934.470">def visit_index_expr(self, e: IndexExpr) -&gt; Optional[Key]:
    if literal(e.index) == LITERAL_YES:
        return ('Index', literal_hash(e.base), literal_hash(e.index))
    return None

</t>
<t tx="ekr.20220525082934.471">def visit_assignment_expr(self, e: AssignmentExpr) -&gt; Optional[Key]:
    return literal_hash(e.target)

</t>
<t tx="ekr.20220525082934.472">def visit_call_expr(self, e: CallExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.473">def visit_slice_expr(self, e: SliceExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.474">def visit_cast_expr(self, e: CastExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.475">def visit_assert_type_expr(self, e: AssertTypeExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.476">def visit_conditional_expr(self, e: ConditionalExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.477">def visit_ellipsis(self, e: EllipsisExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.478">def visit_yield_from_expr(self, e: YieldFromExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.479">def visit_yield_expr(self, e: YieldExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.48">def translate_expr_list(self, l: Sequence[AST]) -&gt; List[Expression]:
    return cast(List[Expression], self.translate_opt_expr_list(l))

</t>
<t tx="ekr.20220525082934.480">def visit_reveal_expr(self, e: RevealExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.481">def visit_super_expr(self, e: SuperExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.482">def visit_type_application(self, e: TypeApplication) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.483">def visit_lambda_expr(self, e: LambdaExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.484">def visit_list_comprehension(self, e: ListComprehension) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.485">def visit_set_comprehension(self, e: SetComprehension) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.486">def visit_dictionary_comprehension(self, e: DictionaryComprehension) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.487">def visit_generator_expr(self, e: GeneratorExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.488">def visit_backquote_expr(self, e: BackquoteExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.489">def visit_type_var_expr(self, e: TypeVarExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.49">def get_lineno(self, node: Union[ast3.expr, ast3.stmt]) -&gt; int:
    if (isinstance(node, (ast3.AsyncFunctionDef, ast3.ClassDef, ast3.FunctionDef))
            and node.decorator_list):
        return node.decorator_list[0].lineno
    return node.lineno

</t>
<t tx="ekr.20220525082934.490">def visit_paramspec_expr(self, e: ParamSpecExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.491">def visit_type_var_tuple_expr(self, e: TypeVarTupleExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.492">def visit_type_alias_expr(self, e: TypeAliasExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.493">def visit_namedtuple_expr(self, e: NamedTupleExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.494">def visit_enum_call_expr(self, e: EnumCallExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.495">def visit_typeddict_expr(self, e: TypedDictExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.496">def visit_newtype_expr(self, e: NewTypeExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.497">def visit__promote_expr(self, e: PromoteExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.498">def visit_await_expr(self, e: AwaitExpr) -&gt; None:
    return None

</t>
<t tx="ekr.20220525082934.499">def visit_temp_node(self, e: TempNode) -&gt; None:
    return None


</t>
<t tx="ekr.20220525082934.5">class ExpandTypeVisitor(TypeVisitor[Type]):
    """Visitor that substitutes type variables with values."""

    variables: Mapping[TypeVarId, Type]  # TypeVar id -&gt; TypeVar value

    @others
</t>
<t tx="ekr.20220525082934.50">def translate_stmt_list(self,
                        stmts: Sequence[ast3.stmt],
                        ismodule: bool = False) -&gt; List[Statement]:
    # A "# type: ignore" comment before the first statement of a module
    # ignores the whole module:
    if (ismodule and stmts and self.type_ignores
            and min(self.type_ignores) &lt; self.get_lineno(stmts[0])):
        self.errors.used_ignored_lines[self.errors.file][min(self.type_ignores)].append(
            codes.FILE.code)
        block = Block(self.fix_function_overloads(self.translate_stmt_list(stmts)))
        mark_block_unreachable(block)
        return [block]

    res: List[Statement] = []
    for stmt in stmts:
        node = self.visit(stmt)
        res.append(node)

    return res

</t>
<t tx="ekr.20220525082934.500">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""
This is a module for various lookup functions:
functions that will find a semantic node by its name.
"""

from mypy.nodes import MypyFile, SymbolTableNode, TypeInfo
from typing import Dict, Optional

# TODO: gradually move existing lookup functions to this module.


@others
</t>
<t tx="ekr.20220525082934.501">def lookup_fully_qualified(name: str, modules: Dict[str, MypyFile], *,
                           raise_on_missing: bool = False) -&gt; Optional[SymbolTableNode]:
    """Find a symbol using it fully qualified name.

    The algorithm has two steps: first we try splitting the name on '.' to find
    the module, then iteratively look for each next chunk after a '.' (e.g. for
    nested classes).

    This function should *not* be used to find a module. Those should be looked
    in the modules dictionary.
    """
    head = name
    rest = []
    # 1. Find a module tree in modules dictionary.
    while True:
        if '.' not in head:
            if raise_on_missing:
                assert '.' in head, f"Cannot find module for {name}"
            return None
        head, tail = head.rsplit('.', maxsplit=1)
        rest.append(tail)
        mod = modules.get(head)
        if mod is not None:
            break
    names = mod.names
    # 2. Find the symbol in the module tree.
    if not rest:
        # Looks like a module, don't use this to avoid confusions.
        if raise_on_missing:
            assert rest, f"Cannot find {name}, got a module symbol"
        return None
    while True:
        key = rest.pop()
        if key not in names:
            if raise_on_missing:
                assert key in names, f"Cannot find component {key!r} for {name!r}"
            return None
        stnode = names[key]
        if not rest:
            return stnode
        node = stnode.node
        # In fine-grained mode, could be a cross-reference to a deleted module
        # or a Var made up for a missing module.
        if not isinstance(node, TypeInfo):
            if raise_on_missing:
                assert node, f"Cannot find {name}"
            return None
        names = node.names
</t>
<t tx="ekr.20220525082934.502">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Mypy type checker command line tool."""

import argparse
from gettext import gettext
import os
import subprocess
import sys
import time

from typing import Any, Dict, IO, List, Optional, Sequence, Tuple, TextIO, Union
from typing_extensions import Final, NoReturn

from mypy import build
from mypy import defaults
from mypy import state
from mypy import util
from mypy.modulefinder import (
    BuildSource, FindModuleCache, SearchPaths,
    get_site_packages_dirs, mypy_path,
)
from mypy.find_sources import create_source_list, InvalidSourceList
from mypy.fscache import FileSystemCache
from mypy.errors import CompileError
from mypy.errorcodes import error_codes
from mypy.options import Options, BuildType
from mypy.config_parser import get_config_module_names, parse_version, parse_config_file
from mypy.split_namespace import SplitNamespace

from mypy.version import __version__

orig_stat: Final = os.stat
MEM_PROFILE: Final = False  # If True, dump memory profile


@others
</t>
<t tx="ekr.20220525082934.503">def stat_proxy(path: str) -&gt; os.stat_result:
    try:
        st = orig_stat(path)
    except os.error as err:
        print(f"stat({path!r}) -&gt; {err}")
        raise
    else:
        print("stat(%r) -&gt; (st_mode=%o, st_mtime=%d, st_size=%d)" %
              (path, st.st_mode, st.st_mtime, st.st_size))
        return st


</t>
<t tx="ekr.20220525082934.504">def main(script_path: Optional[str],
         stdout: TextIO,
         stderr: TextIO,
         args: Optional[List[str]] = None,
         clean_exit: bool = False,
         ) -&gt; None:
    """Main entry point to the type checker.

    Args:
        script_path: Path to the 'mypy' script (used for finding data files).
        args: Custom command-line arguments.  If not given, sys.argv[1:] will
            be used.
        clean_exit: Don't hard kill the process on exit. This allows catching
            SystemExit.
    """
    util.check_python_version('mypy')
    t0 = time.time()
    # To log stat() calls: os.stat = stat_proxy
    sys.setrecursionlimit(2 ** 14)
    if args is None:
        args = sys.argv[1:]

    fscache = FileSystemCache()
    sources, options = process_options(args, stdout=stdout, stderr=stderr,
                                       fscache=fscache)
    if clean_exit:
        options.fast_exit = False

    formatter = util.FancyFormatter(stdout, stderr, options.show_error_codes)

    if options.install_types and (stdout is not sys.stdout or stderr is not sys.stderr):
        # Since --install-types performs user input, we want regular stdout and stderr.
        fail("error: --install-types not supported in this mode of running mypy", stderr, options)

    if options.non_interactive and not options.install_types:
        fail("error: --non-interactive is only supported with --install-types", stderr, options)

    if options.install_types and not options.incremental:
        fail("error: --install-types not supported with incremental mode disabled",
             stderr, options)

    if options.install_types and options.python_executable is None:
        fail("error: --install-types not supported without python executable or site packages",
             stderr, options)

    if options.install_types and not sources:
        install_types(formatter, options, non_interactive=options.non_interactive)
        return

    res, messages, blockers = run_build(sources, options, fscache, t0, stdout, stderr)

    if options.non_interactive:
        missing_pkgs = read_types_packages_to_install(options.cache_dir, after_run=True)
        if missing_pkgs:
            # Install missing type packages and rerun build.
            install_types(formatter, options, after_run=True, non_interactive=True)
            fscache.flush()
            print()
            res, messages, blockers = run_build(sources, options, fscache, t0, stdout, stderr)
        show_messages(messages, stderr, formatter, options)

    if MEM_PROFILE:
        from mypy.memprofile import print_memory_profile
        print_memory_profile()

    code = 0
    if messages:
        code = 2 if blockers else 1
    if options.error_summary:
        n_errors, n_notes, n_files = util.count_stats(messages)
        if n_errors:
            summary = formatter.format_error(
                n_errors, n_files, len(sources), blockers=blockers,
                use_color=options.color_output
            )
            stdout.write(summary + '\n')
        # Only notes should also output success
        elif not messages or n_notes == len(messages):
            stdout.write(formatter.format_success(len(sources), options.color_output) + '\n')
        stdout.flush()

    if options.install_types and not options.non_interactive:
        result = install_types(formatter, options, after_run=True, non_interactive=False)
        if result:
            print()
            print("note: Run mypy again for up-to-date results with installed types")
            code = 2

    if options.fast_exit:
        # Exit without freeing objects -- it's faster.
        #
        # NOTE: We don't flush all open files on exit (or run other destructors)!
        util.hard_exit(code)
    elif code:
        sys.exit(code)

    # HACK: keep res alive so that mypyc won't free it before the hard_exit
    list([res])


</t>
<t tx="ekr.20220525082934.505">def run_build(sources: List[BuildSource],
              options: Options,
              fscache: FileSystemCache,
              t0: float,
              stdout: TextIO,
              stderr: TextIO) -&gt; Tuple[Optional[build.BuildResult], List[str], bool]:
    formatter = util.FancyFormatter(stdout, stderr, options.show_error_codes)

    messages = []

    @others
    serious = False
    blockers = False
    res = None
    try:
        # Keep a dummy reference (res) for memory profiling afterwards, as otherwise
        # the result could be freed.
        res = build.build(sources, options, None, flush_errors, fscache, stdout, stderr)
    except CompileError as e:
        blockers = True
        if not e.use_stdout:
            serious = True
    if (options.warn_unused_configs
            and options.unused_configs
            and not options.incremental
            and not options.non_interactive):
        print("Warning: unused section(s) in %s: %s" %
              (options.config_file,
              get_config_module_names(options.config_file,
                                      [glob for glob in options.per_module_options.keys()
                                      if glob in options.unused_configs])),
              file=stderr)
    maybe_write_junit_xml(time.time() - t0, serious, messages, options)
    return res, messages, blockers


</t>
<t tx="ekr.20220525082934.506">def flush_errors(new_messages: List[str], serious: bool) -&gt; None:
    if options.pretty:
        new_messages = formatter.fit_in_terminal(new_messages)
    messages.extend(new_messages)
    if options.non_interactive:
        # Collect messages and possibly show them later.
        return
    f = stderr if serious else stdout
    show_messages(new_messages, f, formatter, options)

</t>
<t tx="ekr.20220525082934.507">def show_messages(messages: List[str],
                  f: TextIO,
                  formatter: util.FancyFormatter,
                  options: Options) -&gt; None:
    for msg in messages:
        if options.color_output:
            msg = formatter.colorize(msg)
        f.write(msg + '\n')
    f.flush()


</t>
<t tx="ekr.20220525082934.508"># Make the help output a little less jarring.
class AugmentedHelpFormatter(argparse.RawDescriptionHelpFormatter):
    @others
</t>
<t tx="ekr.20220525082934.509">def __init__(self, prog: str) -&gt; None:
    super().__init__(prog=prog, max_help_position=28)

</t>
<t tx="ekr.20220525082934.51">def translate_type_comment(self,
                           n: Union[ast3.stmt, ast3.arg],
                           type_comment: Optional[str]) -&gt; Optional[ProperType]:
    if type_comment is None:
        return None
    else:
        lineno = n.lineno
        extra_ignore, typ = parse_type_comment(type_comment,
                                               lineno,
                                               n.col_offset,
                                               self.errors)
        if extra_ignore is not None:
            self.type_ignores[lineno] = extra_ignore
        return typ

</t>
<t tx="ekr.20220525082934.510">def _fill_text(self, text: str, width: int, indent: str) -&gt; str:
    if '\n' in text:
        # Assume we want to manually format the text
        return super()._fill_text(text, width, indent)
    else:
        # Assume we want argparse to manage wrapping, indenting, and
        # formatting the text for us.
        return argparse.HelpFormatter._fill_text(self, text, width, indent)


</t>
<t tx="ekr.20220525082934.511"># Define pairs of flag prefixes with inverse meaning.
flag_prefix_pairs: Final = [
    ('allow', 'disallow'),
    ('show', 'hide'),
]
flag_prefix_map: Final[Dict[str, str]] = {}
for a, b in flag_prefix_pairs:
    flag_prefix_map[a] = b
    flag_prefix_map[b] = a


</t>
<t tx="ekr.20220525082934.512">def invert_flag_name(flag: str) -&gt; str:
    split = flag[2:].split('-', 1)
    if len(split) == 2:
        prefix, rest = split
        if prefix in flag_prefix_map:
            return f'--{flag_prefix_map[prefix]}-{rest}'
        elif prefix == 'no':
            return f'--{rest}'

    return f'--no-{flag[2:]}'


</t>
<t tx="ekr.20220525082934.513">class PythonExecutableInferenceError(Exception):
    """Represents a failure to infer the version or executable while searching."""


</t>
<t tx="ekr.20220525082934.514">def python_executable_prefix(v: str) -&gt; List[str]:
    if sys.platform == 'win32':
        # on Windows, all Python executables are named `python`. To handle this, there
        # is the `py` launcher, which can be passed a version e.g. `py -3.8`, and it will
        # execute an installed Python 3.8 interpreter. See also:
        # https://docs.python.org/3/using/windows.html#python-launcher-for-windows
        return ['py', f'-{v}']
    else:
        return [f'python{v}']


</t>
<t tx="ekr.20220525082934.515">def _python_executable_from_version(python_version: Tuple[int, int]) -&gt; str:
    if sys.version_info[:2] == python_version:
        return sys.executable
    str_ver = '.'.join(map(str, python_version))
    try:
        sys_exe = subprocess.check_output(python_executable_prefix(str_ver) +
                                          ['-c', 'import sys; print(sys.executable)'],
                                          stderr=subprocess.STDOUT).decode().strip()
        return sys_exe
    except (subprocess.CalledProcessError, FileNotFoundError) as e:
        raise PythonExecutableInferenceError(
            'failed to find a Python executable matching version {},'
            ' perhaps try --python-executable, or --no-site-packages?'.format(python_version)
        ) from e


</t>
<t tx="ekr.20220525082934.516">def infer_python_executable(options: Options,
                            special_opts: argparse.Namespace) -&gt; None:
    """Infer the Python executable from the given version.

    This function mutates options based on special_opts to infer the correct Python executable
    to use.
    """
    # TODO: (ethanhs) Look at folding these checks and the site packages subprocess calls into
    # one subprocess call for speed.

    # Use the command line specified executable, or fall back to one set in the
    # config file. If an executable is not specified, infer it from the version
    # (unless no_executable is set)
    python_executable = special_opts.python_executable or options.python_executable

    if python_executable is None:
        if not special_opts.no_executable and not options.no_site_packages:
            python_executable = _python_executable_from_version(options.python_version)
    options.python_executable = python_executable


</t>
<t tx="ekr.20220525082934.517">HEADER: Final = """%(prog)s [-h] [-v] [-V] [more options; see below]
            [-m MODULE] [-p PACKAGE] [-c PROGRAM_TEXT] [files ...]"""


DESCRIPTION: Final = """
Mypy is a program that will type check your Python code.

Pass in any files or folders you want to type check. Mypy will
recursively traverse any provided folders to find .py files:

    $ mypy my_program.py my_src_folder

For more information on getting started, see:

- https://mypy.readthedocs.io/en/stable/getting_started.html

For more details on both running mypy and using the flags below, see:

- https://mypy.readthedocs.io/en/stable/running_mypy.html
- https://mypy.readthedocs.io/en/stable/command_line.html

You can also use a config file to configure mypy instead of using
command line flags. For more details, see:

- https://mypy.readthedocs.io/en/stable/config_file.html
"""

FOOTER: Final = """Environment variables:
  Define MYPYPATH for additional module search path entries.
  Define MYPY_CACHE_DIR to override configuration cache_dir path."""


</t>
<t tx="ekr.20220525082934.518">class CapturableArgumentParser(argparse.ArgumentParser):

    """Override ArgumentParser methods that use sys.stdout/sys.stderr directly.

    This is needed because hijacking sys.std* is not thread-safe,
    yet output must be captured to properly support mypy.api.run.
    """

    @others
</t>
<t tx="ekr.20220525082934.519">def __init__(self, *args: Any, **kwargs: Any):
    self.stdout = kwargs.pop('stdout', sys.stdout)
    self.stderr = kwargs.pop('stderr', sys.stderr)
    super().__init__(*args, **kwargs)

</t>
<t tx="ekr.20220525082934.52">op_map: Final[Dict[typing.Type[AST], str]] = {
    ast3.Add: '+',
    ast3.Sub: '-',
    ast3.Mult: '*',
    ast3.MatMult: '@',
    ast3.Div: '/',
    ast3.Mod: '%',
    ast3.Pow: '**',
    ast3.LShift: '&lt;&lt;',
    ast3.RShift: '&gt;&gt;',
    ast3.BitOr: '|',
    ast3.BitXor: '^',
    ast3.BitAnd: '&amp;',
    ast3.FloorDiv: '//'
}

</t>
<t tx="ekr.20220525082934.520"># =====================
# Help-printing methods
# =====================
def print_usage(self, file: Optional[IO[str]] = None) -&gt; None:
    if file is None:
        file = self.stdout
    self._print_message(self.format_usage(), file)

</t>
<t tx="ekr.20220525082934.521">def print_help(self, file: Optional[IO[str]] = None) -&gt; None:
    if file is None:
        file = self.stdout
    self._print_message(self.format_help(), file)

</t>
<t tx="ekr.20220525082934.522">def _print_message(self, message: str, file: Optional[IO[str]] = None) -&gt; None:
    if message:
        if file is None:
            file = self.stderr
        file.write(message)

</t>
<t tx="ekr.20220525082934.523"># ===============
# Exiting methods
# ===============
def exit(self, status: int = 0, message: Optional[str] = None) -&gt; NoReturn:
    if message:
        self._print_message(message, self.stderr)
    sys.exit(status)

</t>
<t tx="ekr.20220525082934.524">def error(self, message: str) -&gt; NoReturn:
    """error(message: string)

    Prints a usage message incorporating the message to stderr and
    exits.

    If you override this in a subclass, it should not return -- it
    should either exit or raise an exception.
    """
    self.print_usage(self.stderr)
    args = {'prog': self.prog, 'message': message}
    self.exit(2, gettext('%(prog)s: error: %(message)s\n') % args)


</t>
<t tx="ekr.20220525082934.525">class CapturableVersionAction(argparse.Action):

    """Supplement CapturableArgumentParser to handle --version.

    This is nearly identical to argparse._VersionAction except,
    like CapturableArgumentParser, it allows output to be captured.

    Another notable difference is that version is mandatory.
    This allows removing a line in __call__ that falls back to parser.version
    (which does not appear to exist).
    """

    @others
</t>
<t tx="ekr.20220525082934.526">def __init__(self,
             option_strings: Sequence[str],
             version: str,
             dest: str = argparse.SUPPRESS,
             default: str = argparse.SUPPRESS,
             help: str = "show program's version number and exit",
             stdout: Optional[IO[str]] = None):
    super().__init__(
        option_strings=option_strings,
        dest=dest,
        default=default,
        nargs=0,
        help=help)
    self.version = version
    self.stdout = stdout or sys.stdout

</t>
<t tx="ekr.20220525082934.527">def __call__(self,
             parser: argparse.ArgumentParser,
             namespace: argparse.Namespace,
             values: Union[str, Sequence[Any], None],
             option_string: Optional[str] = None) -&gt; NoReturn:
    formatter = parser._get_formatter()
    formatter.add_text(self.version)
    parser._print_message(formatter.format_help(), self.stdout)
    parser.exit()


</t>
<t tx="ekr.20220525082934.528">def process_options(args: List[str],
                    stdout: Optional[TextIO] = None,
                    stderr: Optional[TextIO] = None,
                    require_targets: bool = True,
                    server_options: bool = False,
                    fscache: Optional[FileSystemCache] = None,
                    program: str = 'mypy',
                    header: str = HEADER,
                    ) -&gt; Tuple[List[BuildSource], Options]:
    """Parse command line arguments.

    If a FileSystemCache is passed in, and package_root options are given,
    call fscache.set_package_root() to set the cache's package root.
    """
    stdout = stdout or sys.stdout
    stderr = stderr or sys.stderr

    parser = CapturableArgumentParser(prog=program,
                                      usage=header,
                                      description=DESCRIPTION,
                                      epilog=FOOTER,
                                      fromfile_prefix_chars='@',
                                      formatter_class=AugmentedHelpFormatter,
                                      add_help=False,
                                      stdout=stdout,
                                      stderr=stderr)

    strict_flag_names: List[str] = []
    strict_flag_assignments: List[Tuple[str, bool]] = []

    @others
    # Parse config file first, so command line can override.
    parse_config_file(options, set_strict_flags, config_file, stdout, stderr)

    # Set strict flags before parsing (if strict mode enabled), so other command
    # line options can override.
    if getattr(dummy, 'special-opts:strict'):  # noqa
        set_strict_flags()

    # Override cache_dir if provided in the environment
    environ_cache_dir = os.getenv('MYPY_CACHE_DIR', '')
    if environ_cache_dir.strip():
        options.cache_dir = environ_cache_dir
    options.cache_dir = os.path.expanduser(options.cache_dir)

    # Parse command line for real, using a split namespace.
    special_opts = argparse.Namespace()
    parser.parse_args(args, SplitNamespace(options, special_opts, 'special-opts:'))

    # The python_version is either the default, which can be overridden via a config file,
    # or stored in special_opts and is passed via the command line.
    options.python_version = special_opts.python_version or options.python_version
    try:
        infer_python_executable(options, special_opts)
    except PythonExecutableInferenceError as e:
        parser.error(str(e))

    if special_opts.no_executable or options.no_site_packages:
        options.python_executable = None

    # Paths listed in the config file will be ignored if any paths, modules or packages
    # are passed on the command line.
    if options.files and not (special_opts.files or special_opts.packages or special_opts.modules):
        special_opts.files = options.files

    # Check for invalid argument combinations.
    if require_targets:
        code_methods = sum(bool(c) for c in [special_opts.modules + special_opts.packages,
                                             special_opts.command,
                                             special_opts.files])
        if code_methods == 0 and not options.install_types:
            parser.error("Missing target module, package, files, or command.")
        elif code_methods &gt; 1:
            parser.error("May only specify one of: module/package, files, or command.")
    if options.explicit_package_bases and not options.namespace_packages:
        parser.error(
            "Can only use --explicit-package-bases with --namespace-packages, since otherwise "
            "examining __init__.py's is sufficient to determine module names for files"
        )

    # Check for overlapping `--always-true` and `--always-false` flags.
    overlap = set(options.always_true) &amp; set(options.always_false)
    if overlap:
        parser.error("You can't make a variable always true and always false (%s)" %
                     ', '.join(sorted(overlap)))

    # Process `--enable-error-code` and `--disable-error-code` flags
    disabled_codes = set(options.disable_error_code)
    enabled_codes = set(options.enable_error_code)

    valid_error_codes = set(error_codes.keys())

    invalid_codes = (enabled_codes | disabled_codes) - valid_error_codes
    if invalid_codes:
        parser.error(f"Invalid error code(s): {', '.join(sorted(invalid_codes))}")

    options.disabled_error_codes |= {error_codes[code] for code in disabled_codes}
    options.enabled_error_codes |= {error_codes[code] for code in enabled_codes}

    # Enabling an error code always overrides disabling
    options.disabled_error_codes -= options.enabled_error_codes

    # Set build flags.
    if options.strict_optional_whitelist is not None:
        # TODO: Deprecate, then kill this flag
        options.strict_optional = True
    if special_opts.find_occurrences:
        state.find_occurrences = special_opts.find_occurrences.split('.')
        assert state.find_occurrences is not None
        if len(state.find_occurrences) &lt; 2:
            parser.error("Can only find occurrences of class members.")
        if len(state.find_occurrences) != 2:
            parser.error("Can only find occurrences of non-nested class members.")

    # Set reports.
    for flag, val in vars(special_opts).items():
        if flag.endswith('_report') and val is not None:
            report_type = flag[:-7].replace('_', '-')
            report_dir = val
            options.report_dirs[report_type] = report_dir

    # Process --package-root.
    if options.package_root:
        process_package_roots(fscache, parser, options)

    # Process --cache-map.
    if special_opts.cache_map:
        if options.sqlite_cache:
            parser.error("--cache-map is incompatible with --sqlite-cache")

        process_cache_map(parser, special_opts, options)

    # An explicitly specified cache_fine_grained implies local_partial_types
    # (because otherwise the cache is not compatible with dmypy)
    if options.cache_fine_grained:
        options.local_partial_types = True

    # Let logical_deps imply cache_fine_grained (otherwise the former is useless).
    if options.logical_deps:
        options.cache_fine_grained = True

    # Set target.
    if special_opts.modules + special_opts.packages:
        options.build_type = BuildType.MODULE
        egg_dirs, site_packages = get_site_packages_dirs(options.python_executable)
        search_paths = SearchPaths((os.getcwd(),),
                                   tuple(mypy_path() + options.mypy_path),
                                   tuple(egg_dirs + site_packages),
                                   ())
        targets = []
        # TODO: use the same cache that the BuildManager will
        cache = FindModuleCache(search_paths, fscache, options)
        for p in special_opts.packages:
            if os.sep in p or os.altsep and os.altsep in p:
                fail(f"Package name '{p}' cannot have a slash in it.",
                     stderr, options)
            p_targets = cache.find_modules_recursive(p)
            if not p_targets:
                fail(f"Can't find package '{p}'", stderr, options)
            targets.extend(p_targets)
        for m in special_opts.modules:
            targets.append(BuildSource(None, m, None))
        return targets, options
    elif special_opts.command:
        options.build_type = BuildType.PROGRAM_TEXT
        targets = [BuildSource(None, None, '\n'.join(special_opts.command))]
        return targets, options
    else:
        try:
            targets = create_source_list(special_opts.files, options, fscache)
        # Variable named e2 instead of e to work around mypyc bug #620
        # which causes issues when using the same variable to catch
        # exceptions of different types.
        except InvalidSourceList as e2:
            fail(str(e2), stderr, options)
        return targets, options


</t>
<t tx="ekr.20220525082934.529">def add_invertible_flag(flag: str,
                        *,
                        inverse: Optional[str] = None,
                        default: bool,
                        dest: Optional[str] = None,
                        help: str,
                        strict_flag: bool = False,
                        group: Optional[argparse._ActionsContainer] = None
                        ) -&gt; None:
    if inverse is None:
        inverse = invert_flag_name(flag)
    if group is None:
        group = parser

    if help is not argparse.SUPPRESS:
        help += f" (inverse: {inverse})"

    arg = group.add_argument(flag,
                             action='store_false' if default else 'store_true',
                             dest=dest,
                             help=help)
    dest = arg.dest
    arg = group.add_argument(inverse,
                             action='store_true' if default else 'store_false',
                             dest=dest,
                             help=argparse.SUPPRESS)
    if strict_flag:
        assert dest is not None
        strict_flag_names.append(flag)
        strict_flag_assignments.append((dest, not default))

</t>
<t tx="ekr.20220525082934.53">def from_operator(self, op: ast3.operator) -&gt; str:
    op_name = ASTConverter.op_map.get(type(op))
    if op_name is None:
        raise RuntimeError('Unknown operator ' + str(type(op)))
    else:
        return op_name

</t>
<t tx="ekr.20220525082934.530"># Unless otherwise specified, arguments will be parsed directly onto an
# Options object.  Options that require further processing should have
# their `dest` prefixed with `special-opts:`, which will cause them to be
# parsed into the separate special_opts namespace object.

# Note: we have a style guide for formatting the mypy --help text. See
# https://github.com/python/mypy/wiki/Documentation-Conventions

general_group = parser.add_argument_group(
    title='Optional arguments')
general_group.add_argument(
    '-h', '--help', action='help',
    help="Show this help message and exit")
general_group.add_argument(
    '-v', '--verbose', action='count', dest='verbosity',
    help="More verbose messages")

compilation_status = "no" if __file__.endswith(".py") else "yes"
general_group.add_argument(
    '-V', '--version', action=CapturableVersionAction,
    version='%(prog)s ' + __version__ + f" (compiled: {compilation_status})",
    help="Show program's version number and exit",
    stdout=stdout)

config_group = parser.add_argument_group(
    title='Config file',
    description="Use a config file instead of command line arguments. "
                "This is useful if you are using many flags or want "
                "to set different options per each module.")
config_group.add_argument(
    '--config-file',
    help="Configuration file, must have a [mypy] section "
         "(defaults to {})".format(', '.join(defaults.CONFIG_FILES)))
add_invertible_flag('--warn-unused-configs', default=False, strict_flag=True,
                    help="Warn about unused '[mypy-&lt;pattern&gt;]' or '[[tool.mypy.overrides]]' "
                         "config sections",
                    group=config_group)

imports_group = parser.add_argument_group(
    title='Import discovery',
    description="Configure how imports are discovered and followed.")
add_invertible_flag(
    '--namespace-packages', default=False,
    help="Support namespace packages (PEP 420, __init__.py-less)",
    group=imports_group)
imports_group.add_argument(
    '--ignore-missing-imports', action='store_true',
    help="Silently ignore imports of missing modules")
imports_group.add_argument(
    '--follow-imports', choices=['normal', 'silent', 'skip', 'error'],
    default='normal', help="How to treat imports (default normal)")
imports_group.add_argument(
    '--python-executable', action='store', metavar='EXECUTABLE',
    help="Python executable used for finding PEP 561 compliant installed"
         " packages and stubs",
    dest='special-opts:python_executable')
imports_group.add_argument(
    '--no-site-packages', action='store_true',
    dest='special-opts:no_executable',
    help="Do not search for installed PEP 561 compliant packages")
imports_group.add_argument(
    '--no-silence-site-packages', action='store_true',
    help="Do not silence errors in PEP 561 compliant installed packages")

platform_group = parser.add_argument_group(
    title='Platform configuration',
    description="Type check code assuming it will be run under certain "
                "runtime conditions. By default, mypy assumes your code "
                "will be run using the same operating system and Python "
                "version you are using to run mypy itself.")
platform_group.add_argument(
    '--python-version', type=parse_version, metavar='x.y',
    help='Type check code assuming it will be running on Python x.y',
    dest='special-opts:python_version')
platform_group.add_argument(
    '-2', '--py2', dest='special-opts:python_version', action='store_const',
    const=defaults.PYTHON2_VERSION,
    help="Use Python 2 mode (same as --python-version 2.7)")
platform_group.add_argument(
    '--platform', action='store', metavar='PLATFORM',
    help="Type check special-cased code for the given OS platform "
         "(defaults to sys.platform)")
platform_group.add_argument(
    '--always-true', metavar='NAME', action='append', default=[],
    help="Additional variable to be considered True (may be repeated)")
platform_group.add_argument(
    '--always-false', metavar='NAME', action='append', default=[],
    help="Additional variable to be considered False (may be repeated)")

disallow_any_group = parser.add_argument_group(
    title='Disallow dynamic typing',
    description="Disallow the use of the dynamic 'Any' type under certain conditions.")
disallow_any_group.add_argument(
    '--disallow-any-unimported', default=False, action='store_true',
    help="Disallow Any types resulting from unfollowed imports")
disallow_any_group.add_argument(
    '--disallow-any-expr', default=False, action='store_true',
    help='Disallow all expressions that have type Any')
disallow_any_group.add_argument(
    '--disallow-any-decorated', default=False, action='store_true',
    help='Disallow functions that have Any in their signature '
         'after decorator transformation')
disallow_any_group.add_argument(
    '--disallow-any-explicit', default=False, action='store_true',
    help='Disallow explicit Any in type positions')
add_invertible_flag('--disallow-any-generics', default=False, strict_flag=True,
                    help='Disallow usage of generic types that do not specify explicit type '
                    'parameters', group=disallow_any_group)
add_invertible_flag('--disallow-subclassing-any', default=False, strict_flag=True,
                    help="Disallow subclassing values of type 'Any' when defining classes",
                    group=disallow_any_group)

untyped_group = parser.add_argument_group(
    title='Untyped definitions and calls',
    description="Configure how untyped definitions and calls are handled. "
                "Note: by default, mypy ignores any untyped function definitions "
                "and assumes any calls to such functions have a return "
                "type of 'Any'.")
add_invertible_flag('--disallow-untyped-calls', default=False, strict_flag=True,
                    help="Disallow calling functions without type annotations"
                    " from functions with type annotations",
                    group=untyped_group)
add_invertible_flag('--disallow-untyped-defs', default=False, strict_flag=True,
                    help="Disallow defining functions without type annotations"
                    " or with incomplete type annotations",
                    group=untyped_group)
add_invertible_flag('--disallow-incomplete-defs', default=False, strict_flag=True,
                    help="Disallow defining functions with incomplete type annotations",
                    group=untyped_group)
add_invertible_flag('--check-untyped-defs', default=False, strict_flag=True,
                    help="Type check the interior of functions without type annotations",
                    group=untyped_group)
add_invertible_flag('--disallow-untyped-decorators', default=False, strict_flag=True,
                    help="Disallow decorating typed functions with untyped decorators",
                    group=untyped_group)

none_group = parser.add_argument_group(
    title='None and Optional handling',
    description="Adjust how values of type 'None' are handled. For more context on "
                "how mypy handles values of type 'None', see: "
                "https://mypy.readthedocs.io/en/stable/kinds_of_types.html#no-strict-optional")
add_invertible_flag('--no-implicit-optional', default=False, strict_flag=True,
                    help="Don't assume arguments with default values of None are Optional",
                    group=none_group)
none_group.add_argument(
    '--strict-optional', action='store_true',
    help=argparse.SUPPRESS)
none_group.add_argument(
    '--no-strict-optional', action='store_false', dest='strict_optional',
    help="Disable strict Optional checks (inverse: --strict-optional)")
none_group.add_argument(
    '--strict-optional-whitelist', metavar='GLOB', nargs='*',
    help=argparse.SUPPRESS)

lint_group = parser.add_argument_group(
    title='Configuring warnings',
    description="Detect code that is sound but redundant or problematic.")
add_invertible_flag('--warn-redundant-casts', default=False, strict_flag=True,
                    help="Warn about casting an expression to its inferred type",
                    group=lint_group)
add_invertible_flag('--warn-unused-ignores', default=False, strict_flag=True,
                    help="Warn about unneeded '# type: ignore' comments",
                    group=lint_group)
add_invertible_flag('--no-warn-no-return', dest='warn_no_return', default=True,
                    help="Do not warn about functions that end without returning",
                    group=lint_group)
add_invertible_flag('--warn-return-any', default=False, strict_flag=True,
                    help="Warn about returning values of type Any"
                         " from non-Any typed functions",
                    group=lint_group)
add_invertible_flag('--warn-unreachable', default=False, strict_flag=False,
                    help="Warn about statements or expressions inferred to be"
                         " unreachable",
                    group=lint_group)

# Note: this group is intentionally added here even though we don't add
# --strict to this group near the end.
#
# That way, this group will appear after the various strictness groups
# but before the remaining flags.
# We add `--strict` near the end so we don't accidentally miss any strictness
# flags that are added after this group.
strictness_group = parser.add_argument_group(
    title='Miscellaneous strictness flags')

add_invertible_flag('--allow-untyped-globals', default=False, strict_flag=False,
                    help="Suppress toplevel errors caused by missing annotations",
                    group=strictness_group)

add_invertible_flag('--allow-redefinition', default=False, strict_flag=False,
                    help="Allow unconditional variable redefinition with a new type",
                    group=strictness_group)

add_invertible_flag('--no-implicit-reexport', default=True, strict_flag=True,
                    dest='implicit_reexport',
                    help="Treat imports as private unless aliased",
                    group=strictness_group)

add_invertible_flag('--strict-equality', default=False, strict_flag=True,
                    help="Prohibit equality, identity, and container checks for"
                         " non-overlapping types",
                    group=strictness_group)

add_invertible_flag('--strict-concatenate', default=False, strict_flag=True,
                    help="Make arguments prepended via Concatenate be truly positional-only",
                    group=strictness_group)

strict_help = "Strict mode; enables the following flags: {}".format(
    ", ".join(strict_flag_names))
strictness_group.add_argument(
    '--strict', action='store_true', dest='special-opts:strict',
    help=strict_help)

strictness_group.add_argument(
    '--disable-error-code', metavar='NAME', action='append', default=[],
    help="Disable a specific error code")
strictness_group.add_argument(
    '--enable-error-code', metavar='NAME', action='append', default=[],
    help="Enable a specific error code"
)

error_group = parser.add_argument_group(
    title='Configuring error messages',
    description="Adjust the amount of detail shown in error messages.")
add_invertible_flag('--show-error-context', default=False,
                    dest='show_error_context',
                    help='Precede errors with "note:" messages explaining context',
                    group=error_group)
add_invertible_flag('--show-column-numbers', default=False,
                    help="Show column numbers in error messages",
                    group=error_group)
add_invertible_flag('--show-error-codes', default=False,
                    help="Show error codes in error messages",
                    group=error_group)
add_invertible_flag('--pretty', default=False,
                    help="Use visually nicer output in error messages:"
                         " Use soft word wrap, show source code snippets,"
                         " and show error location markers",
                    group=error_group)
add_invertible_flag('--no-color-output', dest='color_output', default=True,
                    help="Do not colorize error messages",
                    group=error_group)
add_invertible_flag('--no-error-summary', dest='error_summary', default=True,
                    help="Do not show error stats summary",
                    group=error_group)
add_invertible_flag('--show-absolute-path', default=False,
                    help="Show absolute paths to files",
                    group=error_group)
error_group.add_argument('--soft-error-limit', default=defaults.MANY_ERRORS_THRESHOLD,
                         type=int, dest="many_errors_threshold", help=argparse.SUPPRESS)

incremental_group = parser.add_argument_group(
    title='Incremental mode',
    description="Adjust how mypy incrementally type checks and caches modules. "
                "Mypy caches type information about modules into a cache to "
                "let you speed up future invocations of mypy. Also see "
                "mypy's daemon mode: "
                "mypy.readthedocs.io/en/stable/mypy_daemon.html#mypy-daemon")
incremental_group.add_argument(
    '-i', '--incremental', action='store_true',
    help=argparse.SUPPRESS)
incremental_group.add_argument(
    '--no-incremental', action='store_false', dest='incremental',
    help="Disable module cache (inverse: --incremental)")
incremental_group.add_argument(
    '--cache-dir', action='store', metavar='DIR',
    help="Store module cache info in the given folder in incremental mode "
         "(defaults to '{}')".format(defaults.CACHE_DIR))
add_invertible_flag('--sqlite-cache', default=False,
                    help="Use a sqlite database to store the cache",
                    group=incremental_group)
incremental_group.add_argument(
    '--cache-fine-grained', action='store_true',
    help="Include fine-grained dependency information in the cache for the mypy daemon")
incremental_group.add_argument(
    '--skip-version-check', action='store_true',
    help="Allow using cache written by older mypy version")
incremental_group.add_argument(
    '--skip-cache-mtime-checks', action='store_true',
    help="Skip cache internal consistency checks based on mtime")

internals_group = parser.add_argument_group(
    title='Advanced options',
    description="Debug and customize mypy internals.")
internals_group.add_argument(
    '--pdb', action='store_true', help="Invoke pdb on fatal error")
internals_group.add_argument(
    '--show-traceback', '--tb', action='store_true',
    help="Show traceback on fatal error")
internals_group.add_argument(
    '--raise-exceptions', action='store_true', help="Raise exception on fatal error"
)
internals_group.add_argument(
    '--custom-typing-module', metavar='MODULE', dest='custom_typing_module',
    help="Use a custom typing module")
internals_group.add_argument(
    '--custom-typeshed-dir', metavar='DIR',
    help="Use the custom typeshed in DIR")
add_invertible_flag('--warn-incomplete-stub', default=False,
                    help="Warn if missing type annotation in typeshed, only relevant with"
                         " --disallow-untyped-defs or --disallow-incomplete-defs enabled",
                    group=internals_group)
internals_group.add_argument(
    '--shadow-file', nargs=2, metavar=('SOURCE_FILE', 'SHADOW_FILE'),
    dest='shadow_file', action='append',
    help="When encountering SOURCE_FILE, read and type check "
         "the contents of SHADOW_FILE instead.")
add_invertible_flag('--fast-exit', default=True, help=argparse.SUPPRESS,
                    group=internals_group)

report_group = parser.add_argument_group(
    title='Report generation',
    description='Generate a report in the specified format.')
for report_type in sorted(defaults.REPORTER_NAMES):
    if report_type not in {'memory-xml'}:
        report_group.add_argument(f"--{report_type.replace('_', '-')}-report",
                                  metavar='DIR',
                                  dest=f'special-opts:{report_type}_report')

other_group = parser.add_argument_group(
    title='Miscellaneous')
other_group.add_argument(
    '--quickstart-file', help=argparse.SUPPRESS)
other_group.add_argument(
    '--junit-xml', help="Write junit.xml to the given file")
other_group.add_argument(
    '--find-occurrences', metavar='CLASS.MEMBER',
    dest='special-opts:find_occurrences',
    help="Print out all usages of a class member (experimental)")
other_group.add_argument(
    '--scripts-are-modules', action='store_true',
    help="Script x becomes module x instead of __main__")

add_invertible_flag('--install-types', default=False, strict_flag=False,
                    help="Install detected missing library stub packages using pip",
                    group=other_group)
add_invertible_flag('--non-interactive', default=False, strict_flag=False,
                    help=("Install stubs without asking for confirmation and hide " +
                          "errors, with --install-types"),
                    group=other_group, inverse="--interactive")

if server_options:
    # TODO: This flag is superfluous; remove after a short transition (2018-03-16)
    other_group.add_argument(
        '--experimental', action='store_true', dest='fine_grained_incremental',
        help="Enable fine-grained incremental mode")
    other_group.add_argument(
        '--use-fine-grained-cache', action='store_true',
        help="Use the cache in fine-grained incremental mode")

# hidden options
parser.add_argument(
    '--stats', action='store_true', dest='dump_type_stats', help=argparse.SUPPRESS)
parser.add_argument(
    '--inferstats', action='store_true', dest='dump_inference_stats',
    help=argparse.SUPPRESS)
parser.add_argument(
    '--dump-build-stats', action='store_true',
    help=argparse.SUPPRESS)
# dump timing  stats for each processed file into the given output file
parser.add_argument(
    '--timing-stats', dest='timing_stats', help=argparse.SUPPRESS)
# --debug-cache will disable any cache-related compressions/optimizations,
# which will make the cache writing process output pretty-printed JSON (which
# is easier to debug).
parser.add_argument('--debug-cache', action='store_true', help=argparse.SUPPRESS)
# --dump-deps will dump all fine-grained dependencies to stdout
parser.add_argument('--dump-deps', action='store_true', help=argparse.SUPPRESS)
# --dump-graph will dump the contents of the graph of SCCs and exit.
parser.add_argument('--dump-graph', action='store_true', help=argparse.SUPPRESS)
# --semantic-analysis-only does exactly that.
parser.add_argument('--semantic-analysis-only', action='store_true', help=argparse.SUPPRESS)
# --local-partial-types disallows partial types spanning module top level and a function
# (implicitly defined in fine-grained incremental mode)
parser.add_argument('--local-partial-types', action='store_true', help=argparse.SUPPRESS)
# --logical-deps adds some more dependencies that are not semantically needed, but
# may be helpful to determine relative importance of classes and functions for overall
# type precision in a code base. It also _removes_ some deps, so this flag should be never
# used except for generating code stats. This also automatically enables --cache-fine-grained.
# NOTE: This is an experimental option that may be modified or removed at any time.
parser.add_argument('--logical-deps', action='store_true', help=argparse.SUPPRESS)
# --bazel changes some behaviors for use with Bazel (https://bazel.build).
parser.add_argument('--bazel', action='store_true', help=argparse.SUPPRESS)
# --package-root adds a directory below which directories are considered
# packages even without __init__.py.  May be repeated.
parser.add_argument('--package-root', metavar='ROOT', action='append', default=[],
                    help=argparse.SUPPRESS)
# --cache-map FILE ... gives a mapping from source files to cache files.
# Each triple of arguments is a source file, a cache meta file, and a cache data file.
# Modules not mentioned in the file will go through cache_dir.
# Must be followed by another flag or by '--' (and then only file args may follow).
parser.add_argument('--cache-map', nargs='+', dest='special-opts:cache_map',
                    help=argparse.SUPPRESS)
parser.add_argument('--enable-incomplete-features', action='store_true',
                    help=argparse.SUPPRESS)

# options specifying code to check
code_group = parser.add_argument_group(
    title="Running code",
    description="Specify the code you want to type check. For more details, see "
                "mypy.readthedocs.io/en/stable/running_mypy.html#running-mypy")
add_invertible_flag(
    '--explicit-package-bases', default=False,
    help="Use current directory and MYPYPATH to determine module names of files passed",
    group=code_group)
add_invertible_flag(
    '--fast-module-lookup', default=False,
    help=argparse.SUPPRESS,
    group=code_group)
code_group.add_argument(
    "--exclude",
    action="append",
    metavar="PATTERN",
    default=[],
    help=(
        "Regular expression to match file names, directory names or paths which mypy should "
        "ignore while recursively discovering files to check, e.g. --exclude '/setup\\.py$'. "
        "May be specified more than once, eg. --exclude a --exclude b"
    )
)
code_group.add_argument(
    '-m', '--module', action='append', metavar='MODULE',
    default=[],
    dest='special-opts:modules',
    help="Type-check module; can repeat for more modules")
code_group.add_argument(
    '-p', '--package', action='append', metavar='PACKAGE',
    default=[],
    dest='special-opts:packages',
    help="Type-check package recursively; can be repeated")
code_group.add_argument(
    '-c', '--command', action='append', metavar='PROGRAM_TEXT',
    dest='special-opts:command',
    help="Type-check program passed in as string")
code_group.add_argument(
    metavar='files', nargs='*', dest='special-opts:files',
    help="Type-check given files or directories")

# Parse arguments once into a dummy namespace so we can get the
# filename for the config file and know if the user requested all strict options.
dummy = argparse.Namespace()
parser.parse_args(args, dummy)
config_file = dummy.config_file
# Don't explicitly test if "config_file is not None" for this check.
# This lets `--config-file=` (an empty string) be used to disable all config files.
if config_file and not os.path.exists(config_file):
    parser.error(f"Cannot find config file '{config_file}'")

options = Options()

</t>
<t tx="ekr.20220525082934.531">def set_strict_flags() -&gt; None:
    for dest, value in strict_flag_assignments:
        setattr(options, dest, value)

</t>
<t tx="ekr.20220525082934.532">def process_package_roots(fscache: Optional[FileSystemCache],
                          parser: argparse.ArgumentParser,
                          options: Options) -&gt; None:
    """Validate and normalize package_root."""
    if fscache is None:
        parser.error("--package-root does not work here (no fscache)")
    assert fscache is not None  # Since mypy doesn't know parser.error() raises.
    # Do some stuff with drive letters to make Windows happy (esp. tests).
    current_drive, _ = os.path.splitdrive(os.getcwd())
    dot = os.curdir
    dotslash = os.curdir + os.sep
    dotdotslash = os.pardir + os.sep
    trivial_paths = {dot, dotslash}
    package_root = []
    for root in options.package_root:
        if os.path.isabs(root):
            parser.error(f"Package root cannot be absolute: {root!r}")
        drive, root = os.path.splitdrive(root)
        if drive and drive != current_drive:
            parser.error(f"Package root must be on current drive: {drive + root!r}")
        # Empty package root is always okay.
        if root:
            root = os.path.relpath(root)  # Normalize the heck out of it.
            if not root.endswith(os.sep):
                root = root + os.sep
            if root.startswith(dotdotslash):
                parser.error(f"Package root cannot be above current directory: {root!r}")
            if root in trivial_paths:
                root = ''
        package_root.append(root)
    options.package_root = package_root
    # Pass the package root on the the filesystem cache.
    fscache.set_package_root(package_root)


</t>
<t tx="ekr.20220525082934.533">def process_cache_map(parser: argparse.ArgumentParser,
                      special_opts: argparse.Namespace,
                      options: Options) -&gt; None:
    """Validate cache_map and copy into options.cache_map."""
    n = len(special_opts.cache_map)
    if n % 3 != 0:
        parser.error("--cache-map requires one or more triples (see source)")
    for i in range(0, n, 3):
        source, meta_file, data_file = special_opts.cache_map[i:i + 3]
        if source in options.cache_map:
            parser.error(f"Duplicate --cache-map source {source})")
        if not source.endswith('.py') and not source.endswith('.pyi'):
            parser.error(f"Invalid --cache-map source {source} (triple[0] must be *.py[i])")
        if not meta_file.endswith('.meta.json'):
            parser.error("Invalid --cache-map meta_file %s (triple[1] must be *.meta.json)" %
                         meta_file)
        if not data_file.endswith('.data.json'):
            parser.error("Invalid --cache-map data_file %s (triple[2] must be *.data.json)" %
                         data_file)
        options.cache_map[source] = (meta_file, data_file)


</t>
<t tx="ekr.20220525082934.534">def maybe_write_junit_xml(td: float, serious: bool, messages: List[str], options: Options) -&gt; None:
    if options.junit_xml:
        py_version = f'{options.python_version[0]}_{options.python_version[1]}'
        util.write_junit_xml(
            td, serious, messages, options.junit_xml, py_version, options.platform)


</t>
<t tx="ekr.20220525082934.535">def fail(msg: str, stderr: TextIO, options: Options) -&gt; NoReturn:
    """Fail with a serious error."""
    stderr.write(f'{msg}\n')
    maybe_write_junit_xml(0.0, serious=True, messages=[msg], options=options)
    sys.exit(2)


</t>
<t tx="ekr.20220525082934.536">def read_types_packages_to_install(cache_dir: str, after_run: bool) -&gt; List[str]:
    if not os.path.isdir(cache_dir):
        if not after_run:
            sys.stderr.write(
                "error: Can't determine which types to install with no files to check " +
                "(and no cache from previous mypy run)\n"
            )
        else:
            sys.stderr.write(
                "error: --install-types failed (no mypy cache directory)\n"
            )
        sys.exit(2)
    fnam = build.missing_stubs_file(cache_dir)
    if not os.path.isfile(fnam):
        # No missing stubs.
        return []
    with open(fnam) as f:
        return [line.strip() for line in f.readlines()]


</t>
<t tx="ekr.20220525082934.537">def install_types(formatter: util.FancyFormatter,
                  options: Options,
                  *,
                  after_run: bool = False,
                  non_interactive: bool = False) -&gt; bool:
    """Install stub packages using pip if some missing stubs were detected."""
    packages = read_types_packages_to_install(options.cache_dir, after_run)
    if not packages:
        # If there are no missing stubs, generate no output.
        return False
    if after_run and not non_interactive:
        print()
    print('Installing missing stub packages:')
    assert options.python_executable, 'Python executable required to install types'
    cmd = [options.python_executable, '-m', 'pip', 'install'] + packages
    print(formatter.style(' '.join(cmd), 'none', bold=True))
    print()
    if not non_interactive:
        x = input('Install? [yN] ')
        if not x.strip() or not x.lower().startswith('y'):
            print(formatter.style('mypy: Skipping installation', 'red', bold=True))
            sys.exit(2)
        print()
    subprocess.run(cmd)
    return True
</t>
<t tx="ekr.20220525082934.538">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from typing import Dict, List

from mypy.expandtype import expand_type
from mypy.nodes import TypeInfo
from mypy.types import Type, TypeVarId, Instance, AnyType, TypeOfAny, ProperType


@others
</t>
<t tx="ekr.20220525082934.539">def map_instance_to_supertype(instance: Instance,
                              superclass: TypeInfo) -&gt; Instance:
    """Produce a supertype of `instance` that is an Instance
    of `superclass`, mapping type arguments up the chain of bases.

    If `superclass` is not a nominal superclass of `instance.type`,
    then all type arguments are mapped to 'Any'.
    """
    if instance.type == superclass:
        # Fast path: `instance` already belongs to `superclass`.
        return instance

    if not superclass.type_vars:
        # Fast path: `superclass` has no type variables to map to.
        return Instance(superclass, [])

    return map_instance_to_supertypes(instance, superclass)[0]


</t>
<t tx="ekr.20220525082934.54">comp_op_map: Final[Dict[typing.Type[AST], str]] = {
    ast3.Gt: '&gt;',
    ast3.Lt: '&lt;',
    ast3.Eq: '==',
    ast3.GtE: '&gt;=',
    ast3.LtE: '&lt;=',
    ast3.NotEq: '!=',
    ast3.Is: 'is',
    ast3.IsNot: 'is not',
    ast3.In: 'in',
    ast3.NotIn: 'not in'
}

</t>
<t tx="ekr.20220525082934.540">def map_instance_to_supertypes(instance: Instance,
                               supertype: TypeInfo) -&gt; List[Instance]:
    # FIX: Currently we should only have one supertype per interface, so no
    #      need to return an array
    result: List[Instance] = []
    for path in class_derivation_paths(instance.type, supertype):
        types = [instance]
        for sup in path:
            a: List[Instance] = []
            for t in types:
                a.extend(map_instance_to_direct_supertypes(t, sup))
            types = a
        result.extend(types)
    if result:
        return result
    else:
        # Nothing. Presumably due to an error. Construct a dummy using Any.
        any_type = AnyType(TypeOfAny.from_error)
        return [Instance(supertype, [any_type] * len(supertype.type_vars))]


</t>
<t tx="ekr.20220525082934.541">def class_derivation_paths(typ: TypeInfo,
                           supertype: TypeInfo) -&gt; List[List[TypeInfo]]:
    """Return an array of non-empty paths of direct base classes from
    type to supertype.  Return [] if no such path could be found.

      InterfaceImplementationPaths(A, B) == [[B]] if A inherits B
      InterfaceImplementationPaths(A, C) == [[B, C]] if A inherits B and
                                                        B inherits C
    """
    # FIX: Currently we might only ever have a single path, so this could be
    #      simplified
    result: List[List[TypeInfo]] = []

    for base in typ.bases:
        btype = base.type
        if btype == supertype:
            result.append([btype])
        else:
            # Try constructing a longer path via the base class.
            for path in class_derivation_paths(btype, supertype):
                result.append([btype] + path)

    return result


</t>
<t tx="ekr.20220525082934.542">def map_instance_to_direct_supertypes(instance: Instance,
                                      supertype: TypeInfo) -&gt; List[Instance]:
    # FIX: There should only be one supertypes, always.
    typ = instance.type
    result: List[Instance] = []

    for b in typ.bases:
        if b.type == supertype:
            env = instance_to_type_environment(instance)
            t = expand_type(b, env)
            assert isinstance(t, ProperType)
            assert isinstance(t, Instance)
            result.append(t)

    if result:
        return result
    else:
        # Relationship with the supertype not specified explicitly. Use dynamic
        # type arguments implicitly.
        any_type = AnyType(TypeOfAny.unannotated)
        return [Instance(supertype, [any_type] * len(supertype.type_vars))]


</t>
<t tx="ekr.20220525082934.543">def instance_to_type_environment(instance: Instance) -&gt; Dict[TypeVarId, Type]:
    """Given an Instance, produce the resulting type environment for type
    variables bound by the Instance's class definition.

    An Instance is a type application of a class (a TypeInfo) to its
    required number of type arguments.  So this environment consists
    of the class's type variables mapped to the Instance's actual
    arguments.  The type variables are mapped by their `id`.

    """
    return {binder.id: arg for binder, arg in zip(instance.type.defn.type_vars, instance.args)}
</t>
<t tx="ekr.20220525082934.544">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from mypy.backports import OrderedDict
from typing import List, Optional, Tuple, Callable

from mypy.types import (
    Type, AnyType, TypeVisitor, UnboundType, NoneType, TypeVarType, Instance, CallableType,
    TupleType, TypedDictType, ErasedType, UnionType, PartialType, DeletedType,
    UninhabitedType, TypeType, TypeOfAny, Overloaded, FunctionLike, LiteralType,
    ProperType, get_proper_type, get_proper_types, TypeAliasType, TypeGuardedType,
    ParamSpecType, Parameters, UnpackType, TypeVarTupleType,
)
from mypy.subtypes import is_equivalent, is_subtype, is_callable_compatible, is_proper_subtype
from mypy.erasetype import erase_type
from mypy.maptype import map_instance_to_supertype
from mypy.typeops import tuple_fallback, make_simplified_union, is_recursive_pair
from mypy.state import state
from mypy import join

# TODO Describe this module.


@others
</t>
<t tx="ekr.20220525082934.545">def trivial_meet(s: Type, t: Type) -&gt; ProperType:
    """Return one of types (expanded) if it is a subtype of other, otherwise bottom type."""
    if is_subtype(s, t):
        return get_proper_type(s)
    elif is_subtype(t, s):
        return get_proper_type(t)
    else:
        if state.strict_optional:
            return UninhabitedType()
        else:
            return NoneType()


</t>
<t tx="ekr.20220525082934.546">def meet_types(s: Type, t: Type) -&gt; ProperType:
    """Return the greatest lower bound of two types."""
    if is_recursive_pair(s, t):
        # This case can trigger an infinite recursion, general support for this will be
        # tricky so we use a trivial meet (like for protocols).
        return trivial_meet(s, t)
    s = get_proper_type(s)
    t = get_proper_type(t)

    if isinstance(s, ErasedType):
        return s
    if isinstance(s, AnyType):
        return t
    if isinstance(s, UnionType) and not isinstance(t, UnionType):
        s, t = t, s
    return t.accept(TypeMeetVisitor(s))


</t>
<t tx="ekr.20220525082934.547">def narrow_declared_type(declared: Type, narrowed: Type) -&gt; Type:
    """Return the declared type narrowed down to another type."""
    # TODO: check infinite recursion for aliases here.
    if isinstance(narrowed, TypeGuardedType):  # type: ignore[misc]
        # A type guard forces the new type even if it doesn't overlap the old.
        return narrowed.type_guard

    declared = get_proper_type(declared)
    narrowed = get_proper_type(narrowed)

    if declared == narrowed:
        return declared
    if isinstance(declared, UnionType):
        return make_simplified_union([narrow_declared_type(x, narrowed)
                                      for x in declared.relevant_items()])
    if is_enum_overlapping_union(declared, narrowed):
        return narrowed
    elif not is_overlapping_types(declared, narrowed,
                                  prohibit_none_typevar_overlap=True):
        if state.strict_optional:
            return UninhabitedType()
        else:
            return NoneType()
    elif isinstance(narrowed, UnionType):
        return make_simplified_union([narrow_declared_type(declared, x)
                                      for x in narrowed.relevant_items()])
    elif isinstance(narrowed, AnyType):
        return narrowed
    elif isinstance(narrowed, TypeVarType) and is_subtype(narrowed.upper_bound, declared):
        return narrowed
    elif isinstance(declared, TypeType) and isinstance(narrowed, TypeType):
        return TypeType.make_normalized(narrow_declared_type(declared.item, narrowed.item))
    elif (isinstance(declared, TypeType)
          and isinstance(narrowed, Instance)
          and narrowed.type.is_metaclass()):
        # We'd need intersection types, so give up.
        return declared
    elif isinstance(declared, (Instance, TupleType, TypeType, LiteralType)):
        return meet_types(declared, narrowed)
    elif isinstance(declared, TypedDictType) and isinstance(narrowed, Instance):
        # Special case useful for selecting TypedDicts from unions using isinstance(x, dict).
        if (narrowed.type.fullname == 'builtins.dict' and
                all(isinstance(t, AnyType) for t in get_proper_types(narrowed.args))):
            return declared
        return meet_types(declared, narrowed)
    return narrowed


</t>
<t tx="ekr.20220525082934.548">def get_possible_variants(typ: Type) -&gt; List[Type]:
    """This function takes any "Union-like" type and returns a list of the available "options".

    Specifically, there are currently exactly three different types that can have
    "variants" or are "union-like":

    - Unions
    - TypeVars with value restrictions
    - Overloads

    This function will return a list of each "option" present in those types.

    If this function receives any other type, we return a list containing just that
    original type. (E.g. pretend the type was contained within a singleton union).

    The only exception is regular TypeVars: we return a list containing that TypeVar's
    upper bound.

    This function is useful primarily when checking to see if two types are overlapping:
    the algorithm to check if two unions are overlapping is fundamentally the same as
    the algorithm for checking if two overloads are overlapping.

    Normalizing both kinds of types in the same way lets us reuse the same algorithm
    for both.
    """
    typ = get_proper_type(typ)

    if isinstance(typ, TypeVarType):
        if len(typ.values) &gt; 0:
            return typ.values
        else:
            return [typ.upper_bound]
    elif isinstance(typ, UnionType):
        return list(typ.items)
    elif isinstance(typ, Overloaded):
        # Note: doing 'return typ.items()' makes mypy
        # infer a too-specific return type of List[CallableType]
        return list(typ.items)
    else:
        return [typ]


</t>
<t tx="ekr.20220525082934.549">def is_enum_overlapping_union(x: ProperType, y: ProperType) -&gt; bool:
    """Return True if x is an Enum, and y is an Union with at least one Literal from x"""
    return (
        isinstance(x, Instance) and x.type.is_enum and
        isinstance(y, UnionType) and
        any(isinstance(p, LiteralType) and x.type == p.fallback.type
            for p in (get_proper_type(z) for z in y.relevant_items()))
    )


</t>
<t tx="ekr.20220525082934.55">def from_comp_operator(self, op: ast3.cmpop) -&gt; str:
    op_name = ASTConverter.comp_op_map.get(type(op))
    if op_name is None:
        raise RuntimeError('Unknown comparison operator ' + str(type(op)))
    else:
        return op_name

</t>
<t tx="ekr.20220525082934.550">def is_literal_in_union(x: ProperType, y: ProperType) -&gt; bool:
    """Return True if x is a Literal and y is an Union that includes x"""
    return (isinstance(x, LiteralType) and isinstance(y, UnionType) and
            any(x == get_proper_type(z) for z in y.items))


</t>
<t tx="ekr.20220525082934.551">def is_overlapping_types(left: Type,
                         right: Type,
                         ignore_promotions: bool = False,
                         prohibit_none_typevar_overlap: bool = False) -&gt; bool:
    """Can a value of type 'left' also be of type 'right' or vice-versa?

    If 'ignore_promotions' is True, we ignore promotions while checking for overlaps.
    If 'prohibit_none_typevar_overlap' is True, we disallow None from overlapping with
    TypeVars (in both strict-optional and non-strict-optional mode).
    """
    if (
        isinstance(left, TypeGuardedType)  # type: ignore[misc]
        or isinstance(right, TypeGuardedType)  # type: ignore[misc]
    ):
        # A type guard forces the new type even if it doesn't overlap the old.
        return True

    left, right = get_proper_types((left, right))

    @others
    if isinstance(left, TypeType) or isinstance(right, TypeType):
        return _type_object_overlap(left, right) or _type_object_overlap(right, left)

    if isinstance(left, CallableType) and isinstance(right, CallableType):
        return is_callable_compatible(left, right,
                                      is_compat=_is_overlapping_types,
                                      ignore_pos_arg_names=True,
                                      allow_partial_overlap=True)
    elif isinstance(left, CallableType):
        left = left.fallback
    elif isinstance(right, CallableType):
        right = right.fallback

    if isinstance(left, LiteralType) and isinstance(right, LiteralType):
        if left.value == right.value:
            # If values are the same, we still need to check if fallbacks are overlapping,
            # this is done below.
            left = left.fallback
            right = right.fallback
        else:
            return False
    elif isinstance(left, LiteralType):
        left = left.fallback
    elif isinstance(right, LiteralType):
        right = right.fallback

    # Finally, we handle the case where left and right are instances.

    if isinstance(left, Instance) and isinstance(right, Instance):
        # First we need to handle promotions and structural compatibility for instances
        # that came as fallbacks, so simply call is_subtype() to avoid code duplication.
        if (is_subtype(left, right, ignore_promotions=ignore_promotions)
                or is_subtype(right, left, ignore_promotions=ignore_promotions)):
            return True

        # Two unrelated types cannot be partially overlapping: they're disjoint.
        if left.type.has_base(right.type.fullname):
            left = map_instance_to_supertype(left, right.type)
        elif right.type.has_base(left.type.fullname):
            right = map_instance_to_supertype(right, left.type)
        else:
            return False

        if len(left.args) == len(right.args):
            # Note: we don't really care about variance here, since the overlapping check
            # is symmetric and since we want to return 'True' even for partial overlaps.
            #
            # For example, suppose we have two types Wrapper[Parent] and Wrapper[Child].
            # It doesn't matter whether Wrapper is covariant or contravariant since
            # either way, one of the two types will overlap with the other.
            #
            # Similarly, if Wrapper was invariant, the two types could still be partially
            # overlapping -- what if Wrapper[Parent] happened to contain only instances of
            # specifically Child?
            #
            # Or, to use a more concrete example, List[Union[A, B]] and List[Union[B, C]]
            # would be considered partially overlapping since it's possible for both lists
            # to contain only instances of B at runtime.
            if all(_is_overlapping_types(left_arg, right_arg)
                   for left_arg, right_arg in zip(left.args, right.args)):
                return True

        return False

    # We ought to have handled every case by now: we conclude the
    # two types are not overlapping, either completely or partially.
    #
    # Note: it's unclear however, whether returning False is the right thing
    # to do when inferring reachability -- see  https://github.com/python/mypy/issues/5529

    assert type(left) != type(right)
    return False


</t>
<t tx="ekr.20220525082934.552">def _is_overlapping_types(left: Type, right: Type) -&gt; bool:
    '''Encode the kind of overlapping check to perform.

    This function mostly exists so we don't have to repeat keyword arguments everywhere.'''
    return is_overlapping_types(
        left, right,
        ignore_promotions=ignore_promotions,
        prohibit_none_typevar_overlap=prohibit_none_typevar_overlap)

</t>
<t tx="ekr.20220525082934.553"># We should never encounter this type.
if isinstance(left, PartialType) or isinstance(right, PartialType):
    assert False, "Unexpectedly encountered partial type"

# We should also never encounter these types, but it's possible a few
# have snuck through due to unrelated bugs. For now, we handle these
# in the same way we handle 'Any'.
#
# TODO: Replace these with an 'assert False' once we are more confident.
illegal_types = (UnboundType, ErasedType, DeletedType)
if isinstance(left, illegal_types) or isinstance(right, illegal_types):
    return True

# When running under non-strict optional mode, simplify away types of
# the form 'Union[A, B, C, None]' into just 'Union[A, B, C]'.

if not state.strict_optional:
    if isinstance(left, UnionType):
        left = UnionType.make_union(left.relevant_items())
    if isinstance(right, UnionType):
        right = UnionType.make_union(right.relevant_items())
    left, right = get_proper_types((left, right))

# 'Any' may or may not be overlapping with the other type
if isinstance(left, AnyType) or isinstance(right, AnyType):
    return True

# We check for complete overlaps next as a general-purpose failsafe.
# If this check fails, we start checking to see if there exists a
# *partial* overlap between types.
#
# These checks will also handle the NoneType and UninhabitedType cases for us.

# enums are sometimes expanded into an Union of Literals
# when that happens we want to make sure we treat the two as overlapping
# and crucially, we want to do that *fast* in case the enum is large
# so we do it before expanding variants below to avoid O(n**2) behavior
if (
    is_enum_overlapping_union(left, right)
    or is_enum_overlapping_union(right, left)
    or is_literal_in_union(left, right)
    or is_literal_in_union(right, left)
):
    return True

if (is_proper_subtype(left, right, ignore_promotions=ignore_promotions)
        or is_proper_subtype(right, left, ignore_promotions=ignore_promotions)):
    return True

# See the docstring for 'get_possible_variants' for more info on what the
# following lines are doing.

left_possible = get_possible_variants(left)
right_possible = get_possible_variants(right)

# We start by checking multi-variant types like Unions first. We also perform
# the same logic if either type happens to be a TypeVar.
#
# Handling the TypeVars now lets us simulate having them bind to the corresponding
# type -- if we deferred these checks, the "return-early" logic of the other
# checks will prevent us from detecting certain overlaps.
#
# If both types are singleton variants (and are not TypeVars), we've hit the base case:
# we skip these checks to avoid infinitely recursing.

</t>
<t tx="ekr.20220525082934.554">def is_none_typevar_overlap(t1: Type, t2: Type) -&gt; bool:
    t1, t2 = get_proper_types((t1, t2))
    return isinstance(t1, NoneType) and isinstance(t2, TypeVarType)

</t>
<t tx="ekr.20220525082934.555">if prohibit_none_typevar_overlap:
    if is_none_typevar_overlap(left, right) or is_none_typevar_overlap(right, left):
        return False

if (len(left_possible) &gt; 1 or len(right_possible) &gt; 1
        or isinstance(left, TypeVarType) or isinstance(right, TypeVarType)):
    for l in left_possible:
        for r in right_possible:
            if _is_overlapping_types(l, r):
                return True
    return False

# Now that we've finished handling TypeVars, we're free to end early
# if one one of the types is None and we're running in strict-optional mode.
# (None only overlaps with None in strict-optional mode).
#
# We must perform this check after the TypeVar checks because
# a TypeVar could be bound to None, for example.

if state.strict_optional and isinstance(left, NoneType) != isinstance(right, NoneType):
    return False

# Next, we handle single-variant types that may be inherently partially overlapping:
#
# - TypedDicts
# - Tuples
#
# If we cannot identify a partial overlap and end early, we degrade these two types
# into their 'Instance' fallbacks.

if isinstance(left, TypedDictType) and isinstance(right, TypedDictType):
    return are_typed_dicts_overlapping(left, right, ignore_promotions=ignore_promotions)
elif typed_dict_mapping_pair(left, right):
    # Overlaps between TypedDicts and Mappings require dedicated logic.
    return typed_dict_mapping_overlap(left, right,
                                      overlapping=_is_overlapping_types)
elif isinstance(left, TypedDictType):
    left = left.fallback
elif isinstance(right, TypedDictType):
    right = right.fallback

if is_tuple(left) and is_tuple(right):
    return are_tuples_overlapping(left, right, ignore_promotions=ignore_promotions)
elif isinstance(left, TupleType):
    left = tuple_fallback(left)
elif isinstance(right, TupleType):
    right = tuple_fallback(right)

# Next, we handle single-variant types that cannot be inherently partially overlapping,
# but do require custom logic to inspect.
#
# As before, we degrade into 'Instance' whenever possible.

if isinstance(left, TypeType) and isinstance(right, TypeType):
    return _is_overlapping_types(left.item, right.item)

</t>
<t tx="ekr.20220525082934.556">def _type_object_overlap(left: Type, right: Type) -&gt; bool:
    """Special cases for type object types overlaps."""
    # TODO: these checks are a bit in gray area, adjust if they cause problems.
    left, right = get_proper_types((left, right))
    # 1. Type[C] vs Callable[..., C], where the latter is class object.
    if isinstance(left, TypeType) and isinstance(right, CallableType) and right.is_type_obj():
        return _is_overlapping_types(left.item, right.ret_type)
    # 2. Type[C] vs Meta, where Meta is a metaclass for C.
    if isinstance(left, TypeType) and isinstance(right, Instance):
        if isinstance(left.item, Instance):
            left_meta = left.item.type.metaclass_type
            if left_meta is not None:
                return _is_overlapping_types(left_meta, right)
            # builtins.type (default metaclass) overlaps with all metaclasses
            return right.type.has_base('builtins.type')
        elif isinstance(left.item, AnyType):
            return right.type.has_base('builtins.type')
    # 3. Callable[..., C] vs Meta is considered below, when we switch to fallbacks.
    return False

</t>
<t tx="ekr.20220525082934.557">def is_overlapping_erased_types(left: Type, right: Type, *,
                                ignore_promotions: bool = False) -&gt; bool:
    """The same as 'is_overlapping_erased_types', except the types are erased first."""
    return is_overlapping_types(erase_type(left), erase_type(right),
                                ignore_promotions=ignore_promotions,
                                prohibit_none_typevar_overlap=True)


</t>
<t tx="ekr.20220525082934.558">def are_typed_dicts_overlapping(left: TypedDictType, right: TypedDictType, *,
                                ignore_promotions: bool = False,
                                prohibit_none_typevar_overlap: bool = False) -&gt; bool:
    """Returns 'true' if left and right are overlapping TypeDictTypes."""
    # All required keys in left are present and overlapping with something in right
    for key in left.required_keys:
        if key not in right.items:
            return False
        if not is_overlapping_types(left.items[key], right.items[key],
                                    ignore_promotions=ignore_promotions,
                                    prohibit_none_typevar_overlap=prohibit_none_typevar_overlap):
            return False

    # Repeat check in the other direction
    for key in right.required_keys:
        if key not in left.items:
            return False
        if not is_overlapping_types(left.items[key], right.items[key],
                                    ignore_promotions=ignore_promotions):
            return False

    # The presence of any additional optional keys does not affect whether the two
    # TypedDicts are partially overlapping: the dicts would be overlapping if the
    # keys happened to be missing.
    return True


</t>
<t tx="ekr.20220525082934.559">def are_tuples_overlapping(left: Type, right: Type, *,
                           ignore_promotions: bool = False,
                           prohibit_none_typevar_overlap: bool = False) -&gt; bool:
    """Returns true if left and right are overlapping tuples."""
    left, right = get_proper_types((left, right))
    left = adjust_tuple(left, right) or left
    right = adjust_tuple(right, left) or right
    assert isinstance(left, TupleType), f'Type {left} is not a tuple'
    assert isinstance(right, TupleType), f'Type {right} is not a tuple'
    if len(left.items) != len(right.items):
        return False
    return all(is_overlapping_types(l, r,
                                    ignore_promotions=ignore_promotions,
                                    prohibit_none_typevar_overlap=prohibit_none_typevar_overlap)
               for l, r in zip(left.items, right.items))


</t>
<t tx="ekr.20220525082934.56">def as_block(self, stmts: List[ast3.stmt], lineno: int) -&gt; Optional[Block]:
    b = None
    if stmts:
        b = Block(self.fix_function_overloads(self.translate_stmt_list(stmts)))
        b.set_line(lineno)
    return b

</t>
<t tx="ekr.20220525082934.560">def adjust_tuple(left: ProperType, r: ProperType) -&gt; Optional[TupleType]:
    """Find out if `left` is a Tuple[A, ...], and adjust its length to `right`"""
    if isinstance(left, Instance) and left.type.fullname == 'builtins.tuple':
        n = r.length() if isinstance(r, TupleType) else 1
        return TupleType([left.args[0]] * n, left)
    return None


</t>
<t tx="ekr.20220525082934.561">def is_tuple(typ: Type) -&gt; bool:
    typ = get_proper_type(typ)
    return (isinstance(typ, TupleType)
            or (isinstance(typ, Instance) and typ.type.fullname == 'builtins.tuple'))


</t>
<t tx="ekr.20220525082934.562">class TypeMeetVisitor(TypeVisitor[ProperType]):
    @others
</t>
<t tx="ekr.20220525082934.563">def __init__(self, s: ProperType) -&gt; None:
    self.s = s

</t>
<t tx="ekr.20220525082934.564">def visit_unbound_type(self, t: UnboundType) -&gt; ProperType:
    if isinstance(self.s, NoneType):
        if state.strict_optional:
            return AnyType(TypeOfAny.special_form)
        else:
            return self.s
    elif isinstance(self.s, UninhabitedType):
        return self.s
    else:
        return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082934.565">def visit_any(self, t: AnyType) -&gt; ProperType:
    return self.s

</t>
<t tx="ekr.20220525082934.566">def visit_union_type(self, t: UnionType) -&gt; ProperType:
    if isinstance(self.s, UnionType):
        meets: List[Type] = []
        for x in t.items:
            for y in self.s.items:
                meets.append(meet_types(x, y))
    else:
        meets = [meet_types(x, self.s)
                 for x in t.items]
    return make_simplified_union(meets)

</t>
<t tx="ekr.20220525082934.567">def visit_none_type(self, t: NoneType) -&gt; ProperType:
    if state.strict_optional:
        if isinstance(self.s, NoneType) or (isinstance(self.s, Instance) and
                                           self.s.type.fullname == 'builtins.object'):
            return t
        else:
            return UninhabitedType()
    else:
        return t

</t>
<t tx="ekr.20220525082934.568">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; ProperType:
    return t

</t>
<t tx="ekr.20220525082934.569">def visit_deleted_type(self, t: DeletedType) -&gt; ProperType:
    if isinstance(self.s, NoneType):
        if state.strict_optional:
            return t
        else:
            return self.s
    elif isinstance(self.s, UninhabitedType):
        return self.s
    else:
        return t

</t>
<t tx="ekr.20220525082934.57">def as_required_block(self, stmts: List[ast3.stmt], lineno: int) -&gt; Block:
    assert stmts  # must be non-empty
    b = Block(self.fix_function_overloads(self.translate_stmt_list(stmts)))
    b.set_line(lineno)
    return b

</t>
<t tx="ekr.20220525082934.570">def visit_erased_type(self, t: ErasedType) -&gt; ProperType:
    return self.s

</t>
<t tx="ekr.20220525082934.571">def visit_type_var(self, t: TypeVarType) -&gt; ProperType:
    if isinstance(self.s, TypeVarType) and self.s.id == t.id:
        return self.s
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20220525082934.572">def visit_param_spec(self, t: ParamSpecType) -&gt; ProperType:
    if self.s == t:
        return self.s
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20220525082934.573">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; ProperType:
    if self.s == t:
        return self.s
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20220525082934.574">def visit_unpack_type(self, t: UnpackType) -&gt; ProperType:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082934.575">def visit_parameters(self, t: Parameters) -&gt; ProperType:
    # TODO: is this the right variance?
    if isinstance(self.s, Parameters) or isinstance(self.s, CallableType):
        if len(t.arg_types) != len(self.s.arg_types):
            return self.default(self.s)
        return t.copy_modified(
            arg_types=[meet_types(s_a, t_a) for s_a, t_a in zip(self.s.arg_types, t.arg_types)]
        )
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20220525082934.576">def visit_instance(self, t: Instance) -&gt; ProperType:
    if isinstance(self.s, Instance):
        if t.type == self.s.type:
            if is_subtype(t, self.s) or is_subtype(self.s, t):
                # Combine type arguments. We could have used join below
                # equivalently.
                args: List[Type] = []
                # N.B: We use zip instead of indexing because the lengths might have
                # mismatches during daemon reprocessing.
                for ta, sia in zip(t.args, self.s.args):
                    args.append(self.meet(ta, sia))
                return Instance(t.type, args)
            else:
                if state.strict_optional:
                    return UninhabitedType()
                else:
                    return NoneType()
        else:
            if is_subtype(t, self.s):
                return t
            elif is_subtype(self.s, t):
                # See also above comment.
                return self.s
            else:
                if state.strict_optional:
                    return UninhabitedType()
                else:
                    return NoneType()
    elif isinstance(self.s, FunctionLike) and t.type.is_protocol:
        call = join.unpack_callback_protocol(t)
        if call:
            return meet_types(call, self.s)
    elif isinstance(self.s, FunctionLike) and self.s.is_type_obj() and t.type.is_metaclass():
        if is_subtype(self.s.fallback, t):
            return self.s
        return self.default(self.s)
    elif isinstance(self.s, TypeType):
        return meet_types(t, self.s)
    elif isinstance(self.s, TupleType):
        return meet_types(t, self.s)
    elif isinstance(self.s, LiteralType):
        return meet_types(t, self.s)
    elif isinstance(self.s, TypedDictType):
        return meet_types(t, self.s)
    return self.default(self.s)

</t>
<t tx="ekr.20220525082934.577">def visit_callable_type(self, t: CallableType) -&gt; ProperType:
    if isinstance(self.s, CallableType) and join.is_similar_callables(t, self.s):
        if is_equivalent(t, self.s):
            return join.combine_similar_callables(t, self.s)
        result = meet_similar_callables(t, self.s)
        # We set the from_type_type flag to suppress error when a collection of
        # concrete class objects gets inferred as their common abstract superclass.
        if not ((t.is_type_obj() and t.type_object().is_abstract) or
                (self.s.is_type_obj() and self.s.type_object().is_abstract)):
            result.from_type_type = True
        if isinstance(get_proper_type(result.ret_type), UninhabitedType):
            # Return a plain None or &lt;uninhabited&gt; instead of a weird function.
            return self.default(self.s)
        return result
    elif isinstance(self.s, TypeType) and t.is_type_obj() and not t.is_generic():
        # In this case we are able to potentially produce a better meet.
        res = meet_types(self.s.item, t.ret_type)
        if not isinstance(res, (NoneType, UninhabitedType)):
            return TypeType.make_normalized(res)
        return self.default(self.s)
    elif isinstance(self.s, Instance) and self.s.type.is_protocol:
        call = join.unpack_callback_protocol(self.s)
        if call:
            return meet_types(t, call)
    return self.default(self.s)

</t>
<t tx="ekr.20220525082934.578">def visit_overloaded(self, t: Overloaded) -&gt; ProperType:
    # TODO: Implement a better algorithm that covers at least the same cases
    # as TypeJoinVisitor.visit_overloaded().
    s = self.s
    if isinstance(s, FunctionLike):
        if s.items == t.items:
            return Overloaded(t.items)
        elif is_subtype(s, t):
            return s
        elif is_subtype(t, s):
            return t
        else:
            return meet_types(t.fallback, s.fallback)
    elif isinstance(self.s, Instance) and self.s.type.is_protocol:
        call = join.unpack_callback_protocol(self.s)
        if call:
            return meet_types(t, call)
    return meet_types(t.fallback, s)

</t>
<t tx="ekr.20220525082934.579">def visit_tuple_type(self, t: TupleType) -&gt; ProperType:
    if isinstance(self.s, TupleType) and self.s.length() == t.length():
        items: List[Type] = []
        for i in range(t.length()):
            items.append(self.meet(t.items[i], self.s.items[i]))
        # TODO: What if the fallbacks are different?
        return TupleType(items, tuple_fallback(t))
    elif isinstance(self.s, Instance):
        # meet(Tuple[t1, t2, &lt;...&gt;], Tuple[s, ...]) == Tuple[meet(t1, s), meet(t2, s), &lt;...&gt;].
        if self.s.type.fullname == 'builtins.tuple' and self.s.args:
            return t.copy_modified(items=[meet_types(it, self.s.args[0]) for it in t.items])
        elif is_proper_subtype(t, self.s):
            # A named tuple that inherits from a normal class
            return t
    return self.default(self.s)

</t>
<t tx="ekr.20220525082934.58">def fix_function_overloads(self, stmts: List[Statement]) -&gt; List[Statement]:
    ret: List[Statement] = []
    current_overload: List[OverloadPart] = []
    current_overload_name: Optional[str] = None
    seen_unconditional_func_def = False
    last_if_stmt: Optional[IfStmt] = None
    last_if_overload: Optional[Union[Decorator, FuncDef, OverloadedFuncDef]] = None
    last_if_stmt_overload_name: Optional[str] = None
    last_if_unknown_truth_value: Optional[IfStmt] = None
    skipped_if_stmts: List[IfStmt] = []
    for stmt in stmts:
        if_overload_name: Optional[str] = None
        if_block_with_overload: Optional[Block] = None
        if_unknown_truth_value: Optional[IfStmt] = None
        if isinstance(stmt, IfStmt) and seen_unconditional_func_def is False:
            # Check IfStmt block to determine if function overloads can be merged
            if_overload_name = self._check_ifstmt_for_overloads(stmt, current_overload_name)
            if if_overload_name is not None:
                if_block_with_overload, if_unknown_truth_value = \
                    self._get_executable_if_block_with_overloads(stmt)

        if (current_overload_name is not None
                and isinstance(stmt, (Decorator, FuncDef))
                and stmt.name == current_overload_name):
            if last_if_stmt is not None:
                skipped_if_stmts.append(last_if_stmt)
            if last_if_overload is not None:
                # Last stmt was an IfStmt with same overload name
                # Add overloads to current_overload
                if isinstance(last_if_overload, OverloadedFuncDef):
                    current_overload.extend(last_if_overload.items)
                else:
                    current_overload.append(last_if_overload)
                last_if_stmt, last_if_overload = None, None
            if last_if_unknown_truth_value:
                self.fail_merge_overload(last_if_unknown_truth_value)
                last_if_unknown_truth_value = None
            current_overload.append(stmt)
            if isinstance(stmt, FuncDef):
                seen_unconditional_func_def = True
        elif (
            current_overload_name is not None
            and isinstance(stmt, IfStmt)
            and if_overload_name == current_overload_name
        ):
            # IfStmt only contains stmts relevant to current_overload.
            # Check if stmts are reachable and add them to current_overload,
            # otherwise skip IfStmt to allow subsequent overload
            # or function definitions.
            skipped_if_stmts.append(stmt)
            if if_block_with_overload is None:
                if if_unknown_truth_value is not None:
                    self.fail_merge_overload(if_unknown_truth_value)
                continue
            if last_if_overload is not None:
                # Last stmt was an IfStmt with same overload name
                # Add overloads to current_overload
                if isinstance(last_if_overload, OverloadedFuncDef):
                    current_overload.extend(last_if_overload.items)
                else:
                    current_overload.append(last_if_overload)
                last_if_stmt, last_if_overload = None, None
            if isinstance(if_block_with_overload.body[-1], OverloadedFuncDef):
                skipped_if_stmts.extend(
                    cast(List[IfStmt], if_block_with_overload.body[:-1])
                )
                current_overload.extend(if_block_with_overload.body[-1].items)
            else:
                current_overload.append(
                    cast(Union[Decorator, FuncDef], if_block_with_overload.body[0])
                )
        else:
            if last_if_stmt is not None:
                ret.append(last_if_stmt)
                last_if_stmt_overload_name = current_overload_name
                last_if_stmt, last_if_overload = None, None
                last_if_unknown_truth_value = None

            if current_overload and current_overload_name == last_if_stmt_overload_name:
                # Remove last stmt (IfStmt) from ret if the overload names matched
                # Only happens if no executable block had been found in IfStmt
                skipped_if_stmts.append(cast(IfStmt, ret.pop()))
            if current_overload and skipped_if_stmts:
                # Add bare IfStmt (without overloads) to ret
                # Required for mypy to be able to still check conditions
                for if_stmt in skipped_if_stmts:
                    self._strip_contents_from_if_stmt(if_stmt)
                    ret.append(if_stmt)
                skipped_if_stmts = []
            if len(current_overload) == 1:
                ret.append(current_overload[0])
            elif len(current_overload) &gt; 1:
                ret.append(OverloadedFuncDef(current_overload))

            # If we have multiple decorated functions named "_" next to each, we want to treat
            # them as a series of regular FuncDefs instead of one OverloadedFuncDef because
            # most of mypy/mypyc assumes that all the functions in an OverloadedFuncDef are
            # related, but multiple underscore functions next to each other aren't necessarily
            # related
            seen_unconditional_func_def = False
            if isinstance(stmt, Decorator) and not unnamed_function(stmt.name):
                current_overload = [stmt]
                current_overload_name = stmt.name
            elif (
                isinstance(stmt, IfStmt)
                and if_overload_name is not None
            ):
                current_overload = []
                current_overload_name = if_overload_name
                last_if_stmt = stmt
                last_if_stmt_overload_name = None
                if if_block_with_overload is not None:
                    skipped_if_stmts.extend(
                        cast(List[IfStmt], if_block_with_overload.body[:-1])
                    )
                    last_if_overload = cast(
                        Union[Decorator, FuncDef, OverloadedFuncDef],
                        if_block_with_overload.body[-1]
                    )
                last_if_unknown_truth_value = if_unknown_truth_value
            else:
                current_overload = []
                current_overload_name = None
                ret.append(stmt)

    if current_overload and skipped_if_stmts:
        # Add bare IfStmt (without overloads) to ret
        # Required for mypy to be able to still check conditions
        for if_stmt in skipped_if_stmts:
            self._strip_contents_from_if_stmt(if_stmt)
            ret.append(if_stmt)
    if len(current_overload) == 1:
        ret.append(current_overload[0])
    elif len(current_overload) &gt; 1:
        ret.append(OverloadedFuncDef(current_overload))
    elif last_if_overload is not None:
        ret.append(last_if_overload)
    elif last_if_stmt is not None:
        ret.append(last_if_stmt)
    return ret

</t>
<t tx="ekr.20220525082934.580">def visit_typeddict_type(self, t: TypedDictType) -&gt; ProperType:
    if isinstance(self.s, TypedDictType):
        for (name, l, r) in self.s.zip(t):
            if (not is_equivalent(l, r) or
                    (name in t.required_keys) != (name in self.s.required_keys)):
                return self.default(self.s)
        item_list: List[Tuple[str, Type]] = []
        for (item_name, s_item_type, t_item_type) in self.s.zipall(t):
            if s_item_type is not None:
                item_list.append((item_name, s_item_type))
            else:
                # at least one of s_item_type and t_item_type is not None
                assert t_item_type is not None
                item_list.append((item_name, t_item_type))
        items = OrderedDict(item_list)
        fallback = self.s.create_anonymous_fallback()
        required_keys = t.required_keys | self.s.required_keys
        return TypedDictType(items, required_keys, fallback)
    elif isinstance(self.s, Instance) and is_subtype(t, self.s):
        return t
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20220525082934.581">def visit_literal_type(self, t: LiteralType) -&gt; ProperType:
    if isinstance(self.s, LiteralType) and self.s == t:
        return t
    elif isinstance(self.s, Instance) and is_subtype(t.fallback, self.s):
        return t
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20220525082934.582">def visit_partial_type(self, t: PartialType) -&gt; ProperType:
    # We can't determine the meet of partial types. We should never get here.
    assert False, 'Internal error'

</t>
<t tx="ekr.20220525082934.583">def visit_type_type(self, t: TypeType) -&gt; ProperType:
    if isinstance(self.s, TypeType):
        typ = self.meet(t.item, self.s.item)
        if not isinstance(typ, NoneType):
            typ = TypeType.make_normalized(typ, line=t.line)
        return typ
    elif isinstance(self.s, Instance) and self.s.type.fullname == 'builtins.type':
        return t
    elif isinstance(self.s, CallableType):
        return self.meet(t, self.s)
    else:
        return self.default(self.s)

</t>
<t tx="ekr.20220525082934.584">def visit_type_alias_type(self, t: TypeAliasType) -&gt; ProperType:
    assert False, f"This should be never called, got {t}"

</t>
<t tx="ekr.20220525082934.585">def meet(self, s: Type, t: Type) -&gt; ProperType:
    return meet_types(s, t)

</t>
<t tx="ekr.20220525082934.586">def default(self, typ: Type) -&gt; ProperType:
    if isinstance(typ, UnboundType):
        return AnyType(TypeOfAny.special_form)
    else:
        if state.strict_optional:
            return UninhabitedType()
        else:
            return NoneType()


</t>
<t tx="ekr.20220525082934.587">def meet_similar_callables(t: CallableType, s: CallableType) -&gt; CallableType:
    from mypy.join import join_types

    arg_types: List[Type] = []
    for i in range(len(t.arg_types)):
        arg_types.append(join_types(t.arg_types[i], s.arg_types[i]))
    # TODO in combine_similar_callables also applies here (names and kinds)
    # The fallback type can be either 'function' or 'type'. The result should have 'function' as
    # fallback only if both operands have it as 'function'.
    if t.fallback.type.fullname != 'builtins.function':
        fallback = t.fallback
    else:
        fallback = s.fallback
    return t.copy_modified(arg_types=arg_types,
                           ret_type=meet_types(t.ret_type, s.ret_type),
                           fallback=fallback,
                           name=None)


</t>
<t tx="ekr.20220525082934.588">def meet_type_list(types: List[Type]) -&gt; Type:
    if not types:
        # This should probably be builtins.object but that is hard to get and
        # it doesn't matter for any current users.
        return AnyType(TypeOfAny.implementation_artifact)
    met = types[0]
    for t in types[1:]:
        met = meet_types(met, t)
    return met


</t>
<t tx="ekr.20220525082934.589">def typed_dict_mapping_pair(left: Type, right: Type) -&gt; bool:
    """Is this a pair where one type is a TypedDict and another one is an instance of Mapping?

    This case requires a precise/principled consideration because there are two use cases
    that push the boundary the opposite ways: we need to avoid spurious overlaps to avoid
    false positives for overloads, but we also need to avoid spuriously non-overlapping types
    to avoid false positives with --strict-equality.
    """
    left, right = get_proper_types((left, right))
    assert not isinstance(left, TypedDictType) or not isinstance(right, TypedDictType)

    if isinstance(left, TypedDictType):
        _, other = left, right
    elif isinstance(right, TypedDictType):
        _, other = right, left
    else:
        return False
    return isinstance(other, Instance) and other.type.has_base('typing.Mapping')


</t>
<t tx="ekr.20220525082934.59">def _check_ifstmt_for_overloads(
    self, stmt: IfStmt, current_overload_name: Optional[str] = None
) -&gt; Optional[str]:
    """Check if IfStmt contains only overloads with the same name.
    Return overload_name if found, None otherwise.
    """
    # Check that block only contains a single Decorator, FuncDef, or OverloadedFuncDef.
    # Multiple overloads have already been merged as OverloadedFuncDef.
    if not (
        len(stmt.body[0].body) == 1
        and (
            isinstance(stmt.body[0].body[0], (Decorator, OverloadedFuncDef))
            or current_overload_name is not None
            and isinstance(stmt.body[0].body[0], FuncDef)
        )
        or len(stmt.body[0].body) &gt; 1
        and isinstance(stmt.body[0].body[-1], OverloadedFuncDef)
        and all(
            self._is_stripped_if_stmt(if_stmt)
            for if_stmt in stmt.body[0].body[:-1]
        )
    ):
        return None

    overload_name = cast(
        Union[Decorator, FuncDef, OverloadedFuncDef], stmt.body[0].body[-1]).name
    if stmt.else_body is None:
        return overload_name

    if isinstance(stmt.else_body, Block) and len(stmt.else_body.body) == 1:
        # For elif: else_body contains an IfStmt itself -&gt; do a recursive check.
        if (
            isinstance(stmt.else_body.body[0], (Decorator, FuncDef, OverloadedFuncDef))
            and stmt.else_body.body[0].name == overload_name
        ):
            return overload_name
        if (
            isinstance(stmt.else_body.body[0], IfStmt)
            and self._check_ifstmt_for_overloads(
                stmt.else_body.body[0], current_overload_name
            ) == overload_name
        ):
            return overload_name

    return None

</t>
<t tx="ekr.20220525082934.590">def typed_dict_mapping_overlap(left: Type, right: Type,
                               overlapping: Callable[[Type, Type], bool]) -&gt; bool:
    """Check if a TypedDict type is overlapping with a Mapping.

    The basic logic here consists of two rules:

    * A TypedDict with some required keys is overlapping with Mapping[str, &lt;some type&gt;]
      if and only if every key type is overlapping with &lt;some type&gt;. For example:

      - TypedDict(x=int, y=str) overlaps with Dict[str, Union[str, int]]
      - TypedDict(x=int, y=str) doesn't overlap with Dict[str, int]

      Note that any additional non-required keys can't change the above result.

    * A TypedDict with no required keys overlaps with Mapping[str, &lt;some type&gt;] if and
      only if at least one of key types overlaps with &lt;some type&gt;. For example:

      - TypedDict(x=str, y=str, total=False) overlaps with Dict[str, str]
      - TypedDict(x=str, y=str, total=False) doesn't overlap with Dict[str, int]
      - TypedDict(x=int, y=str, total=False) overlaps with Dict[str, str]

    As usual empty, dictionaries lie in a gray area. In general, List[str] and List[str]
    are considered non-overlapping despite empty list belongs to both. However, List[int]
    and List[&lt;nothing&gt;] are considered overlapping.

    So here we follow the same logic: a TypedDict with no required keys is considered
    non-overlapping with Mapping[str, &lt;some type&gt;], but is considered overlapping with
    Mapping[&lt;nothing&gt;, &lt;nothing&gt;]. This way we avoid false positives for overloads, and also
    avoid false positives for comparisons like SomeTypedDict == {} under --strict-equality.
    """
    left, right = get_proper_types((left, right))
    assert not isinstance(left, TypedDictType) or not isinstance(right, TypedDictType)

    if isinstance(left, TypedDictType):
        assert isinstance(right, Instance)
        typed, other = left, right
    else:
        assert isinstance(left, Instance)
        assert isinstance(right, TypedDictType)
        typed, other = right, left

    mapping = next(base for base in other.type.mro if base.fullname == 'typing.Mapping')
    other = map_instance_to_supertype(other, mapping)
    key_type, value_type = get_proper_types(other.args)

    # TODO: is there a cleaner way to get str_type here?
    fallback = typed.as_anonymous().fallback
    str_type = fallback.type.bases[0].args[0]  # typing._TypedDict inherits Mapping[str, object]

    # Special case: a TypedDict with no required keys overlaps with an empty dict.
    if isinstance(key_type, UninhabitedType) and isinstance(value_type, UninhabitedType):
        return not typed.required_keys

    if typed.required_keys:
        if not overlapping(key_type, str_type):
            return False
        return all(overlapping(typed.items[k], value_type) for k in typed.required_keys)
    else:
        if not overlapping(key_type, str_type):
            return False
        non_required = set(typed.items.keys()) - typed.required_keys
        return any(overlapping(typed.items[k], value_type) for k in non_required)
</t>
<t tx="ekr.20220525082934.591">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Utility for dumping memory usage stats.

This is tailored to mypy and knows (a little) about which list objects are
owned by particular AST nodes, etc.
"""

from collections import defaultdict
import gc
import sys
from typing import List, Dict, Iterable, Tuple, cast

from mypy.nodes import FakeInfo, Node
from mypy.types import Type
from mypy.util import get_class_descriptors


@others
</t>
<t tx="ekr.20220525082934.592">def collect_memory_stats() -&gt; Tuple[Dict[str, int],
                                    Dict[str, int]]:
    """Return stats about memory use.

    Return a tuple with these items:
      - Dict from object kind to number of instances of that kind
      - Dict from object kind to total bytes used by all instances of that kind
    """
    objs = gc.get_objects()
    find_recursive_objects(objs)

    inferred = {}
    for obj in objs:
        if type(obj) is FakeInfo:
            # Processing these would cause a crash.
            continue
        n = type(obj).__name__
        if hasattr(obj, '__dict__'):
            # Keep track of which class a particular __dict__ is associated with.
            inferred[id(obj.__dict__)] = f'{n} (__dict__)'
        if isinstance(obj, (Node, Type)):  # type: ignore
            if hasattr(obj, '__dict__'):
                for x in obj.__dict__.values():
                    if isinstance(x, list):
                        # Keep track of which node a list is associated with.
                        inferred[id(x)] = f'{n} (list)'
                    if isinstance(x, tuple):
                        # Keep track of which node a list is associated with.
                        inferred[id(x)] = f'{n} (tuple)'

            for k in get_class_descriptors(type(obj)):
                x = getattr(obj, k, None)
                if isinstance(x, list):
                    inferred[id(x)] = f'{n} (list)'
                if isinstance(x, tuple):
                    inferred[id(x)] = f'{n} (tuple)'

    freqs: Dict[str, int] = {}
    memuse: Dict[str, int] = {}
    for obj in objs:
        if id(obj) in inferred:
            name = inferred[id(obj)]
        else:
            name = type(obj).__name__
        freqs[name] = freqs.get(name, 0) + 1
        memuse[name] = memuse.get(name, 0) + sys.getsizeof(obj)

    return freqs, memuse


</t>
<t tx="ekr.20220525082934.593">def print_memory_profile(run_gc: bool = True) -&gt; None:
    if not sys.platform.startswith('win'):
        import resource
        system_memuse = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    else:
        system_memuse = -1  # TODO: Support this on Windows
    if run_gc:
        gc.collect()
    freqs, memuse = collect_memory_stats()
    print('%7s  %7s  %7s  %s' % ('Freq', 'Size(k)', 'AvgSize', 'Type'))
    print('-------------------------------------------')
    totalmem = 0
    i = 0
    for n, mem in sorted(memuse.items(), key=lambda x: -x[1]):
        f = freqs[n]
        if i &lt; 50:
            print('%7d  %7d  %7.0f  %s' % (f, mem // 1024, mem / f, n))
        i += 1
        totalmem += mem
    print()
    print('Mem usage RSS   ', system_memuse // 1024)
    print('Total reachable ', totalmem // 1024)


</t>
<t tx="ekr.20220525082934.594">def find_recursive_objects(objs: List[object]) -&gt; None:
    """Find additional objects referenced by objs and append them to objs.

    We use this since gc.get_objects() does not return objects without pointers
    in them such as strings.
    """
    seen = {id(o) for o in objs}

    @others
    for obj in objs[:]:
        if type(obj) is FakeInfo:
            # Processing these would cause a crash.
            continue
        if type(obj) in (dict, defaultdict):
            for key, val in cast(Dict[object, object], obj).items():
                visit(key)
                visit(val)
        if type(obj) in (list, tuple, set):
            for x in cast(Iterable[object], obj):
                visit(x)
        if hasattr(obj, '__slots__'):
            for base in type.mro(type(obj)):
                for slot in getattr(base, '__slots__', ()):
                    if hasattr(obj, slot):
                        visit(getattr(obj, slot))
</t>
<t tx="ekr.20220525082934.595">def visit(o: object) -&gt; None:
    if id(o) not in seen:
        objs.append(o)
        seen.add(id(o))

</t>
<t tx="ekr.20220525082934.596">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Facilities for generating error messages during type checking.

Don't add any non-trivial message construction logic to the type
checker, as it can compromise clarity and make messages less
consistent. Add such logic to this module instead. Literal messages, including those
with format args, should be defined as constants in mypy.message_registry.

Historically we tried to avoid all message string literals in the type
checker but we are moving away from this convention.
"""
from contextlib import contextmanager

from mypy.backports import OrderedDict
import re
import difflib
from textwrap import dedent

from typing import (
    cast, List, Dict, Any, Sequence, Iterable, Iterator, Tuple, Set, Optional, Union, Callable
)
from typing_extensions import Final

from mypy.erasetype import erase_type
from mypy.errors import Errors, ErrorWatcher, ErrorInfo
from mypy.types import (
    Type, CallableType, Instance, TypeVarType, TupleType, TypedDictType, LiteralType,
    UnionType, NoneType, AnyType, Overloaded, FunctionLike, DeletedType, TypeType,
    UninhabitedType, TypeOfAny, UnboundType, PartialType, get_proper_type, ProperType,
    ParamSpecType, Parameters, get_proper_types
)
from mypy.typetraverser import TypeTraverserVisitor
from mypy.nodes import (
    TypeInfo, Context, MypyFile, FuncDef, reverse_builtin_aliases,
    ArgKind, ARG_POS, ARG_OPT, ARG_NAMED, ARG_NAMED_OPT, ARG_STAR, ARG_STAR2,
    ReturnStmt, NameExpr, Var, CONTRAVARIANT, COVARIANT, SymbolNode,
    CallExpr, IndexExpr, StrExpr, SymbolTable, SYMBOL_FUNCBASE_TYPES
)
from mypy.operators import op_methods, op_methods_to_symbols
from mypy.subtypes import (
    is_subtype, find_member, get_member_flags,
    IS_SETTABLE, IS_CLASSVAR, IS_CLASS_OR_STATIC,
)
from mypy.sametypes import is_same_type
from mypy.typeops import separate_union_literals
from mypy.util import unmangle, plural_s
from mypy.errorcodes import ErrorCode
from mypy import message_registry, errorcodes as codes

TYPES_FOR_UNIMPORTED_HINTS: Final = {
    'typing.Any',
    'typing.Callable',
    'typing.Dict',
    'typing.Iterable',
    'typing.Iterator',
    'typing.List',
    'typing.Optional',
    'typing.Set',
    'typing.Tuple',
    'typing.TypeVar',
    'typing.Union',
    'typing.cast',
}


ARG_CONSTRUCTOR_NAMES: Final = {
    ARG_POS: "Arg",
    ARG_OPT: "DefaultArg",
    ARG_NAMED: "NamedArg",
    ARG_NAMED_OPT: "DefaultNamedArg",
    ARG_STAR: "VarArg",
    ARG_STAR2: "KwArg",
}


# Map from the full name of a missing definition to the test fixture (under
# test-data/unit/fixtures/) that provides the definition. This is used for
# generating better error messages when running mypy tests only.
SUGGESTED_TEST_FIXTURES: Final = {
    'builtins.list': 'list.pyi',
    'builtins.dict': 'dict.pyi',
    'builtins.set': 'set.pyi',
    'builtins.tuple': 'tuple.pyi',
    'builtins.bool': 'bool.pyi',
    'builtins.Exception': 'exception.pyi',
    'builtins.BaseException': 'exception.pyi',
    'builtins.isinstance': 'isinstancelist.pyi',
    'builtins.property': 'property.pyi',
    'builtins.classmethod': 'classmethod.pyi',
}


@others
</t>
<t tx="ekr.20220525082934.597">class MessageBuilder:
    """Helper class for reporting type checker error messages with parameters.

    The methods of this class need to be provided with the context within a
    file; the errors member manages the wider context.

    IDEA: Support a 'verbose mode' that includes full information about types
          in error messages and that may otherwise produce more detailed error
          messages.
    """

    # Report errors using this instance. It knows about the current file and
    # import context.
    errors: Errors

    modules: Dict[str, MypyFile]

    # Hack to deduplicate error messages from union types
    _disable_type_names: List[bool]

    @others
</t>
<t tx="ekr.20220525082934.598">def __init__(self, errors: Errors, modules: Dict[str, MypyFile]) -&gt; None:
    self.errors = errors
    self.modules = modules
    self._disable_type_names = []

</t>
<t tx="ekr.20220525082934.599">#
# Helpers
#

</t>
<t tx="ekr.20220525082934.6">def __init__(self, variables: Mapping[TypeVarId, Type]) -&gt; None:
    self.variables = variables

</t>
<t tx="ekr.20220525082934.60">def _get_executable_if_block_with_overloads(
    self, stmt: IfStmt
) -&gt; Tuple[Optional[Block], Optional[IfStmt]]:
    """Return block from IfStmt that will get executed.

    Return
        0 -&gt; A block if sure that alternative blocks are unreachable.
        1 -&gt; An IfStmt if the reachability of it can't be inferred,
             i.e. the truth value is unknown.
    """
    infer_reachability_of_if_statement(stmt, self.options)
    if (
        stmt.else_body is None
        and stmt.body[0].is_unreachable is True
    ):
        # always False condition with no else
        return None, None
    if (
        stmt.else_body is None
        or stmt.body[0].is_unreachable is False
        and stmt.else_body.is_unreachable is False
    ):
        # The truth value is unknown, thus not conclusive
        return None, stmt
    if stmt.else_body.is_unreachable is True:
        # else_body will be set unreachable if condition is always True
        return stmt.body[0], None
    if stmt.body[0].is_unreachable is True:
        # body will be set unreachable if condition is always False
        # else_body can contain an IfStmt itself (for elif) -&gt; do a recursive check
        if isinstance(stmt.else_body.body[0], IfStmt):
            return self._get_executable_if_block_with_overloads(stmt.else_body.body[0])
        return stmt.else_body, None
    return None, stmt

</t>
<t tx="ekr.20220525082934.600">def filter_errors(self, *, filter_errors: bool = True,
                  save_filtered_errors: bool = False) -&gt; ErrorWatcher:
    return ErrorWatcher(self.errors, filter_errors=filter_errors,
                        save_filtered_errors=save_filtered_errors)

</t>
<t tx="ekr.20220525082934.601">def add_errors(self, errors: List[ErrorInfo]) -&gt; None:
    """Add errors in messages to this builder."""
    for info in errors:
        self.errors.add_error_info(info)

</t>
<t tx="ekr.20220525082934.602">@contextmanager
def disable_type_names(self) -&gt; Iterator[None]:
    self._disable_type_names.append(True)
    try:
        yield
    finally:
        self._disable_type_names.pop()

</t>
<t tx="ekr.20220525082934.603">def are_type_names_disabled(self) -&gt; bool:
    return len(self._disable_type_names) &gt; 0 and self._disable_type_names[-1]

</t>
<t tx="ekr.20220525082934.604">def report(self,
           msg: str,
           context: Optional[Context],
           severity: str,
           *,
           code: Optional[ErrorCode] = None,
           file: Optional[str] = None,
           origin: Optional[Context] = None,
           offset: int = 0,
           allow_dups: bool = False) -&gt; None:
    """Report an error or note (unless disabled)."""
    if origin is not None:
        end_line = origin.end_line
    elif context is not None:
        end_line = context.end_line
    else:
        end_line = None
    self.errors.report(context.get_line() if context else -1,
                       context.get_column() if context else -1,
                       msg, severity=severity, file=file, offset=offset,
                       origin_line=origin.get_line() if origin else None,
                       end_line=end_line, code=code, allow_dups=allow_dups)

</t>
<t tx="ekr.20220525082934.605">def fail(self,
         msg: str,
         context: Optional[Context],
         *,
         code: Optional[ErrorCode] = None,
         file: Optional[str] = None,
         origin: Optional[Context] = None,
         allow_dups: bool = False) -&gt; None:
    """Report an error message (unless disabled)."""
    self.report(msg, context, 'error', code=code, file=file,
                origin=origin, allow_dups=allow_dups)

</t>
<t tx="ekr.20220525082934.606">def note(self,
         msg: str,
         context: Context,
         file: Optional[str] = None,
         origin: Optional[Context] = None,
         offset: int = 0,
         allow_dups: bool = False,
         *,
         code: Optional[ErrorCode] = None) -&gt; None:
    """Report a note (unless disabled)."""
    self.report(msg, context, 'note', file=file, origin=origin,
                offset=offset, allow_dups=allow_dups, code=code)

</t>
<t tx="ekr.20220525082934.607">def note_multiline(self, messages: str, context: Context, file: Optional[str] = None,
                   origin: Optional[Context] = None, offset: int = 0,
                   allow_dups: bool = False,
                   code: Optional[ErrorCode] = None) -&gt; None:
    """Report as many notes as lines in the message (unless disabled)."""
    for msg in messages.splitlines():
        self.report(msg, context, 'note', file=file, origin=origin,
                    offset=offset, allow_dups=allow_dups, code=code)

</t>
<t tx="ekr.20220525082934.608">#
# Specific operations
#

# The following operations are for generating specific error messages. They
# get some information as arguments, and they build an error message based
# on them.

</t>
<t tx="ekr.20220525082934.609">def has_no_attr(self,
                original_type: Type,
                typ: Type,
                member: str,
                context: Context,
                module_symbol_table: Optional[SymbolTable] = None) -&gt; Type:
    """Report a missing or non-accessible member.

    original_type is the top-level type on which the error occurred.
    typ is the actual type that is missing the member. These can be
    different, e.g., in a union, original_type will be the union and typ
    will be the specific item in the union that does not have the member
    attribute.

    'module_symbol_table' is passed to this function if the type for which we
    are trying to get a member was originally a module. The SymbolTable allows
    us to look up and suggests attributes of the module since they are not
    directly available on original_type

    If member corresponds to an operator, use the corresponding operator
    name in the messages. Return type Any.
    """
    original_type = get_proper_type(original_type)
    typ = get_proper_type(typ)

    if (isinstance(original_type, Instance) and
            original_type.type.has_readable_member(member)):
        self.fail(f'Member "{member}" is not assignable', context)
    elif member == '__contains__':
        self.fail('Unsupported right operand type for in ({})'.format(
            format_type(original_type)), context, code=codes.OPERATOR)
    elif member in op_methods.values():
        # Access to a binary operator member (e.g. _add). This case does
        # not handle indexing operations.
        for op, method in op_methods.items():
            if method == member:
                self.unsupported_left_operand(op, original_type, context)
                break
    elif member == '__neg__':
        self.fail('Unsupported operand type for unary - ({})'.format(
            format_type(original_type)), context, code=codes.OPERATOR)
    elif member == '__pos__':
        self.fail('Unsupported operand type for unary + ({})'.format(
            format_type(original_type)), context, code=codes.OPERATOR)
    elif member == '__invert__':
        self.fail('Unsupported operand type for ~ ({})'.format(
            format_type(original_type)), context, code=codes.OPERATOR)
    elif member == '__getitem__':
        # Indexed get.
        # TODO: Fix this consistently in format_type
        if isinstance(original_type, CallableType) and original_type.is_type_obj():
            self.fail('The type {} is not generic and not indexable'.format(
                format_type(original_type)), context)
        else:
            self.fail('Value of type {} is not indexable'.format(
                format_type(original_type)), context, code=codes.INDEX)
    elif member == '__setitem__':
        # Indexed set.
        self.fail('Unsupported target for indexed assignment ({})'.format(
            format_type(original_type)), context, code=codes.INDEX)
    elif member == '__call__':
        if isinstance(original_type, Instance) and \
                (original_type.type.fullname == 'builtins.function'):
            # "'function' not callable" is a confusing error message.
            # Explain that the problem is that the type of the function is not known.
            self.fail('Cannot call function of unknown type', context, code=codes.OPERATOR)
        else:
            self.fail(message_registry.NOT_CALLABLE.format(
                format_type(original_type)), context, code=codes.OPERATOR)
    else:
        # The non-special case: a missing ordinary attribute.
        extra = ''
        if member == '__iter__':
            extra = ' (not iterable)'
        elif member == '__aiter__':
            extra = ' (not async iterable)'
        if not self.are_type_names_disabled():
            failed = False
            if isinstance(original_type, Instance) and original_type.type.names:
                alternatives = set(original_type.type.names.keys())

                if module_symbol_table is not None:
                    alternatives |= {key for key in module_symbol_table.keys()}

                # in some situations, the member is in the alternatives set
                # but since we're in this function, we shouldn't suggest it
                if member in alternatives:
                    alternatives.remove(member)

                matches = [m for m in COMMON_MISTAKES.get(member, []) if m in alternatives]
                matches.extend(best_matches(member, alternatives)[:3])
                if member == '__aiter__' and matches == ['__iter__']:
                    matches = []  # Avoid misleading suggestion
                if member == '__div__' and matches == ['__truediv__']:
                    # TODO: Handle differences in division between Python 2 and 3 more cleanly
                    matches = []
                if matches:
                    self.fail(
                        '{} has no attribute "{}"; maybe {}?{}'.format(
                            format_type(original_type),
                            member,
                            pretty_seq(matches, "or"),
                            extra,
                        ),
                        context,
                        code=codes.ATTR_DEFINED)
                    failed = True
            if not failed:
                self.fail(
                    '{} has no attribute "{}"{}'.format(
                        format_type(original_type), member, extra),
                    context,
                    code=codes.ATTR_DEFINED)
        elif isinstance(original_type, UnionType):
            # The checker passes "object" in lieu of "None" for attribute
            # checks, so we manually convert it back.
            typ_format, orig_type_format = format_type_distinctly(typ, original_type)
            if typ_format == '"object"' and \
                    any(type(item) == NoneType for item in original_type.items):
                typ_format = '"None"'
            self.fail('Item {} of {} has no attribute "{}"{}'.format(
                typ_format, orig_type_format, member, extra), context,
                code=codes.UNION_ATTR)
        elif isinstance(original_type, TypeVarType):
            bound = get_proper_type(original_type.upper_bound)
            if isinstance(bound, UnionType):
                typ_fmt, bound_fmt = format_type_distinctly(typ, bound)
                original_type_fmt = format_type(original_type)
                self.fail(
                    'Item {} of the upper bound {} of type variable {} has no '
                    'attribute "{}"{}'.format(
                        typ_fmt, bound_fmt, original_type_fmt, member, extra),
                    context, code=codes.UNION_ATTR)
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20220525082934.61">def _strip_contents_from_if_stmt(self, stmt: IfStmt) -&gt; None:
    """Remove contents from IfStmt.

    Needed to still be able to check the conditions after the contents
    have been merged with the surrounding function overloads.
    """
    if len(stmt.body) == 1:
        stmt.body[0].body = []
    if stmt.else_body and len(stmt.else_body.body) == 1:
        if isinstance(stmt.else_body.body[0], IfStmt):
            self._strip_contents_from_if_stmt(stmt.else_body.body[0])
        else:
            stmt.else_body.body = []

</t>
<t tx="ekr.20220525082934.610">def unsupported_operand_types(self,
                              op: str,
                              left_type: Any,
                              right_type: Any,
                              context: Context,
                              *,
                              code: ErrorCode = codes.OPERATOR) -&gt; None:
    """Report unsupported operand types for a binary operation.

    Types can be Type objects or strings.
    """
    left_str = ''
    if isinstance(left_type, str):
        left_str = left_type
    else:
        left_str = format_type(left_type)

    right_str = ''
    if isinstance(right_type, str):
        right_str = right_type
    else:
        right_str = format_type(right_type)

    if self.are_type_names_disabled():
        msg = f'Unsupported operand types for {op} (likely involving Union)'
    else:
        msg = f'Unsupported operand types for {op} ({left_str} and {right_str})'
    self.fail(msg, context, code=code)

</t>
<t tx="ekr.20220525082934.611">def unsupported_left_operand(self, op: str, typ: Type,
                             context: Context) -&gt; None:
    if self.are_type_names_disabled():
        msg = f'Unsupported left operand type for {op} (some union)'
    else:
        msg = f'Unsupported left operand type for {op} ({format_type(typ)})'
    self.fail(msg, context, code=codes.OPERATOR)

</t>
<t tx="ekr.20220525082934.612">def not_callable(self, typ: Type, context: Context) -&gt; Type:
    self.fail(message_registry.NOT_CALLABLE.format(format_type(typ)), context)
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20220525082934.613">def untyped_function_call(self, callee: CallableType, context: Context) -&gt; Type:
    name = callable_name(callee) or '(unknown)'
    self.fail(f'Call to untyped function {name} in typed context', context,
              code=codes.NO_UNTYPED_CALL)
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20220525082934.614">def incompatible_argument(self,
                          n: int,
                          m: int,
                          callee: CallableType,
                          arg_type: Type,
                          arg_kind: ArgKind,
                          object_type: Optional[Type],
                          context: Context,
                          outer_context: Context) -&gt; Optional[ErrorCode]:
    """Report an error about an incompatible argument type.

    The argument type is arg_type, argument number is n and the
    callee type is 'callee'. If the callee represents a method
    that corresponds to an operator, use the corresponding
    operator name in the messages.

    Return the error code that used for the argument (multiple error
    codes are possible).
    """
    arg_type = get_proper_type(arg_type)

    target = ''
    callee_name = callable_name(callee)
    if callee_name is not None:
        name = callee_name
        if callee.bound_args and callee.bound_args[0] is not None:
            base = format_type(callee.bound_args[0])
        else:
            base = extract_type(name)

        for method, op in op_methods_to_symbols.items():
            for variant in method, '__r' + method[2:]:
                # FIX: do not rely on textual formatting
                if name.startswith(f'"{variant}" of'):
                    if op == 'in' or variant != method:
                        # Reversed order of base/argument.
                        self.unsupported_operand_types(op, arg_type, base,
                                                       context, code=codes.OPERATOR)
                    else:
                        self.unsupported_operand_types(op, base, arg_type,
                                                       context, code=codes.OPERATOR)
                    return codes.OPERATOR

        if name.startswith('"__cmp__" of'):
            self.unsupported_operand_types("comparison", arg_type, base,
                                           context, code=codes.OPERATOR)
            return codes.INDEX

        if name.startswith('"__getitem__" of'):
            self.invalid_index_type(arg_type, callee.arg_types[n - 1], base, context,
                                    code=codes.INDEX)
            return codes.INDEX

        if name.startswith('"__setitem__" of'):
            if n == 1:
                self.invalid_index_type(arg_type, callee.arg_types[n - 1], base, context,
                                        code=codes.INDEX)
                return codes.INDEX
            else:
                msg = '{} (expression has type {}, target has type {})'
                arg_type_str, callee_type_str = format_type_distinctly(arg_type,
                                                                       callee.arg_types[n - 1])
                self.fail(msg.format(message_registry.INCOMPATIBLE_TYPES_IN_ASSIGNMENT,
                                     arg_type_str, callee_type_str),
                          context, code=codes.ASSIGNMENT)
                return codes.ASSIGNMENT

        target = f'to {name} '

    msg = ''
    code = codes.MISC
    notes: List[str] = []
    if callee_name == '&lt;list&gt;':
        name = callee_name[1:-1]
        n -= 1
        actual_type_str, expected_type_str = format_type_distinctly(arg_type,
                                                                    callee.arg_types[0])
        msg = '{} item {} has incompatible type {}; expected {}'.format(
            name.title(), n, actual_type_str, expected_type_str)
        code = codes.LIST_ITEM
    elif callee_name == '&lt;dict&gt;':
        name = callee_name[1:-1]
        n -= 1
        key_type, value_type = cast(TupleType, arg_type).items
        expected_key_type, expected_value_type = cast(TupleType, callee.arg_types[0]).items

        # don't increase verbosity unless there is need to do so
        if is_subtype(key_type, expected_key_type):
            key_type_str = format_type(key_type)
            expected_key_type_str = format_type(expected_key_type)
        else:
            key_type_str, expected_key_type_str = format_type_distinctly(
                key_type, expected_key_type)
        if is_subtype(value_type, expected_value_type):
            value_type_str = format_type(value_type)
            expected_value_type_str = format_type(expected_value_type)
        else:
            value_type_str, expected_value_type_str = format_type_distinctly(
                value_type, expected_value_type)

        msg = '{} entry {} has incompatible type {}: {}; expected {}: {}'.format(
            name.title(), n, key_type_str, value_type_str,
            expected_key_type_str, expected_value_type_str)
        code = codes.DICT_ITEM
    elif callee_name == '&lt;list-comprehension&gt;':
        actual_type_str, expected_type_str = map(strip_quotes,
                                                 format_type_distinctly(arg_type,
                                                                        callee.arg_types[0]))
        msg = 'List comprehension has incompatible type List[{}]; expected List[{}]'.format(
            actual_type_str, expected_type_str)
    elif callee_name == '&lt;set-comprehension&gt;':
        actual_type_str, expected_type_str = map(strip_quotes,
                                                 format_type_distinctly(arg_type,
                                                                        callee.arg_types[0]))
        msg = 'Set comprehension has incompatible type Set[{}]; expected Set[{}]'.format(
            actual_type_str, expected_type_str)
    elif callee_name == '&lt;dictionary-comprehension&gt;':
        actual_type_str, expected_type_str = format_type_distinctly(arg_type,
                                                                    callee.arg_types[n - 1])
        msg = ('{} expression in dictionary comprehension has incompatible type {}; '
               'expected type {}').format(
            'Key' if n == 1 else 'Value',
            actual_type_str,
            expected_type_str)
    elif callee_name == '&lt;generator&gt;':
        actual_type_str, expected_type_str = format_type_distinctly(arg_type,
                                                                    callee.arg_types[0])
        msg = 'Generator has incompatible item type {}; expected {}'.format(
            actual_type_str, expected_type_str)
    else:
        try:
            expected_type = callee.arg_types[m - 1]
        except IndexError:  # Varargs callees
            expected_type = callee.arg_types[-1]
        arg_type_str, expected_type_str = format_type_distinctly(
            arg_type, expected_type, bare=True)
        if arg_kind == ARG_STAR:
            arg_type_str = '*' + arg_type_str
        elif arg_kind == ARG_STAR2:
            arg_type_str = '**' + arg_type_str

        # For function calls with keyword arguments, display the argument name rather than the
        # number.
        arg_label = str(n)
        if isinstance(outer_context, CallExpr) and len(outer_context.arg_names) &gt;= n:
            arg_name = outer_context.arg_names[n - 1]
            if arg_name is not None:
                arg_label = f'"{arg_name}"'
        if (arg_kind == ARG_STAR2
                and isinstance(arg_type, TypedDictType)
                and m &lt;= len(callee.arg_names)
                and callee.arg_names[m - 1] is not None
                and callee.arg_kinds[m - 1] != ARG_STAR2):
            arg_name = callee.arg_names[m - 1]
            assert arg_name is not None
            arg_type_str, expected_type_str = format_type_distinctly(
                arg_type.items[arg_name],
                expected_type,
                bare=True)
            arg_label = f'"{arg_name}"'
        if isinstance(outer_context, IndexExpr) and isinstance(outer_context.index, StrExpr):
            msg = 'Value of "{}" has incompatible type {}; expected {}' .format(
                outer_context.index.value, quote_type_string(arg_type_str),
                quote_type_string(expected_type_str))
        else:
            msg = 'Argument {} {}has incompatible type {}; expected {}'.format(
                arg_label, target, quote_type_string(arg_type_str),
                quote_type_string(expected_type_str))
        object_type = get_proper_type(object_type)
        if isinstance(object_type, TypedDictType):
            code = codes.TYPEDDICT_ITEM
        else:
            code = codes.ARG_TYPE
        expected_type = get_proper_type(expected_type)
        if isinstance(expected_type, UnionType):
            expected_types = list(expected_type.items)
        else:
            expected_types = [expected_type]
        for type in get_proper_types(expected_types):
            if isinstance(arg_type, Instance) and isinstance(type, Instance):
                notes = append_invariance_notes(notes, arg_type, type)
    self.fail(msg, context, code=code)
    if notes:
        for note_msg in notes:
            self.note(note_msg, context, code=code)
    return code

</t>
<t tx="ekr.20220525082934.615">def incompatible_argument_note(self,
                               original_caller_type: ProperType,
                               callee_type: ProperType,
                               context: Context,
                               code: Optional[ErrorCode]) -&gt; None:
    if isinstance(original_caller_type, (Instance, TupleType, TypedDictType)):
        if isinstance(callee_type, Instance) and callee_type.type.is_protocol:
            self.report_protocol_problems(original_caller_type, callee_type,
                                          context, code=code)
        if isinstance(callee_type, UnionType):
            for item in callee_type.items:
                item = get_proper_type(item)
                if isinstance(item, Instance) and item.type.is_protocol:
                    self.report_protocol_problems(original_caller_type, item,
                                                  context, code=code)
    if (isinstance(callee_type, CallableType) and
            isinstance(original_caller_type, Instance)):
        call = find_member('__call__', original_caller_type, original_caller_type,
                           is_operator=True)
        if call:
            self.note_call(original_caller_type, call, context, code=code)

    self.maybe_note_concatenate_pos_args(original_caller_type, callee_type, context, code)

</t>
<t tx="ekr.20220525082934.616">def maybe_note_concatenate_pos_args(self,
                                    original_caller_type: ProperType,
                                    callee_type: ProperType,
                                    context: Context,
                                    code: Optional[ErrorCode] = None) -&gt; None:
    # pos-only vs positional can be confusing, with Concatenate
    if (isinstance(callee_type, CallableType) and
            isinstance(original_caller_type, CallableType) and
            (original_caller_type.from_concatenate or callee_type.from_concatenate)):
        names: List[str] = []
        for c, o in zip(
                          callee_type.formal_arguments(),
                          original_caller_type.formal_arguments()):
            if None in (c.pos, o.pos):
                # non-positional
                continue
            if c.name != o.name and c.name is None and o.name is not None:
                names.append(o.name)

        if names:
            missing_arguments = '"' + '", "'.join(names) + '"'
            self.note(f'This may be because "{original_caller_type.name}" has arguments '
                      f'named: {missing_arguments}', context, code=code)

</t>
<t tx="ekr.20220525082934.617">def invalid_index_type(self, index_type: Type, expected_type: Type, base_str: str,
                       context: Context, *, code: ErrorCode) -&gt; None:
    index_str, expected_str = format_type_distinctly(index_type, expected_type)
    self.fail('Invalid index type {} for {}; expected type {}'.format(
        index_str, base_str, expected_str), context, code=code)

</t>
<t tx="ekr.20220525082934.618">def too_few_arguments(self, callee: CallableType, context: Context,
                      argument_names: Optional[Sequence[Optional[str]]]) -&gt; None:
    if argument_names is not None:
        num_positional_args = sum(k is None for k in argument_names)
        arguments_left = callee.arg_names[num_positional_args:callee.min_args]
        diff = [k for k in arguments_left if k not in argument_names]
        if len(diff) == 1:
            msg = 'Missing positional argument'
        else:
            msg = 'Missing positional arguments'
        callee_name = callable_name(callee)
        if callee_name is not None and diff and all(d is not None for d in diff):
            args = '", "'.join(cast(List[str], diff))
            msg += f' "{args}" in call to {callee_name}'
        else:
            msg = 'Too few arguments' + for_function(callee)

    else:
        msg = 'Too few arguments' + for_function(callee)
    self.fail(msg, context, code=codes.CALL_ARG)

</t>
<t tx="ekr.20220525082934.619">def missing_named_argument(self, callee: CallableType, context: Context, name: str) -&gt; None:
    msg = f'Missing named argument "{name}"' + for_function(callee)
    self.fail(msg, context, code=codes.CALL_ARG)

</t>
<t tx="ekr.20220525082934.62">def _is_stripped_if_stmt(self, stmt: Statement) -&gt; bool:
    """Check stmt to make sure it is a stripped IfStmt.

    See also: _strip_contents_from_if_stmt
    """
    if not isinstance(stmt, IfStmt):
        return False

    if not (len(stmt.body) == 1 and len(stmt.body[0].body) == 0):
        # Body not empty
        return False

    if not stmt.else_body or len(stmt.else_body.body) == 0:
        # No or empty else_body
        return True

    # For elif, IfStmt are stored recursively in else_body
    return self._is_stripped_if_stmt(stmt.else_body.body[0])

</t>
<t tx="ekr.20220525082934.620">def too_many_arguments(self, callee: CallableType, context: Context) -&gt; None:
    msg = 'Too many arguments' + for_function(callee)
    self.fail(msg, context, code=codes.CALL_ARG)
    self.maybe_note_about_special_args(callee, context)

</t>
<t tx="ekr.20220525082934.621">def too_many_arguments_from_typed_dict(self,
                                       callee: CallableType,
                                       arg_type: TypedDictType,
                                       context: Context) -&gt; None:
    # Try to determine the name of the extra argument.
    for key in arg_type.items:
        if key not in callee.arg_names:
            msg = f'Extra argument "{key}" from **args' + for_function(callee)
            break
    else:
        self.too_many_arguments(callee, context)
        return
    self.fail(msg, context)

</t>
<t tx="ekr.20220525082934.622">def too_many_positional_arguments(self, callee: CallableType,
                                  context: Context) -&gt; None:
    msg = 'Too many positional arguments' + for_function(callee)
    self.fail(msg, context)
    self.maybe_note_about_special_args(callee, context)

</t>
<t tx="ekr.20220525082934.623">def maybe_note_about_special_args(self, callee: CallableType, context: Context) -&gt; None:
    # https://github.com/python/mypy/issues/11309
    first_arg = callee.def_extras.get('first_arg')
    if first_arg and first_arg not in {'self', 'cls', 'mcs'}:
        self.note(
            'Looks like the first special argument in a method '
            'is not named "self", "cls", or "mcs", '
            'maybe it is missing?',
            context,
        )

</t>
<t tx="ekr.20220525082934.624">def unexpected_keyword_argument(self, callee: CallableType, name: str, arg_type: Type,
                                context: Context) -&gt; None:
    msg = f'Unexpected keyword argument "{name}"' + for_function(callee)
    # Suggest intended keyword, look for type match else fallback on any match.
    matching_type_args = []
    not_matching_type_args = []
    for i, kwarg_type in enumerate(callee.arg_types):
        callee_arg_name = callee.arg_names[i]
        if callee_arg_name is not None and callee.arg_kinds[i] != ARG_STAR:
            if is_subtype(arg_type, kwarg_type):
                matching_type_args.append(callee_arg_name)
            else:
                not_matching_type_args.append(callee_arg_name)
    matches = best_matches(name, matching_type_args)
    if not matches:
        matches = best_matches(name, not_matching_type_args)
    if matches:
        msg += f"; did you mean {pretty_seq(matches[:3], 'or')}?"
    self.fail(msg, context, code=codes.CALL_ARG)
    module = find_defining_module(self.modules, callee)
    if module:
        assert callee.definition is not None
        fname = callable_name(callee)
        if not fname:  # an alias to function with a different name
            fname = 'Called function'
        self.note(f'{fname} defined here', callee.definition,
                  file=module.path, origin=context, code=codes.CALL_ARG)

</t>
<t tx="ekr.20220525082934.625">def duplicate_argument_value(self, callee: CallableType, index: int,
                             context: Context) -&gt; None:
    self.fail('{} gets multiple values for keyword argument "{}"'.
              format(callable_name(callee) or 'Function', callee.arg_names[index]),
              context)

</t>
<t tx="ekr.20220525082934.626">def does_not_return_value(self, callee_type: Optional[Type], context: Context) -&gt; None:
    """Report an error about use of an unusable type."""
    name: Optional[str] = None
    callee_type = get_proper_type(callee_type)
    if isinstance(callee_type, FunctionLike):
        name = callable_name(callee_type)
    if name is not None:
        self.fail(f'{capitalize(name)} does not return a value', context,
                  code=codes.FUNC_RETURNS_VALUE)
    else:
        self.fail('Function does not return a value', context, code=codes.FUNC_RETURNS_VALUE)

</t>
<t tx="ekr.20220525082934.627">def underscore_function_call(self, context: Context) -&gt; None:
    self.fail('Calling function named "_" is not allowed', context)

</t>
<t tx="ekr.20220525082934.628">def deleted_as_rvalue(self, typ: DeletedType, context: Context) -&gt; None:
    """Report an error about using an deleted type as an rvalue."""
    if typ.source is None:
        s = ""
    else:
        s = f' "{typ.source}"'
    self.fail(f'Trying to read deleted variable{s}', context)

</t>
<t tx="ekr.20220525082934.629">def deleted_as_lvalue(self, typ: DeletedType, context: Context) -&gt; None:
    """Report an error about using an deleted type as an lvalue.

    Currently, this only occurs when trying to assign to an
    exception variable outside the local except: blocks.
    """
    if typ.source is None:
        s = ""
    else:
        s = f' "{typ.source}"'
    self.fail(f'Assignment to variable{s} outside except: block', context)

</t>
<t tx="ekr.20220525082934.63">def in_method_scope(self) -&gt; bool:
    return self.class_and_function_stack[-2:] == ['C', 'F']

</t>
<t tx="ekr.20220525082934.630">def no_variant_matches_arguments(self,
                                 overload: Overloaded,
                                 arg_types: List[Type],
                                 context: Context,
                                 *,
                                 code: Optional[ErrorCode] = None) -&gt; None:
    code = code or codes.CALL_OVERLOAD
    name = callable_name(overload)
    if name:
        name_str = f' of {name}'
    else:
        name_str = ''
    arg_types_str = ', '.join(format_type(arg) for arg in arg_types)
    num_args = len(arg_types)
    if num_args == 0:
        self.fail(f'All overload variants{name_str} require at least one argument',
                  context, code=code)
    elif num_args == 1:
        self.fail('No overload variant{} matches argument type {}'
                  .format(name_str, arg_types_str), context, code=code)
    else:
        self.fail('No overload variant{} matches argument types {}'
                  .format(name_str, arg_types_str), context, code=code)

    self.note(
        f'Possible overload variant{plural_s(len(overload.items))}:',
        context, code=code)
    for item in overload.items:
        self.note(pretty_callable(item), context, offset=4, code=code)

</t>
<t tx="ekr.20220525082934.631">def wrong_number_values_to_unpack(self, provided: int, expected: int,
                                  context: Context) -&gt; None:
    if provided &lt; expected:
        if provided == 1:
            self.fail(f'Need more than 1 value to unpack ({expected} expected)',
                      context)
        else:
            self.fail('Need more than {} values to unpack ({} expected)'.format(
                provided, expected), context)
    elif provided &gt; expected:
        self.fail('Too many values to unpack ({} expected, {} provided)'.format(
            expected, provided), context)

</t>
<t tx="ekr.20220525082934.632">def unpacking_strings_disallowed(self, context: Context) -&gt; None:
    self.fail("Unpacking a string is disallowed", context)

</t>
<t tx="ekr.20220525082934.633">def type_not_iterable(self, type: Type, context: Context) -&gt; None:
    self.fail(f'{format_type(type)} object is not iterable', context)

</t>
<t tx="ekr.20220525082934.634">def incompatible_operator_assignment(self, op: str,
                                     context: Context) -&gt; None:
    self.fail(f'Result type of {op} incompatible in assignment',
              context)

</t>
<t tx="ekr.20220525082934.635">def overload_signature_incompatible_with_supertype(
        self, name: str, name_in_super: str, supertype: str,
        context: Context) -&gt; None:
    target = self.override_target(name, name_in_super, supertype)
    self.fail('Signature of "{}" incompatible with {}'.format(
        name, target), context, code=codes.OVERRIDE)

    note_template = 'Overload variants must be defined in the same order as they are in "{}"'
    self.note(note_template.format(supertype), context, code=codes.OVERRIDE)

</t>
<t tx="ekr.20220525082934.636">def signature_incompatible_with_supertype(
        self, name: str, name_in_super: str, supertype: str, context: Context,
        original: Optional[FunctionLike] = None,
        override: Optional[FunctionLike] = None) -&gt; None:
    code = codes.OVERRIDE
    target = self.override_target(name, name_in_super, supertype)
    self.fail('Signature of "{}" incompatible with {}'.format(
        name, target), context, code=code)

    INCLUDE_DECORATOR = True  # Include @classmethod and @staticmethod decorators, if any
    ALLOW_DUPS = True  # Allow duplicate notes, needed when signatures are duplicates
    ALIGN_OFFSET = 1  # One space, to account for the difference between error and note
    OFFSET = 4  # Four spaces, so that notes will look like this:
    # error: Signature of "f" incompatible with supertype "A"
    # note:      Superclass:
    # note:          def f(self) -&gt; str
    # note:      Subclass:
    # note:          def f(self, x: str) -&gt; None
    if original is not None and isinstance(original, (CallableType, Overloaded)) \
            and override is not None and isinstance(override, (CallableType, Overloaded)):
        self.note('Superclass:', context, offset=ALIGN_OFFSET + OFFSET, code=code)
        self.pretty_callable_or_overload(original, context, offset=ALIGN_OFFSET + 2 * OFFSET,
                                        add_class_or_static_decorator=INCLUDE_DECORATOR,
                                        allow_dups=ALLOW_DUPS, code=code)

        self.note('Subclass:', context, offset=ALIGN_OFFSET + OFFSET, code=code)
        self.pretty_callable_or_overload(override, context, offset=ALIGN_OFFSET + 2 * OFFSET,
                                        add_class_or_static_decorator=INCLUDE_DECORATOR,
                                        allow_dups=ALLOW_DUPS, code=code)

</t>
<t tx="ekr.20220525082934.637">def pretty_callable_or_overload(self,
                                tp: Union[CallableType, Overloaded],
                                context: Context,
                                *,
                                offset: int = 0,
                                add_class_or_static_decorator: bool = False,
                                allow_dups: bool = False,
                                code: Optional[ErrorCode] = None) -&gt; None:
    if isinstance(tp, CallableType):
        if add_class_or_static_decorator:
            decorator = pretty_class_or_static_decorator(tp)
            if decorator is not None:
                self.note(decorator, context, offset=offset, allow_dups=allow_dups, code=code)
        self.note(pretty_callable(tp), context,
                  offset=offset, allow_dups=allow_dups, code=code)
    elif isinstance(tp, Overloaded):
        self.pretty_overload(tp, context, offset,
                             add_class_or_static_decorator=add_class_or_static_decorator,
                             allow_dups=allow_dups, code=code)

</t>
<t tx="ekr.20220525082934.638">def argument_incompatible_with_supertype(
        self, arg_num: int, name: str, type_name: Optional[str],
        name_in_supertype: str, arg_type_in_supertype: Type, supertype: str,
        context: Context) -&gt; None:
    target = self.override_target(name, name_in_supertype, supertype)
    arg_type_in_supertype_f = format_type_bare(arg_type_in_supertype)
    self.fail('Argument {} of "{}" is incompatible with {}; '
              'supertype defines the argument type as "{}"'
              .format(arg_num, name, target, arg_type_in_supertype_f),
              context,
              code=codes.OVERRIDE)
    self.note(
        'This violates the Liskov substitution principle',
        context,
        code=codes.OVERRIDE)
    self.note(
        'See https://mypy.readthedocs.io/en/stable/common_issues.html#incompatible-overrides',
        context,
        code=codes.OVERRIDE)

    if name == "__eq__" and type_name:
        multiline_msg = self.comparison_method_example_msg(class_name=type_name)
        self.note_multiline(multiline_msg, context, code=codes.OVERRIDE)

</t>
<t tx="ekr.20220525082934.639">def comparison_method_example_msg(self, class_name: str) -&gt; str:
    return dedent('''\
    It is recommended for "__eq__" to work with arbitrary objects, for example:
        def __eq__(self, other: object) -&gt; bool:
            if not isinstance(other, {class_name}):
                return NotImplemented
            return &lt;logic to compare two {class_name} instances&gt;
    '''.format(class_name=class_name))

</t>
<t tx="ekr.20220525082934.64">def translate_module_id(self, id: str) -&gt; str:
    """Return the actual, internal module id for a source text id.

    For example, translate '__builtin__' in Python 2 to 'builtins'.
    """
    if id == self.options.custom_typing_module:
        return 'typing'
    elif id == '__builtin__' and self.options.python_version[0] == 2:
        # HACK: __builtin__ in Python 2 is aliases to builtins. However, the implementation
        #   is named __builtin__.py (there is another layer of translation elsewhere).
        return 'builtins'
    return id

</t>
<t tx="ekr.20220525082934.640">def return_type_incompatible_with_supertype(
        self, name: str, name_in_supertype: str, supertype: str,
        original: Type, override: Type,
        context: Context) -&gt; None:
    target = self.override_target(name, name_in_supertype, supertype)
    override_str, original_str = format_type_distinctly(override, original)
    self.fail('Return type {} of "{}" incompatible with return type {} in {}'
              .format(override_str, name, original_str, target),
              context,
              code=codes.OVERRIDE)

</t>
<t tx="ekr.20220525082934.641">def override_target(self, name: str, name_in_super: str,
                    supertype: str) -&gt; str:
    target = f'supertype "{supertype}"'
    if name_in_super != name:
        target = f'"{name_in_super}" of {target}'
    return target

</t>
<t tx="ekr.20220525082934.642">def incompatible_type_application(self, expected_arg_count: int,
                                  actual_arg_count: int,
                                  context: Context) -&gt; None:
    if expected_arg_count == 0:
        self.fail('Type application targets a non-generic function or class',
                  context)
    elif actual_arg_count &gt; expected_arg_count:
        self.fail('Type application has too many types ({} expected)'
                  .format(expected_arg_count), context)
    else:
        self.fail('Type application has too few types ({} expected)'
                  .format(expected_arg_count), context)

</t>
<t tx="ekr.20220525082934.643">def could_not_infer_type_arguments(self, callee_type: CallableType, n: int,
                                   context: Context) -&gt; None:
    callee_name = callable_name(callee_type)
    if callee_name is not None and n &gt; 0:
        self.fail(f'Cannot infer type argument {n} of {callee_name}', context)
    else:
        self.fail('Cannot infer function type argument', context)

</t>
<t tx="ekr.20220525082934.644">def invalid_var_arg(self, typ: Type, context: Context) -&gt; None:
    self.fail('List or tuple expected as variadic arguments', context)

</t>
<t tx="ekr.20220525082934.645">def invalid_keyword_var_arg(self, typ: Type, is_mapping: bool, context: Context) -&gt; None:
    typ = get_proper_type(typ)
    if isinstance(typ, Instance) and is_mapping:
        self.fail('Keywords must be strings', context)
    else:
        self.fail(
            f'Argument after ** must be a mapping, not {format_type(typ)}',
            context, code=codes.ARG_TYPE)

</t>
<t tx="ekr.20220525082934.646">def undefined_in_superclass(self, member: str, context: Context) -&gt; None:
    self.fail(f'"{member}" undefined in superclass', context)

</t>
<t tx="ekr.20220525082934.647">def first_argument_for_super_must_be_type(self, actual: Type, context: Context) -&gt; None:
    actual = get_proper_type(actual)
    if isinstance(actual, Instance):
        # Don't include type of instance, because it can look confusingly like a type
        # object.
        type_str = 'a non-type instance'
    else:
        type_str = format_type(actual)
    self.fail(f'Argument 1 for "super" must be a type object; got {type_str}', context,
              code=codes.ARG_TYPE)

</t>
<t tx="ekr.20220525082934.648">def too_few_string_formatting_arguments(self, context: Context) -&gt; None:
    self.fail('Not enough arguments for format string', context,
              code=codes.STRING_FORMATTING)

</t>
<t tx="ekr.20220525082934.649">def too_many_string_formatting_arguments(self, context: Context) -&gt; None:
    self.fail('Not all arguments converted during string formatting', context,
              code=codes.STRING_FORMATTING)

</t>
<t tx="ekr.20220525082934.65">def visit_Module(self, mod: ast3.Module) -&gt; MypyFile:
    self.type_ignores = {}
    for ti in mod.type_ignores:
        parsed = parse_type_ignore_tag(ti.tag)  # type: ignore[attr-defined]
        if parsed is not None:
            self.type_ignores[ti.lineno] = parsed
        else:
            self.fail(INVALID_TYPE_IGNORE, ti.lineno, -1)
    body = self.fix_function_overloads(self.translate_stmt_list(mod.body, ismodule=True))
    return MypyFile(body,
                    self.imports,
                    False,
                    self.type_ignores,
                    )

</t>
<t tx="ekr.20220525082934.650">def unsupported_placeholder(self, placeholder: str, context: Context) -&gt; None:
    self.fail(f'Unsupported format character "{placeholder}"', context,
              code=codes.STRING_FORMATTING)

</t>
<t tx="ekr.20220525082934.651">def string_interpolation_with_star_and_key(self, context: Context) -&gt; None:
    self.fail('String interpolation contains both stars and mapping keys', context,
              code=codes.STRING_FORMATTING)

</t>
<t tx="ekr.20220525082934.652">def requires_int_or_single_byte(self, context: Context,
                                format_call: bool = False) -&gt; None:
    self.fail('"{}c" requires an integer in range(256) or a single byte'
              .format(':' if format_call else '%'),
              context, code=codes.STRING_FORMATTING)

</t>
<t tx="ekr.20220525082934.653">def requires_int_or_char(self, context: Context,
                         format_call: bool = False) -&gt; None:
    self.fail('"{}c" requires int or char'.format(':' if format_call else '%'),
              context, code=codes.STRING_FORMATTING)

</t>
<t tx="ekr.20220525082934.654">def key_not_in_mapping(self, key: str, context: Context) -&gt; None:
    self.fail(f'Key "{key}" not found in mapping', context,
              code=codes.STRING_FORMATTING)

</t>
<t tx="ekr.20220525082934.655">def string_interpolation_mixing_key_and_non_keys(self, context: Context) -&gt; None:
    self.fail('String interpolation mixes specifier with and without mapping keys', context,
              code=codes.STRING_FORMATTING)

</t>
<t tx="ekr.20220525082934.656">def cannot_determine_type(self, name: str, context: Context) -&gt; None:
    self.fail(f'Cannot determine type of "{name}"', context, code=codes.HAS_TYPE)

</t>
<t tx="ekr.20220525082934.657">def cannot_determine_type_in_base(self, name: str, base: str, context: Context) -&gt; None:
    self.fail(f'Cannot determine type of "{name}" in base class "{base}"', context)

</t>
<t tx="ekr.20220525082934.658">def no_formal_self(self, name: str, item: CallableType, context: Context) -&gt; None:
    self.fail('Attribute function "%s" with type %s does not accept self argument'
              % (name, format_type(item)), context)

</t>
<t tx="ekr.20220525082934.659">def incompatible_self_argument(self, name: str, arg: Type, sig: CallableType,
                               is_classmethod: bool, context: Context) -&gt; None:
    kind = 'class attribute function' if is_classmethod else 'attribute function'
    self.fail('Invalid self argument %s to %s "%s" with type %s'
              % (format_type(arg), kind, name, format_type(sig)), context)

</t>
<t tx="ekr.20220525082934.66"># --- stmt ---
# FunctionDef(identifier name, arguments args,
#             stmt* body, expr* decorator_list, expr? returns, string? type_comment)
# arguments = (arg* args, arg? vararg, arg* kwonlyargs, expr* kw_defaults,
#              arg? kwarg, expr* defaults)
def visit_FunctionDef(self, n: ast3.FunctionDef) -&gt; Union[FuncDef, Decorator]:
    return self.do_func_def(n)

</t>
<t tx="ekr.20220525082934.660">def incompatible_conditional_function_def(self, defn: FuncDef) -&gt; None:
    self.fail('All conditional function variants must have identical '
              'signatures', defn)

</t>
<t tx="ekr.20220525082934.661">def cannot_instantiate_abstract_class(self, class_name: str,
                                      abstract_attributes: List[str],
                                      context: Context) -&gt; None:
    attrs = format_string_list([f'"{a}"' for a in abstract_attributes])
    self.fail('Cannot instantiate abstract class "%s" with abstract '
              'attribute%s %s' % (class_name, plural_s(abstract_attributes),
                               attrs),
              context, code=codes.ABSTRACT)

</t>
<t tx="ekr.20220525082934.662">def base_class_definitions_incompatible(self, name: str, base1: TypeInfo,
                                        base2: TypeInfo,
                                        context: Context) -&gt; None:
    self.fail('Definition of "{}" in base class "{}" is incompatible '
              'with definition in base class "{}"'.format(
                  name, base1.name, base2.name), context)

</t>
<t tx="ekr.20220525082934.663">def cant_assign_to_method(self, context: Context) -&gt; None:
    self.fail(message_registry.CANNOT_ASSIGN_TO_METHOD, context,
              code=codes.ASSIGNMENT)

</t>
<t tx="ekr.20220525082934.664">def cant_assign_to_classvar(self, name: str, context: Context) -&gt; None:
    self.fail(f'Cannot assign to class variable "{name}" via instance', context)

</t>
<t tx="ekr.20220525082934.665">def final_cant_override_writable(self, name: str, ctx: Context) -&gt; None:
    self.fail(f'Cannot override writable attribute "{name}" with a final one', ctx)

</t>
<t tx="ekr.20220525082934.666">def cant_override_final(self, name: str, base_name: str, ctx: Context) -&gt; None:
    self.fail('Cannot override final attribute "{}"'
              ' (previously declared in base class "{}")'.format(name, base_name), ctx)

</t>
<t tx="ekr.20220525082934.667">def cant_assign_to_final(self, name: str, attr_assign: bool, ctx: Context) -&gt; None:
    """Warn about a prohibited assignment to a final attribute.

    Pass `attr_assign=True` if the assignment assigns to an attribute.
    """
    kind = "attribute" if attr_assign else "name"
    self.fail(f'Cannot assign to final {kind} "{unmangle(name)}"', ctx)

</t>
<t tx="ekr.20220525082934.668">def protocol_members_cant_be_final(self, ctx: Context) -&gt; None:
    self.fail("Protocol member cannot be final", ctx)

</t>
<t tx="ekr.20220525082934.669">def final_without_value(self, ctx: Context) -&gt; None:
    self.fail("Final name must be initialized with a value", ctx)

</t>
<t tx="ekr.20220525082934.67"># AsyncFunctionDef(identifier name, arguments args,
#                  stmt* body, expr* decorator_list, expr? returns, string? type_comment)
def visit_AsyncFunctionDef(self, n: ast3.AsyncFunctionDef) -&gt; Union[FuncDef, Decorator]:
    return self.do_func_def(n, is_coroutine=True)

</t>
<t tx="ekr.20220525082934.670">def read_only_property(self, name: str, type: TypeInfo,
                       context: Context) -&gt; None:
    self.fail(f'Property "{name}" defined in "{type.name}" is read-only', context)

</t>
<t tx="ekr.20220525082934.671">def incompatible_typevar_value(self,
                               callee: CallableType,
                               typ: Type,
                               typevar_name: str,
                               context: Context) -&gt; None:
    self.fail(message_registry.INCOMPATIBLE_TYPEVAR_VALUE
              .format(typevar_name, callable_name(callee) or 'function', format_type(typ)),
              context,
              code=codes.TYPE_VAR)

</t>
<t tx="ekr.20220525082934.672">def dangerous_comparison(self, left: Type, right: Type, kind: str, ctx: Context) -&gt; None:
    left_str = 'element' if kind == 'container' else 'left operand'
    right_str = 'container item' if kind == 'container' else 'right operand'
    message = 'Non-overlapping {} check ({} type: {}, {} type: {})'
    left_typ, right_typ = format_type_distinctly(left, right)
    self.fail(message.format(kind, left_str, left_typ, right_str, right_typ), ctx,
              code=codes.COMPARISON_OVERLAP)

</t>
<t tx="ekr.20220525082934.673">def overload_inconsistently_applies_decorator(self, decorator: str, context: Context) -&gt; None:
    self.fail(
        f'Overload does not consistently use the "@{decorator}" '
        + 'decorator on all function signatures.',
        context)

</t>
<t tx="ekr.20220525082934.674">def overloaded_signatures_overlap(self, index1: int, index2: int, context: Context) -&gt; None:
    self.fail('Overloaded function signatures {} and {} overlap with '
              'incompatible return types'.format(index1, index2), context)

</t>
<t tx="ekr.20220525082934.675">def overloaded_signature_will_never_match(self, index1: int, index2: int,
                                          context: Context) -&gt; None:
    self.fail(
        'Overloaded function signature {index2} will never be matched: '
        'signature {index1}\'s parameter type(s) are the same or broader'.format(
            index1=index1,
            index2=index2),
        context)

</t>
<t tx="ekr.20220525082934.676">def overloaded_signatures_typevar_specific(self, index: int, context: Context) -&gt; None:
    self.fail(f'Overloaded function implementation cannot satisfy signature {index} ' +
              'due to inconsistencies in how they use type variables', context)

</t>
<t tx="ekr.20220525082934.677">def overloaded_signatures_arg_specific(self, index: int, context: Context) -&gt; None:
    self.fail('Overloaded function implementation does not accept all possible arguments '
              'of signature {}'.format(index), context)

</t>
<t tx="ekr.20220525082934.678">def overloaded_signatures_ret_specific(self, index: int, context: Context) -&gt; None:
    self.fail('Overloaded function implementation cannot produce return type '
              'of signature {}'.format(index), context)

</t>
<t tx="ekr.20220525082934.679">def warn_both_operands_are_from_unions(self, context: Context) -&gt; None:
    self.note('Both left and right operands are unions', context, code=codes.OPERATOR)

</t>
<t tx="ekr.20220525082934.68">def do_func_def(self, n: Union[ast3.FunctionDef, ast3.AsyncFunctionDef],
                is_coroutine: bool = False) -&gt; Union[FuncDef, Decorator]:
    """Helper shared between visit_FunctionDef and visit_AsyncFunctionDef."""
    self.class_and_function_stack.append('F')
    no_type_check = bool(n.decorator_list and
                         any(is_no_type_check_decorator(d) for d in n.decorator_list))

    lineno = n.lineno
    args = self.transform_args(n.args, lineno, no_type_check=no_type_check)
    if special_function_elide_names(n.name):
        for arg in args:
            arg.pos_only = True

    arg_kinds = [arg.kind for arg in args]
    arg_names = [None if arg.pos_only else arg.variable.name for arg in args]

    arg_types: List[Optional[Type]] = []
    if no_type_check:
        arg_types = [None] * len(args)
        return_type = None
    elif n.type_comment is not None:
        try:
            func_type_ast = ast3_parse(n.type_comment, '&lt;func_type&gt;', 'func_type')
            assert isinstance(func_type_ast, FunctionType)
            # for ellipsis arg
            if (len(func_type_ast.argtypes) == 1 and
                    isinstance(func_type_ast.argtypes[0], ast3_Ellipsis)):
                if n.returns:
                    # PEP 484 disallows both type annotations and type comments
                    self.fail(message_registry.DUPLICATE_TYPE_SIGNATURES, lineno, n.col_offset)
                arg_types = [a.type_annotation
                             if a.type_annotation is not None
                             else AnyType(TypeOfAny.unannotated)
                             for a in args]
            else:
                # PEP 484 disallows both type annotations and type comments
                if n.returns or any(a.type_annotation is not None for a in args):
                    self.fail(message_registry.DUPLICATE_TYPE_SIGNATURES, lineno, n.col_offset)
                translated_args = (TypeConverter(self.errors,
                                                 line=lineno,
                                                 override_column=n.col_offset)
                                   .translate_expr_list(func_type_ast.argtypes))
                arg_types = [a if a is not None else AnyType(TypeOfAny.unannotated)
                            for a in translated_args]
            return_type = TypeConverter(self.errors,
                                        line=lineno).visit(func_type_ast.returns)

            # add implicit self type
            if self.in_method_scope() and len(arg_types) &lt; len(args):
                arg_types.insert(0, AnyType(TypeOfAny.special_form))
        except SyntaxError:
            stripped_type = n.type_comment.split("#", 2)[0].strip()
            err_msg = f'{TYPE_COMMENT_SYNTAX_ERROR} "{stripped_type}"'
            self.fail(err_msg, lineno, n.col_offset)
            if n.type_comment and n.type_comment[0] not in ["(", "#"]:
                self.note('Suggestion: wrap argument types in parentheses',
                          lineno, n.col_offset)
            arg_types = [AnyType(TypeOfAny.from_error)] * len(args)
            return_type = AnyType(TypeOfAny.from_error)
    else:
        arg_types = [a.type_annotation for a in args]
        return_type = TypeConverter(self.errors, line=n.returns.lineno
                                    if n.returns else lineno).visit(n.returns)

    for arg, arg_type in zip(args, arg_types):
        self.set_type_optional(arg_type, arg.initializer)

    func_type = None
    if any(arg_types) or return_type:
        if len(arg_types) != 1 and any(isinstance(t, EllipsisType)
                                       for t in arg_types):
            self.fail("Ellipses cannot accompany other argument types "
                      "in function type signature", lineno, n.col_offset)
        elif len(arg_types) &gt; len(arg_kinds):
            self.fail('Type signature has too many arguments', lineno, n.col_offset,
                      blocker=False)
        elif len(arg_types) &lt; len(arg_kinds):
            self.fail('Type signature has too few arguments', lineno, n.col_offset,
                      blocker=False)
        else:
            func_type = CallableType([a if a is not None else
                                      AnyType(TypeOfAny.unannotated) for a in arg_types],
                                     arg_kinds,
                                     arg_names,
                                     return_type if return_type is not None else
                                     AnyType(TypeOfAny.unannotated),
                                     _dummy_fallback)

    func_def = FuncDef(
        n.name,
        args,
        self.as_required_block(n.body, lineno),
        func_type)
    if isinstance(func_def.type, CallableType):
        # semanal.py does some in-place modifications we want to avoid
        func_def.unanalyzed_type = func_def.type.copy_modified()
    if is_coroutine:
        func_def.is_coroutine = True
    if func_type is not None:
        func_type.definition = func_def
        func_type.line = lineno

    if n.decorator_list:
        if sys.version_info &lt; (3, 8):
            # Before 3.8, [typed_]ast the line number points to the first decorator.
            # In 3.8, it points to the 'def' line, where we want it.
            lineno += len(n.decorator_list)
            end_lineno: Optional[int] = None
        else:
            # Set end_lineno to the old pre-3.8 lineno, in order to keep
            # existing "# type: ignore" comments working:
            end_lineno = n.decorator_list[0].lineno + len(n.decorator_list)

        var = Var(func_def.name)
        var.is_ready = False
        var.set_line(lineno)

        func_def.is_decorated = True
        func_def.set_line(lineno, n.col_offset, end_lineno)
        func_def.body.set_line(lineno)  # TODO: Why?

        deco = Decorator(func_def, self.translate_expr_list(n.decorator_list), var)
        first = n.decorator_list[0]
        deco.set_line(first.lineno, first.col_offset)
        retval: Union[FuncDef, Decorator] = deco
    else:
        # FuncDef overrides set_line -- can't use self.set_line
        func_def.set_line(lineno, n.col_offset)
        retval = func_def
    self.class_and_function_stack.pop()
    return retval

</t>
<t tx="ekr.20220525082934.680">def warn_operand_was_from_union(self, side: str, original: Type, context: Context) -&gt; None:
    self.note(f'{side} operand is of type {format_type(original)}', context,
              code=codes.OPERATOR)

</t>
<t tx="ekr.20220525082934.681">def operator_method_signatures_overlap(
        self, reverse_class: TypeInfo, reverse_method: str, forward_class: Type,
        forward_method: str, context: Context) -&gt; None:
    self.fail('Signatures of "{}" of "{}" and "{}" of {} '
              'are unsafely overlapping'.format(
                  reverse_method, reverse_class.name,
                  forward_method, format_type(forward_class)),
              context)

</t>
<t tx="ekr.20220525082934.682">def forward_operator_not_callable(
        self, forward_method: str, context: Context) -&gt; None:
    self.fail(f'Forward operator "{forward_method}" is not callable', context)

</t>
<t tx="ekr.20220525082934.683">def signatures_incompatible(self, method: str, other_method: str,
                            context: Context) -&gt; None:
    self.fail('Signatures of "{}" and "{}" are incompatible'.format(
        method, other_method), context)

</t>
<t tx="ekr.20220525082934.684">def yield_from_invalid_operand_type(self, expr: Type, context: Context) -&gt; Type:
    text = format_type(expr) if format_type(expr) != 'object' else expr
    self.fail(f'"yield from" can\'t be applied to {text}', context)
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20220525082934.685">def invalid_signature(self, func_type: Type, context: Context) -&gt; None:
    self.fail(f'Invalid signature {format_type(func_type)}', context)

</t>
<t tx="ekr.20220525082934.686">def invalid_signature_for_special_method(
        self, func_type: Type, context: Context, method_name: str) -&gt; None:
    self.fail(f'Invalid signature {format_type(func_type)} for "{method_name}"',
              context)

</t>
<t tx="ekr.20220525082934.687">def reveal_type(self, typ: Type, context: Context) -&gt; None:
    self.note(f'Revealed type is "{typ}"', context)

</t>
<t tx="ekr.20220525082934.688">def reveal_locals(self, type_map: Dict[str, Optional[Type]], context: Context) -&gt; None:
    # To ensure that the output is predictable on Python &lt; 3.6,
    # use an ordered dictionary sorted by variable name
    sorted_locals = OrderedDict(sorted(type_map.items(), key=lambda t: t[0]))
    if sorted_locals:
        self.note("Revealed local types are:", context)
        for k, v in sorted_locals.items():
            self.note(f'    {k}: {v}', context)
    else:
        self.note("There are no locals to reveal", context)

</t>
<t tx="ekr.20220525082934.689">def unsupported_type_type(self, item: Type, context: Context) -&gt; None:
    self.fail(f'Cannot instantiate type "Type[{format_type_bare(item)}]"', context)

</t>
<t tx="ekr.20220525082934.69">def set_type_optional(self, type: Optional[Type], initializer: Optional[Expression]) -&gt; None:
    if self.options.no_implicit_optional:
        return
    # Indicate that type should be wrapped in an Optional if arg is initialized to None.
    optional = isinstance(initializer, NameExpr) and initializer.name == 'None'
    if isinstance(type, UnboundType):
        type.optional = optional

</t>
<t tx="ekr.20220525082934.690">def redundant_cast(self, typ: Type, context: Context) -&gt; None:
    self.fail(f'Redundant cast to {format_type(typ)}', context,
              code=codes.REDUNDANT_CAST)

</t>
<t tx="ekr.20220525082934.691">def assert_type_fail(self, source_type: Type, target_type: Type, context: Context) -&gt; None:
    self.fail(f"Expression is of type {format_type(source_type)}, "
              f"not {format_type(target_type)}", context,
              code=codes.ASSERT_TYPE)

</t>
<t tx="ekr.20220525082934.692">def unimported_type_becomes_any(self, prefix: str, typ: Type, ctx: Context) -&gt; None:
    self.fail(f"{prefix} becomes {format_type(typ)} due to an unfollowed import",
              ctx, code=codes.NO_ANY_UNIMPORTED)

</t>
<t tx="ekr.20220525082934.693">def need_annotation_for_var(self, node: SymbolNode, context: Context,
                            python_version: Optional[Tuple[int, int]] = None) -&gt; None:
    hint = ''
    has_variable_annotations = not python_version or python_version &gt;= (3, 6)
    # Only gives hint if it's a variable declaration and the partial type is a builtin type
    if (python_version and isinstance(node, Var) and isinstance(node.type, PartialType) and
            node.type.type and node.type.type.fullname in reverse_builtin_aliases):
        alias = reverse_builtin_aliases[node.type.type.fullname]
        alias = alias.split('.')[-1]
        type_dec = '&lt;type&gt;'
        if alias == 'Dict':
            type_dec = f'{type_dec}, {type_dec}'
        if has_variable_annotations:
            hint = f' (hint: "{node.name}: {alias}[{type_dec}] = ...")'
        else:
            hint = f' (hint: "{node.name} = ...  # type: {alias}[{type_dec}]")'

    if has_variable_annotations:
        needed = 'annotation'
    else:
        needed = 'comment'

    self.fail(f'Need type {needed} for "{unmangle(node.name)}"{hint}', context,
              code=codes.VAR_ANNOTATED)

</t>
<t tx="ekr.20220525082934.694">def explicit_any(self, ctx: Context) -&gt; None:
    self.fail('Explicit "Any" is not allowed', ctx)

</t>
<t tx="ekr.20220525082934.695">def unexpected_typeddict_keys(
        self,
        typ: TypedDictType,
        expected_keys: List[str],
        actual_keys: List[str],
        context: Context) -&gt; None:
    actual_set = set(actual_keys)
    expected_set = set(expected_keys)
    if not typ.is_anonymous():
        # Generate simpler messages for some common special cases.
        if actual_set &lt; expected_set:
            # Use list comprehension instead of set operations to preserve order.
            missing = [key for key in expected_keys if key not in actual_set]
            self.fail('Missing {} for TypedDict {}'.format(
                format_key_list(missing, short=True), format_type(typ)),
                context, code=codes.TYPEDDICT_ITEM)
            return
        else:
            extra = [key for key in actual_keys if key not in expected_set]
            if extra:
                # If there are both extra and missing keys, only report extra ones for
                # simplicity.
                self.fail('Extra {} for TypedDict {}'.format(
                    format_key_list(extra, short=True), format_type(typ)),
                    context, code=codes.TYPEDDICT_ITEM)
                return
    found = format_key_list(actual_keys, short=True)
    if not expected_keys:
        self.fail(f'Unexpected TypedDict {found}', context)
        return
    expected = format_key_list(expected_keys)
    if actual_keys and actual_set &lt; expected_set:
        found = f'only {found}'
    self.fail(f'Expected {expected} but found {found}', context,
              code=codes.TYPEDDICT_ITEM)

</t>
<t tx="ekr.20220525082934.696">def typeddict_key_must_be_string_literal(
        self,
        typ: TypedDictType,
        context: Context) -&gt; None:
    self.fail(
        'TypedDict key must be a string literal; expected one of {}'.format(
            format_item_name_list(typ.items.keys())), context, code=codes.LITERAL_REQ)

</t>
<t tx="ekr.20220525082934.697">def typeddict_key_not_found(
        self,
        typ: TypedDictType,
        item_name: str,
        context: Context) -&gt; None:
    if typ.is_anonymous():
        self.fail('"{}" is not a valid TypedDict key; expected one of {}'.format(
            item_name, format_item_name_list(typ.items.keys())), context)
    else:
        self.fail('TypedDict {} has no key "{}"'.format(
            format_type(typ), item_name), context, code=codes.TYPEDDICT_ITEM)
        matches = best_matches(item_name, typ.items.keys())
        if matches:
            self.note("Did you mean {}?".format(
                pretty_seq(matches[:3], "or")), context, code=codes.TYPEDDICT_ITEM)

</t>
<t tx="ekr.20220525082934.698">def typeddict_context_ambiguous(
        self,
        types: List[TypedDictType],
        context: Context) -&gt; None:
    formatted_types = ', '.join(list(format_type_distinctly(*types)))
    self.fail('Type of TypedDict is ambiguous, could be any of ({})'.format(
              formatted_types), context)

</t>
<t tx="ekr.20220525082934.699">def typeddict_key_cannot_be_deleted(
        self,
        typ: TypedDictType,
        item_name: str,
        context: Context) -&gt; None:
    if typ.is_anonymous():
        self.fail(f'TypedDict key "{item_name}" cannot be deleted',
                  context)
    else:
        self.fail('Key "{}" of TypedDict {} cannot be deleted'.format(
            item_name, format_type(typ)), context)

</t>
<t tx="ekr.20220525082934.7">def visit_unbound_type(self, t: UnboundType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082934.70">def transform_args(self,
                   args: ast3.arguments,
                   line: int,
                   no_type_check: bool = False,
                   ) -&gt; List[Argument]:
    new_args = []
    names: List[ast3.arg] = []
    posonlyargs = getattr(args, "posonlyargs", cast(List[ast3.arg], []))
    args_args = posonlyargs + args.args
    args_defaults = args.defaults
    num_no_defaults = len(args_args) - len(args_defaults)
    # positional arguments without defaults
    for i, a in enumerate(args_args[:num_no_defaults]):
        pos_only = i &lt; len(posonlyargs)
        new_args.append(self.make_argument(a, None, ARG_POS, no_type_check, pos_only))
        names.append(a)

    # positional arguments with defaults
    for i, (a, d) in enumerate(zip(args_args[num_no_defaults:], args_defaults)):
        pos_only = num_no_defaults + i &lt; len(posonlyargs)
        new_args.append(self.make_argument(a, d, ARG_OPT, no_type_check, pos_only))
        names.append(a)

    # *arg
    if args.vararg is not None:
        new_args.append(self.make_argument(args.vararg, None, ARG_STAR, no_type_check))
        names.append(args.vararg)

    # keyword-only arguments with defaults
    for a, kd in zip(args.kwonlyargs, args.kw_defaults):
        new_args.append(self.make_argument(
            a,
            kd,
            ARG_NAMED if kd is None else ARG_NAMED_OPT,
            no_type_check))
        names.append(a)

    # **kwarg
    if args.kwarg is not None:
        new_args.append(self.make_argument(args.kwarg, None, ARG_STAR2, no_type_check))
        names.append(args.kwarg)

    check_arg_names([arg.variable.name for arg in new_args], names, self.fail_arg)

    return new_args

</t>
<t tx="ekr.20220525082934.700">def typeddict_setdefault_arguments_inconsistent(
        self,
        default: Type,
        expected: Type,
        context: Context) -&gt; None:
    msg = 'Argument 2 to "setdefault" of "TypedDict" has incompatible type {}; expected {}'
    self.fail(msg.format(format_type(default), format_type(expected)), context,
              code=codes.TYPEDDICT_ITEM)

</t>
<t tx="ekr.20220525082934.701">def type_arguments_not_allowed(self, context: Context) -&gt; None:
    self.fail('Parameterized generics cannot be used with class or instance checks', context)

</t>
<t tx="ekr.20220525082934.702">def disallowed_any_type(self, typ: Type, context: Context) -&gt; None:
    typ = get_proper_type(typ)
    if isinstance(typ, AnyType):
        message = 'Expression has type "Any"'
    else:
        message = f'Expression type contains "Any" (has type {format_type(typ)})'
    self.fail(message, context)

</t>
<t tx="ekr.20220525082934.703">def incorrectly_returning_any(self, typ: Type, context: Context) -&gt; None:
    message = f'Returning Any from function declared to return {format_type(typ)}'
    self.fail(message, context, code=codes.NO_ANY_RETURN)

</t>
<t tx="ekr.20220525082934.704">def incorrect__exit__return(self, context: Context) -&gt; None:
    self.fail(
        '"bool" is invalid as return type for "__exit__" that always returns False', context,
        code=codes.EXIT_RETURN)
    self.note(
        'Use "typing_extensions.Literal[False]" as the return type or change it to "None"',
        context, code=codes.EXIT_RETURN)
    self.note(
        'If return type of "__exit__" implies that it may return True, '
        'the context manager may swallow exceptions',
        context, code=codes.EXIT_RETURN)

</t>
<t tx="ekr.20220525082934.705">def untyped_decorated_function(self, typ: Type, context: Context) -&gt; None:
    typ = get_proper_type(typ)
    if isinstance(typ, AnyType):
        self.fail("Function is untyped after decorator transformation", context)
    else:
        self.fail('Type of decorated function contains type "Any" ({})'.format(
            format_type(typ)), context)

</t>
<t tx="ekr.20220525082934.706">def typed_function_untyped_decorator(self, func_name: str, context: Context) -&gt; None:
    self.fail(f'Untyped decorator makes function "{func_name}" untyped', context)

</t>
<t tx="ekr.20220525082934.707">def bad_proto_variance(self, actual: int, tvar_name: str, expected: int,
                       context: Context) -&gt; None:
    msg = capitalize('{} type variable "{}" used in protocol where'
                     ' {} one is expected'.format(variance_string(actual),
                                                  tvar_name,
                                                  variance_string(expected)))
    self.fail(msg, context)

</t>
<t tx="ekr.20220525082934.708">def concrete_only_assign(self, typ: Type, context: Context) -&gt; None:
    self.fail("Can only assign concrete classes to a variable of type {}"
              .format(format_type(typ)), context)

</t>
<t tx="ekr.20220525082934.709">def concrete_only_call(self, typ: Type, context: Context) -&gt; None:
    self.fail("Only concrete class can be given where {} is expected"
              .format(format_type(typ)), context)

</t>
<t tx="ekr.20220525082934.71">def make_argument(self, arg: ast3.arg, default: Optional[ast3.expr], kind: ArgKind,
                  no_type_check: bool, pos_only: bool = False) -&gt; Argument:
    if no_type_check:
        arg_type = None
    else:
        annotation = arg.annotation
        type_comment = arg.type_comment
        if annotation is not None and type_comment is not None:
            self.fail(message_registry.DUPLICATE_TYPE_SIGNATURES, arg.lineno, arg.col_offset)
        arg_type = None
        if annotation is not None:
            arg_type = TypeConverter(self.errors, line=arg.lineno).visit(annotation)
        else:
            arg_type = self.translate_type_comment(arg, type_comment)
    if argument_elide_name(arg.arg):
        pos_only = True

    return Argument(Var(arg.arg), arg_type, self.visit(default), kind, pos_only)

</t>
<t tx="ekr.20220525082934.710">def cannot_use_function_with_type(
        self, method_name: str, type_name: str, context: Context) -&gt; None:
    self.fail(f"Cannot use {method_name}() with {type_name} type", context)

</t>
<t tx="ekr.20220525082934.711">def report_non_method_protocol(self, tp: TypeInfo, members: List[str],
                               context: Context) -&gt; None:
    self.fail("Only protocols that don't have non-method members can be"
              " used with issubclass()", context)
    if len(members) &lt; 3:
        attrs = ', '.join(members)
        self.note('Protocol "{}" has non-method member(s): {}'
                  .format(tp.name, attrs), context)

</t>
<t tx="ekr.20220525082934.712">def note_call(self,
              subtype: Type,
              call: Type,
              context: Context,
              *,
              code: Optional[ErrorCode]) -&gt; None:
    self.note('"{}.__call__" has type {}'.format(format_type_bare(subtype),
                                                 format_type(call, verbosity=1)),
              context, code=code)

</t>
<t tx="ekr.20220525082934.713">def unreachable_statement(self, context: Context) -&gt; None:
    self.fail("Statement is unreachable", context, code=codes.UNREACHABLE)

</t>
<t tx="ekr.20220525082934.714">def redundant_left_operand(self, op_name: str, context: Context) -&gt; None:
    """Indicates that the left operand of a boolean expression is redundant:
    it does not change the truth value of the entire condition as a whole.
    'op_name' should either be the string "and" or the string "or".
    """
    self.redundant_expr(f'Left operand of "{op_name}"', op_name == 'and', context)

</t>
<t tx="ekr.20220525082934.715">def unreachable_right_operand(self, op_name: str, context: Context) -&gt; None:
    """Indicates that the right operand of a boolean expression is redundant:
    it does not change the truth value of the entire condition as a whole.
    'op_name' should either be the string "and" or the string "or".
    """
    self.fail(f'Right operand of "{op_name}" is never evaluated',
              context, code=codes.UNREACHABLE)

</t>
<t tx="ekr.20220525082934.716">def redundant_condition_in_comprehension(self, truthiness: bool, context: Context) -&gt; None:
    self.redundant_expr("If condition in comprehension", truthiness, context)

</t>
<t tx="ekr.20220525082934.717">def redundant_condition_in_if(self, truthiness: bool, context: Context) -&gt; None:
    self.redundant_expr("If condition", truthiness, context)

</t>
<t tx="ekr.20220525082934.718">def redundant_expr(self, description: str, truthiness: bool, context: Context) -&gt; None:
    self.fail(f"{description} is always {str(truthiness).lower()}",
              context, code=codes.REDUNDANT_EXPR)

</t>
<t tx="ekr.20220525082934.719">def impossible_intersection(self,
                            formatted_base_class_list: str,
                            reason: str,
                            context: Context,
                            ) -&gt; None:
    template = "Subclass of {} cannot exist: would have {}"
    self.fail(template.format(formatted_base_class_list, reason), context,
              code=codes.UNREACHABLE)

</t>
<t tx="ekr.20220525082934.72">def fail_arg(self, msg: str, arg: ast3.arg) -&gt; None:
    self.fail(msg, arg.lineno, arg.col_offset)

</t>
<t tx="ekr.20220525082934.720">def report_protocol_problems(self,
                             subtype: Union[Instance, TupleType, TypedDictType],
                             supertype: Instance,
                             context: Context,
                             *,
                             code: Optional[ErrorCode]) -&gt; None:
    """Report possible protocol conflicts between 'subtype' and 'supertype'.

    This includes missing members, incompatible types, and incompatible
    attribute flags, such as settable vs read-only or class variable vs
    instance variable.
    """
    OFFSET = 4  # Four spaces, so that notes will look like this:
    # note: 'Cls' is missing following 'Proto' members:
    # note:     method, attr
    MAX_ITEMS = 2  # Maximum number of conflicts, missing members, and overloads shown
    # List of special situations where we don't want to report additional problems
    exclusions: Dict[type, List[str]] = {
        TypedDictType: ["typing.Mapping"],
        TupleType: ["typing.Iterable", "typing.Sequence"],
        Instance: [],
    }
    if supertype.type.fullname in exclusions[type(subtype)]:
        return
    if any(isinstance(tp, UninhabitedType) for tp in get_proper_types(supertype.args)):
        # We don't want to add notes for failed inference (e.g. Iterable[&lt;nothing&gt;]).
        # This will be only confusing a user even more.
        return

    if isinstance(subtype, TupleType):
        if not isinstance(subtype.partial_fallback, Instance):
            return
        subtype = subtype.partial_fallback
    elif isinstance(subtype, TypedDictType):
        if not isinstance(subtype.fallback, Instance):
            return
        subtype = subtype.fallback

    # Report missing members
    missing = get_missing_protocol_members(subtype, supertype)
    if (missing and len(missing) &lt; len(supertype.type.protocol_members) and
            len(missing) &lt;= MAX_ITEMS):
        self.note('"{}" is missing following "{}" protocol member{}:'
                  .format(subtype.type.name, supertype.type.name, plural_s(missing)),
                  context,
                  code=code)
        self.note(', '.join(missing), context, offset=OFFSET, code=code)
    elif len(missing) &gt; MAX_ITEMS or len(missing) == len(supertype.type.protocol_members):
        # This is an obviously wrong type: too many missing members
        return

    # Report member type conflicts
    conflict_types = get_conflict_protocol_types(subtype, supertype)
    if conflict_types and (not is_subtype(subtype, erase_type(supertype)) or
                           not subtype.type.defn.type_vars or
                           not supertype.type.defn.type_vars):
        self.note(f'Following member(s) of {format_type(subtype)} have conflicts:',
                  context,
                  code=code)
        for name, got, exp in conflict_types[:MAX_ITEMS]:
            exp = get_proper_type(exp)
            got = get_proper_type(got)
            if (not isinstance(exp, (CallableType, Overloaded)) or
                    not isinstance(got, (CallableType, Overloaded))):
                self.note('{}: expected {}, got {}'.format(name,
                                                           *format_type_distinctly(exp, got)),
                          context,
                          offset=OFFSET,
                          code=code)
            else:
                self.note('Expected:', context, offset=OFFSET, code=code)
                if isinstance(exp, CallableType):
                    self.note(pretty_callable(exp), context, offset=2 * OFFSET, code=code)
                else:
                    assert isinstance(exp, Overloaded)
                    self.pretty_overload(exp, context, 2 * OFFSET, code=code)
                self.note('Got:', context, offset=OFFSET, code=code)
                if isinstance(got, CallableType):
                    self.note(pretty_callable(got), context, offset=2 * OFFSET, code=code)
                else:
                    assert isinstance(got, Overloaded)
                    self.pretty_overload(got, context, 2 * OFFSET, code=code)
        self.print_more(conflict_types, context, OFFSET, MAX_ITEMS, code=code)

    # Report flag conflicts (i.e. settable vs read-only etc.)
    conflict_flags = get_bad_protocol_flags(subtype, supertype)
    for name, subflags, superflags in conflict_flags[:MAX_ITEMS]:
        if IS_CLASSVAR in subflags and IS_CLASSVAR not in superflags:
            self.note('Protocol member {}.{} expected instance variable,'
                      ' got class variable'.format(supertype.type.name, name),
                      context,
                      code=code)
        if IS_CLASSVAR in superflags and IS_CLASSVAR not in subflags:
            self.note('Protocol member {}.{} expected class variable,'
                      ' got instance variable'.format(supertype.type.name, name),
                      context,
                      code=code)
        if IS_SETTABLE in superflags and IS_SETTABLE not in subflags:
            self.note('Protocol member {}.{} expected settable variable,'
                      ' got read-only attribute'.format(supertype.type.name, name),
                      context,
                      code=code)
        if IS_CLASS_OR_STATIC in superflags and IS_CLASS_OR_STATIC not in subflags:
            self.note('Protocol member {}.{} expected class or static method'
                      .format(supertype.type.name, name),
                      context,
                      code=code)
    self.print_more(conflict_flags, context, OFFSET, MAX_ITEMS, code=code)

</t>
<t tx="ekr.20220525082934.721">def pretty_overload(self,
                    tp: Overloaded,
                    context: Context,
                    offset: int,
                    *,
                    add_class_or_static_decorator: bool = False,
                    allow_dups: bool = False,
                    code: Optional[ErrorCode] = None) -&gt; None:
    for item in tp.items:
        self.note('@overload', context, offset=offset, allow_dups=allow_dups, code=code)

        if add_class_or_static_decorator:
            decorator = pretty_class_or_static_decorator(item)
            if decorator is not None:
                self.note(decorator, context, offset=offset, allow_dups=allow_dups, code=code)

        self.note(pretty_callable(item), context,
                  offset=offset, allow_dups=allow_dups, code=code)

</t>
<t tx="ekr.20220525082934.722">def print_more(self,
               conflicts: Sequence[Any],
               context: Context,
               offset: int,
               max_items: int,
               *,
               code: Optional[ErrorCode] = None) -&gt; None:
    if len(conflicts) &gt; max_items:
        self.note(f'&lt;{len(conflicts) - max_items} more conflict(s) not shown&gt;',
                  context, offset=offset, code=code)

</t>
<t tx="ekr.20220525082934.723">def try_report_long_tuple_assignment_error(self,
                                           subtype: ProperType,
                                           supertype: ProperType,
                                           context: Context,
                                           msg: str = message_registry.INCOMPATIBLE_TYPES,
                                           subtype_label: Optional[str] = None,
                                           supertype_label: Optional[str] = None,
                                           code: Optional[ErrorCode] = None) -&gt; bool:
    """Try to generate meaningful error message for very long tuple assignment

    Returns a bool: True when generating long tuple assignment error,
    False when no such error reported
    """
    if isinstance(subtype, TupleType):
        if (len(subtype.items) &gt; 10 and
            isinstance(supertype, Instance) and
                supertype.type.fullname == 'builtins.tuple'):
            lhs_type = supertype.args[0]
            lhs_types = [lhs_type] * len(subtype.items)
            self.generate_incompatible_tuple_error(lhs_types,
                                subtype.items, context, msg, code)
            return True
        elif (isinstance(supertype, TupleType) and
                (len(subtype.items) &gt; 10 or len(supertype.items) &gt; 10)):
            if len(subtype.items) != len(supertype.items):
                if supertype_label is not None and subtype_label is not None:
                    error_msg = "{} ({} {}, {} {})".format(msg, subtype_label,
                                    self.format_long_tuple_type(subtype), supertype_label,
                                    self.format_long_tuple_type(supertype))
                    self.fail(error_msg, context, code=code)
                    return True
            self.generate_incompatible_tuple_error(supertype.items,
                                subtype.items, context, msg, code)
            return True
    return False

</t>
<t tx="ekr.20220525082934.724">def format_long_tuple_type(self, typ: TupleType) -&gt; str:
    """Format very long tuple type using an ellipsis notation"""
    item_cnt = len(typ.items)
    if item_cnt &gt; 10:
        return 'Tuple[{}, {}, ... &lt;{} more items&gt;]'\
                .format(format_type_bare(typ.items[0]),
                    format_type_bare(typ.items[1]), str(item_cnt - 2))
    else:
        return format_type_bare(typ)

</t>
<t tx="ekr.20220525082934.725">def generate_incompatible_tuple_error(self,
                                      lhs_types: List[Type],
                                      rhs_types: List[Type],
                                      context: Context,
                                      msg: str = message_registry.INCOMPATIBLE_TYPES,
                                      code: Optional[ErrorCode] = None) -&gt; None:
    """Generate error message for individual incompatible tuple pairs"""
    error_cnt = 0
    notes = []  # List[str]
    for i, (lhs_t, rhs_t) in enumerate(zip(lhs_types, rhs_types)):
        if not is_subtype(lhs_t, rhs_t):
            if error_cnt &lt; 3:
                notes.append('Expression tuple item {} has type {}; {} expected; '
                    .format(str(i), format_type(rhs_t), format_type(lhs_t)))
            error_cnt += 1

    error_msg = msg + f' ({str(error_cnt)} tuple items are incompatible'
    if error_cnt - 3 &gt; 0:
        error_msg += f'; {str(error_cnt - 3)} items are omitted)'
    else:
        error_msg += ')'
    self.fail(error_msg, context, code=code)
    for note in notes:
        self.note(note, context, code=code)

</t>
<t tx="ekr.20220525082934.726">def add_fixture_note(self, fullname: str, ctx: Context) -&gt; None:
    self.note(f'Maybe your test fixture does not define "{fullname}"?', ctx)
    if fullname in SUGGESTED_TEST_FIXTURES:
        self.note(
            'Consider adding [builtins fixtures/{}] to your test description'.format(
                SUGGESTED_TEST_FIXTURES[fullname]), ctx)


</t>
<t tx="ekr.20220525082934.727">def quote_type_string(type_string: str) -&gt; str:
    """Quotes a type representation for use in messages."""
    no_quote_regex = r'^&lt;(tuple|union): \d+ items&gt;$'
    if (type_string in ['Module', 'overloaded function', '&lt;nothing&gt;', '&lt;deleted&gt;']
            or re.match(no_quote_regex, type_string) is not None or type_string.endswith('?')):
        # Messages are easier to read if these aren't quoted.  We use a
        # regex to match strings with variable contents.
        return type_string
    return f'"{type_string}"'


</t>
<t tx="ekr.20220525082934.728">def format_callable_args(arg_types: List[Type], arg_kinds: List[ArgKind],
                         arg_names: List[Optional[str]], format: Callable[[Type], str],
                         verbosity: int) -&gt; str:
    """Format a bunch of Callable arguments into a string"""
    arg_strings = []
    for arg_name, arg_type, arg_kind in zip(
            arg_names, arg_types, arg_kinds):
        if (arg_kind == ARG_POS and arg_name is None
                or verbosity == 0 and arg_kind.is_positional()):

            arg_strings.append(format(arg_type))
        else:
            constructor = ARG_CONSTRUCTOR_NAMES[arg_kind]
            if arg_kind.is_star() or arg_name is None:
                arg_strings.append(f"{constructor}({format(arg_type)})")
            else:
                arg_strings.append("{}({}, {})".format(
                    constructor,
                    format(arg_type),
                    repr(arg_name)))

    return ", ".join(arg_strings)


</t>
<t tx="ekr.20220525082934.729">def format_type_inner(typ: Type,
                      verbosity: int,
                      fullnames: Optional[Set[str]]) -&gt; str:
    """
    Convert a type to a relatively short string suitable for error messages.

    Args:
      verbosity: a coarse grained control on the verbosity of the type
      fullnames: a set of names that should be printed in full
    """
    @others
    # TODO: show type alias names in errors.
    typ = get_proper_type(typ)

    if isinstance(typ, Instance):
        itype = typ
        # Get the short name of the type.
        if itype.type.fullname in ('types.ModuleType', '_importlib_modulespec.ModuleType'):
            # Make some common error messages simpler and tidier.
            return 'Module'
        if verbosity &gt;= 2 or (fullnames and itype.type.fullname in fullnames):
            base_str = itype.type.fullname
        else:
            base_str = itype.type.name
        if not itype.args:
            # No type arguments, just return the type name
            return base_str
        elif itype.type.fullname == 'builtins.tuple':
            item_type_str = format(itype.args[0])
            return f'Tuple[{item_type_str}, ...]'
        elif itype.type.fullname in reverse_builtin_aliases:
            alias = reverse_builtin_aliases[itype.type.fullname]
            alias = alias.split('.')[-1]
            return f'{alias}[{format_list(itype.args)}]'
        else:
            # There are type arguments. Convert the arguments to strings.
            return f'{base_str}[{format_list(itype.args)}]'
    elif isinstance(typ, TypeVarType):
        # This is similar to non-generic instance types.
        return typ.name
    elif isinstance(typ, ParamSpecType):
        # Concatenate[..., P]
        if typ.prefix.arg_types:
            args = format_callable_args(
                typ.prefix.arg_types,
                typ.prefix.arg_kinds,
                typ.prefix.arg_names,
                format,
                verbosity)

            return f'[{args}, **{typ.name_with_suffix()}]'
        else:
            return typ.name_with_suffix()
    elif isinstance(typ, TupleType):
        # Prefer the name of the fallback class (if not tuple), as it's more informative.
        if typ.partial_fallback.type.fullname != 'builtins.tuple':
            return format(typ.partial_fallback)
        s = f'Tuple[{format_list(typ.items)}]'
        return s
    elif isinstance(typ, TypedDictType):
        # If the TypedDictType is named, return the name
        if not typ.is_anonymous():
            return format(typ.fallback)
        items = []
        for (item_name, item_type) in typ.items.items():
            modifier = '' if item_name in typ.required_keys else '?'
            items.append(f'{item_name!r}{modifier}: {format(item_type)}')
        s = f"TypedDict({{{', '.join(items)}}})"
        return s
    elif isinstance(typ, LiteralType):
        return f'Literal[{format_literal_value(typ)}]'
    elif isinstance(typ, UnionType):
        literal_items, union_items = separate_union_literals(typ)

        # Coalesce multiple Literal[] members. This also changes output order.
        # If there's just one Literal item, retain the original ordering.
        if len(literal_items) &gt; 1:
            literal_str = 'Literal[{}]'.format(
                ', '.join(format_literal_value(t) for t in literal_items)
            )

            if len(union_items) == 1 and isinstance(get_proper_type(union_items[0]), NoneType):
                return f'Optional[{literal_str}]'
            elif union_items:
                return f'Union[{format_list(union_items)}, {literal_str}]'
            else:
                return literal_str
        else:
            # Only print Union as Optional if the Optional wouldn't have to contain another Union
            print_as_optional = (len(typ.items) -
                                 sum(isinstance(get_proper_type(t), NoneType)
                                     for t in typ.items) == 1)
            if print_as_optional:
                rest = [t for t in typ.items if not isinstance(get_proper_type(t), NoneType)]
                return f'Optional[{format(rest[0])}]'
            else:
                s = f'Union[{format_list(typ.items)}]'

            return s
    elif isinstance(typ, NoneType):
        return 'None'
    elif isinstance(typ, AnyType):
        return 'Any'
    elif isinstance(typ, DeletedType):
        return '&lt;deleted&gt;'
    elif isinstance(typ, UninhabitedType):
        if typ.is_noreturn:
            return 'NoReturn'
        else:
            return '&lt;nothing&gt;'
    elif isinstance(typ, TypeType):
        return f'Type[{format(typ.item)}]'
    elif isinstance(typ, FunctionLike):
        func = typ
        if func.is_type_obj():
            # The type of a type object type can be derived from the
            # return type (this always works).
            return format(TypeType.make_normalized(erase_type(func.items[0].ret_type)))
        elif isinstance(func, CallableType):
            if func.type_guard is not None:
                return_type = f'TypeGuard[{format(func.type_guard)}]'
            else:
                return_type = format(func.ret_type)
            if func.is_ellipsis_args:
                return f'Callable[..., {return_type}]'
            param_spec = func.param_spec()
            if param_spec is not None:
                return f'Callable[{format(param_spec)}, {return_type}]'
            args = format_callable_args(
                func.arg_types,
                func.arg_kinds,
                func.arg_names,
                format,
                verbosity)
            return f'Callable[[{args}], {return_type}]'
        else:
            # Use a simple representation for function types; proper
            # function types may result in long and difficult-to-read
            # error messages.
            return 'overloaded function'
    elif isinstance(typ, UnboundType):
        return str(typ)
    elif isinstance(typ, Parameters):
        args = format_callable_args(
            typ.arg_types,
            typ.arg_kinds,
            typ.arg_names,
            format,
            verbosity)
        return f'[{args}]'
    elif typ is None:
        raise RuntimeError('Type is None')
    else:
        # Default case; we simply have to return something meaningful here.
        return 'object'


</t>
<t tx="ekr.20220525082934.73"># ClassDef(identifier name,
#  expr* bases,
#  keyword* keywords,
#  stmt* body,
#  expr* decorator_list)
def visit_ClassDef(self, n: ast3.ClassDef) -&gt; ClassDef:
    self.class_and_function_stack.append('C')
    keywords = [(kw.arg, self.visit(kw.value))
                for kw in n.keywords if kw.arg]

    cdef = ClassDef(n.name,
                    self.as_required_block(n.body, n.lineno),
                    None,
                    self.translate_expr_list(n.bases),
                    metaclass=dict(keywords).get('metaclass'),
                    keywords=keywords)
    cdef.decorators = self.translate_expr_list(n.decorator_list)
    # Set end_lineno to the old mypy 0.700 lineno, in order to keep
    # existing "# type: ignore" comments working:
    if sys.version_info &lt; (3, 8):
        cdef.line = n.lineno + len(n.decorator_list)
        cdef.end_line = n.lineno
    else:
        cdef.line = n.lineno
        cdef.end_line = n.decorator_list[0].lineno if n.decorator_list else None
    cdef.column = n.col_offset
    self.class_and_function_stack.pop()
    return cdef

</t>
<t tx="ekr.20220525082934.730">def format(typ: Type) -&gt; str:
    return format_type_inner(typ, verbosity, fullnames)

</t>
<t tx="ekr.20220525082934.731">def format_list(types: Sequence[Type]) -&gt; str:
    return ', '.join(format(typ) for typ in types)

</t>
<t tx="ekr.20220525082934.732">def format_literal_value(typ: LiteralType) -&gt; str:
    if typ.is_enum_literal():
        underlying_type = format(typ.fallback)
        return f'{underlying_type}.{typ.value}'
    else:
        return typ.value_repr()

</t>
<t tx="ekr.20220525082934.733">def collect_all_instances(t: Type) -&gt; List[Instance]:
    """Return all instances that `t` contains (including `t`).

    This is similar to collect_all_inner_types from typeanal but only
    returns instances and will recurse into fallbacks.
    """
    visitor = CollectAllInstancesQuery()
    t.accept(visitor)
    return visitor.instances


</t>
<t tx="ekr.20220525082934.734">class CollectAllInstancesQuery(TypeTraverserVisitor):
    def __init__(self) -&gt; None:
        self.instances: List[Instance] = []

    def visit_instance(self, t: Instance) -&gt; None:
        self.instances.append(t)
        super().visit_instance(t)


</t>
<t tx="ekr.20220525082934.735">def find_type_overlaps(*types: Type) -&gt; Set[str]:
    """Return a set of fullnames that share a short name and appear in either type.

    This is used to ensure that distinct types with the same short name are printed
    with their fullname.
    """
    d: Dict[str, Set[str]] = {}
    for type in types:
        for inst in collect_all_instances(type):
            d.setdefault(inst.type.name, set()).add(inst.type.fullname)
    for shortname in d.keys():
        if f'typing.{shortname}' in TYPES_FOR_UNIMPORTED_HINTS:
            d[shortname].add(f'typing.{shortname}')

    overlaps: Set[str] = set()
    for fullnames in d.values():
        if len(fullnames) &gt; 1:
            overlaps.update(fullnames)
    return overlaps


</t>
<t tx="ekr.20220525082934.736">def format_type(typ: Type, verbosity: int = 0) -&gt; str:
    """
    Convert a type to a relatively short string suitable for error messages.

    `verbosity` is a coarse grained control on the verbosity of the type

    This function returns a string appropriate for unmodified use in error
    messages; this means that it will be quoted in most cases.  If
    modification of the formatted string is required, callers should use
    format_type_bare.
    """
    return quote_type_string(format_type_bare(typ, verbosity))


</t>
<t tx="ekr.20220525082934.737">def format_type_bare(typ: Type,
                     verbosity: int = 0) -&gt; str:
    """
    Convert a type to a relatively short string suitable for error messages.

    `verbosity` is a coarse grained control on the verbosity of the type
    `fullnames` specifies a set of names that should be printed in full

    This function will return an unquoted string.  If a caller doesn't need to
    perform post-processing on the string output, format_type should be used
    instead.  (The caller may want to use quote_type_string after
    processing has happened, to maintain consistent quoting in messages.)
    """
    return format_type_inner(typ, verbosity, find_type_overlaps(typ))


</t>
<t tx="ekr.20220525082934.738">def format_type_distinctly(*types: Type, bare: bool = False) -&gt; Tuple[str, ...]:
    """Jointly format types to distinct strings.

    Increase the verbosity of the type strings until they become distinct
    while also requiring that distinct types with the same short name are
    formatted distinctly.

    By default, the returned strings are created using format_type() and will be
    quoted accordingly. If ``bare`` is True, the returned strings will not
    be quoted; callers who need to do post-processing of the strings before
    quoting them (such as prepending * or **) should use this.
    """
    overlapping = find_type_overlaps(*types)
    for verbosity in range(2):
        strs = [
            format_type_inner(type, verbosity=verbosity, fullnames=overlapping)
            for type in types
        ]
        if len(set(strs)) == len(strs):
            break
    if bare:
        return tuple(strs)
    else:
        return tuple(quote_type_string(s) for s in strs)


</t>
<t tx="ekr.20220525082934.739">def pretty_class_or_static_decorator(tp: CallableType) -&gt; Optional[str]:
    """Return @classmethod or @staticmethod, if any, for the given callable type."""
    if tp.definition is not None and isinstance(tp.definition, SYMBOL_FUNCBASE_TYPES):
        if tp.definition.is_class:
            return '@classmethod'
        if tp.definition.is_static:
            return '@staticmethod'
    return None


</t>
<t tx="ekr.20220525082934.74"># Return(expr? value)
def visit_Return(self, n: ast3.Return) -&gt; ReturnStmt:
    node = ReturnStmt(self.visit(n.value))
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.740">def pretty_callable(tp: CallableType) -&gt; str:
    """Return a nice easily-readable representation of a callable type.
    For example:
        def [T &lt;: int] f(self, x: int, y: T) -&gt; None
    """
    s = ''
    asterisk = False
    for i in range(len(tp.arg_types)):
        if s:
            s += ', '
        if tp.arg_kinds[i].is_named() and not asterisk:
            s += '*, '
            asterisk = True
        if tp.arg_kinds[i] == ARG_STAR:
            s += '*'
            asterisk = True
        if tp.arg_kinds[i] == ARG_STAR2:
            s += '**'
        name = tp.arg_names[i]
        if name:
            s += name + ': '
        s += format_type_bare(tp.arg_types[i])
        if tp.arg_kinds[i].is_optional():
            s += ' = ...'

    # If we got a "special arg" (i.e: self, cls, etc...), prepend it to the arg list
    if isinstance(tp.definition, FuncDef) and tp.definition.name is not None:
        definition_args = [arg.variable.name for arg in tp.definition.arguments]
        if definition_args and tp.arg_names != definition_args \
                and len(definition_args) &gt; 0 and definition_args[0]:
            if s:
                s = ', ' + s
            s = definition_args[0] + s
        s = f'{tp.definition.name}({s})'
    elif tp.name:
        first_arg = tp.def_extras.get('first_arg')
        if first_arg:
            if s:
                s = ', ' + s
            s = first_arg + s
        s = f'{tp.name.split()[0]}({s})'  # skip "of Class" part
    else:
        s = f'({s})'

    s += ' -&gt; '
    if tp.type_guard is not None:
        s += f'TypeGuard[{format_type_bare(tp.type_guard)}]'
    else:
        s += format_type_bare(tp.ret_type)

    if tp.variables:
        tvars = []
        for tvar in tp.variables:
            if isinstance(tvar, TypeVarType):
                upper_bound = get_proper_type(tvar.upper_bound)
                if (isinstance(upper_bound, Instance) and
                        upper_bound.type.fullname != 'builtins.object'):
                    tvars.append(f'{tvar.name} &lt;: {format_type_bare(upper_bound)}')
                elif tvar.values:
                    tvars.append('{} in ({})'
                                 .format(tvar.name, ', '.join([format_type_bare(tp)
                                                               for tp in tvar.values])))
                else:
                    tvars.append(tvar.name)
            else:
                # For other TypeVarLikeTypes, just use the repr
                tvars.append(repr(tvar))
        s = f"[{', '.join(tvars)}] {s}"
    return f'def {s}'


</t>
<t tx="ekr.20220525082934.741">def variance_string(variance: int) -&gt; str:
    if variance == COVARIANT:
        return 'covariant'
    elif variance == CONTRAVARIANT:
        return 'contravariant'
    else:
        return 'invariant'


</t>
<t tx="ekr.20220525082934.742">def get_missing_protocol_members(left: Instance, right: Instance) -&gt; List[str]:
    """Find all protocol members of 'right' that are not implemented
    (i.e. completely missing) in 'left'.
    """
    assert right.type.is_protocol
    missing: List[str] = []
    for member in right.type.protocol_members:
        if not find_member(member, left, left):
            missing.append(member)
    return missing


</t>
<t tx="ekr.20220525082934.743">def get_conflict_protocol_types(left: Instance, right: Instance) -&gt; List[Tuple[str, Type, Type]]:
    """Find members that are defined in 'left' but have incompatible types.
    Return them as a list of ('member', 'got', 'expected').
    """
    assert right.type.is_protocol
    conflicts: List[Tuple[str, Type, Type]] = []
    for member in right.type.protocol_members:
        if member in ('__init__', '__new__'):
            continue
        supertype = find_member(member, right, left)
        assert supertype is not None
        subtype = find_member(member, left, left)
        if not subtype:
            continue
        is_compat = is_subtype(subtype, supertype, ignore_pos_arg_names=True)
        if IS_SETTABLE in get_member_flags(member, right.type):
            is_compat = is_compat and is_subtype(supertype, subtype)
        if not is_compat:
            conflicts.append((member, subtype, supertype))
    return conflicts


</t>
<t tx="ekr.20220525082934.744">def get_bad_protocol_flags(left: Instance, right: Instance
                           ) -&gt; List[Tuple[str, Set[int], Set[int]]]:
    """Return all incompatible attribute flags for members that are present in both
    'left' and 'right'.
    """
    assert right.type.is_protocol
    all_flags: List[Tuple[str, Set[int], Set[int]]] = []
    for member in right.type.protocol_members:
        if find_member(member, left, left):
            item = (member,
                    get_member_flags(member, left.type),
                    get_member_flags(member, right.type))
            all_flags.append(item)
    bad_flags = []
    for name, subflags, superflags in all_flags:
        if (IS_CLASSVAR in subflags and IS_CLASSVAR not in superflags or
                IS_CLASSVAR in superflags and IS_CLASSVAR not in subflags or
                IS_SETTABLE in superflags and IS_SETTABLE not in subflags or
                IS_CLASS_OR_STATIC in superflags and IS_CLASS_OR_STATIC not in subflags):
            bad_flags.append((name, subflags, superflags))
    return bad_flags


</t>
<t tx="ekr.20220525082934.745">def capitalize(s: str) -&gt; str:
    """Capitalize the first character of a string."""
    if s == '':
        return ''
    else:
        return s[0].upper() + s[1:]


</t>
<t tx="ekr.20220525082934.746">def extract_type(name: str) -&gt; str:
    """If the argument is the name of a method (of form C.m), return
    the type portion in quotes (e.g. "y"). Otherwise, return the string
    unmodified.
    """
    name = re.sub('^"[a-zA-Z0-9_]+" of ', '', name)
    return name


</t>
<t tx="ekr.20220525082934.747">def strip_quotes(s: str) -&gt; str:
    """Strip a double quote at the beginning and end of the string, if any."""
    s = re.sub('^"', '', s)
    s = re.sub('"$', '', s)
    return s


</t>
<t tx="ekr.20220525082934.748"></t>
<t tx="ekr.20220525082934.749">def format_string_list(lst: List[str]) -&gt; str:
    assert len(lst) &gt; 0
    if len(lst) == 1:
        return lst[0]
    elif len(lst) &lt;= 5:
        return f"{', '.join(lst[:-1])} and {lst[-1]}"
    else:
        return '%s, ... and %s (%i methods suppressed)' % (
            ', '.join(lst[:2]), lst[-1], len(lst) - 3)


</t>
<t tx="ekr.20220525082934.75"># Delete(expr* targets)
def visit_Delete(self, n: ast3.Delete) -&gt; DelStmt:
    if len(n.targets) &gt; 1:
        tup = TupleExpr(self.translate_expr_list(n.targets))
        tup.set_line(n.lineno)
        node = DelStmt(tup)
    else:
        node = DelStmt(self.visit(n.targets[0]))
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.750">def format_item_name_list(s: Iterable[str]) -&gt; str:
    lst = list(s)
    if len(lst) &lt;= 5:
        return '(' + ', '.join([f'"{name}"' for name in lst]) + ')'
    else:
        return '(' + ', '.join([f'"{name}"' for name in lst[:5]]) + ', ...)'


</t>
<t tx="ekr.20220525082934.751">def callable_name(type: FunctionLike) -&gt; Optional[str]:
    name = type.get_name()
    if name is not None and name[0] != '&lt;':
        return f'"{name}"'.replace(' of ', '" of "')
    return name


</t>
<t tx="ekr.20220525082934.752">def for_function(callee: CallableType) -&gt; str:
    name = callable_name(callee)
    if name is not None:
        return f' for {name}'
    return ''


</t>
<t tx="ekr.20220525082934.753">def find_defining_module(modules: Dict[str, MypyFile], typ: CallableType) -&gt; Optional[MypyFile]:
    if not typ.definition:
        return None
    fullname = typ.definition.fullname
    if fullname is not None and '.' in fullname:
        for i in range(fullname.count('.')):
            module_name = fullname.rsplit('.', i + 1)[0]
            try:
                return modules[module_name]
            except KeyError:
                pass
        assert False, "Couldn't determine module from CallableType"
    return None


</t>
<t tx="ekr.20220525082934.754"># For hard-coding suggested missing member alternatives.
COMMON_MISTAKES: Final[Dict[str, Sequence[str]]] = {
    'add': ('append', 'extend'),
}


</t>
<t tx="ekr.20220525082934.755">def best_matches(current: str, options: Iterable[str]) -&gt; List[str]:
    ratios = {v: difflib.SequenceMatcher(a=current, b=v).ratio() for v in options}
    return sorted((o for o in options if ratios[o] &gt; 0.75),
                  reverse=True, key=lambda v: (ratios[v], v))


</t>
<t tx="ekr.20220525082934.756">def pretty_seq(args: Sequence[str], conjunction: str) -&gt; str:
    quoted = ['"' + a + '"' for a in args]
    if len(quoted) == 1:
        return quoted[0]
    if len(quoted) == 2:
        return f"{quoted[0]} {conjunction} {quoted[1]}"
    last_sep = ", " + conjunction + " "
    return ", ".join(quoted[:-1]) + last_sep + quoted[-1]


</t>
<t tx="ekr.20220525082934.757">def append_invariance_notes(notes: List[str], arg_type: Instance,
                            expected_type: Instance) -&gt; List[str]:
    """Explain that the type is invariant and give notes for how to solve the issue."""
    invariant_type = ''
    covariant_suggestion = ''
    if (arg_type.type.fullname == 'builtins.list' and
            expected_type.type.fullname == 'builtins.list' and
            is_subtype(arg_type.args[0], expected_type.args[0])):
        invariant_type = 'List'
        covariant_suggestion = 'Consider using "Sequence" instead, which is covariant'
    elif (arg_type.type.fullname == 'builtins.dict' and
          expected_type.type.fullname == 'builtins.dict' and
          is_same_type(arg_type.args[0], expected_type.args[0]) and
          is_subtype(arg_type.args[1], expected_type.args[1])):
        invariant_type = 'Dict'
        covariant_suggestion = ('Consider using "Mapping" instead, '
                                'which is covariant in the value type')
    if invariant_type and covariant_suggestion:
        notes.append(
            f'"{invariant_type}" is invariant -- see ' +
            "https://mypy.readthedocs.io/en/stable/common_issues.html#variance")
        notes.append(covariant_suggestion)
    return notes


</t>
<t tx="ekr.20220525082934.758">def make_inferred_type_note(context: Context,
                            subtype: Type,
                            supertype: Type,
                            supertype_str: str) -&gt; str:
    """Explain that the user may have forgotten to type a variable.

    The user does not expect an error if the inferred container type is the same as the return
    type of a function and the argument type(s) are a subtype of the argument type(s) of the
    return type. This note suggests that they add a type annotation with the return type instead
    of relying on the inferred type.
    """
    subtype = get_proper_type(subtype)
    supertype = get_proper_type(supertype)
    if (isinstance(subtype, Instance) and
            isinstance(supertype, Instance) and
            subtype.type.fullname == supertype.type.fullname and
            subtype.args and
            supertype.args and
            isinstance(context, ReturnStmt) and
            isinstance(context.expr, NameExpr) and
            isinstance(context.expr.node, Var) and
            context.expr.node.is_inferred):
        for subtype_arg, supertype_arg in zip(subtype.args, supertype.args):
            if not is_subtype(subtype_arg, supertype_arg):
                return ''
        var_name = context.expr.name
        return 'Perhaps you need a type annotation for "{}"? Suggestion: {}'.format(
            var_name, supertype_str)
    return ''


</t>
<t tx="ekr.20220525082934.759">def format_key_list(keys: List[str], *, short: bool = False) -&gt; str:
    formatted_keys = [f'"{key}"' for key in keys]
    td = '' if short else 'TypedDict '
    if len(keys) == 0:
        return f'no {td}keys'
    elif len(keys) == 1:
        return f'{td}key {formatted_keys[0]}'
    else:
        return f"{td}keys ({', '.join(formatted_keys)})"
</t>
<t tx="ekr.20220525082934.76"># Assign(expr* targets, expr? value, string? type_comment, expr? annotation)
def visit_Assign(self, n: ast3.Assign) -&gt; AssignmentStmt:
    lvalues = self.translate_expr_list(n.targets)
    rvalue = self.visit(n.value)
    typ = self.translate_type_comment(n, n.type_comment)
    s = AssignmentStmt(lvalues, rvalue, type=typ, new_syntax=False)
    return self.set_line(s, n)

</t>
<t tx="ekr.20220525082934.760">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Message constants for generating error messages during type checking.

Literal messages should be defined as constants in this module so they won't get out of sync
if used in more than one place, and so that they can be easily introspected. These messages are
ultimately consumed by messages.MessageBuilder.fail(). For more non-trivial message generation,
add a method to MessageBuilder and call this instead.
"""

from typing import NamedTuple, Optional
from typing_extensions import Final

from mypy import errorcodes as codes


@others
# Invalid types
INVALID_TYPE_RAW_ENUM_VALUE: Final = "Invalid type: try using Literal[{}.{}] instead?"

# Type checker error message constants
NO_RETURN_VALUE_EXPECTED: Final = ErrorMessage("No return value expected", codes.RETURN_VALUE)
MISSING_RETURN_STATEMENT: Final = ErrorMessage("Missing return statement", codes.RETURN)
INVALID_IMPLICIT_RETURN: Final = ErrorMessage("Implicit return in function which does not return")
INCOMPATIBLE_RETURN_VALUE_TYPE: Final = ErrorMessage(
    "Incompatible return value type", codes.RETURN_VALUE
)
RETURN_VALUE_EXPECTED: Final = ErrorMessage("Return value expected", codes.RETURN_VALUE)
NO_RETURN_EXPECTED: Final = ErrorMessage("Return statement in function which does not return")
INVALID_EXCEPTION: Final = ErrorMessage("Exception must be derived from BaseException")
INVALID_EXCEPTION_TYPE: Final = ErrorMessage("Exception type must be derived from BaseException")
RETURN_IN_ASYNC_GENERATOR: Final = ErrorMessage(
    '"return" with value in async generator is not allowed'
)
INVALID_RETURN_TYPE_FOR_GENERATOR: Final = ErrorMessage(
    'The return type of a generator function should be "Generator"' " or one of its supertypes"
)
INVALID_RETURN_TYPE_FOR_ASYNC_GENERATOR: Final = ErrorMessage(
    'The return type of an async generator function should be "AsyncGenerator" or one of its '
    "supertypes"
)
INVALID_GENERATOR_RETURN_ITEM_TYPE: Final = ErrorMessage(
    "The return type of a generator function must be None in"
    " its third type parameter in Python 2"
)
YIELD_VALUE_EXPECTED: Final = ErrorMessage("Yield value expected")
INCOMPATIBLE_TYPES: Final = "Incompatible types"
INCOMPATIBLE_TYPES_IN_ASSIGNMENT: Final = "Incompatible types in assignment"
INCOMPATIBLE_TYPES_IN_AWAIT: Final = ErrorMessage('Incompatible types in "await"')
INCOMPATIBLE_REDEFINITION: Final = ErrorMessage("Incompatible redefinition")
INCOMPATIBLE_TYPES_IN_ASYNC_WITH_AENTER: Final = (
    'Incompatible types in "async with" for "__aenter__"'
)
INCOMPATIBLE_TYPES_IN_ASYNC_WITH_AEXIT: Final = (
    'Incompatible types in "async with" for "__aexit__"'
)
INCOMPATIBLE_TYPES_IN_ASYNC_FOR: Final = 'Incompatible types in "async for"'
INVALID_TYPE_FOR_SLOTS: Final = 'Invalid type for "__slots__"'

ASYNC_FOR_OUTSIDE_COROUTINE: Final = '"async for" outside async function'
ASYNC_WITH_OUTSIDE_COROUTINE: Final = '"async with" outside async function'

INCOMPATIBLE_TYPES_IN_YIELD: Final = ErrorMessage('Incompatible types in "yield"')
INCOMPATIBLE_TYPES_IN_YIELD_FROM: Final = ErrorMessage('Incompatible types in "yield from"')
INCOMPATIBLE_TYPES_IN_STR_INTERPOLATION: Final = "Incompatible types in string interpolation"
INCOMPATIBLE_TYPES_IN_CAPTURE: Final = ErrorMessage('Incompatible types in capture pattern')
MUST_HAVE_NONE_RETURN_TYPE: Final = ErrorMessage('The return type of "{}" must be None')
INVALID_TUPLE_INDEX_TYPE: Final = ErrorMessage("Invalid tuple index type")
TUPLE_INDEX_OUT_OF_RANGE: Final = ErrorMessage("Tuple index out of range")
INVALID_SLICE_INDEX: Final = ErrorMessage("Slice index must be an integer or None")
CANNOT_INFER_LAMBDA_TYPE: Final = ErrorMessage("Cannot infer type of lambda")
CANNOT_ACCESS_INIT: Final = 'Cannot access "__init__" directly'
NON_INSTANCE_NEW_TYPE: Final = ErrorMessage('"__new__" must return a class instance (got {})')
INVALID_NEW_TYPE: Final = ErrorMessage('Incompatible return type for "__new__"')
BAD_CONSTRUCTOR_TYPE: Final = ErrorMessage("Unsupported decorated constructor type")
CANNOT_ASSIGN_TO_METHOD: Final = "Cannot assign to a method"
CANNOT_ASSIGN_TO_TYPE: Final = "Cannot assign to a type"
INCONSISTENT_ABSTRACT_OVERLOAD: Final = ErrorMessage(
    "Overloaded method has both abstract and non-abstract variants"
)
MULTIPLE_OVERLOADS_REQUIRED: Final = ErrorMessage("Single overload definition, multiple required")
READ_ONLY_PROPERTY_OVERRIDES_READ_WRITE: Final = ErrorMessage(
    "Read-only property cannot override read-write property"
)
FORMAT_REQUIRES_MAPPING: Final = "Format requires a mapping"
RETURN_TYPE_CANNOT_BE_CONTRAVARIANT: Final = ErrorMessage(
    "Cannot use a contravariant type variable as return type"
)
FUNCTION_PARAMETER_CANNOT_BE_COVARIANT: Final = ErrorMessage(
    "Cannot use a covariant type variable as a parameter"
)
INCOMPATIBLE_IMPORT_OF: Final = "Incompatible import of"
FUNCTION_TYPE_EXPECTED: Final = ErrorMessage(
    "Function is missing a type annotation", codes.NO_UNTYPED_DEF
)
ONLY_CLASS_APPLICATION: Final = ErrorMessage(
    "Type application is only supported for generic classes"
)
RETURN_TYPE_EXPECTED: Final = ErrorMessage(
    "Function is missing a return type annotation", codes.NO_UNTYPED_DEF
)
ARGUMENT_TYPE_EXPECTED: Final = ErrorMessage(
    "Function is missing a type annotation for one or more arguments", codes.NO_UNTYPED_DEF
)
KEYWORD_ARGUMENT_REQUIRES_STR_KEY_TYPE: Final = ErrorMessage(
    'Keyword argument only valid with "str" key type in call to "dict"'
)
ALL_MUST_BE_SEQ_STR: Final = ErrorMessage("Type of __all__ must be {}, not {}")
INVALID_TYPEDDICT_ARGS: Final = ErrorMessage(
    "Expected keyword arguments, {...}, or dict(...) in TypedDict constructor"
)
TYPEDDICT_KEY_MUST_BE_STRING_LITERAL: Final = ErrorMessage(
    "Expected TypedDict key to be string literal"
)
MALFORMED_ASSERT: Final = ErrorMessage("Assertion is always true, perhaps remove parentheses?")
DUPLICATE_TYPE_SIGNATURES: Final = "Function has duplicate type signatures"
DESCRIPTOR_SET_NOT_CALLABLE: Final = ErrorMessage("{}.__set__ is not callable")
DESCRIPTOR_GET_NOT_CALLABLE: Final = "{}.__get__ is not callable"
MODULE_LEVEL_GETATTRIBUTE: Final = ErrorMessage(
    "__getattribute__ is not valid at the module level"
)
NAME_NOT_IN_SLOTS: Final = ErrorMessage(
    'Trying to assign name "{}" that is not in "__slots__" of type "{}"'
)
TYPE_ALWAYS_TRUE: Final = ErrorMessage(
    "{} which does not implement __bool__ or __len__ "
    "so it could always be true in boolean context",
    code=codes.TRUTHY_BOOL,
)
TYPE_ALWAYS_TRUE_UNIONTYPE: Final = ErrorMessage(
    "{} of which no members implement __bool__ or __len__ "
    "so it could always be true in boolean context",
    code=codes.TRUTHY_BOOL,
)
FUNCTION_ALWAYS_TRUE: Final = ErrorMessage(
    'Function {} could always be true in boolean context',
    code=codes.TRUTHY_BOOL,
)
NOT_CALLABLE: Final = '{} not callable'
PYTHON2_PRINT_FILE_TYPE: Final = (
    'Argument "file" to "print" has incompatible type "{}"; expected "{}"'
)
TYPE_MUST_BE_USED: Final = 'Value of type {} must be used'

# Generic
GENERIC_INSTANCE_VAR_CLASS_ACCESS: Final = (
    "Access to generic instance variables via class is ambiguous"
)
GENERIC_CLASS_VAR_ACCESS: Final = "Access to generic class variables is ambiguous"
BARE_GENERIC: Final = "Missing type parameters for generic type {}"
IMPLICIT_GENERIC_ANY_BUILTIN: Final = (
    'Implicit generic "Any". Use "{}" and specify generic parameters'
)
INVALID_UNPACK = "{} cannot be unpacked (must be tuple or TypeVarTuple)"

# TypeVar
INCOMPATIBLE_TYPEVAR_VALUE: Final = 'Value of type variable "{}" of {} cannot be {}'
CANNOT_USE_TYPEVAR_AS_EXPRESSION: Final = 'Type variable "{}.{}" cannot be used as an expression'
INVALID_TYPEVAR_AS_TYPEARG: Final = 'Type variable "{}" not valid as type argument value for "{}"'
INVALID_TYPEVAR_ARG_BOUND: Final = 'Type argument {} of "{}" must be a subtype of {}'
INVALID_TYPEVAR_ARG_VALUE: Final = 'Invalid type argument value for "{}"'
TYPEVAR_VARIANCE_DEF: Final = 'TypeVar "{}" may only be a literal bool'
TYPEVAR_BOUND_MUST_BE_TYPE: Final = 'TypeVar "bound" must be a type'
TYPEVAR_UNEXPECTED_ARGUMENT: Final = 'Unexpected argument to "TypeVar()"'

# Super
TOO_MANY_ARGS_FOR_SUPER: Final = ErrorMessage('Too many arguments for "super"')
TOO_FEW_ARGS_FOR_SUPER: Final = ErrorMessage('Too few arguments for "super"', codes.CALL_ARG)
SUPER_WITH_SINGLE_ARG_NOT_SUPPORTED: Final = ErrorMessage(
    '"super" with a single argument not supported'
)
UNSUPPORTED_ARG_1_FOR_SUPER: Final = ErrorMessage('Unsupported argument 1 for "super"')
UNSUPPORTED_ARG_2_FOR_SUPER: Final = ErrorMessage('Unsupported argument 2 for "super"')
SUPER_VARARGS_NOT_SUPPORTED: Final = ErrorMessage('Varargs not supported with "super"')
SUPER_POSITIONAL_ARGS_REQUIRED: Final = ErrorMessage('"super" only accepts positional arguments')
SUPER_ARG_2_NOT_INSTANCE_OF_ARG_1: Final = ErrorMessage(
    'Argument 2 for "super" not an instance of argument 1'
)
TARGET_CLASS_HAS_NO_BASE_CLASS: Final = ErrorMessage("Target class has no base class")
SUPER_OUTSIDE_OF_METHOD_NOT_SUPPORTED: Final = ErrorMessage(
    "super() outside of a method is not supported"
)
SUPER_ENCLOSING_POSITIONAL_ARGS_REQUIRED: Final = ErrorMessage(
    "super() requires one or more positional arguments in enclosing function"
)

# Self-type
MISSING_OR_INVALID_SELF_TYPE: Final = ErrorMessage(
    "Self argument missing for a non-static method (or an invalid type for self)"
)
ERASED_SELF_TYPE_NOT_SUPERTYPE: Final = ErrorMessage(
    'The erased type of self "{}" is not a supertype of its class "{}"'
)
INVALID_SELF_TYPE_OR_EXTRA_ARG: Final = ErrorMessage(
    "Invalid type for self, or extra argument type in function annotation"
)

# Final
CANNOT_INHERIT_FROM_FINAL: Final = ErrorMessage('Cannot inherit from final class "{}"')
DEPENDENT_FINAL_IN_CLASS_BODY: Final = ErrorMessage(
    "Final name declared in class body cannot depend on type variables"
)
CANNOT_ACCESS_FINAL_INSTANCE_ATTR: Final = (
    'Cannot access final instance attribute "{}" on class object'
)
CANNOT_MAKE_DELETABLE_FINAL: Final = ErrorMessage("Deletable attribute cannot be final")

# Enum
ENUM_MEMBERS_ATTR_WILL_BE_OVERRIDEN: Final = ErrorMessage(
    'Assigned "__members__" will be overridden by "Enum" internally'
)

# ClassVar
CANNOT_OVERRIDE_INSTANCE_VAR: Final = ErrorMessage(
    'Cannot override instance variable (previously declared on base class "{}") with class '
    "variable"
)
CANNOT_OVERRIDE_CLASS_VAR: Final = ErrorMessage(
    'Cannot override class variable (previously declared on base class "{}") with instance '
    "variable"
)
CLASS_VAR_WITH_TYPEVARS: Final = 'ClassVar cannot contain type variables'
CLASS_VAR_OUTSIDE_OF_CLASS: Final = (
    'ClassVar can only be used for assignments in class body'
)

# Protocol
RUNTIME_PROTOCOL_EXPECTED: Final = ErrorMessage(
    "Only @runtime_checkable protocols can be used with instance and class checks"
)
CANNOT_INSTANTIATE_PROTOCOL: Final = ErrorMessage('Cannot instantiate protocol class "{}"')
TOO_MANY_UNION_COMBINATIONS: Final = ErrorMessage(
    "Not all union combinations were tried because there are too many unions"
)

CONTIGUOUS_ITERABLE_EXPECTED: Final = ErrorMessage("Contiguous iterable with same type expected")
ITERABLE_TYPE_EXPECTED: Final = ErrorMessage("Invalid type '{}' for *expr (iterable expected)")
TYPE_GUARD_POS_ARG_REQUIRED: Final = ErrorMessage("Type guard requires positional argument")

# Match Statement
MISSING_MATCH_ARGS: Final = 'Class "{}" doesn\'t define "__match_args__"'
OR_PATTERN_ALTERNATIVE_NAMES: Final = "Alternative patterns bind different names"
CLASS_PATTERN_GENERIC_TYPE_ALIAS: Final = (
    "Class pattern class must not be a type alias with type parameters"
)
CLASS_PATTERN_TYPE_REQUIRED: Final = 'Expected type in class pattern; found "{}"'
CLASS_PATTERN_TOO_MANY_POSITIONAL_ARGS: Final = "Too many positional patterns for class pattern"
CLASS_PATTERN_KEYWORD_MATCHES_POSITIONAL: Final = (
    'Keyword "{}" already matches a positional pattern'
)
CLASS_PATTERN_DUPLICATE_KEYWORD_PATTERN: Final = 'Duplicate keyword pattern "{}"'
CLASS_PATTERN_UNKNOWN_KEYWORD: Final = 'Class "{}" has no attribute "{}"'
MULTIPLE_ASSIGNMENTS_IN_PATTERN: Final = 'Multiple assignments to name "{}" in pattern'
CANNOT_MODIFY_MATCH_ARGS: Final = 'Cannot assign to "__match_args__"'
</t>
<t tx="ekr.20220525082934.761">class ErrorMessage(NamedTuple):
    value: str
    code: Optional[codes.ErrorCode] = None

    def format(self, *args: object, **kwargs: object) -&gt; "ErrorMessage":
        return ErrorMessage(self.value.format(*args, **kwargs), code=self.code)


</t>
<t tx="ekr.20220525082934.762">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Interfaces for accessing metadata.

We provide two implementations.
 * The "classic" file system implementation, which uses a directory
   structure of files.
 * A hokey sqlite backed implementation, which basically simulates
   the file system in an effort to work around poor file system performance
   on OS X.
"""

import binascii
import os
import time

from abc import abstractmethod
from typing import List, Iterable, Any, Optional
from typing_extensions import TYPE_CHECKING
if TYPE_CHECKING:
    # We avoid importing sqlite3 unless we are using it so we can mostly work
    # on semi-broken pythons that are missing it.
    import sqlite3


@others
</t>
<t tx="ekr.20220525082934.763">class MetadataStore:
    """Generic interface for metadata storage."""

    @others
</t>
<t tx="ekr.20220525082934.764">@abstractmethod
def getmtime(self, name: str) -&gt; float:
    """Read the mtime of a metadata entry..

    Raises FileNotFound if the entry does not exist.
    """
    pass

</t>
<t tx="ekr.20220525082934.765">@abstractmethod
def read(self, name: str) -&gt; str:
    """Read the contents of a metadata entry.

    Raises FileNotFound if the entry does not exist.
    """
    pass

</t>
<t tx="ekr.20220525082934.766">@abstractmethod
def write(self, name: str, data: str, mtime: Optional[float] = None) -&gt; bool:
    """Write a metadata entry.

    If mtime is specified, set it as the mtime of the entry. Otherwise,
    the current time is used.

    Returns True if the entry is successfully written, False otherwise.
    """

</t>
<t tx="ekr.20220525082934.767">@abstractmethod
def remove(self, name: str) -&gt; None:
    """Delete a metadata entry"""
    pass

</t>
<t tx="ekr.20220525082934.768">@abstractmethod
def commit(self) -&gt; None:
    """If the backing store requires a commit, do it.

    But N.B. that this is not *guaranteed* to do anything, and
    there is no guarantee that changes are not made until it is
    called.
    """
    pass

</t>
<t tx="ekr.20220525082934.769">@abstractmethod
def list_all(self) -&gt; Iterable[str]: ...


</t>
<t tx="ekr.20220525082934.77"># AnnAssign(expr target, expr annotation, expr? value, int simple)
def visit_AnnAssign(self, n: ast3.AnnAssign) -&gt; AssignmentStmt:
    line = n.lineno
    if n.value is None:  # always allow 'x: int'
        rvalue: Expression = TempNode(AnyType(TypeOfAny.special_form), no_rhs=True)
        rvalue.line = line
        rvalue.column = n.col_offset
    else:
        rvalue = self.visit(n.value)
    typ = TypeConverter(self.errors, line=line).visit(n.annotation)
    assert typ is not None
    typ.column = n.annotation.col_offset
    s = AssignmentStmt([self.visit(n.target)], rvalue, type=typ, new_syntax=True)
    return self.set_line(s, n)

</t>
<t tx="ekr.20220525082934.770">def random_string() -&gt; str:
    return binascii.hexlify(os.urandom(8)).decode('ascii')


</t>
<t tx="ekr.20220525082934.771">class FilesystemMetadataStore(MetadataStore):
    @others
</t>
<t tx="ekr.20220525082934.772">def __init__(self, cache_dir_prefix: str) -&gt; None:
    # We check startswith instead of equality because the version
    # will have already been appended by the time the cache dir is
    # passed here.
    if cache_dir_prefix.startswith(os.devnull):
        self.cache_dir_prefix = None
    else:
        self.cache_dir_prefix = cache_dir_prefix

</t>
<t tx="ekr.20220525082934.773">def getmtime(self, name: str) -&gt; float:
    if not self.cache_dir_prefix:
        raise FileNotFoundError()

    return int(os.path.getmtime(os.path.join(self.cache_dir_prefix, name)))

</t>
<t tx="ekr.20220525082934.774">def read(self, name: str) -&gt; str:
    assert os.path.normpath(name) != os.path.abspath(name), "Don't use absolute paths!"

    if not self.cache_dir_prefix:
        raise FileNotFoundError()

    with open(os.path.join(self.cache_dir_prefix, name)) as f:
        return f.read()

</t>
<t tx="ekr.20220525082934.775">def write(self, name: str, data: str, mtime: Optional[float] = None) -&gt; bool:
    assert os.path.normpath(name) != os.path.abspath(name), "Don't use absolute paths!"

    if not self.cache_dir_prefix:
        return False

    path = os.path.join(self.cache_dir_prefix, name)
    tmp_filename = path + '.' + random_string()
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(tmp_filename, 'w') as f:
            f.write(data)
        os.replace(tmp_filename, path)
        if mtime is not None:
            os.utime(path, times=(mtime, mtime))

    except os.error:
        return False
    return True

</t>
<t tx="ekr.20220525082934.776">def remove(self, name: str) -&gt; None:
    if not self.cache_dir_prefix:
        raise FileNotFoundError()

    os.remove(os.path.join(self.cache_dir_prefix, name))

</t>
<t tx="ekr.20220525082934.777">def commit(self) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082934.778">def list_all(self) -&gt; Iterable[str]:
    if not self.cache_dir_prefix:
        return

    for dir, _, files in os.walk(self.cache_dir_prefix):
        dir = os.path.relpath(dir, self.cache_dir_prefix)
        for file in files:
            yield os.path.join(dir, file)


</t>
<t tx="ekr.20220525082934.779">SCHEMA = '''
CREATE TABLE IF NOT EXISTS files (
    path TEXT UNIQUE NOT NULL,
    mtime REAL,
    data TEXT
);
CREATE INDEX IF NOT EXISTS path_idx on files(path);
'''
# No migrations yet
MIGRATIONS: List[str] = []


</t>
<t tx="ekr.20220525082934.78"># AugAssign(expr target, operator op, expr value)
def visit_AugAssign(self, n: ast3.AugAssign) -&gt; OperatorAssignmentStmt:
    s = OperatorAssignmentStmt(self.from_operator(n.op),
                               self.visit(n.target),
                               self.visit(n.value))
    return self.set_line(s, n)

</t>
<t tx="ekr.20220525082934.780">def connect_db(db_file: str) -&gt; 'sqlite3.Connection':
    import sqlite3.dbapi2

    db = sqlite3.dbapi2.connect(db_file)
    db.executescript(SCHEMA)
    for migr in MIGRATIONS:
        try:
            db.executescript(migr)
        except sqlite3.OperationalError:
            pass
    return db


</t>
<t tx="ekr.20220525082934.781">class SqliteMetadataStore(MetadataStore):
    @others
</t>
<t tx="ekr.20220525082934.782">def __init__(self, cache_dir_prefix: str) -&gt; None:
    # We check startswith instead of equality because the version
    # will have already been appended by the time the cache dir is
    # passed here.
    if cache_dir_prefix.startswith(os.devnull):
        self.db = None
        return

    os.makedirs(cache_dir_prefix, exist_ok=True)
    self.db = connect_db(os.path.join(cache_dir_prefix, 'cache.db'))

</t>
<t tx="ekr.20220525082934.783">def _query(self, name: str, field: str) -&gt; Any:
    # Raises FileNotFound for consistency with the file system version
    if not self.db:
        raise FileNotFoundError()

    cur = self.db.execute(f'SELECT {field} FROM files WHERE path = ?', (name,))
    results = cur.fetchall()
    if not results:
        raise FileNotFoundError()
    assert len(results) == 1
    return results[0][0]

</t>
<t tx="ekr.20220525082934.784">def getmtime(self, name: str) -&gt; float:
    return self._query(name, 'mtime')

</t>
<t tx="ekr.20220525082934.785">def read(self, name: str) -&gt; str:
    return self._query(name, 'data')

</t>
<t tx="ekr.20220525082934.786">def write(self, name: str, data: str, mtime: Optional[float] = None) -&gt; bool:
    import sqlite3

    if not self.db:
        return False
    try:
        if mtime is None:
            mtime = time.time()
        self.db.execute('INSERT OR REPLACE INTO files(path, mtime, data) VALUES(?, ?, ?)',
                        (name, mtime, data))
    except sqlite3.OperationalError:
        return False
    return True

</t>
<t tx="ekr.20220525082934.787">def remove(self, name: str) -&gt; None:
    if not self.db:
        raise FileNotFoundError()

    self.db.execute('DELETE FROM files WHERE path = ?', (name,))

</t>
<t tx="ekr.20220525082934.788">def commit(self) -&gt; None:
    if self.db:
        self.db.commit()

</t>
<t tx="ekr.20220525082934.789">def list_all(self) -&gt; Iterable[str]:
    if self.db:
        for row in self.db.execute('SELECT path FROM files'):
            yield row[0]
</t>
<t tx="ekr.20220525082934.79"># For(expr target, expr iter, stmt* body, stmt* orelse, string? type_comment)
def visit_For(self, n: ast3.For) -&gt; ForStmt:
    target_type = self.translate_type_comment(n, n.type_comment)
    node = ForStmt(self.visit(n.target),
                   self.visit(n.iter),
                   self.as_required_block(n.body, n.lineno),
                   self.as_block(n.orelse, n.lineno),
                   target_type)
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.790">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from typing import Optional

from mypy.nodes import (
    AssertTypeExpr, Var, FuncItem, ClassDef, AssignmentStmt, ForStmt, WithStmt,
    CastExpr, TypeApplication, TypeAliasExpr, TypeVarExpr, TypedDictExpr, NamedTupleExpr,
    PromoteExpr, NewTypeExpr
)
from mypy.types import Type
from mypy.traverser import TraverserVisitor
from mypy.typetraverser import TypeTraverserVisitor


@others
</t>
<t tx="ekr.20220525082934.791">class MixedTraverserVisitor(TraverserVisitor, TypeTraverserVisitor):
    """Recursive traversal of both Node and Type objects."""

    # Symbol nodes

    @others
</t>
<t tx="ekr.20220525082934.792">def visit_var(self, var: Var) -&gt; None:
    self.visit_optional_type(var.type)

</t>
<t tx="ekr.20220525082934.793">def visit_func(self, o: FuncItem) -&gt; None:
    super().visit_func(o)
    self.visit_optional_type(o.type)

</t>
<t tx="ekr.20220525082934.794">def visit_class_def(self, o: ClassDef) -&gt; None:
    # TODO: Should we visit generated methods/variables as well, either here or in
    #       TraverserVisitor?
    super().visit_class_def(o)
    info = o.info
    if info:
        for base in info.bases:
            base.accept(self)

</t>
<t tx="ekr.20220525082934.795">def visit_type_alias_expr(self, o: TypeAliasExpr) -&gt; None:
    super().visit_type_alias_expr(o)
    o.type.accept(self)

</t>
<t tx="ekr.20220525082934.796">def visit_type_var_expr(self, o: TypeVarExpr) -&gt; None:
    super().visit_type_var_expr(o)
    o.upper_bound.accept(self)
    for value in o.values:
        value.accept(self)

</t>
<t tx="ekr.20220525082934.797">def visit_typeddict_expr(self, o: TypedDictExpr) -&gt; None:
    super().visit_typeddict_expr(o)
    self.visit_optional_type(o.info.typeddict_type)

</t>
<t tx="ekr.20220525082934.798">def visit_namedtuple_expr(self, o: NamedTupleExpr) -&gt; None:
    super().visit_namedtuple_expr(o)
    assert o.info.tuple_type
    o.info.tuple_type.accept(self)

</t>
<t tx="ekr.20220525082934.799">def visit__promote_expr(self, o: PromoteExpr) -&gt; None:
    super().visit__promote_expr(o)
    o.type.accept(self)

</t>
<t tx="ekr.20220525082934.8">def visit_any(self, t: AnyType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082934.80"># AsyncFor(expr target, expr iter, stmt* body, stmt* orelse, string? type_comment)
def visit_AsyncFor(self, n: ast3.AsyncFor) -&gt; ForStmt:
    target_type = self.translate_type_comment(n, n.type_comment)
    node = ForStmt(self.visit(n.target),
                   self.visit(n.iter),
                   self.as_required_block(n.body, n.lineno),
                   self.as_block(n.orelse, n.lineno),
                   target_type)
    node.is_async = True
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.800">def visit_newtype_expr(self, o: NewTypeExpr) -&gt; None:
    super().visit_newtype_expr(o)
    self.visit_optional_type(o.old_type)

</t>
<t tx="ekr.20220525082934.801"># Statements

</t>
<t tx="ekr.20220525082934.802">def visit_assignment_stmt(self, o: AssignmentStmt) -&gt; None:
    super().visit_assignment_stmt(o)
    self.visit_optional_type(o.type)

</t>
<t tx="ekr.20220525082934.803">def visit_for_stmt(self, o: ForStmt) -&gt; None:
    super().visit_for_stmt(o)
    self.visit_optional_type(o.index_type)

</t>
<t tx="ekr.20220525082934.804">def visit_with_stmt(self, o: WithStmt) -&gt; None:
    super().visit_with_stmt(o)
    for typ in o.analyzed_types:
        typ.accept(self)

</t>
<t tx="ekr.20220525082934.805"># Expressions

</t>
<t tx="ekr.20220525082934.806">def visit_cast_expr(self, o: CastExpr) -&gt; None:
    super().visit_cast_expr(o)
    o.type.accept(self)

</t>
<t tx="ekr.20220525082934.807">def visit_assert_type_expr(self, o: AssertTypeExpr) -&gt; None:
    super().visit_assert_type_expr(o)
    o.type.accept(self)

</t>
<t tx="ekr.20220525082934.808">def visit_type_application(self, o: TypeApplication) -&gt; None:
    super().visit_type_application(o)
    for t in o.types:
        t.accept(self)

</t>
<t tx="ekr.20220525082934.809"># Helpers

</t>
<t tx="ekr.20220525082934.81"># While(expr test, stmt* body, stmt* orelse)
def visit_While(self, n: ast3.While) -&gt; WhileStmt:
    node = WhileStmt(self.visit(n.test),
                     self.as_required_block(n.body, n.lineno),
                     self.as_block(n.orelse, n.lineno))
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.810">def visit_optional_type(self, t: Optional[Type]) -&gt; None:
    if t:
        t.accept(self)
</t>
<t tx="ekr.20220525082934.811">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Low-level infrastructure to find modules.

This builds on fscache.py; find_sources.py builds on top of this.
"""

import ast
import collections
import functools
import os
import re
import subprocess
import sys
from enum import Enum, unique

from mypy.errors import CompileError

if sys.version_info &gt;= (3, 11):
    import tomllib
else:
    import tomli as tomllib

from typing import Dict, Iterator, List, NamedTuple, Optional, Set, Tuple, Union
from typing_extensions import Final, TypeAlias as _TypeAlias

from mypy.fscache import FileSystemCache
from mypy.nodes import MypyFile
from mypy.options import Options
from mypy.stubinfo import is_legacy_bundled_package
from mypy import pyinfo


# Paths to be searched in find_module().
SearchPaths = NamedTuple(
    'SearchPaths',
    [
        ('python_path', Tuple[str, ...]),  # where user code is found
        ('mypy_path', Tuple[str, ...]),  # from $MYPYPATH or config variable
        ('package_path', Tuple[str, ...]),  # from get_site_packages_dirs()
        ('typeshed_path', Tuple[str, ...]),  # paths in typeshed
    ]
)


# Package dirs are a two-tuple of path to search and whether to verify the module
OnePackageDir = Tuple[str, bool]
PackageDirs = List[OnePackageDir]

# Minimum and maximum Python versions for modules in stdlib as (major, minor)
StdlibVersions: _TypeAlias = Dict[str, Tuple[Tuple[int, int], Optional[Tuple[int, int]]]]

PYTHON_EXTENSIONS: Final = [".pyi", ".py"]

PYTHON2_STUB_DIR: Final = "@python2"


@others
</t>
<t tx="ekr.20220525082934.812"># TODO: Consider adding more reasons here?
# E.g. if we deduce a module would likely be found if the user were
# to set the --namespace-packages flag.
@unique
class ModuleNotFoundReason(Enum):
    # The module was not found: we found neither stubs nor a plausible code
    # implementation (with or without a py.typed file).
    NOT_FOUND = 0

    # The implementation for this module plausibly exists (e.g. we
    # found a matching folder or *.py file), but either the parent package
    # did not contain a py.typed file or we were unable to find a
    # corresponding *-stubs package.
    FOUND_WITHOUT_TYPE_HINTS = 1

    # The module was not found in the current working directory, but
    # was able to be found in the parent directory.
    WRONG_WORKING_DIRECTORY = 2

    # Stub PyPI package (typically types-pkgname) known to exist but not installed.
    APPROVED_STUBS_NOT_INSTALLED = 3

    @others
</t>
<t tx="ekr.20220525082934.813">def error_message_templates(self, daemon: bool) -&gt; Tuple[str, List[str]]:
    doc_link = "See https://mypy.readthedocs.io/en/stable/running_mypy.html#missing-imports"
    if self is ModuleNotFoundReason.NOT_FOUND:
        msg = 'Cannot find implementation or library stub for module named "{module}"'
        notes = [doc_link]
    elif self is ModuleNotFoundReason.WRONG_WORKING_DIRECTORY:
        msg = 'Cannot find implementation or library stub for module named "{module}"'
        notes = ["You may be running mypy in a subpackage, "
                 "mypy should be run on the package root"]
    elif self is ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS:
        msg = (
            'Skipping analyzing "{module}": module is installed, but missing library stubs '
            'or py.typed marker'
        )
        notes = [doc_link]
    elif self is ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED:
        msg = (
            'Library stubs not installed for "{module}" (or incompatible with Python {pyver})'
        )
        notes = ['Hint: "python3 -m pip install {stub_dist}"']
        if not daemon:
            notes.append(
                '(or run "mypy --install-types" to install all missing stub packages)')
        notes.append(doc_link)
    else:
        assert False
    return msg, notes


</t>
<t tx="ekr.20220525082934.814"># If we found the module, returns the path to the module as a str.
# Otherwise, returns the reason why the module wasn't found.
ModuleSearchResult = Union[str, ModuleNotFoundReason]


</t>
<t tx="ekr.20220525082934.815">class BuildSource:
    """A single source file."""

    @others
</t>
<t tx="ekr.20220525082934.816">def __init__(self, path: Optional[str], module: Optional[str],
             text: Optional[str] = None, base_dir: Optional[str] = None) -&gt; None:
    self.path = path  # File where it's found (e.g. 'xxx/yyy/foo/bar.py')
    self.module = module or '__main__'  # Module name (e.g. 'foo.bar')
    self.text = text  # Source code, if initially supplied, else None
    self.base_dir = base_dir  # Directory where the package is rooted (e.g. 'xxx/yyy')

</t>
<t tx="ekr.20220525082934.817">def __repr__(self) -&gt; str:
    return 'BuildSource(path={!r}, module={!r}, has_text={}, base_dir={!r})'.format(
        self.path,
        self.module,
        self.text is not None,
        self.base_dir)


</t>
<t tx="ekr.20220525082934.818">class BuildSourceSet:
    """Helper to efficiently test a file's membership in a set of build sources."""

    @others
</t>
<t tx="ekr.20220525082934.819">def __init__(self, sources: List[BuildSource]) -&gt; None:
    self.source_text_present = False
    self.source_modules = {}  # type: Dict[str, str]
    self.source_paths = set()  # type: Set[str]

    for source in sources:
        if source.text is not None:
            self.source_text_present = True
        if source.path:
            self.source_paths.add(source.path)
        if source.module:
            self.source_modules[source.module] = source.path or ''

</t>
<t tx="ekr.20220525082934.82"># If(expr test, stmt* body, stmt* orelse)
def visit_If(self, n: ast3.If) -&gt; IfStmt:
    lineno = n.lineno
    node = IfStmt([self.visit(n.test)],
                  [self.as_required_block(n.body, lineno)],
                  self.as_block(n.orelse, lineno))
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.820">def is_source(self, file: MypyFile) -&gt; bool:
    if file.path and file.path in self.source_paths:
        return True
    elif file._fullname in self.source_modules:
        return True
    elif self.source_text_present:
        return True
    else:
        return False


</t>
<t tx="ekr.20220525082934.821">class FindModuleCache:
    """Module finder with integrated cache.

    Module locations and some intermediate results are cached internally
    and can be cleared with the clear() method.

    All file system accesses are performed through a FileSystemCache,
    which is not ever cleared by this class. If necessary it must be
    cleared by client code.
    """

    @others
</t>
<t tx="ekr.20220525082934.822">def __init__(self,
             search_paths: SearchPaths,
             fscache: Optional[FileSystemCache],
             options: Optional[Options],
             stdlib_py_versions: Optional[StdlibVersions] = None,
             source_set: Optional[BuildSourceSet] = None) -&gt; None:
    self.search_paths = search_paths
    self.source_set = source_set
    self.fscache = fscache or FileSystemCache()
    # Cache for get_toplevel_possibilities:
    # search_paths -&gt; (toplevel_id -&gt; list(package_dirs))
    self.initial_components: Dict[Tuple[str, ...], Dict[str, List[str]]] = {}
    # Cache find_module: id -&gt; result
    self.results: Dict[str, ModuleSearchResult] = {}
    self.ns_ancestors: Dict[str, str] = {}
    self.options = options
    custom_typeshed_dir = None
    if options:
        custom_typeshed_dir = options.custom_typeshed_dir
    self.stdlib_py_versions = (
        stdlib_py_versions or load_stdlib_py_versions(custom_typeshed_dir)
    )
    self.python_major_ver = 3 if options is None else options.python_version[0]

</t>
<t tx="ekr.20220525082934.823">def clear(self) -&gt; None:
    self.results.clear()
    self.initial_components.clear()
    self.ns_ancestors.clear()

</t>
<t tx="ekr.20220525082934.824">def find_module_via_source_set(self, id: str) -&gt; Optional[ModuleSearchResult]:
    """Fast path to find modules by looking through the input sources

    This is only used when --fast-module-lookup is passed on the command line."""
    if not self.source_set:
        return None

    p = self.source_set.source_modules.get(id, None)
    if p and self.fscache.isfile(p):
        # We need to make sure we still have __init__.py all the way up
        # otherwise we might have false positives compared to slow path
        # in case of deletion of init files, which is covered by some tests.
        # TODO: are there some combination of flags in which this check should be skipped?
        d = os.path.dirname(p)
        for _ in range(id.count('.')):
            if not any(self.fscache.isfile(os.path.join(d, '__init__' + x))
                       for x in PYTHON_EXTENSIONS):
                return None
            d = os.path.dirname(d)
        return p

    idx = id.rfind('.')
    if idx != -1:
        # When we're looking for foo.bar.baz and can't find a matching module
        # in the source set, look up for a foo.bar module.
        parent = self.find_module_via_source_set(id[:idx])
        if parent is None or not isinstance(parent, str):
            return None

        basename, ext = os.path.splitext(parent)
        if (not any(parent.endswith('__init__' + x) for x in PYTHON_EXTENSIONS)
                and (ext in PYTHON_EXTENSIONS and not self.fscache.isdir(basename))):
            # If we do find such a *module* (and crucially, we don't want a package,
            # hence the filtering out of __init__ files, and checking for the presence
            # of a folder with a matching name), then we can be pretty confident that
            # 'baz' will either be a top-level variable in foo.bar, or will not exist.
            #
            # Either way, spelunking in other search paths for another 'foo.bar.baz'
            # module should be avoided because:
            #  1. in the unlikely event that one were found, it's highly likely that
            #     it would be unrelated to the source being typechecked and therefore
            #     more likely to lead to erroneous results
            #  2. as described in _find_module, in some cases the search itself could
            #  potentially waste significant amounts of time
            return ModuleNotFoundReason.NOT_FOUND
    return None

</t>
<t tx="ekr.20220525082934.825">def find_lib_path_dirs(self, id: str, lib_path: Tuple[str, ...]) -&gt; PackageDirs:
    """Find which elements of a lib_path have the directory a module needs to exist.

    This is run for the python_path, mypy_path, and typeshed_path search paths.
    """
    components = id.split('.')
    dir_chain = os.sep.join(components[:-1])  # e.g., 'foo/bar'

    dirs = []
    for pathitem in self.get_toplevel_possibilities(lib_path, components[0]):
        # e.g., '/usr/lib/python3.4/foo/bar'
        dir = os.path.normpath(os.path.join(pathitem, dir_chain))
        if self.fscache.isdir(dir):
            dirs.append((dir, True))
    return dirs

</t>
<t tx="ekr.20220525082934.826">def get_toplevel_possibilities(self, lib_path: Tuple[str, ...], id: str) -&gt; List[str]:
    """Find which elements of lib_path could contain a particular top-level module.

    In practice, almost all modules can be routed to the correct entry in
    lib_path by looking at just the first component of the module name.

    We take advantage of this by enumerating the contents of all of the
    directories on the lib_path and building a map of which entries in
    the lib_path could contain each potential top-level module that appears.
    """

    if lib_path in self.initial_components:
        return self.initial_components[lib_path].get(id, [])

    # Enumerate all the files in the directories on lib_path and produce the map
    components: Dict[str, List[str]] = {}
    for dir in lib_path:
        try:
            contents = self.fscache.listdir(dir)
        except OSError:
            contents = []
        # False positives are fine for correctness here, since we will check
        # precisely later, so we only look at the root of every filename without
        # any concern for the exact details.
        for name in contents:
            name = os.path.splitext(name)[0]
            components.setdefault(name, []).append(dir)

    if self.python_major_ver == 2:
        components = {id: filter_redundant_py2_dirs(dirs)
                      for id, dirs in components.items()}

    self.initial_components[lib_path] = components
    return components.get(id, [])

</t>
<t tx="ekr.20220525082934.827">def find_module(self, id: str, *, fast_path: bool = False) -&gt; ModuleSearchResult:
    """Return the path of the module source file or why it wasn't found.

    If fast_path is True, prioritize performance over generating detailed
    error descriptions.
    """
    if id not in self.results:
        top_level = id.partition('.')[0]
        use_typeshed = True
        if id in self.stdlib_py_versions:
            use_typeshed = self._typeshed_has_version(id)
        elif top_level in self.stdlib_py_versions:
            use_typeshed = self._typeshed_has_version(top_level)
        self.results[id] = self._find_module(id, use_typeshed)
        if (not (fast_path or (self.options is not None and self.options.fast_module_lookup))
                and self.results[id] is ModuleNotFoundReason.NOT_FOUND
                and self._can_find_module_in_parent_dir(id)):
            self.results[id] = ModuleNotFoundReason.WRONG_WORKING_DIRECTORY
    return self.results[id]

</t>
<t tx="ekr.20220525082934.828">def _typeshed_has_version(self, module: str) -&gt; bool:
    if not self.options:
        return True
    version = typeshed_py_version(self.options)
    min_version, max_version = self.stdlib_py_versions[module]
    return version &gt;= min_version and (max_version is None or version &lt;= max_version)

</t>
<t tx="ekr.20220525082934.829">def _find_module_non_stub_helper(self, components: List[str],
                                 pkg_dir: str) -&gt; Union[OnePackageDir, ModuleNotFoundReason]:
    plausible_match = False
    dir_path = pkg_dir
    for index, component in enumerate(components):
        dir_path = os.path.join(dir_path, component)
        if self.fscache.isfile(os.path.join(dir_path, 'py.typed')):
            return os.path.join(pkg_dir, *components[:-1]), index == 0
        elif not plausible_match and (self.fscache.isdir(dir_path)
                                      or self.fscache.isfile(dir_path + ".py")):
            plausible_match = True
    if is_legacy_bundled_package(components[0], self.python_major_ver):
        if (len(components) == 1
                or (self.find_module(components[0]) is
                    ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED)):
            return ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED
    if is_legacy_bundled_package('.'.join(components[:2]), self.python_major_ver):
        return ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED
    if plausible_match:
        return ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS
    else:
        return ModuleNotFoundReason.NOT_FOUND

</t>
<t tx="ekr.20220525082934.83"># With(withitem* items, stmt* body, string? type_comment)
def visit_With(self, n: ast3.With) -&gt; WithStmt:
    target_type = self.translate_type_comment(n, n.type_comment)
    node = WithStmt([self.visit(i.context_expr) for i in n.items],
                    [self.visit(i.optional_vars) for i in n.items],
                    self.as_required_block(n.body, n.lineno),
                    target_type)
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.830">def _update_ns_ancestors(self, components: List[str], match: Tuple[str, bool]) -&gt; None:
    path, verify = match
    for i in range(1, len(components)):
        pkg_id = '.'.join(components[:-i])
        if pkg_id not in self.ns_ancestors and self.fscache.isdir(path):
            self.ns_ancestors[pkg_id] = path
        path = os.path.dirname(path)

</t>
<t tx="ekr.20220525082934.831">def _can_find_module_in_parent_dir(self, id: str) -&gt; bool:
    """Test if a module can be found by checking the parent directories
    of the current working directory.
    """
    working_dir = os.getcwd()
    parent_search = FindModuleCache(
        SearchPaths((), (), (), ()),
        self.fscache,
        self.options,
        stdlib_py_versions=self.stdlib_py_versions
    )
    while any(file.endswith(("__init__.py", "__init__.pyi"))
              for file in os.listdir(working_dir)):
        working_dir = os.path.dirname(working_dir)
        parent_search.search_paths = SearchPaths((working_dir,), (), (), ())
        if not isinstance(parent_search._find_module(id, False), ModuleNotFoundReason):
            return True
    return False

</t>
<t tx="ekr.20220525082934.832">def _find_module(self, id: str, use_typeshed: bool) -&gt; ModuleSearchResult:
    fscache = self.fscache

    # Fast path for any modules in the current source set.
    # This is particularly important when there are a large number of search
    # paths which share the first (few) component(s) due to the use of namespace
    # packages, for instance:
    # foo/
    #    company/
    #        __init__.py
    #        foo/
    # bar/
    #    company/
    #        __init__.py
    #        bar/
    # baz/
    #    company/
    #        __init__.py
    #        baz/
    #
    # mypy gets [foo/company/foo, bar/company/bar, baz/company/baz, ...] as input
    # and computes [foo, bar, baz, ...] as the module search path.
    #
    # This would result in O(n) search for every import of company.*, leading to
    # O(n**2) behavior in load_graph as such imports are unsurprisingly present
    # at least once, and usually many more times than that, in each and every file
    # being parsed.
    #
    # Thankfully, such cases are efficiently handled by looking up the module path
    # via BuildSourceSet.
    p = (self.find_module_via_source_set(id)
         if (self.options is not None and self.options.fast_module_lookup)
         else None)
    if p:
        return p

    # If we're looking for a module like 'foo.bar.baz', it's likely that most of the
    # many elements of lib_path don't even have a subdirectory 'foo/bar'.  Discover
    # that only once and cache it for when we look for modules like 'foo.bar.blah'
    # that will require the same subdirectory.
    components = id.split('.')
    dir_chain = os.sep.join(components[:-1])  # e.g., 'foo/bar'

    # We have two sets of folders so that we collect *all* stubs folders and
    # put them in the front of the search path
    third_party_inline_dirs: PackageDirs = []
    third_party_stubs_dirs: PackageDirs = []
    found_possible_third_party_missing_type_hints = False
    need_installed_stubs = False
    # Third-party stub/typed packages
    for pkg_dir in self.search_paths.package_path:
        stub_name = components[0] + '-stubs'
        stub_dir = os.path.join(pkg_dir, stub_name)
        if self.python_major_ver == 2:
            alt_stub_name = components[0] + '-python2-stubs'
            alt_stub_dir = os.path.join(pkg_dir, alt_stub_name)
            if fscache.isdir(alt_stub_dir):
                stub_name = alt_stub_name
                stub_dir = alt_stub_dir
        if fscache.isdir(stub_dir) and self._is_compatible_stub_package(stub_dir):
            stub_typed_file = os.path.join(stub_dir, 'py.typed')
            stub_components = [stub_name] + components[1:]
            path = os.path.join(pkg_dir, *stub_components[:-1])
            if fscache.isdir(path):
                if fscache.isfile(stub_typed_file):
                    # Stub packages can have a py.typed file, which must include
                    # 'partial\n' to make the package partial
                    # Partial here means that mypy should look at the runtime
                    # package if installed.
                    if fscache.read(stub_typed_file).decode().strip() == 'partial':
                        runtime_path = os.path.join(pkg_dir, dir_chain)
                        third_party_inline_dirs.append((runtime_path, True))
                        # if the package is partial, we don't verify the module, as
                        # the partial stub package may not have a __init__.pyi
                        third_party_stubs_dirs.append((path, False))
                    else:
                        # handle the edge case where people put a py.typed file
                        # in a stub package, but it isn't partial
                        third_party_stubs_dirs.append((path, True))
                else:
                    third_party_stubs_dirs.append((path, True))
        non_stub_match = self._find_module_non_stub_helper(components, pkg_dir)
        if isinstance(non_stub_match, ModuleNotFoundReason):
            if non_stub_match is ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS:
                found_possible_third_party_missing_type_hints = True
            elif non_stub_match is ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED:
                need_installed_stubs = True
        else:
            third_party_inline_dirs.append(non_stub_match)
            self._update_ns_ancestors(components, non_stub_match)
    if self.options and self.options.use_builtins_fixtures:
        # Everything should be in fixtures.
        third_party_inline_dirs.clear()
        third_party_stubs_dirs.clear()
        found_possible_third_party_missing_type_hints = False
    python_mypy_path = self.search_paths.mypy_path + self.search_paths.python_path
    candidate_base_dirs = self.find_lib_path_dirs(id, python_mypy_path)
    if use_typeshed:
        # Search for stdlib stubs in typeshed before installed
        # stubs to avoid picking up backports (dataclasses, for
        # example) when the library is included in stdlib.
        candidate_base_dirs += self.find_lib_path_dirs(id, self.search_paths.typeshed_path)
    candidate_base_dirs += third_party_stubs_dirs + third_party_inline_dirs

    # If we're looking for a module like 'foo.bar.baz', then candidate_base_dirs now
    # contains just the subdirectories 'foo/bar' that actually exist under the
    # elements of lib_path.  This is probably much shorter than lib_path itself.
    # Now just look for 'baz.pyi', 'baz/__init__.py', etc., inside those directories.
    seplast = os.sep + components[-1]  # so e.g. '/baz'
    sepinit = os.sep + '__init__'
    near_misses = []  # Collect near misses for namespace mode (see below).
    for base_dir, verify in candidate_base_dirs:
        base_path = base_dir + seplast  # so e.g. '/usr/lib/python3.4/foo/bar/baz'
        has_init = False
        dir_prefix = base_dir
        for _ in range(len(components) - 1):
            dir_prefix = os.path.dirname(dir_prefix)
        # Prefer package over module, i.e. baz/__init__.py* over baz.py*.
        for extension in PYTHON_EXTENSIONS:
            path = base_path + sepinit + extension
            suffix = '-stubs'
            if self.python_major_ver == 2:
                if os.path.isdir(base_path + '-python2-stubs'):
                    suffix = '-python2-stubs'
            path_stubs = base_path + suffix + sepinit + extension
            if fscache.isfile_case(path, dir_prefix):
                has_init = True
                if verify and not verify_module(fscache, id, path, dir_prefix):
                    near_misses.append((path, dir_prefix))
                    continue
                return path
            elif fscache.isfile_case(path_stubs, dir_prefix):
                if verify and not verify_module(fscache, id, path_stubs, dir_prefix):
                    near_misses.append((path_stubs, dir_prefix))
                    continue
                return path_stubs

        # In namespace mode, register a potential namespace package
        if self.options and self.options.namespace_packages:
            if fscache.exists_case(base_path, dir_prefix) and not has_init:
                near_misses.append((base_path, dir_prefix))

        # No package, look for module.
        for extension in PYTHON_EXTENSIONS:
            path = base_path + extension
            if fscache.isfile_case(path, dir_prefix):
                if verify and not verify_module(fscache, id, path, dir_prefix):
                    near_misses.append((path, dir_prefix))
                    continue
                return path

    # In namespace mode, re-check those entries that had 'verify'.
    # Assume search path entries xxx, yyy and zzz, and we're
    # looking for foo.bar.baz.  Suppose near_misses has:
    #
    # - xxx/foo/bar/baz.py
    # - yyy/foo/bar/baz/__init__.py
    # - zzz/foo/bar/baz.pyi
    #
    # If any of the foo directories has __init__.py[i], it wins.
    # Else, we look for foo/bar/__init__.py[i], etc.  If there are
    # none, the first hit wins.  Note that this does not take into
    # account whether the lowest-level module is a file (baz.py),
    # a package (baz/__init__.py), or a stub file (baz.pyi) -- for
    # these the first one encountered along the search path wins.
    #
    # The helper function highest_init_level() returns an int that
    # indicates the highest level at which a __init__.py[i] file
    # is found; if no __init__ was found it returns 0, if we find
    # only foo/bar/__init__.py it returns 1, and if we have
    # foo/__init__.py it returns 2 (regardless of what's in
    # foo/bar).  It doesn't look higher than that.
    if self.options and self.options.namespace_packages and near_misses:
        levels = [highest_init_level(fscache, id, path, dir_prefix)
                  for path, dir_prefix in near_misses]
        index = levels.index(max(levels))
        return near_misses[index][0]

    # Finally, we may be asked to produce an ancestor for an
    # installed package with a py.typed marker that is a
    # subpackage of a namespace package.  We only fess up to these
    # if we would otherwise return "not found".
    ancestor = self.ns_ancestors.get(id)
    if ancestor is not None:
        return ancestor

    if need_installed_stubs:
        return ModuleNotFoundReason.APPROVED_STUBS_NOT_INSTALLED
    elif found_possible_third_party_missing_type_hints:
        return ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS
    else:
        return ModuleNotFoundReason.NOT_FOUND

</t>
<t tx="ekr.20220525082934.833">def _is_compatible_stub_package(self, stub_dir: str) -&gt; bool:
    """Does a stub package support the target Python version?

    Stub packages may contain a metadata file which specifies
    whether the stubs are compatible with Python 2 and 3.
    """
    metadata_fnam = os.path.join(stub_dir, 'METADATA.toml')
    if os.path.isfile(metadata_fnam):
        with open(metadata_fnam, "rb") as f:
            metadata = tomllib.load(f)
        if self.python_major_ver == 2:
            return bool(metadata.get('python2', False))
        else:
            return bool(metadata.get('python3', True))
    return True

</t>
<t tx="ekr.20220525082934.834">def find_modules_recursive(self, module: str) -&gt; List[BuildSource]:
    module_path = self.find_module(module)
    if isinstance(module_path, ModuleNotFoundReason):
        return []
    sources = [BuildSource(module_path, module, None)]

    package_path = None
    if module_path.endswith(('__init__.py', '__init__.pyi')):
        package_path = os.path.dirname(module_path)
    elif self.fscache.isdir(module_path):
        package_path = module_path
    if package_path is None:
        return sources

    # This logic closely mirrors that in find_sources. One small but important difference is
    # that we do not sort names with keyfunc. The recursive call to find_modules_recursive
    # calls find_module, which will handle the preference between packages, pyi and py.
    # Another difference is it doesn't handle nested search paths / package roots.

    seen: Set[str] = set()
    names = sorted(self.fscache.listdir(package_path))
    for name in names:
        # Skip certain names altogether
        if name in ("__pycache__", "site-packages", "node_modules") or name.startswith("."):
            continue
        subpath = os.path.join(package_path, name)

        if self.options and matches_exclude(
            subpath, self.options.exclude, self.fscache, self.options.verbosity &gt;= 2
        ):
            continue

        if self.fscache.isdir(subpath):
            # Only recurse into packages
            if (self.options and self.options.namespace_packages) or (
                self.fscache.isfile(os.path.join(subpath, "__init__.py"))
                or self.fscache.isfile(os.path.join(subpath, "__init__.pyi"))
            ):
                seen.add(name)
                sources.extend(self.find_modules_recursive(module + '.' + name))
        else:
            stem, suffix = os.path.splitext(name)
            if stem == '__init__':
                continue
            if stem not in seen and '.' not in stem and suffix in PYTHON_EXTENSIONS:
                # (If we sorted names by keyfunc) we could probably just make the BuildSource
                # ourselves, but this ensures compatibility with find_module / the cache
                seen.add(stem)
                sources.extend(self.find_modules_recursive(module + '.' + stem))
    return sources


</t>
<t tx="ekr.20220525082934.835">def matches_exclude(subpath: str,
                    excludes: List[str],
                    fscache: FileSystemCache,
                    verbose: bool) -&gt; bool:
    if not excludes:
        return False
    subpath_str = os.path.relpath(subpath).replace(os.sep, "/")
    if fscache.isdir(subpath):
        subpath_str += "/"
    for exclude in excludes:
        if re.search(exclude, subpath_str):
            if verbose:
                print(f"TRACE: Excluding {subpath_str} (matches pattern {exclude})",
                      file=sys.stderr)
            return True
    return False


</t>
<t tx="ekr.20220525082934.836">def verify_module(fscache: FileSystemCache, id: str, path: str, prefix: str) -&gt; bool:
    """Check that all packages containing id have a __init__ file."""
    if path.endswith(('__init__.py', '__init__.pyi')):
        path = os.path.dirname(path)
    for i in range(id.count('.')):
        path = os.path.dirname(path)
        if not any(fscache.isfile_case(os.path.join(path, f'__init__{extension}'),
                                       prefix)
                   for extension in PYTHON_EXTENSIONS):
            return False
    return True


</t>
<t tx="ekr.20220525082934.837">def highest_init_level(fscache: FileSystemCache, id: str, path: str, prefix: str) -&gt; int:
    """Compute the highest level where an __init__ file is found."""
    if path.endswith(('__init__.py', '__init__.pyi')):
        path = os.path.dirname(path)
    level = 0
    for i in range(id.count('.')):
        path = os.path.dirname(path)
        if any(fscache.isfile_case(os.path.join(path, f'__init__{extension}'),
                                   prefix)
               for extension in PYTHON_EXTENSIONS):
            level = i + 1
    return level


</t>
<t tx="ekr.20220525082934.838">def mypy_path() -&gt; List[str]:
    path_env = os.getenv('MYPYPATH')
    if not path_env:
        return []
    return path_env.split(os.pathsep)


</t>
<t tx="ekr.20220525082934.839">def default_lib_path(data_dir: str,
                     pyversion: Tuple[int, int],
                     custom_typeshed_dir: Optional[str]) -&gt; List[str]:
    """Return default standard library search paths."""
    path: List[str] = []

    if custom_typeshed_dir:
        typeshed_dir = os.path.join(custom_typeshed_dir, "stdlib")
        mypy_extensions_dir = os.path.join(custom_typeshed_dir, "stubs", "mypy-extensions")
        versions_file = os.path.join(typeshed_dir, "VERSIONS")
        if not os.path.isdir(typeshed_dir) or not os.path.isfile(versions_file):
            print("error: --custom-typeshed-dir does not point to a valid typeshed ({})".format(
                custom_typeshed_dir))
            sys.exit(2)
    else:
        auto = os.path.join(data_dir, 'stubs-auto')
        if os.path.isdir(auto):
            data_dir = auto
        typeshed_dir = os.path.join(data_dir, "typeshed", "stdlib")
        mypy_extensions_dir = os.path.join(data_dir, "typeshed", "stubs", "mypy-extensions")
    if pyversion[0] == 2:
        # Python 2 variants of certain stdlib modules are in a separate directory.
        python2_dir = os.path.join(typeshed_dir, PYTHON2_STUB_DIR)
        path.append(python2_dir)
    path.append(typeshed_dir)

    # Get mypy-extensions stubs from typeshed, since we treat it as an
    # "internal" library, similar to typing and typing-extensions.
    path.append(mypy_extensions_dir)

    # Add fallback path that can be used if we have a broken installation.
    if sys.platform != 'win32':
        path.append('/usr/local/lib/mypy')
    if not path:
        print("Could not resolve typeshed subdirectories. Your mypy install is broken.\n"
              "Python executable is located at {}.\nMypy located at {}".format(
                  sys.executable, data_dir), file=sys.stderr)
        sys.exit(1)
    return path


</t>
<t tx="ekr.20220525082934.84"># AsyncWith(withitem* items, stmt* body, string? type_comment)
def visit_AsyncWith(self, n: ast3.AsyncWith) -&gt; WithStmt:
    target_type = self.translate_type_comment(n, n.type_comment)
    s = WithStmt([self.visit(i.context_expr) for i in n.items],
                 [self.visit(i.optional_vars) for i in n.items],
                 self.as_required_block(n.body, n.lineno),
                 target_type)
    s.is_async = True
    return self.set_line(s, n)

</t>
<t tx="ekr.20220525082934.840">@functools.lru_cache(maxsize=None)
def get_prefixes(python_executable: Optional[str]) -&gt; Tuple[str, str]:
    """Get the sys.base_prefix and sys.prefix for the given python.

    This runs a subprocess call to get the prefix paths of the given Python executable.
    To avoid repeatedly calling a subprocess (which can be slow!) we
    lru_cache the results.
    """
    if python_executable is None:
        return '', ''
    elif python_executable == sys.executable:
        # Use running Python's package dirs
        return pyinfo.getprefixes()
    else:
        # Use subprocess to get the package directory of given Python
        # executable
        return ast.literal_eval(
            subprocess.check_output([python_executable, pyinfo.__file__, 'getprefixes'],
            stderr=subprocess.PIPE).decode())


</t>
<t tx="ekr.20220525082934.841">@functools.lru_cache(maxsize=None)
def get_site_packages_dirs(python_executable: Optional[str]) -&gt; Tuple[List[str], List[str]]:
    """Find package directories for given python.

    This runs a subprocess call, which generates a list of the egg directories, and the site
    package directories. To avoid repeatedly calling a subprocess (which can be slow!) we
    lru_cache the results.
    """

    if python_executable is None:
        return [], []
    elif python_executable == sys.executable:
        # Use running Python's package dirs
        site_packages = pyinfo.getsitepackages()
    else:
        # Use subprocess to get the package directory of given Python
        # executable
        try:
            site_packages = ast.literal_eval(
                subprocess.check_output([python_executable, pyinfo.__file__, 'getsitepackages'],
                stderr=subprocess.PIPE).decode())
        except OSError as err:
            reason = os.strerror(err.errno)
            raise CompileError(
                [f"mypy: Invalid python executable '{python_executable}': {reason}"]
            ) from err
    return expand_site_packages(site_packages)


</t>
<t tx="ekr.20220525082934.842">def expand_site_packages(site_packages: List[str]) -&gt; Tuple[List[str], List[str]]:
    """Expands .pth imports in site-packages directories"""
    egg_dirs: List[str] = []
    for dir in site_packages:
        if not os.path.isdir(dir):
            continue
        pth_filenames = sorted(name for name in os.listdir(dir) if name.endswith(".pth"))
        for pth_filename in pth_filenames:
            egg_dirs.extend(_parse_pth_file(dir, pth_filename))

    return egg_dirs, site_packages


</t>
<t tx="ekr.20220525082934.843">def _parse_pth_file(dir: str, pth_filename: str) -&gt; Iterator[str]:
    """
    Mimics a subset of .pth import hook from Lib/site.py
    See https://github.com/python/cpython/blob/3.5/Lib/site.py#L146-L185
    """

    pth_file = os.path.join(dir, pth_filename)
    try:
        f = open(pth_file)
    except OSError:
        return
    with f:
        for line in f.readlines():
            if line.startswith("#"):
                # Skip comment lines
                continue
            if line.startswith(("import ", "import\t")):
                # import statements in .pth files are not supported
                continue

            yield _make_abspath(line.rstrip(), dir)


</t>
<t tx="ekr.20220525082934.844">def _make_abspath(path: str, root: str) -&gt; str:
    """Take a path and make it absolute relative to root if not already absolute."""
    if os.path.isabs(path):
        return os.path.normpath(path)
    else:
        return os.path.join(root, os.path.normpath(path))


</t>
<t tx="ekr.20220525082934.845">def add_py2_mypypath_entries(mypypath: List[str]) -&gt; List[str]:
    """Add corresponding @python2 subdirectories to mypypath.

    For each path entry 'x', add 'x/@python2' before 'x' if the latter is
    a directory.
    """
    result = []
    for item in mypypath:
        python2_dir = os.path.join(item, PYTHON2_STUB_DIR)
        if os.path.isdir(python2_dir):
            # @python2 takes precedence, but we also look into the parent
            # directory.
            result.append(python2_dir)
            result.append(item)
        else:
            result.append(item)
    return result


</t>
<t tx="ekr.20220525082934.846">def compute_search_paths(sources: List[BuildSource],
                         options: Options,
                         data_dir: str,
                         alt_lib_path: Optional[str] = None) -&gt; SearchPaths:
    """Compute the search paths as specified in PEP 561.

    There are the following 4 members created:
    - User code (from `sources`)
    - MYPYPATH (set either via config or environment variable)
    - installed package directories (which will later be split into stub-only and inline)
    - typeshed
     """
    # Determine the default module search path.
    lib_path = collections.deque(
        default_lib_path(data_dir,
                         options.python_version,
                         custom_typeshed_dir=options.custom_typeshed_dir))

    if options.use_builtins_fixtures:
        # Use stub builtins (to speed up test cases and to make them easier to
        # debug).  This is a test-only feature, so assume our files are laid out
        # as in the source tree.
        # We also need to allow overriding where to look for it. Argh.
        root_dir = os.getenv('MYPY_TEST_PREFIX', None)
        if not root_dir:
            root_dir = os.path.dirname(os.path.dirname(__file__))
        lib_path.appendleft(os.path.join(root_dir, 'test-data', 'unit', 'lib-stub'))
    # alt_lib_path is used by some tests to bypass the normal lib_path mechanics.
    # If we don't have one, grab directories of source files.
    python_path: List[str] = []
    if not alt_lib_path:
        for source in sources:
            # Include directory of the program file in the module search path.
            if source.base_dir:
                dir = source.base_dir
                if dir not in python_path:
                    python_path.append(dir)

        # Do this even if running as a file, for sanity (mainly because with
        # multiple builds, there could be a mix of files/modules, so its easier
        # to just define the semantics that we always add the current director
        # to the lib_path
        # TODO: Don't do this in some cases; for motivation see see
        # https://github.com/python/mypy/issues/4195#issuecomment-341915031
        if options.bazel:
            dir = '.'
        else:
            dir = os.getcwd()
        if dir not in lib_path:
            python_path.insert(0, dir)

    # Start with a MYPYPATH environment variable at the front of the mypy_path, if defined.
    mypypath = mypy_path()

    # Add a config-defined mypy path.
    mypypath.extend(options.mypy_path)

    # If provided, insert the caller-supplied extra module path to the
    # beginning (highest priority) of the search path.
    if alt_lib_path:
        mypypath.insert(0, alt_lib_path)

    # When type checking in Python 2 module, add @python2 subdirectories of
    # path items into the search path.
    if options.python_version[0] == 2:
        mypypath = add_py2_mypypath_entries(mypypath)

    egg_dirs, site_packages = get_site_packages_dirs(options.python_executable)
    base_prefix, prefix = get_prefixes(options.python_executable)
    is_venv = base_prefix != prefix
    for site_dir in site_packages:
        assert site_dir not in lib_path
        if (site_dir in mypypath or
                any(p.startswith(site_dir + os.path.sep) for p in mypypath) or
                os.path.altsep and any(p.startswith(site_dir + os.path.altsep) for p in mypypath)):
            print(f"{site_dir} is in the MYPYPATH. Please remove it.", file=sys.stderr)
            print("See https://mypy.readthedocs.io/en/stable/running_mypy.html"
                  "#how-mypy-handles-imports for more info", file=sys.stderr)
            sys.exit(1)
        elif site_dir in python_path and (is_venv and not site_dir.startswith(prefix)):
            print("{} is in the PYTHONPATH. Please change directory"
                  " so it is not.".format(site_dir),
                  file=sys.stderr)
            sys.exit(1)

    return SearchPaths(python_path=tuple(reversed(python_path)),
                       mypy_path=tuple(mypypath),
                       package_path=tuple(egg_dirs + site_packages),
                       typeshed_path=tuple(lib_path))


</t>
<t tx="ekr.20220525082934.847">def load_stdlib_py_versions(custom_typeshed_dir: Optional[str]) -&gt; StdlibVersions:
    """Return dict with minimum and maximum Python versions of stdlib modules.

    The contents look like
    {..., 'secrets': ((3, 6), None), 'symbol': ((2, 7), (3, 9)), ...}

    None means there is no maximum version.
    """
    typeshed_dir = custom_typeshed_dir or os.path.join(os.path.dirname(__file__), "typeshed")
    stdlib_dir = os.path.join(typeshed_dir, "stdlib")
    result = {}

    versions_path = os.path.join(stdlib_dir, "VERSIONS")
    assert os.path.isfile(versions_path), (custom_typeshed_dir, versions_path, __file__)
    with open(versions_path) as f:
        for line in f:
            line = line.split("#")[0].strip()
            if line == "":
                continue
            module, version_range = line.split(":")
            versions = version_range.split("-")
            min_version = parse_version(versions[0])
            max_version = (parse_version(versions[1])
                           if len(versions) &gt;= 2 and versions[1].strip() else None)
            result[module] = min_version, max_version

    # Modules that are Python 2 only or have separate Python 2 stubs
    # have stubs in @python2/ and may need an override.
    python2_dir = os.path.join(stdlib_dir, PYTHON2_STUB_DIR)
    try:
        for fnam in os.listdir(python2_dir):
            fnam = fnam.replace(".pyi", "")
            max_version = result.get(fnam, ((2, 7), None))[1]
            result[fnam] = (2, 7), max_version
    except FileNotFoundError:
        # Ignore error to support installations where Python 2 stubs aren't available.
        pass

    return result


</t>
<t tx="ekr.20220525082934.848">def parse_version(version: str) -&gt; Tuple[int, int]:
    major, minor = version.strip().split(".")
    return int(major), int(minor)


</t>
<t tx="ekr.20220525082934.849">def typeshed_py_version(options: Options) -&gt; Tuple[int, int]:
    """Return Python version used for checking whether module supports typeshed."""
    # Typeshed no longer covers Python 3.x versions before 3.6, so 3.6 is
    # the earliest we can support.
    if options.python_version[0] &gt;= 3:
        return max(options.python_version, (3, 6))
    else:
        return options.python_version


</t>
<t tx="ekr.20220525082934.85"># Raise(expr? exc, expr? cause)
def visit_Raise(self, n: ast3.Raise) -&gt; RaiseStmt:
    node = RaiseStmt(self.visit(n.exc), self.visit(n.cause))
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.850">def filter_redundant_py2_dirs(dirs: List[str]) -&gt; List[str]:
    """If dirs has &lt;dir&gt;/@python2 followed by &lt;dir&gt;, filter out the latter."""
    if len(dirs) &lt;= 1 or not any(d.endswith(PYTHON2_STUB_DIR) for d in dirs):
        # Fast path -- nothing to do
        return dirs
    seen = []
    result = []
    for d in dirs:
        if d.endswith(PYTHON2_STUB_DIR):
            seen.append(os.path.dirname(d))
            result.append(d)
        elif d not in seen:
            result.append(d)
    return result
</t>
<t tx="ekr.20220525082934.851">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Basic introspection of modules."""

from typing import List, Optional, Union
from types import ModuleType
from multiprocessing import Process, Queue
import importlib
import inspect
import os
import pkgutil
import queue
import sys


@others
</t>
<t tx="ekr.20220525082934.852">class ModuleProperties:
    @others
</t>
<t tx="ekr.20220525082934.853"># Note that all __init__ args must have default values
def __init__(self,
             name: str = "",
             file: Optional[str] = None,
             path: Optional[List[str]] = None,
             all: Optional[List[str]] = None,
             is_c_module: bool = False,
             subpackages: Optional[List[str]] = None) -&gt; None:
    self.name = name  # __name__ attribute
    self.file = file  # __file__ attribute
    self.path = path  # __path__ attribute
    self.all = all  # __all__ attribute
    self.is_c_module = is_c_module
    self.subpackages = subpackages or []


</t>
<t tx="ekr.20220525082934.854">def is_c_module(module: ModuleType) -&gt; bool:
    if module.__dict__.get('__file__') is None:
        # Could be a namespace package. These must be handled through
        # introspection, since there is no source file.
        return True
    return os.path.splitext(module.__dict__['__file__'])[-1] in ['.so', '.pyd']


</t>
<t tx="ekr.20220525082934.855">class InspectError(Exception):
    pass


</t>
<t tx="ekr.20220525082934.856">def get_package_properties(package_id: str) -&gt; ModuleProperties:
    """Use runtime introspection to get information about a module/package."""
    try:
        package = importlib.import_module(package_id)
    except BaseException as e:
        raise InspectError(str(e)) from e
    name = getattr(package, "__name__", package_id)
    file = getattr(package, "__file__", None)
    path: Optional[List[str]] = getattr(package, "__path__", None)
    if not isinstance(path, list):
        path = None
    pkg_all = getattr(package, '__all__', None)
    if pkg_all is not None:
        try:
            pkg_all = list(pkg_all)
        except Exception:
            pkg_all = None
    is_c = is_c_module(package)

    if path is None:
        # Object has no path; this means it's either a module inside a package
        # (and thus no sub-packages), or it could be a C extension package.
        if is_c:
            # This is a C extension module, now get the list of all sub-packages
            # using the inspect module
            subpackages = [package.__name__ + "." + name
                           for name, val in inspect.getmembers(package)
                           if inspect.ismodule(val)
                           and val.__name__ == package.__name__ + "." + name]
        else:
            # It's a module inside a package.  There's nothing else to walk/yield.
            subpackages = []
    else:
        all_packages = pkgutil.walk_packages(path, prefix=package.__name__ + ".",
                                             onerror=lambda r: None)
        subpackages = [qualified_name for importer, qualified_name, ispkg in all_packages]
    return ModuleProperties(name=name,
                            file=file,
                            path=path,
                            all=pkg_all,
                            is_c_module=is_c,
                            subpackages=subpackages)


</t>
<t tx="ekr.20220525082934.857">def worker(tasks: 'Queue[str]',
           results: 'Queue[Union[str, ModuleProperties]]',
           sys_path: List[str]) -&gt; None:
    """The main loop of a worker introspection process."""
    sys.path = sys_path
    while True:
        mod = tasks.get()
        try:
            prop = get_package_properties(mod)
        except InspectError as e:
            results.put(str(e))
            continue
        results.put(prop)


</t>
<t tx="ekr.20220525082934.858">class ModuleInspect:
    """Perform runtime introspection of modules in a separate process.

    Reuse the process for multiple modules for efficiency. However, if there is an
    error, retry using a fresh process to avoid cross-contamination of state between
    modules.

    We use a separate process to isolate us from many side effects. For example, the
    import of a module may kill the current process, and we want to recover from that.

    Always use in a with statement for proper clean-up:

      with ModuleInspect() as m:
          p = m.get_package_properties('urllib.parse')
    """

    @others
</t>
<t tx="ekr.20220525082934.859">def __init__(self) -&gt; None:
    self._start()

</t>
<t tx="ekr.20220525082934.86"># Try(stmt* body, excepthandler* handlers, stmt* orelse, stmt* finalbody)
def visit_Try(self, n: ast3.Try) -&gt; TryStmt:
    vs = [
        self.set_line(NameExpr(h.name), h) if h.name is not None else None for h in n.handlers
    ]
    types = [self.visit(h.type) for h in n.handlers]
    handlers = [self.as_required_block(h.body, h.lineno) for h in n.handlers]

    node = TryStmt(self.as_required_block(n.body, n.lineno),
                   vs,
                   types,
                   handlers,
                   self.as_block(n.orelse, n.lineno),
                   self.as_block(n.finalbody, n.lineno))
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.860">def _start(self) -&gt; None:
    self.tasks: Queue[str] = Queue()
    self.results: Queue[Union[ModuleProperties, str]] = Queue()
    self.proc = Process(target=worker, args=(self.tasks, self.results, sys.path))
    self.proc.start()
    self.counter = 0  # Number of successful roundtrips

</t>
<t tx="ekr.20220525082934.861">def close(self) -&gt; None:
    """Free any resources used."""
    self.proc.terminate()

</t>
<t tx="ekr.20220525082934.862">def get_package_properties(self, package_id: str) -&gt; ModuleProperties:
    """Return some properties of a module/package using runtime introspection.

    Raise InspectError if the target couldn't be imported.
    """
    self.tasks.put(package_id)
    res = self._get_from_queue()
    if res is None:
        # The process died; recover and report error.
        self._start()
        raise InspectError(f'Process died when importing {package_id!r}')
    if isinstance(res, str):
        # Error importing module
        if self.counter &gt; 0:
            # Also try with a fresh process. Maybe one of the previous imports has
            # corrupted some global state.
            self.close()
            self._start()
            return self.get_package_properties(package_id)
        raise InspectError(res)
    self.counter += 1
    return res

</t>
<t tx="ekr.20220525082934.863">def _get_from_queue(self) -&gt; Union[ModuleProperties, str, None]:
    """Get value from the queue.

    Return the value read from the queue, or None if the process unexpectedly died.
    """
    max_iter = 100
    n = 0
    while True:
        if n == max_iter:
            raise RuntimeError('Timeout waiting for subprocess')
        try:
            return self.results.get(timeout=0.05)
        except queue.Empty:
            if not self.proc.is_alive():
                return None
        n += 1

</t>
<t tx="ekr.20220525082934.864">def __enter__(self) -&gt; 'ModuleInspect':
    return self

</t>
<t tx="ekr.20220525082934.865">def __exit__(self, *args: object) -&gt; None:
    self.close()
</t>
<t tx="ekr.20220525082934.866">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from typing import Optional, Callable, List

from mypy.nodes import TypeInfo
from mypy.types import Instance
from mypy.typestate import TypeState


@others
</t>
<t tx="ekr.20220525082934.867">def calculate_mro(info: TypeInfo, obj_type: Optional[Callable[[], Instance]] = None) -&gt; None:
    """Calculate and set mro (method resolution order).

    Raise MroError if cannot determine mro.
    """
    mro = linearize_hierarchy(info, obj_type)
    assert mro, f"Could not produce a MRO at all for {info}"
    info.mro = mro
    # The property of falling back to Any is inherited.
    info.fallback_to_any = any(baseinfo.fallback_to_any for baseinfo in info.mro)
    TypeState.reset_all_subtype_caches_for(info)


</t>
<t tx="ekr.20220525082934.868">class MroError(Exception):
    """Raised if a consistent mro cannot be determined for a class."""


</t>
<t tx="ekr.20220525082934.869">def linearize_hierarchy(info: TypeInfo,
                        obj_type: Optional[Callable[[], Instance]] = None) -&gt; List[TypeInfo]:
    # TODO describe
    if info.mro:
        return info.mro
    bases = info.direct_base_classes()
    if (not bases and info.fullname != 'builtins.object' and
            obj_type is not None):
        # Second pass in import cycle, add a dummy `object` base class,
        # otherwise MRO calculation may spuriously fail.
        # MRO will be re-calculated for real in the third pass.
        bases = [obj_type().type]
    lin_bases = []
    for base in bases:
        assert base is not None, f"Cannot linearize bases for {info.fullname} {bases}"
        lin_bases.append(linearize_hierarchy(base, obj_type))
    lin_bases.append(bases)
    return [info] + merge(lin_bases)


</t>
<t tx="ekr.20220525082934.87"># Assert(expr test, expr? msg)
def visit_Assert(self, n: ast3.Assert) -&gt; AssertStmt:
    node = AssertStmt(self.visit(n.test), self.visit(n.msg))
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.870">def merge(seqs: List[List[TypeInfo]]) -&gt; List[TypeInfo]:
    seqs = [s[:] for s in seqs]
    result: List[TypeInfo] = []
    while True:
        seqs = [s for s in seqs if s]
        if not seqs:
            return result
        for seq in seqs:
            head = seq[0]
            if not [s for s in seqs if head in s[1:]]:
                break
        else:
            raise MroError()
        result.append(head)
        for s in seqs:
            if s[0] is head:
                del s[0]
</t>
<t tx="ekr.20220525082934.871">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Abstract syntax tree node classes (i.e. parse tree)."""

import os
from enum import Enum, unique
from abc import abstractmethod
from mypy.backports import OrderedDict
from collections import defaultdict
from typing import (
    Any, TypeVar, List, Tuple, cast, Set, Dict, Union, Optional, Callable, Sequence, Iterator
)
from typing_extensions import DefaultDict, Final, TYPE_CHECKING, TypeAlias as _TypeAlias
from mypy_extensions import trait

import mypy.strconv
from mypy.util import short_type
from mypy.visitor import NodeVisitor, StatementVisitor, ExpressionVisitor

from mypy.bogus_type import Bogus

if TYPE_CHECKING:
    from mypy.patterns import Pattern


@others
</t>
<t tx="ekr.20220525082934.872">class Context:
    """Base type for objects that are valid as error message locations."""
    __slots__ = ('line', 'column', 'end_line')

    @others
</t>
<t tx="ekr.20220525082934.873">def __init__(self, line: int = -1, column: int = -1) -&gt; None:
    self.line = line
    self.column = column
    self.end_line: Optional[int] = None

</t>
<t tx="ekr.20220525082934.874">def set_line(self,
             target: Union['Context', int],
             column: Optional[int] = None,
             end_line: Optional[int] = None) -&gt; None:
    """If target is a node, pull line (and column) information
    into this node. If column is specified, this will override any column
    information coming from a node.
    """
    if isinstance(target, int):
        self.line = target
    else:
        self.line = target.line
        self.column = target.column
        self.end_line = target.end_line

    if column is not None:
        self.column = column

    if end_line is not None:
        self.end_line = end_line

</t>
<t tx="ekr.20220525082934.875">def get_line(self) -&gt; int:
    """Don't use. Use x.line."""
    return self.line

</t>
<t tx="ekr.20220525082934.876">def get_column(self) -&gt; int:
    """Don't use. Use x.column."""
    return self.column


</t>
<t tx="ekr.20220525082934.877">if TYPE_CHECKING:
    # break import cycle only needed for mypy
    import mypy.types


T = TypeVar('T')

JsonDict: _TypeAlias = Dict[str, Any]


# Symbol table node kinds
#
# TODO rename to use more descriptive names

LDEF: Final = 0
GDEF: Final = 1
MDEF: Final = 2

# Placeholder for a name imported via 'from ... import'. Second phase of
# semantic will replace this the actual imported reference. This is
# needed so that we can detect whether a name has been imported during
# XXX what?
UNBOUND_IMPORTED: Final = 3

# RevealExpr node kinds
REVEAL_TYPE: Final = 0
REVEAL_LOCALS: Final = 1

LITERAL_YES: Final = 2
LITERAL_TYPE: Final = 1
LITERAL_NO: Final = 0

node_kinds: Final = {
    LDEF: 'Ldef',
    GDEF: 'Gdef',
    MDEF: 'Mdef',
    UNBOUND_IMPORTED: 'UnboundImported',
}
inverse_node_kinds: Final = {_kind: _name for _name, _kind in node_kinds.items()}


implicit_module_attrs: Final = {
    '__name__': '__builtins__.str',
    '__doc__': None,  # depends on Python version, see semanal.py
    '__path__': None,  # depends on if the module is a package
    '__file__': '__builtins__.str',
    '__package__': '__builtins__.str',
    '__annotations__': None,  # dict[str, Any] bounded in add_implicit_module_attrs()
}


# These aliases exist because built-in class objects are not subscriptable.
# For example `list[int]` fails at runtime. Instead List[int] should be used.
type_aliases: Final = {
    'typing.List': 'builtins.list',
    'typing.Dict': 'builtins.dict',
    'typing.Set': 'builtins.set',
    'typing.FrozenSet': 'builtins.frozenset',
    'typing.ChainMap': 'collections.ChainMap',
    'typing.Counter': 'collections.Counter',
    'typing.DefaultDict': 'collections.defaultdict',
    'typing.Deque': 'collections.deque',
    'typing.OrderedDict': 'collections.OrderedDict',
    # HACK: a lie in lieu of actual support for PEP 675
    'typing.LiteralString': 'builtins.str',
}

# This keeps track of the oldest supported Python version where the corresponding
# alias source is available.
type_aliases_source_versions: Final = {
    'typing.List': (2, 7),
    'typing.Dict': (2, 7),
    'typing.Set': (2, 7),
    'typing.FrozenSet': (2, 7),
    'typing.ChainMap': (3, 3),
    'typing.Counter': (2, 7),
    'typing.DefaultDict': (2, 7),
    'typing.Deque': (2, 7),
    'typing.OrderedDict': (3, 7),
    'typing.LiteralString': (3, 11),
}

# This keeps track of aliases in `typing_extensions`, which we treat specially.
typing_extensions_aliases: Final = {
    # See: https://github.com/python/mypy/issues/11528
    'typing_extensions.OrderedDict': 'collections.OrderedDict',
    # HACK: a lie in lieu of actual support for PEP 675
    'typing_extensions.LiteralString': 'builtins.str',
}

reverse_builtin_aliases: Final = {
    'builtins.list': 'typing.List',
    'builtins.dict': 'typing.Dict',
    'builtins.set': 'typing.Set',
    'builtins.frozenset': 'typing.FrozenSet',
}

_nongen_builtins: Final = {"builtins.tuple": "typing.Tuple", "builtins.enumerate": ""}
_nongen_builtins.update((name, alias) for alias, name in type_aliases.items())
# Drop OrderedDict from this for backward compatibility
del _nongen_builtins['collections.OrderedDict']
# HACK: consequence of hackily treating LiteralString as an alias for str
del _nongen_builtins['builtins.str']


</t>
<t tx="ekr.20220525082934.878">def get_nongen_builtins(python_version: Tuple[int, int]) -&gt; Dict[str, str]:
    # After 3.9 with pep585 generic builtins are allowed.
    return _nongen_builtins if python_version &lt; (3, 9) else {}


</t>
<t tx="ekr.20220525082934.879">RUNTIME_PROTOCOL_DECOS: Final = (
    "typing.runtime_checkable",
    "typing_extensions.runtime",
    "typing_extensions.runtime_checkable",
)


</t>
<t tx="ekr.20220525082934.88"># Import(alias* names)
def visit_Import(self, n: ast3.Import) -&gt; Import:
    names: List[Tuple[str, Optional[str]]] = []
    for alias in n.names:
        name = self.translate_module_id(alias.name)
        asname = alias.asname
        if asname is None and name != alias.name:
            # if the module name has been translated (and it's not already
            # an explicit import-as), make it an implicit import-as the
            # original name
            asname = alias.name
        names.append((name, asname))
    i = Import(names)
    self.imports.append(i)
    return self.set_line(i, n)

</t>
<t tx="ekr.20220525082934.880">class Node(Context):
    """Common base class for all non-type parse tree nodes."""

    __slots__ = ()

    @others
</t>
<t tx="ekr.20220525082934.881">def __str__(self) -&gt; str:
    ans = self.accept(mypy.strconv.StrConv())
    if ans is None:
        return repr(self)
    return ans

</t>
<t tx="ekr.20220525082934.882">def accept(self, visitor: NodeVisitor[T]) -&gt; T:
    raise RuntimeError('Not implemented')


</t>
<t tx="ekr.20220525082934.883">@trait
class Statement(Node):
    """A statement node."""

    __slots__ = ()

    @others
</t>
<t tx="ekr.20220525082934.884">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    raise RuntimeError('Not implemented')


</t>
<t tx="ekr.20220525082934.885">@trait
class Expression(Node):
    """An expression node."""

    __slots__ = ()

    @others
</t>
<t tx="ekr.20220525082934.886">def accept(self, visitor: ExpressionVisitor[T]) -&gt; T:
    raise RuntimeError('Not implemented')


</t>
<t tx="ekr.20220525082934.887">class FakeExpression(Expression):
    """A dummy expression.

    We need a dummy expression in one place, and can't instantiate Expression
    because it is a trait and mypyc barfs.
    """

    __slots__ = ()


</t>
<t tx="ekr.20220525082934.888"># TODO:
# Lvalue = Union['NameExpr', 'MemberExpr', 'IndexExpr', 'SuperExpr', 'StarExpr'
#                'TupleExpr']; see #1783.
Lvalue: _TypeAlias = Expression


</t>
<t tx="ekr.20220525082934.889">@trait
class SymbolNode(Node):
    """Nodes that can be stored in a symbol table."""

    __slots__ = ()

    @others
</t>
<t tx="ekr.20220525082934.89"># ImportFrom(identifier? module, alias* names, int? level)
def visit_ImportFrom(self, n: ast3.ImportFrom) -&gt; ImportBase:
    assert n.level is not None
    if len(n.names) == 1 and n.names[0].name == '*':
        mod = n.module if n.module is not None else ''
        i: ImportBase = ImportAll(mod, n.level)
    else:
        i = ImportFrom(self.translate_module_id(n.module) if n.module is not None else '',
                       n.level,
                       [(a.name, a.asname) for a in n.names])
    self.imports.append(i)
    return self.set_line(i, n)

</t>
<t tx="ekr.20220525082934.890">@property
@abstractmethod
def name(self) -&gt; str: pass

</t>
<t tx="ekr.20220525082934.891"># fullname can often be None even though the type system
# disagrees. We mark this with Bogus to let mypyc know not to
# worry about it.
@property
@abstractmethod
def fullname(self) -&gt; Bogus[str]: pass

</t>
<t tx="ekr.20220525082934.892">@abstractmethod
def serialize(self) -&gt; JsonDict: pass

</t>
<t tx="ekr.20220525082934.893">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'SymbolNode':
    classname = data['.class']
    method = deserialize_map.get(classname)
    if method is not None:
        return method(data)
    raise NotImplementedError(f'unexpected .class {classname}')


</t>
<t tx="ekr.20220525082934.894"># Items: fullname, related symbol table node, surrounding type (if any)
Definition: _TypeAlias = Tuple[str, 'SymbolTableNode', Optional['TypeInfo']]


</t>
<t tx="ekr.20220525082934.895">class MypyFile(SymbolNode):
    """The abstract syntax tree of a single source file."""

    __slots__ = ('_fullname', 'path', 'defs', 'alias_deps',
                 'is_bom', 'names', 'imports', 'ignored_lines', 'is_stub',
                 'is_cache_skeleton', 'is_partial_stub_package', 'plugin_deps',
                 'future_import_flags')

    # Fully qualified module name
    _fullname: Bogus[str]
    # Path to the file (empty string if not known)
    path: str
    # Top-level definitions and statements
    defs: List[Statement]
    # Type alias dependencies as mapping from target to set of alias full names
    alias_deps: DefaultDict[str, Set[str]]
    # Is there a UTF-8 BOM at the start?
    is_bom: bool
    names: "SymbolTable"
    # All import nodes within the file (also ones within functions etc.)
    imports: List["ImportBase"]
    # Lines on which to ignore certain errors when checking.
    # If the value is empty, ignore all errors; otherwise, the list contains all
    # error codes to ignore.
    ignored_lines: Dict[int, List[str]]
    # Is this file represented by a stub file (.pyi)?
    is_stub: bool
    # Is this loaded from the cache and thus missing the actual body of the file?
    is_cache_skeleton: bool
    # Does this represent an __init__.pyi stub with a module __getattr__
    # (i.e. a partial stub package), for such packages we suppress any missing
    # module errors in addition to missing attribute errors.
    is_partial_stub_package: bool
    # Plugin-created dependencies
    plugin_deps: Dict[str, Set[str]]
    # Future imports defined in this file. Populated during semantic analysis.
    future_import_flags: Set[str]

    @others
</t>
<t tx="ekr.20220525082934.896">def __init__(self,
             defs: List[Statement],
             imports: List['ImportBase'],
             is_bom: bool = False,
             ignored_lines: Optional[Dict[int, List[str]]] = None) -&gt; None:
    super().__init__()
    self.defs = defs
    self.line = 1  # Dummy line number
    self.imports = imports
    self.is_bom = is_bom
    self.alias_deps = defaultdict(set)
    self.plugin_deps = {}
    if ignored_lines:
        self.ignored_lines = ignored_lines
    else:
        self.ignored_lines = {}

    self.path = ''
    self.is_stub = False
    self.is_cache_skeleton = False
    self.is_partial_stub_package = False
    self.future_import_flags = set()

</t>
<t tx="ekr.20220525082934.897">def local_definitions(self) -&gt; Iterator[Definition]:
    """Return all definitions within the module (including nested).

    This doesn't include imported definitions.
    """
    return local_definitions(self.names, self.fullname)

</t>
<t tx="ekr.20220525082934.898">@property
def name(self) -&gt; str:
    return '' if not self._fullname else self._fullname.split('.')[-1]

</t>
<t tx="ekr.20220525082934.899">@property
def fullname(self) -&gt; Bogus[str]:
    return self._fullname

</t>
<t tx="ekr.20220525082934.9">def visit_none_type(self, t: NoneType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082934.90"># Global(identifier* names)
def visit_Global(self, n: ast3.Global) -&gt; GlobalDecl:
    g = GlobalDecl(n.names)
    return self.set_line(g, n)

</t>
<t tx="ekr.20220525082934.900">def accept(self, visitor: NodeVisitor[T]) -&gt; T:
    return visitor.visit_mypy_file(self)

</t>
<t tx="ekr.20220525082934.901">def is_package_init_file(self) -&gt; bool:
    return len(self.path) != 0 and os.path.basename(self.path).startswith('__init__.')

</t>
<t tx="ekr.20220525082934.902">def is_future_flag_set(self, flag: str) -&gt; bool:
    return flag in self.future_import_flags

</t>
<t tx="ekr.20220525082934.903">def serialize(self) -&gt; JsonDict:
    return {'.class': 'MypyFile',
            '_fullname': self._fullname,
            'names': self.names.serialize(self._fullname),
            'is_stub': self.is_stub,
            'path': self.path,
            'is_partial_stub_package': self.is_partial_stub_package,
            'future_import_flags': list(self.future_import_flags),
            }

</t>
<t tx="ekr.20220525082934.904">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'MypyFile':
    assert data['.class'] == 'MypyFile', data
    tree = MypyFile([], [])
    tree._fullname = data['_fullname']
    tree.names = SymbolTable.deserialize(data['names'])
    tree.is_stub = data['is_stub']
    tree.path = data['path']
    tree.is_partial_stub_package = data['is_partial_stub_package']
    tree.is_cache_skeleton = True
    tree.future_import_flags = set(data['future_import_flags'])
    return tree


</t>
<t tx="ekr.20220525082934.905">class ImportBase(Statement):
    """Base class for all import statements."""

    __slots__ = ('is_unreachable', 'is_top_level', 'is_mypy_only', 'assignments')

    is_unreachable: bool  # Set by semanal.SemanticAnalyzerPass1 if inside `if False` etc.
    is_top_level: bool  # Ditto if outside any class or def
    is_mypy_only: bool  # Ditto if inside `if TYPE_CHECKING` or `if MYPY`

    # If an import replaces existing definitions, we construct dummy assignment
    # statements that assign the imported names to the names in the current scope,
    # for type checking purposes. Example:
    #
    #     x = 1
    #     from m import x   &lt;-- add assignment representing "x = m.x"
    assignments: List["AssignmentStmt"]

    @others
</t>
<t tx="ekr.20220525082934.906">def __init__(self) -&gt; None:
    super().__init__()
    self.assignments = []
    self.is_unreachable = False
    self.is_top_level = False
    self.is_mypy_only = False


</t>
<t tx="ekr.20220525082934.907">class Import(ImportBase):
    """import m [as n]"""

    __slots__ = ('ids',)

    ids: List[Tuple[str, Optional[str]]]  # (module id, as id)

    @others
</t>
<t tx="ekr.20220525082934.908">def __init__(self, ids: List[Tuple[str, Optional[str]]]) -&gt; None:
    super().__init__()
    self.ids = ids

</t>
<t tx="ekr.20220525082934.909">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_import(self)


</t>
<t tx="ekr.20220525082934.91"># Nonlocal(identifier* names)
def visit_Nonlocal(self, n: ast3.Nonlocal) -&gt; NonlocalDecl:
    d = NonlocalDecl(n.names)
    return self.set_line(d, n)

</t>
<t tx="ekr.20220525082934.910">class ImportFrom(ImportBase):
    """from m import x [as y], ..."""

    __slots__ = ('id', 'names', 'relative')

    id: str
    relative: int
    names: List[Tuple[str, Optional[str]]]  # Tuples (name, as name)

    @others
</t>
<t tx="ekr.20220525082934.911">def __init__(self, id: str, relative: int, names: List[Tuple[str, Optional[str]]]) -&gt; None:
    super().__init__()
    self.id = id
    self.names = names
    self.relative = relative

</t>
<t tx="ekr.20220525082934.912">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_import_from(self)


</t>
<t tx="ekr.20220525082934.913">class ImportAll(ImportBase):
    """from m import *"""

    __slots__ = ('id', 'relative', 'imported_names')

    id: str
    relative: int
    # NOTE: Only filled and used by old semantic analyzer.
    imported_names: List[str]

    @others
</t>
<t tx="ekr.20220525082934.914">def __init__(self, id: str, relative: int) -&gt; None:
    super().__init__()
    self.id = id
    self.relative = relative
    self.imported_names = []

</t>
<t tx="ekr.20220525082934.915">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_import_all(self)


</t>
<t tx="ekr.20220525082934.916">class ImportedName(SymbolNode):
    """Indirect reference to a fullname stored in symbol table.

    This node is not present in the original program as such. This is
    just a temporary artifact in binding imported names. After semantic
    analysis pass 2, these references should be replaced with direct
    reference to a real AST node.

    Note that this is neither a Statement nor an Expression so this
    can't be visited.
    """

    __slots__ = ('target_fullname',)

    @others
</t>
<t tx="ekr.20220525082934.917">def __init__(self, target_fullname: str) -&gt; None:
    super().__init__()
    self.target_fullname = target_fullname

</t>
<t tx="ekr.20220525082934.918">@property
def name(self) -&gt; str:
    return self.target_fullname.split('.')[-1]

</t>
<t tx="ekr.20220525082934.919">@property
def fullname(self) -&gt; str:
    return self.target_fullname

</t>
<t tx="ekr.20220525082934.92"># Expr(expr value)
def visit_Expr(self, n: ast3.Expr) -&gt; ExpressionStmt:
    value = self.visit(n.value)
    node = ExpressionStmt(value)
    return self.set_line(node, n)

</t>
<t tx="ekr.20220525082934.920">def serialize(self) -&gt; JsonDict:
    assert False, "ImportedName leaked from semantic analysis"

</t>
<t tx="ekr.20220525082934.921">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'ImportedName':
    assert False, "ImportedName should never be serialized"

</t>
<t tx="ekr.20220525082934.922">def __str__(self) -&gt; str:
    return f'ImportedName({self.target_fullname})'


</t>
<t tx="ekr.20220525082934.923">FUNCBASE_FLAGS: Final = ["is_property", "is_class", "is_static", "is_final"]


</t>
<t tx="ekr.20220525082934.924">class FuncBase(Node):
    """Abstract base class for function-like nodes.

    N.B: Although this has SymbolNode subclasses (FuncDef,
    OverloadedFuncDef), avoid calling isinstance(..., FuncBase) on
    something that is typed as SymbolNode.  This is to work around
    mypy bug #3603, in which mypy doesn't understand multiple
    inheritance very well, and will assume that a SymbolNode
    cannot be a FuncBase.

    Instead, test against SYMBOL_FUNCBASE_TYPES, which enumerates
    SymbolNode subclasses that are also FuncBase subclasses.
    """

    __slots__ = ('type',
                 'unanalyzed_type',
                 'info',
                 'is_property',
                 'is_class',        # Uses "@classmethod" (explicit or implicit)
                 'is_static',       # Uses "@staticmethod"
                 'is_final',        # Uses "@final"
                 '_fullname',
                 )

    @others
</t>
<t tx="ekr.20220525082934.925">def __init__(self) -&gt; None:
    super().__init__()
    # Type signature. This is usually CallableType or Overloaded, but it can be
    # something else for decorated functions.
    self.type: Optional[mypy.types.ProperType] = None
    # Original, not semantically analyzed type (used for reprocessing)
    self.unanalyzed_type: Optional[mypy.types.ProperType] = None
    # If method, reference to TypeInfo
    # TODO: Type should be Optional[TypeInfo]
    self.info = FUNC_NO_INFO
    self.is_property = False
    self.is_class = False
    self.is_static = False
    self.is_final = False
    # Name with module prefix
    # TODO: Type should be Optional[str]
    self._fullname = cast(Bogus[str], None)

</t>
<t tx="ekr.20220525082934.926">@property
@abstractmethod
def name(self) -&gt; str: pass

</t>
<t tx="ekr.20220525082934.927">@property
def fullname(self) -&gt; Bogus[str]:
    return self._fullname


</t>
<t tx="ekr.20220525082934.928">OverloadPart: _TypeAlias = Union['FuncDef', 'Decorator']


</t>
<t tx="ekr.20220525082934.929">class OverloadedFuncDef(FuncBase, SymbolNode, Statement):
    """A logical node representing all the variants of a multi-declaration function.

    A multi-declaration function is often an @overload, but can also be a
    @property with a setter and a/or a deleter.

    This node has no explicit representation in the source program.
    Overloaded variants must be consecutive in the source file.
    """

    __slots__ = ('items', 'unanalyzed_items', 'impl')

    items: List[OverloadPart]
    unanalyzed_items: List[OverloadPart]
    impl: Optional[OverloadPart]

    @others
</t>
<t tx="ekr.20220525082934.93"># Pass
def visit_Pass(self, n: ast3.Pass) -&gt; PassStmt:
    s = PassStmt()
    return self.set_line(s, n)

</t>
<t tx="ekr.20220525082934.930">def __init__(self, items: List['OverloadPart']) -&gt; None:
    super().__init__()
    self.items = items
    self.unanalyzed_items = items.copy()
    self.impl = None
    if len(items) &gt; 0:
        self.set_line(items[0].line, items[0].column)
    self.is_final = False

</t>
<t tx="ekr.20220525082934.931">@property
def name(self) -&gt; str:
    if self.items:
        return self.items[0].name
    else:
        # This may happen for malformed overload
        assert self.impl is not None
        return self.impl.name

</t>
<t tx="ekr.20220525082934.932">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_overloaded_func_def(self)

</t>
<t tx="ekr.20220525082934.933">def serialize(self) -&gt; JsonDict:
    return {'.class': 'OverloadedFuncDef',
            'items': [i.serialize() for i in self.items],
            'type': None if self.type is None else self.type.serialize(),
            'fullname': self._fullname,
            'impl': None if self.impl is None else self.impl.serialize(),
            'flags': get_flags(self, FUNCBASE_FLAGS),
            }

</t>
<t tx="ekr.20220525082934.934">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'OverloadedFuncDef':
    assert data['.class'] == 'OverloadedFuncDef'
    res = OverloadedFuncDef([
        cast(OverloadPart, SymbolNode.deserialize(d))
        for d in data['items']])
    if data.get('impl') is not None:
        res.impl = cast(OverloadPart, SymbolNode.deserialize(data['impl']))
        # set line for empty overload items, as not set in __init__
        if len(res.items) &gt; 0:
            res.set_line(res.impl.line)
    if data.get('type') is not None:
        typ = mypy.types.deserialize_type(data['type'])
        assert isinstance(typ, mypy.types.ProperType)
        res.type = typ
    res._fullname = data['fullname']
    set_flags(res, data['flags'])
    # NOTE: res.info will be set in the fixup phase.
    return res


</t>
<t tx="ekr.20220525082934.935">class Argument(Node):
    """A single argument in a FuncItem."""

    __slots__ = ('variable', 'type_annotation', 'initializer', 'kind', 'pos_only')

    @others
</t>
<t tx="ekr.20220525082934.936">def __init__(self,
             variable: 'Var',
             type_annotation: 'Optional[mypy.types.Type]',
             initializer: Optional[Expression],
             kind: 'ArgKind',
             pos_only: bool = False) -&gt; None:
    super().__init__()
    self.variable = variable
    self.type_annotation = type_annotation
    self.initializer = initializer
    self.kind = kind  # must be an ARG_* constant
    self.pos_only = pos_only

</t>
<t tx="ekr.20220525082934.937">def set_line(self,
             target: Union[Context, int],
             column: Optional[int] = None,
             end_line: Optional[int] = None) -&gt; None:
    super().set_line(target, column, end_line)

    if self.initializer and self.initializer.line &lt; 0:
        self.initializer.set_line(self.line, self.column, self.end_line)

    self.variable.set_line(self.line, self.column, self.end_line)


</t>
<t tx="ekr.20220525082934.938">FUNCITEM_FLAGS: Final = FUNCBASE_FLAGS + [
    'is_overload', 'is_generator', 'is_coroutine', 'is_async_generator',
    'is_awaitable_coroutine',
]


</t>
<t tx="ekr.20220525082934.939">class FuncItem(FuncBase):
    """Base class for nodes usable as overloaded function items."""

    __slots__ = ('arguments',  # Note that can be None if deserialized (type is a lie!)
                 'arg_names',  # Names of arguments
                 'arg_kinds',  # Kinds of arguments
                 'min_args',  # Minimum number of arguments
                 'max_pos',  # Maximum number of positional arguments, -1 if no explicit
                             # limit (*args not included)
                 'body',  # Body of the function
                 'is_overload',  # Is this an overload variant of function with more than
                                 # one overload variant?
                 'is_generator',  # Contains a yield statement?
                 'is_coroutine',  # Defined using 'async def' syntax?
                 'is_async_generator',  # Is an async def generator?
                 'is_awaitable_coroutine',  # Decorated with '@{typing,asyncio}.coroutine'?
                 'expanded',  # Variants of function with type variables with values expanded
                 )

    __deletable__ = ('arguments', 'max_pos', 'min_args')

    @others
</t>
<t tx="ekr.20220525082934.94"># Break
def visit_Break(self, n: ast3.Break) -&gt; BreakStmt:
    s = BreakStmt()
    return self.set_line(s, n)

</t>
<t tx="ekr.20220525082934.940">def __init__(self,
             arguments: Optional[List[Argument]] = None,
             body: Optional['Block'] = None,
             typ: 'Optional[mypy.types.FunctionLike]' = None) -&gt; None:
    super().__init__()
    self.arguments = arguments or []
    self.arg_names = [None if arg.pos_only else arg.variable.name for arg in self.arguments]
    self.arg_kinds: List[ArgKind] = [arg.kind for arg in self.arguments]
    self.max_pos: int = (
        self.arg_kinds.count(ARG_POS) + self.arg_kinds.count(ARG_OPT))
    self.body: 'Block' = body or Block([])
    self.type = typ
    self.unanalyzed_type = typ
    self.is_overload: bool = False
    self.is_generator: bool = False
    self.is_coroutine: bool = False
    self.is_async_generator: bool = False
    self.is_awaitable_coroutine: bool = False
    self.expanded: List[FuncItem] = []

    self.min_args = 0
    for i in range(len(self.arguments)):
        if self.arguments[i] is None and i &lt; self.max_fixed_argc():
            self.min_args = i + 1

</t>
<t tx="ekr.20220525082934.941">def max_fixed_argc(self) -&gt; int:
    return self.max_pos

</t>
<t tx="ekr.20220525082934.942">def set_line(self,
             target: Union[Context, int],
             column: Optional[int] = None,
             end_line: Optional[int] = None) -&gt; None:
    super().set_line(target, column, end_line)
    for arg in self.arguments:
        arg.set_line(self.line, self.column, self.end_line)

</t>
<t tx="ekr.20220525082934.943">def is_dynamic(self) -&gt; bool:
    return self.type is None


</t>
<t tx="ekr.20220525082934.944">FUNCDEF_FLAGS: Final = FUNCITEM_FLAGS + [
    'is_decorated', 'is_conditional', 'is_abstract',
]


</t>
<t tx="ekr.20220525082934.945">class FuncDef(FuncItem, SymbolNode, Statement):
    """Function definition.

    This is a non-lambda function defined using 'def'.
    """

    __slots__ = ('_name',
                 'is_decorated',
                 'is_conditional',
                 'is_abstract',
                 'original_def',
                 )

    @others
</t>
<t tx="ekr.20220525082934.946"># Note that all __init__ args must have default values
def __init__(self,
             name: str = '',              # Function name
             arguments: Optional[List[Argument]] = None,
             body: Optional['Block'] = None,
             typ: 'Optional[mypy.types.FunctionLike]' = None) -&gt; None:
    super().__init__(arguments, body, typ)
    self._name = name
    self.is_decorated = False
    self.is_conditional = False  # Defined conditionally (within block)?
    self.is_abstract = False
    self.is_final = False
    # Original conditional definition
    self.original_def: Union[None, FuncDef, Var, Decorator] = None

</t>
<t tx="ekr.20220525082934.947">@property
def name(self) -&gt; str:
    return self._name

</t>
<t tx="ekr.20220525082934.948">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_func_def(self)

</t>
<t tx="ekr.20220525082934.949">def serialize(self) -&gt; JsonDict:
    # We're deliberating omitting arguments and storing only arg_names and
    # arg_kinds for space-saving reasons (arguments is not used in later
    # stages of mypy).
    # TODO: After a FuncDef is deserialized, the only time we use `arg_names`
    # and `arg_kinds` is when `type` is None and we need to infer a type. Can
    # we store the inferred type ahead of time?
    return {'.class': 'FuncDef',
            'name': self._name,
            'fullname': self._fullname,
            'arg_names': self.arg_names,
            'arg_kinds': [int(x.value) for x in self.arg_kinds],
            'type': None if self.type is None else self.type.serialize(),
            'flags': get_flags(self, FUNCDEF_FLAGS),
            # TODO: Do we need expanded, original_def?
            }

</t>
<t tx="ekr.20220525082934.95"># Continue
def visit_Continue(self, n: ast3.Continue) -&gt; ContinueStmt:
    s = ContinueStmt()
    return self.set_line(s, n)

</t>
<t tx="ekr.20220525082934.950">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'FuncDef':
    assert data['.class'] == 'FuncDef'
    body = Block([])
    ret = FuncDef(data['name'],
                  [],
                  body,
                  (None if data['type'] is None
                   else cast(mypy.types.FunctionLike,
                             mypy.types.deserialize_type(data['type']))))
    ret._fullname = data['fullname']
    set_flags(ret, data['flags'])
    # NOTE: ret.info is set in the fixup phase.
    ret.arg_names = data['arg_names']
    ret.arg_kinds = [ArgKind(x) for x in data['arg_kinds']]
    # Leave these uninitialized so that future uses will trigger an error
    del ret.arguments
    del ret.max_pos
    del ret.min_args
    return ret


</t>
<t tx="ekr.20220525082934.951"># All types that are both SymbolNodes and FuncBases. See the FuncBase
# docstring for the rationale.
SYMBOL_FUNCBASE_TYPES = (OverloadedFuncDef, FuncDef)


</t>
<t tx="ekr.20220525082934.952">class Decorator(SymbolNode, Statement):
    """A decorated function.

    A single Decorator object can include any number of function decorators.
    """

    __slots__ = ('func', 'decorators', 'original_decorators', 'var', 'is_overload')

    func: FuncDef  # Decorated function
    decorators: List[Expression]  # Decorators (may be empty)
    # Some decorators are removed by semanal, keep the original here.
    original_decorators: List[Expression]
    # TODO: This is mostly used for the type; consider replacing with a 'type' attribute
    var: "Var"  # Represents the decorated function obj
    is_overload: bool

    @others
</t>
<t tx="ekr.20220525082934.953">def __init__(self, func: FuncDef, decorators: List[Expression],
             var: 'Var') -&gt; None:
    super().__init__()
    self.func = func
    self.decorators = decorators
    self.original_decorators = decorators.copy()
    self.var = var
    self.is_overload = False

</t>
<t tx="ekr.20220525082934.954">@property
def name(self) -&gt; str:
    return self.func.name

</t>
<t tx="ekr.20220525082934.955">@property
def fullname(self) -&gt; Bogus[str]:
    return self.func.fullname

</t>
<t tx="ekr.20220525082934.956">@property
def is_final(self) -&gt; bool:
    return self.func.is_final

</t>
<t tx="ekr.20220525082934.957">@property
def info(self) -&gt; 'TypeInfo':
    return self.func.info

</t>
<t tx="ekr.20220525082934.958">@property
def type(self) -&gt; 'Optional[mypy.types.Type]':
    return self.var.type

</t>
<t tx="ekr.20220525082934.959">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_decorator(self)

</t>
<t tx="ekr.20220525082934.96"># --- expr ---

</t>
<t tx="ekr.20220525082934.960">def serialize(self) -&gt; JsonDict:
    return {'.class': 'Decorator',
            'func': self.func.serialize(),
            'var': self.var.serialize(),
            'is_overload': self.is_overload,
            }

</t>
<t tx="ekr.20220525082934.961">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'Decorator':
    assert data['.class'] == 'Decorator'
    dec = Decorator(FuncDef.deserialize(data['func']),
                    [],
                    Var.deserialize(data['var']))
    dec.is_overload = data['is_overload']
    return dec


</t>
<t tx="ekr.20220525082934.962">VAR_FLAGS: Final = [
    'is_self', 'is_initialized_in_class', 'is_staticmethod',
    'is_classmethod', 'is_property', 'is_settable_property', 'is_suppressed_import',
    'is_classvar', 'is_abstract_var', 'is_final', 'final_unset_in_class', 'final_set_in_init',
    'explicit_self_type', 'is_ready', 'from_module_getattr',
    'has_explicit_value', 'allow_incompatible_override',
]


</t>
<t tx="ekr.20220525082934.963">class Var(SymbolNode):
    """A variable.

    It can refer to global/local variable or a data attribute.
    """

    __slots__ = ('_name',
                 '_fullname',
                 'info',
                 'type',
                 'final_value',
                 'is_self',
                 'is_ready',
                 'is_inferred',
                 'is_initialized_in_class',
                 'is_staticmethod',
                 'is_classmethod',
                 'is_property',
                 'is_settable_property',
                 'is_classvar',
                 'is_abstract_var',
                 'is_final',
                 'final_unset_in_class',
                 'final_set_in_init',
                 'is_suppressed_import',
                 'explicit_self_type',
                 'from_module_getattr',
                 'has_explicit_value',
                 'allow_incompatible_override',
                 )

    @others
</t>
<t tx="ekr.20220525082934.964">def __init__(self, name: str, type: 'Optional[mypy.types.Type]' = None) -&gt; None:
    super().__init__()
    self._name = name   # Name without module prefix
    # TODO: Should be Optional[str]
    self._fullname = cast('Bogus[str]', None)  # Name with module prefix
    # TODO: Should be Optional[TypeInfo]
    self.info = VAR_NO_INFO
    self.type: Optional[mypy.types.Type] = type  # Declared or inferred type, or None
    # Is this the first argument to an ordinary method (usually "self")?
    self.is_self = False
    self.is_ready = True  # If inferred, is the inferred type available?
    self.is_inferred = (self.type is None)
    # Is this initialized explicitly to a non-None value in class body?
    self.is_initialized_in_class = False
    self.is_staticmethod = False
    self.is_classmethod = False
    self.is_property = False
    self.is_settable_property = False
    self.is_classvar = False
    self.is_abstract_var = False
    # Set to true when this variable refers to a module we were unable to
    # parse for some reason (eg a silenced module)
    self.is_suppressed_import = False
    # Was this "variable" (rather a constant) defined as Final[...]?
    self.is_final = False
    # If constant value is a simple literal,
    # store the literal value (unboxed) for the benefit of
    # tools like mypyc.
    self.final_value: Optional[Union[int, float, bool, str]] = None
    # Where the value was set (only for class attributes)
    self.final_unset_in_class = False
    self.final_set_in_init = False
    # This is True for a variable that was declared on self with an explicit type:
    #     class C:
    #         def __init__(self) -&gt; None:
    #             self.x: int
    # This case is important because this defines a new Var, even if there is one
    # present in a superclass (without explicit type this doesn't create a new Var).
    # See SemanticAnalyzer.analyze_member_lvalue() for details.
    self.explicit_self_type = False
    # If True, this is an implicit Var created due to module-level __getattr__.
    self.from_module_getattr = False
    # Var can be created with an explicit value `a = 1` or without one `a: int`,
    # we need a way to tell which one is which.
    self.has_explicit_value = False
    # If True, subclasses can override this with an incompatible type.
    self.allow_incompatible_override = False

</t>
<t tx="ekr.20220525082934.965">@property
def name(self) -&gt; str:
    return self._name

</t>
<t tx="ekr.20220525082934.966">@property
def fullname(self) -&gt; Bogus[str]:
    return self._fullname

</t>
<t tx="ekr.20220525082934.967">def accept(self, visitor: NodeVisitor[T]) -&gt; T:
    return visitor.visit_var(self)

</t>
<t tx="ekr.20220525082934.968">def serialize(self) -&gt; JsonDict:
    # TODO: Leave default values out?
    # NOTE: Sometimes self.is_ready is False here, but we don't care.
    data: JsonDict = {
        ".class": "Var",
        "name": self._name,
        "fullname": self._fullname,
        "type": None if self.type is None else self.type.serialize(),
        "flags": get_flags(self, VAR_FLAGS),
    }
    if self.final_value is not None:
        data['final_value'] = self.final_value
    return data

</t>
<t tx="ekr.20220525082934.969">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'Var':
    assert data['.class'] == 'Var'
    name = data['name']
    type = None if data['type'] is None else mypy.types.deserialize_type(data['type'])
    v = Var(name, type)
    v.is_ready = False  # Override True default set in __init__
    v._fullname = data['fullname']
    set_flags(v, data['flags'])
    v.final_value = data.get('final_value')
    return v


</t>
<t tx="ekr.20220525082934.97">def visit_NamedExpr(self, n: NamedExpr) -&gt; AssignmentExpr:
    s = AssignmentExpr(self.visit(n.target), self.visit(n.value))
    return self.set_line(s, n)

</t>
<t tx="ekr.20220525082934.970">class ClassDef(Statement):
    """Class definition"""

    __slots__ = ('name', 'fullname', 'defs', 'type_vars', 'base_type_exprs',
                 'removed_base_type_exprs', 'info', 'metaclass', 'decorators',
                 'keywords', 'analyzed', 'has_incompatible_baseclass')

    name: str  # Name of the class without module prefix
    fullname: Bogus[str]  # Fully qualified name of the class
    defs: "Block"
    type_vars: List["mypy.types.TypeVarLikeType"]
    # Base class expressions (not semantically analyzed -- can be arbitrary expressions)
    base_type_exprs: List[Expression]
    # Special base classes like Generic[...] get moved here during semantic analysis
    removed_base_type_exprs: List[Expression]
    info: "TypeInfo"  # Related TypeInfo
    metaclass: Optional[Expression]
    decorators: List[Expression]
    keywords: "OrderedDict[str, Expression]"
    analyzed: Optional[Expression]
    has_incompatible_baseclass: bool

    @others
</t>
<t tx="ekr.20220525082934.971">def __init__(self,
             name: str,
             defs: 'Block',
             type_vars: Optional[List['mypy.types.TypeVarLikeType']] = None,
             base_type_exprs: Optional[List[Expression]] = None,
             metaclass: Optional[Expression] = None,
             keywords: Optional[List[Tuple[str, Expression]]] = None) -&gt; None:
    super().__init__()
    self.name = name
    self.fullname = None  # type: ignore
    self.defs = defs
    self.type_vars = type_vars or []
    self.base_type_exprs = base_type_exprs or []
    self.removed_base_type_exprs = []
    self.info = CLASSDEF_NO_INFO
    self.metaclass = metaclass
    self.decorators = []
    self.keywords = OrderedDict(keywords or [])
    self.analyzed = None
    self.has_incompatible_baseclass = False

</t>
<t tx="ekr.20220525082934.972">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_class_def(self)

</t>
<t tx="ekr.20220525082934.973">def is_generic(self) -&gt; bool:
    return self.info.is_generic()

</t>
<t tx="ekr.20220525082934.974">def serialize(self) -&gt; JsonDict:
    # Not serialized: defs, base_type_exprs, metaclass, decorators,
    # analyzed (for named tuples etc.)
    return {'.class': 'ClassDef',
            'name': self.name,
            'fullname': self.fullname,
            'type_vars': [v.serialize() for v in self.type_vars],
            }

</t>
<t tx="ekr.20220525082934.975">@classmethod
def deserialize(self, data: JsonDict) -&gt; 'ClassDef':
    assert data['.class'] == 'ClassDef'
    res = ClassDef(data['name'],
                   Block([]),
                   # https://github.com/python/mypy/issues/12257
                   [cast(mypy.types.TypeVarLikeType, mypy.types.deserialize_type(v))
                    for v in data['type_vars']],
                   )
    res.fullname = data['fullname']
    return res


</t>
<t tx="ekr.20220525082934.976">class GlobalDecl(Statement):
    """Declaration global x, y, ..."""

    __slots__ = ('names',)

    names: List[str]

    @others
</t>
<t tx="ekr.20220525082934.977">def __init__(self, names: List[str]) -&gt; None:
    super().__init__()
    self.names = names

</t>
<t tx="ekr.20220525082934.978">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_global_decl(self)


</t>
<t tx="ekr.20220525082934.979">class NonlocalDecl(Statement):
    """Declaration nonlocal x, y, ..."""

    __slots__ = ('names',)

    names: List[str]

    @others
</t>
<t tx="ekr.20220525082934.98"># BoolOp(boolop op, expr* values)
def visit_BoolOp(self, n: ast3.BoolOp) -&gt; OpExpr:
    # mypy translates (1 and 2 and 3) as (1 and (2 and 3))
    assert len(n.values) &gt;= 2
    op_node = n.op
    if isinstance(op_node, ast3.And):
        op = 'and'
    elif isinstance(op_node, ast3.Or):
        op = 'or'
    else:
        raise RuntimeError('unknown BoolOp ' + str(type(n)))

    # potentially inefficient!
    return self.group(op, self.translate_expr_list(n.values), n)

</t>
<t tx="ekr.20220525082934.980">def __init__(self, names: List[str]) -&gt; None:
    super().__init__()
    self.names = names

</t>
<t tx="ekr.20220525082934.981">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_nonlocal_decl(self)


</t>
<t tx="ekr.20220525082934.982">class Block(Statement):
    __slots__ = ('body', 'is_unreachable')

    @others
</t>
<t tx="ekr.20220525082934.983">def __init__(self, body: List[Statement]) -&gt; None:
    super().__init__()
    self.body = body
    # True if we can determine that this block is not executed during semantic
    # analysis. For example, this applies to blocks that are protected by
    # something like "if PY3:" when using Python 2. However, some code is
    # only considered unreachable during type checking and this is not true
    # in those cases.
    self.is_unreachable = False

</t>
<t tx="ekr.20220525082934.984">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_block(self)


</t>
<t tx="ekr.20220525082934.985"># Statements


</t>
<t tx="ekr.20220525082934.986">class ExpressionStmt(Statement):
    """An expression as a statement, such as print(s)."""

    __slots__ = ('expr',)

    expr: Expression

    @others
</t>
<t tx="ekr.20220525082934.987">def __init__(self, expr: Expression) -&gt; None:
    super().__init__()
    self.expr = expr

</t>
<t tx="ekr.20220525082934.988">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_expression_stmt(self)


</t>
<t tx="ekr.20220525082934.989">class AssignmentStmt(Statement):
    """Assignment statement.

    The same node class is used for single assignment, multiple assignment
    (e.g. x, y = z) and chained assignment (e.g. x = y = z), assignments
    that define new names, and assignments with explicit types ("# type: t"
    or "x: t [= ...]").

    An lvalue can be NameExpr, TupleExpr, ListExpr, MemberExpr, or IndexExpr.
    """

    __slots__ = ('lvalues', 'rvalue', 'type', 'unanalyzed_type', 'new_syntax',
                 'is_alias_def', 'is_final_def')

    lvalues: List[Lvalue]
    # This is a TempNode if and only if no rvalue (x: t).
    rvalue: Expression
    # Declared type in a comment, may be None.
    type: Optional["mypy.types.Type"]
    # Original, not semantically analyzed type in annotation (used for reprocessing)
    unanalyzed_type: Optional["mypy.types.Type"]
    # This indicates usage of PEP 526 type annotation syntax in assignment.
    new_syntax: bool
    # Does this assignment define a type alias?
    is_alias_def: bool
    # Is this a final definition?
    # Final attributes can't be re-assigned once set, and can't be overridden
    # in a subclass. This flag is not set if an attempted declaration was found to
    # be invalid during semantic analysis. It is still set to `True` if
    # a final declaration overrides another final declaration (this is checked
    # during type checking when MROs are known).
    is_final_def: bool

    @others
</t>
<t tx="ekr.20220525082934.99">def group(self, op: str, vals: List[Expression], n: ast3.expr) -&gt; OpExpr:
    if len(vals) == 2:
        e = OpExpr(op, vals[0], vals[1])
    else:
        e = OpExpr(op, vals[0], self.group(op, vals[1:], n))
    return self.set_line(e, n)

</t>
<t tx="ekr.20220525082934.990">def __init__(self, lvalues: List[Lvalue], rvalue: Expression,
             type: 'Optional[mypy.types.Type]' = None, new_syntax: bool = False) -&gt; None:
    super().__init__()
    self.lvalues = lvalues
    self.rvalue = rvalue
    self.type = type
    self.unanalyzed_type = type
    self.new_syntax = new_syntax
    self.is_alias_def = False
    self.is_final_def = False

</t>
<t tx="ekr.20220525082934.991">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_assignment_stmt(self)


</t>
<t tx="ekr.20220525082934.992">class OperatorAssignmentStmt(Statement):
    """Operator assignment statement such as x += 1"""

    __slots__ = ('op', 'lvalue', 'rvalue')

    op: str  # TODO: Enum?
    lvalue: Lvalue
    rvalue: Expression

    @others
</t>
<t tx="ekr.20220525082934.993">def __init__(self, op: str, lvalue: Lvalue, rvalue: Expression) -&gt; None:
    super().__init__()
    self.op = op
    self.lvalue = lvalue
    self.rvalue = rvalue

</t>
<t tx="ekr.20220525082934.994">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_operator_assignment_stmt(self)


</t>
<t tx="ekr.20220525082934.995">class WhileStmt(Statement):
    __slots__ = ('expr', 'body', 'else_body')

    expr: Expression
    body: Block
    else_body: Optional[Block]

    @others
</t>
<t tx="ekr.20220525082934.996">def __init__(self, expr: Expression, body: Block, else_body: Optional[Block]) -&gt; None:
    super().__init__()
    self.expr = expr
    self.body = body
    self.else_body = else_body

</t>
<t tx="ekr.20220525082934.997">def accept(self, visitor: StatementVisitor[T]) -&gt; T:
    return visitor.visit_while_stmt(self)


</t>
<t tx="ekr.20220525082934.998">class ForStmt(Statement):
    __slots__ = ('index', 'index_type', 'unanalyzed_index_type',
                 'inferred_item_type', 'inferred_iterator_type',
                 'expr', 'body', 'else_body', 'is_async')

    # Index variables
    index: Lvalue
    # Type given by type comments for index, can be None
    index_type: Optional["mypy.types.Type"]
    # Original, not semantically analyzed type in annotation (used for reprocessing)
    unanalyzed_index_type: Optional["mypy.types.Type"]
    # Inferred iterable item type
    inferred_item_type: Optional["mypy.types.Type"]
    # Inferred iterator type
    inferred_iterator_type: Optional["mypy.types.Type"]
    # Expression to iterate
    expr: Expression
    body: Block
    else_body: Optional[Block]
    is_async: bool  # True if `async for ...` (PEP 492, Python 3.5)

    @others
</t>
<t tx="ekr.20220525082934.999">def __init__(self,
             index: Lvalue,
             expr: Expression,
             body: Block,
             else_body: Optional[Block],
             index_type: 'Optional[mypy.types.Type]' = None) -&gt; None:
    super().__init__()
    self.index = index
    self.index_type = index_type
    self.unanalyzed_index_type = index_type
    self.inferred_item_type = None
    self.inferred_iterator_type = None
    self.expr = expr
    self.body = body
    self.else_body = else_body
    self.is_async = False

</t>
<t tx="ekr.20220525082935.1">class SemanticAnalyzer(NodeVisitor[None],
                       SemanticAnalyzerInterface,
                       SemanticAnalyzerPluginInterface):
    """Semantically analyze parsed mypy files.

    The analyzer binds names and does various consistency checks for an
    AST. Note that type checking is performed as a separate pass.
    """

    __deletable__ = ['patches', 'options', 'cur_mod_node']

    # Module name space
    modules: Dict[str, MypyFile]
    # Global name space for current module
    globals: SymbolTable
    # Names declared using "global" (separate set for each scope)
    global_decls: List[Set[str]]
    # Names declared using "nonlocal" (separate set for each scope)
    nonlocal_decls: List[Set[str]]
    # Local names of function scopes; None for non-function scopes.
    locals: List[Optional[SymbolTable]]
    # Whether each scope is a comprehension scope.
    is_comprehension_stack: List[bool]
    # Nested block depths of scopes
    block_depth: List[int]
    # TypeInfo of directly enclosing class (or None)
    type: Optional[TypeInfo] = None
    # Stack of outer classes (the second tuple item contains tvars).
    type_stack: List[Optional[TypeInfo]]
    # Type variables bound by the current scope, be it class or function
    tvar_scope: TypeVarLikeScope
    # Per-module options
    options: Options

    # Stack of functions being analyzed
    function_stack: List[FuncItem]

    # Set to True if semantic analysis defines a name, or replaces a
    # placeholder definition. If some iteration makes no progress,
    # there can be at most one additional final iteration (see below).
    progress = False
    deferred = False  # Set to true if another analysis pass is needed
    incomplete = False  # Set to true if current module namespace is missing things
    # Is this the final iteration of semantic analysis (where we report
    # unbound names due to cyclic definitions and should not defer)?
    _final_iteration = False
    # These names couldn't be added to the symbol table due to incomplete deps.
    # Note that missing names are per module, _not_ per namespace. This means that e.g.
    # a missing name at global scope will block adding same name at a class scope.
    # This should not affect correctness and is purely a performance issue,
    # since it can cause unnecessary deferrals. These are represented as
    # PlaceholderNodes in the symbol table. We use this to ensure that the first
    # definition takes precedence even if it's incomplete.
    #
    # Note that a star import adds a special name '*' to the set, this blocks
    # adding _any_ names in the current file.
    missing_names: List[Set[str]]
    # Callbacks that will be called after semantic analysis to tweak things.
    patches: List[Tuple[int, Callable[[], None]]]
    loop_depth = 0         # Depth of breakable loops
    cur_mod_id = ''        # Current module id (or None) (phase 2)
    _is_stub_file = False   # Are we analyzing a stub file?
    _is_typeshed_stub_file = False  # Are we analyzing a typeshed stub file?
    imports: Set[str]  # Imported modules (during phase 2 analysis)
    # Note: some imports (and therefore dependencies) might
    # not be found in phase 1, for example due to * imports.
    errors: Errors  # Keeps track of generated errors
    plugin: Plugin  # Mypy plugin for special casing of library features
    statement: Optional[Statement] = None  # Statement/definition being analyzed

    # Mapping from 'async def' function definitions to their return type wrapped as a
    # 'Coroutine[Any, Any, T]'. Used to keep track of whether a function definition's
    # return type has already been wrapped, by checking if the function definition's
    # type is stored in this mapping and that it still matches.
    wrapped_coro_return_types: Dict[FuncDef, Type] = {}

    @others
</t>
<t tx="ekr.20220525082935.10">#
# Analyzing a target
#

</t>
<t tx="ekr.20220525082935.100">def analyze_lvalue(self,
                   lval: Lvalue,
                   nested: bool = False,
                   explicit_type: bool = False,
                   is_final: bool = False,
                   escape_comprehensions: bool = False,
                   has_explicit_value: bool = False) -&gt; None:
    """Analyze an lvalue or assignment target.

    Args:
        lval: The target lvalue
        nested: If true, the lvalue is within a tuple or list lvalue expression
        explicit_type: Assignment has type annotation
        escape_comprehensions: If we are inside a comprehension, set the variable
            in the enclosing scope instead. This implements
            https://www.python.org/dev/peps/pep-0572/#scope-of-the-target
    """
    if escape_comprehensions:
        assert isinstance(lval, NameExpr), "assignment expression target must be NameExpr"
    if isinstance(lval, NameExpr):
        self.analyze_name_lvalue(
            lval, explicit_type, is_final,
            escape_comprehensions,
            has_explicit_value=has_explicit_value,
        )
    elif isinstance(lval, MemberExpr):
        self.analyze_member_lvalue(lval, explicit_type, is_final)
        if explicit_type and not self.is_self_member_ref(lval):
            self.fail('Type cannot be declared in assignment to non-self '
                      'attribute', lval)
    elif isinstance(lval, IndexExpr):
        if explicit_type:
            self.fail('Unexpected type declaration', lval)
        lval.accept(self)
    elif isinstance(lval, TupleExpr):
        self.analyze_tuple_or_list_lvalue(lval, explicit_type)
    elif isinstance(lval, StarExpr):
        if nested:
            self.analyze_lvalue(lval.expr, nested, explicit_type)
        else:
            self.fail('Starred assignment target must be in a list or tuple', lval)
    else:
        self.fail('Invalid assignment target', lval)

</t>
<t tx="ekr.20220525082935.1000">def visit_if_stmt(self, o: IfStmt) -&gt; None:
    for e in o.expr:
        e.accept(self)
    for b in o.body:
        b.accept(self)
    if o.else_body:
        o.else_body.accept(self)

</t>
<t tx="ekr.20220525082935.1001">def visit_raise_stmt(self, o: RaiseStmt) -&gt; None:
    if o.expr is not None:
        o.expr.accept(self)
    if o.from_expr is not None:
        o.from_expr.accept(self)

</t>
<t tx="ekr.20220525082935.1002">def visit_try_stmt(self, o: TryStmt) -&gt; None:
    o.body.accept(self)
    for i in range(len(o.types)):
        tp = o.types[i]
        if tp is not None:
            tp.accept(self)
        o.handlers[i].accept(self)
    for v in o.vars:
        if v is not None:
            v.accept(self)
    if o.else_body is not None:
        o.else_body.accept(self)
    if o.finally_body is not None:
        o.finally_body.accept(self)

</t>
<t tx="ekr.20220525082935.1003">def visit_with_stmt(self, o: WithStmt) -&gt; None:
    for i in range(len(o.expr)):
        o.expr[i].accept(self)
        targ = o.target[i]
        if targ is not None:
            targ.accept(self)
    o.body.accept(self)

</t>
<t tx="ekr.20220525082935.1004">def visit_match_stmt(self, o: MatchStmt) -&gt; None:
    o.subject.accept(self)
    for i in range(len(o.patterns)):
        o.patterns[i].accept(self)
        guard = o.guards[i]
        if guard is not None:
            guard.accept(self)
        o.bodies[i].accept(self)

</t>
<t tx="ekr.20220525082935.1005">def visit_member_expr(self, o: MemberExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20220525082935.1006">def visit_yield_from_expr(self, o: YieldFromExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20220525082935.1007">def visit_yield_expr(self, o: YieldExpr) -&gt; None:
    if o.expr:
        o.expr.accept(self)

</t>
<t tx="ekr.20220525082935.1008">def visit_call_expr(self, o: CallExpr) -&gt; None:
    for a in o.args:
        a.accept(self)
    o.callee.accept(self)
    if o.analyzed:
        o.analyzed.accept(self)

</t>
<t tx="ekr.20220525082935.1009">def visit_op_expr(self, o: OpExpr) -&gt; None:
    o.left.accept(self)
    o.right.accept(self)

</t>
<t tx="ekr.20220525082935.101">def analyze_name_lvalue(self,
                        lvalue: NameExpr,
                        explicit_type: bool,
                        is_final: bool,
                        escape_comprehensions: bool,
                        has_explicit_value: bool) -&gt; None:
    """Analyze an lvalue that targets a name expression.

    Arguments are similar to "analyze_lvalue".
    """
    if lvalue.node:
        # This has been bound already in a previous iteration.
        return

    name = lvalue.name
    if self.is_alias_for_final_name(name):
        if is_final:
            self.fail("Cannot redefine an existing name as final", lvalue)
        else:
            self.msg.cant_assign_to_final(name, self.type is not None, lvalue)

    kind = self.current_symbol_kind()
    names = self.current_symbol_table(escape_comprehensions=escape_comprehensions)
    existing = names.get(name)

    outer = self.is_global_or_nonlocal(name)
    if kind == MDEF and isinstance(self.type, TypeInfo) and self.type.is_enum:
        # Special case: we need to be sure that `Enum` keys are unique.
        if existing is not None and not isinstance(existing.node, PlaceholderNode):
            self.fail('Attempted to reuse member name "{}" in Enum definition "{}"'.format(
                name, self.type.name,
            ), lvalue)

    if (not existing or isinstance(existing.node, PlaceholderNode)) and not outer:
        # Define new variable.
        var = self.make_name_lvalue_var(lvalue, kind, not explicit_type, has_explicit_value)
        added = self.add_symbol(name, var, lvalue, escape_comprehensions=escape_comprehensions)
        # Only bind expression if we successfully added name to symbol table.
        if added:
            lvalue.is_new_def = True
            lvalue.is_inferred_def = True
            lvalue.kind = kind
            lvalue.node = var
            if kind == GDEF:
                lvalue.fullname = var._fullname
            else:
                lvalue.fullname = lvalue.name
            if self.is_func_scope():
                if unmangle(name) == '_':
                    # Special case for assignment to local named '_': always infer 'Any'.
                    typ = AnyType(TypeOfAny.special_form)
                    self.store_declared_types(lvalue, typ)
        if is_final and self.is_final_redefinition(kind, name):
            self.fail("Cannot redefine an existing name as final", lvalue)
    else:
        self.make_name_lvalue_point_to_existing_def(lvalue, explicit_type, is_final)

</t>
<t tx="ekr.20220525082935.1010">def visit_comparison_expr(self, o: ComparisonExpr) -&gt; None:
    for operand in o.operands:
        operand.accept(self)

</t>
<t tx="ekr.20220525082935.1011">def visit_slice_expr(self, o: SliceExpr) -&gt; None:
    if o.begin_index is not None:
        o.begin_index.accept(self)
    if o.end_index is not None:
        o.end_index.accept(self)
    if o.stride is not None:
        o.stride.accept(self)

</t>
<t tx="ekr.20220525082935.1012">def visit_cast_expr(self, o: CastExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20220525082935.1013">def visit_assert_type_expr(self, o: AssertTypeExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20220525082935.1014">def visit_reveal_expr(self, o: RevealExpr) -&gt; None:
    if o.kind == REVEAL_TYPE:
        assert o.expr is not None
        o.expr.accept(self)
    else:
        # RevealLocalsExpr doesn't have an inner expression
        pass

</t>
<t tx="ekr.20220525082935.1015">def visit_assignment_expr(self, o: AssignmentExpr) -&gt; None:
    o.target.accept(self)
    o.value.accept(self)

</t>
<t tx="ekr.20220525082935.1016">def visit_unary_expr(self, o: UnaryExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20220525082935.1017">def visit_list_expr(self, o: ListExpr) -&gt; None:
    for item in o.items:
        item.accept(self)

</t>
<t tx="ekr.20220525082935.1018">def visit_tuple_expr(self, o: TupleExpr) -&gt; None:
    for item in o.items:
        item.accept(self)

</t>
<t tx="ekr.20220525082935.1019">def visit_dict_expr(self, o: DictExpr) -&gt; None:
    for k, v in o.items:
        if k is not None:
            k.accept(self)
        v.accept(self)

</t>
<t tx="ekr.20220525082935.102">def is_final_redefinition(self, kind: int, name: str) -&gt; bool:
    if kind == GDEF:
        return self.is_mangled_global(name) and not self.is_initial_mangled_global(name)
    elif kind == MDEF and self.type:
        return unmangle(name) + "'" in self.type.names
    return False

</t>
<t tx="ekr.20220525082935.1020">def visit_set_expr(self, o: SetExpr) -&gt; None:
    for item in o.items:
        item.accept(self)

</t>
<t tx="ekr.20220525082935.1021">def visit_index_expr(self, o: IndexExpr) -&gt; None:
    o.base.accept(self)
    o.index.accept(self)
    if o.analyzed:
        o.analyzed.accept(self)

</t>
<t tx="ekr.20220525082935.1022">def visit_generator_expr(self, o: GeneratorExpr) -&gt; None:
    for index, sequence, conditions in zip(o.indices, o.sequences,
                                           o.condlists):
        sequence.accept(self)
        index.accept(self)
        for cond in conditions:
            cond.accept(self)
    o.left_expr.accept(self)

</t>
<t tx="ekr.20220525082935.1023">def visit_dictionary_comprehension(self, o: DictionaryComprehension) -&gt; None:
    for index, sequence, conditions in zip(o.indices, o.sequences,
                                           o.condlists):
        sequence.accept(self)
        index.accept(self)
        for cond in conditions:
            cond.accept(self)
    o.key.accept(self)
    o.value.accept(self)

</t>
<t tx="ekr.20220525082935.1024">def visit_list_comprehension(self, o: ListComprehension) -&gt; None:
    o.generator.accept(self)

</t>
<t tx="ekr.20220525082935.1025">def visit_set_comprehension(self, o: SetComprehension) -&gt; None:
    o.generator.accept(self)

</t>
<t tx="ekr.20220525082935.1026">def visit_conditional_expr(self, o: ConditionalExpr) -&gt; None:
    o.cond.accept(self)
    o.if_expr.accept(self)
    o.else_expr.accept(self)

</t>
<t tx="ekr.20220525082935.1027">def visit_type_application(self, o: TypeApplication) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20220525082935.1028">def visit_lambda_expr(self, o: LambdaExpr) -&gt; None:
    self.visit_func(o)

</t>
<t tx="ekr.20220525082935.1029">def visit_star_expr(self, o: StarExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20220525082935.103">def is_alias_for_final_name(self, name: str) -&gt; bool:
    if self.is_func_scope():
        if not name.endswith("'"):
            # Not a mangled name -- can't be an alias
            return False
        name = unmangle(name)
        assert self.locals[-1] is not None, "No locals at function scope"
        existing = self.locals[-1].get(name)
        return existing is not None and is_final_node(existing.node)
    elif self.type is not None:
        orig_name = unmangle(name) + "'"
        if name == orig_name:
            return False
        existing = self.type.names.get(orig_name)
        return existing is not None and is_final_node(existing.node)
    else:
        orig_name = unmangle(name) + "'"
        if name == orig_name:
            return False
        existing = self.globals.get(orig_name)
        return existing is not None and is_final_node(existing.node)

</t>
<t tx="ekr.20220525082935.1030">def visit_backquote_expr(self, o: BackquoteExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20220525082935.1031">def visit_await_expr(self, o: AwaitExpr) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20220525082935.1032">def visit_super_expr(self, o: SuperExpr) -&gt; None:
    o.call.accept(self)

</t>
<t tx="ekr.20220525082935.1033">def visit_as_pattern(self, o: AsPattern) -&gt; None:
    if o.pattern is not None:
        o.pattern.accept(self)
    if o.name is not None:
        o.name.accept(self)

</t>
<t tx="ekr.20220525082935.1034">def visit_or_pattern(self, o: OrPattern) -&gt; None:
    for p in o.patterns:
        p.accept(self)

</t>
<t tx="ekr.20220525082935.1035">def visit_value_pattern(self, o: ValuePattern) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20220525082935.1036">def visit_sequence_pattern(self, o: SequencePattern) -&gt; None:
    for p in o.patterns:
        p.accept(self)

</t>
<t tx="ekr.20220525082935.1037">def visit_starred_patten(self, o: StarredPattern) -&gt; None:
    if o.capture is not None:
        o.capture.accept(self)

</t>
<t tx="ekr.20220525082935.1038">def visit_mapping_pattern(self, o: MappingPattern) -&gt; None:
    for key in o.keys:
        key.accept(self)
    for value in o.values:
        value.accept(self)
    if o.rest is not None:
        o.rest.accept(self)

</t>
<t tx="ekr.20220525082935.1039">def visit_class_pattern(self, o: ClassPattern) -&gt; None:
    o.class_ref.accept(self)
    for p in o.positionals:
        p.accept(self)
    for v in o.keyword_values:
        v.accept(self)

</t>
<t tx="ekr.20220525082935.104">def make_name_lvalue_var(
    self, lvalue: NameExpr, kind: int, inferred: bool, has_explicit_value: bool,
) -&gt; Var:
    """Return a Var node for an lvalue that is a name expression."""
    name = lvalue.name
    v = Var(name)
    v.set_line(lvalue)
    v.is_inferred = inferred
    if kind == MDEF:
        assert self.type is not None
        v.info = self.type
        v.is_initialized_in_class = True
        v.allow_incompatible_override = name in ALLOW_INCOMPATIBLE_OVERRIDE
    if kind != LDEF:
        v._fullname = self.qualified_name(name)
    else:
        # fullanme should never stay None
        v._fullname = name
    v.is_ready = False  # Type not inferred yet
    v.has_explicit_value = has_explicit_value
    return v

</t>
<t tx="ekr.20220525082935.1040">def visit_import(self, o: Import) -&gt; None:
    for a in o.assignments:
        a.accept(self)

</t>
<t tx="ekr.20220525082935.1041">def visit_import_from(self, o: ImportFrom) -&gt; None:
    for a in o.assignments:
        a.accept(self)

</t>
<t tx="ekr.20220525082935.1042">def visit_print_stmt(self, o: PrintStmt) -&gt; None:
    for arg in o.args:
        arg.accept(self)

</t>
<t tx="ekr.20220525082935.1043">def visit_exec_stmt(self, o: ExecStmt) -&gt; None:
    o.expr.accept(self)
    if o.globals:
        o.globals.accept(self)
    if o.locals:
        o.locals.accept(self)


</t>
<t tx="ekr.20220525082935.1044">class ReturnSeeker(TraverserVisitor):
    @others
</t>
<t tx="ekr.20220525082935.1045">def __init__(self) -&gt; None:
    self.found = False

</t>
<t tx="ekr.20220525082935.1046">def visit_return_stmt(self, o: ReturnStmt) -&gt; None:
    if (o.expr is None or isinstance(o.expr, NameExpr) and o.expr.name == 'None'):
        return
    self.found = True


</t>
<t tx="ekr.20220525082935.1047">def has_return_statement(fdef: FuncBase) -&gt; bool:
    """Find if a function has a non-trivial return statement.

    Plain 'return' and 'return None' don't count.
    """
    seeker = ReturnSeeker()
    fdef.accept(seeker)
    return seeker.found


</t>
<t tx="ekr.20220525082935.1048">class FuncCollectorBase(TraverserVisitor):
    @others
</t>
<t tx="ekr.20220525082935.1049">def __init__(self) -&gt; None:
    self.inside_func = False

</t>
<t tx="ekr.20220525082935.105">def make_name_lvalue_point_to_existing_def(
        self,
        lval: NameExpr,
        explicit_type: bool,
        is_final: bool) -&gt; None:
    """Update an lvalue to point to existing definition in the same scope.

    Arguments are similar to "analyze_lvalue".

    Assume that an existing name exists.
    """
    if is_final:
        # Redefining an existing name with final is always an error.
        self.fail("Cannot redefine an existing name as final", lval)
    original_def = self.lookup(lval.name, lval, suppress_errors=True)
    if original_def is None and self.type and not self.is_func_scope():
        # Workaround to allow "x, x = ..." in class body.
        original_def = self.type.get(lval.name)
    if explicit_type:
        # Don't re-bind if there is a type annotation.
        self.name_already_defined(lval.name, lval, original_def)
    else:
        # Bind to an existing name.
        if original_def:
            self.bind_name_expr(lval, original_def)
        else:
            self.name_not_defined(lval.name, lval)
        self.check_lvalue_validity(lval.node, lval)

</t>
<t tx="ekr.20220525082935.1050">def visit_func_def(self, defn: FuncDef) -&gt; None:
    if not self.inside_func:
        self.inside_func = True
        super().visit_func_def(defn)
        self.inside_func = False


</t>
<t tx="ekr.20220525082935.1051">class YieldSeeker(FuncCollectorBase):
    def __init__(self) -&gt; None:
        super().__init__()
        self.found = False

    def visit_yield_expr(self, o: YieldExpr) -&gt; None:
        self.found = True


</t>
<t tx="ekr.20220525082935.1052">def has_yield_expression(fdef: FuncBase) -&gt; bool:
    seeker = YieldSeeker()
    fdef.accept(seeker)
    return seeker.found


</t>
<t tx="ekr.20220525082935.1053">class ReturnCollector(FuncCollectorBase):
    def __init__(self) -&gt; None:
        super().__init__()
        self.return_statements: List[ReturnStmt] = []

    def visit_return_stmt(self, stmt: ReturnStmt) -&gt; None:
        self.return_statements.append(stmt)


</t>
<t tx="ekr.20220525082935.1054">def all_return_statements(node: Node) -&gt; List[ReturnStmt]:
    v = ReturnCollector()
    node.accept(v)
    return v.return_statements


</t>
<t tx="ekr.20220525082935.1055">class YieldCollector(FuncCollectorBase):
    @others
</t>
<t tx="ekr.20220525082935.1056">def __init__(self) -&gt; None:
    super().__init__()
    self.in_assignment = False
    self.yield_expressions: List[Tuple[YieldExpr, bool]] = []

</t>
<t tx="ekr.20220525082935.1057">def visit_assignment_stmt(self, stmt: AssignmentStmt) -&gt; None:
    self.in_assignment = True
    super().visit_assignment_stmt(stmt)
    self.in_assignment = False

</t>
<t tx="ekr.20220525082935.1058">def visit_yield_expr(self, expr: YieldExpr) -&gt; None:
    self.yield_expressions.append((expr, self.in_assignment))


</t>
<t tx="ekr.20220525082935.1059">def all_yield_expressions(node: Node) -&gt; List[Tuple[YieldExpr, bool]]:
    v = YieldCollector()
    node.accept(v)
    return v.yield_expressions
</t>
<t tx="ekr.20220525082935.106">def analyze_tuple_or_list_lvalue(self, lval: TupleExpr,
                                 explicit_type: bool = False) -&gt; None:
    """Analyze an lvalue or assignment target that is a list or tuple."""
    items = lval.items
    star_exprs = [item for item in items if isinstance(item, StarExpr)]

    if len(star_exprs) &gt; 1:
        self.fail('Two starred expressions in assignment', lval)
    else:
        if len(star_exprs) == 1:
            star_exprs[0].valid = True
        for i in items:
            self.analyze_lvalue(
                lval=i,
                nested=True,
                explicit_type=explicit_type,
                # Lists and tuples always have explicit values defined:
                # `a, b, c = value`
                has_explicit_value=True,
            )

</t>
<t tx="ekr.20220525082935.1060">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Base visitor that implements an identity AST transform.

Subclass TransformVisitor to perform non-trivial transformations.
"""

from typing import List, Dict, cast, Optional, Iterable

from mypy.nodes import (
    AssertTypeExpr, MypyFile, Import, Node, ImportAll, ImportFrom, FuncItem, FuncDef,
    OverloadedFuncDef, ClassDef, Decorator, Block, Var,
    OperatorAssignmentStmt, ExpressionStmt, AssignmentStmt, ReturnStmt,
    RaiseStmt, AssertStmt, DelStmt, BreakStmt, ContinueStmt,
    PassStmt, GlobalDecl, WhileStmt, ForStmt, IfStmt, TryStmt, WithStmt,
    CastExpr, RevealExpr, TupleExpr, GeneratorExpr, ListComprehension, ListExpr,
    ConditionalExpr, DictExpr, SetExpr, NameExpr, IntExpr, StrExpr, BytesExpr,
    UnicodeExpr, FloatExpr, CallExpr, SuperExpr, MemberExpr, IndexExpr,
    SliceExpr, OpExpr, UnaryExpr, LambdaExpr, TypeApplication, PrintStmt,
    SymbolTable, RefExpr, TypeVarExpr, ParamSpecExpr, NewTypeExpr, PromoteExpr,
    ComparisonExpr, TempNode, StarExpr, Statement, Expression,
    YieldFromExpr, NamedTupleExpr, TypedDictExpr, NonlocalDecl, SetComprehension,
    DictionaryComprehension, ComplexExpr, TypeAliasExpr, EllipsisExpr,
    YieldExpr, ExecStmt, Argument, BackquoteExpr, AwaitExpr, AssignmentExpr,
    OverloadPart, EnumCallExpr, REVEAL_TYPE, GDEF, TypeVarTupleExpr
)
from mypy.types import Type, FunctionLike, ProperType
from mypy.traverser import TraverserVisitor
from mypy.visitor import NodeVisitor
from mypy.util import replace_object_state


@others
</t>
<t tx="ekr.20220525082935.1061">class TransformVisitor(NodeVisitor[Node]):
    """Transform a semantically analyzed AST (or subtree) to an identical copy.

    Use the node() method to transform an AST node.

    Subclass to perform a non-identity transform.

    Notes:

     * This can only be used to transform functions or classes, not top-level
       statements, and/or modules as a whole.
     * Do not duplicate TypeInfo nodes. This would generally not be desirable.
     * Only update some name binding cross-references, but only those that
       refer to Var, Decorator or FuncDef nodes, not those targeting ClassDef or
       TypeInfo nodes.
     * Types are not transformed, but you can override type() to also perform
       type transformation.

    TODO nested classes and functions have not been tested well enough
    """

    @others
</t>
<t tx="ekr.20220525082935.1062">def __init__(self) -&gt; None:
    # To simplify testing, set this flag to True if you want to transform
    # all statements in a file (this is prohibited in normal mode).
    self.test_only = False
    # There may be multiple references to a Var node. Keep track of
    # Var translations using a dictionary.
    self.var_map: Dict[Var, Var] = {}
    # These are uninitialized placeholder nodes used temporarily for nested
    # functions while we are transforming a top-level function. This maps an
    # untransformed node to a placeholder (which will later become the
    # transformed node).
    self.func_placeholder_map: Dict[FuncDef, FuncDef] = {}

</t>
<t tx="ekr.20220525082935.1063">def visit_mypy_file(self, node: MypyFile) -&gt; MypyFile:
    assert self.test_only, "This visitor should not be used for whole files."
    # NOTE: The 'names' and 'imports' instance variables will be empty!
    ignored_lines = {line: codes[:]
                     for line, codes in node.ignored_lines.items()}
    new = MypyFile(self.statements(node.defs), [], node.is_bom,
                   ignored_lines=ignored_lines)
    new._fullname = node._fullname
    new.path = node.path
    new.names = SymbolTable()
    return new

</t>
<t tx="ekr.20220525082935.1064">def visit_import(self, node: Import) -&gt; Import:
    return Import(node.ids[:])

</t>
<t tx="ekr.20220525082935.1065">def visit_import_from(self, node: ImportFrom) -&gt; ImportFrom:
    return ImportFrom(node.id, node.relative, node.names[:])

</t>
<t tx="ekr.20220525082935.1066">def visit_import_all(self, node: ImportAll) -&gt; ImportAll:
    return ImportAll(node.id, node.relative)

</t>
<t tx="ekr.20220525082935.1067">def copy_argument(self, argument: Argument) -&gt; Argument:
    arg = Argument(
        self.visit_var(argument.variable),
        argument.type_annotation,
        argument.initializer,
        argument.kind,
    )

    # Refresh lines of the inner things
    arg.set_line(argument.line)

    return arg

</t>
<t tx="ekr.20220525082935.1068">def visit_func_def(self, node: FuncDef) -&gt; FuncDef:
    # Note that a FuncDef must be transformed to a FuncDef.

    # These contortions are needed to handle the case of recursive
    # references inside the function being transformed.
    # Set up placeholder nodes for references within this function
    # to other functions defined inside it.
    # Don't create an entry for this function itself though,
    # since we want self-references to point to the original
    # function if this is the top-level node we are transforming.
    init = FuncMapInitializer(self)
    for stmt in node.body.body:
        stmt.accept(init)

    new = FuncDef(node.name,
                  [self.copy_argument(arg) for arg in node.arguments],
                  self.block(node.body),
                  cast(Optional[FunctionLike], self.optional_type(node.type)))

    self.copy_function_attributes(new, node)

    new._fullname = node._fullname
    new.is_decorated = node.is_decorated
    new.is_conditional = node.is_conditional
    new.is_abstract = node.is_abstract
    new.is_static = node.is_static
    new.is_class = node.is_class
    new.is_property = node.is_property
    new.is_final = node.is_final
    new.original_def = node.original_def

    if node in self.func_placeholder_map:
        # There is a placeholder definition for this function. Replace
        # the attributes of the placeholder with those form the transformed
        # function. We know that the classes will be identical (otherwise
        # this wouldn't work).
        result = self.func_placeholder_map[node]
        replace_object_state(result, new)
        return result
    else:
        return new

</t>
<t tx="ekr.20220525082935.1069">def visit_lambda_expr(self, node: LambdaExpr) -&gt; LambdaExpr:
    new = LambdaExpr([self.copy_argument(arg) for arg in node.arguments],
                     self.block(node.body),
                     cast(Optional[FunctionLike], self.optional_type(node.type)))
    self.copy_function_attributes(new, node)
    return new

</t>
<t tx="ekr.20220525082935.107">def analyze_member_lvalue(self, lval: MemberExpr, explicit_type: bool, is_final: bool) -&gt; None:
    """Analyze lvalue that is a member expression.

    Arguments:
        lval: The target lvalue
        explicit_type: Assignment has type annotation
        is_final: Is the target final
    """
    if lval.node:
        # This has been bound already in a previous iteration.
        return
    lval.accept(self)
    if self.is_self_member_ref(lval):
        assert self.type, "Self member outside a class"
        cur_node = self.type.names.get(lval.name)
        node = self.type.get(lval.name)
        if cur_node and is_final:
            # Overrides will be checked in type checker.
            self.fail("Cannot redefine an existing name as final", lval)
        # On first encounter with this definition, if this attribute was defined before
        # with an inferred type and it's marked with an explicit type now, give an error.
        if (not lval.node and cur_node and isinstance(cur_node.node, Var) and
                cur_node.node.is_inferred and explicit_type):
            self.attribute_already_defined(lval.name, lval, cur_node)
        # If the attribute of self is not defined in superclasses, create a new Var, ...
        if (node is None
                or (isinstance(node.node, Var) and node.node.is_abstract_var)
                # ... also an explicit declaration on self also creates a new Var.
                # Note that `explicit_type` might has been erased for bare `Final`,
                # so we also check if `is_final` is passed.
                or (cur_node is None and (explicit_type or is_final))):
            if self.type.is_protocol and node is None:
                self.fail("Protocol members cannot be defined via assignment to self", lval)
            else:
                # Implicit attribute definition in __init__.
                lval.is_new_def = True
                lval.is_inferred_def = True
                v = Var(lval.name)
                v.set_line(lval)
                v._fullname = self.qualified_name(lval.name)
                v.info = self.type
                v.is_ready = False
                v.explicit_self_type = explicit_type or is_final
                lval.def_var = v
                lval.node = v
                # TODO: should we also set lval.kind = MDEF?
                self.type.names[lval.name] = SymbolTableNode(MDEF, v, implicit=True)
    self.check_lvalue_validity(lval.node, lval)

</t>
<t tx="ekr.20220525082935.1070">def copy_function_attributes(self, new: FuncItem,
                             original: FuncItem) -&gt; None:
    new.info = original.info
    new.min_args = original.min_args
    new.max_pos = original.max_pos
    new.is_overload = original.is_overload
    new.is_generator = original.is_generator
    new.line = original.line

</t>
<t tx="ekr.20220525082935.1071">def visit_overloaded_func_def(self, node: OverloadedFuncDef) -&gt; OverloadedFuncDef:
    items = [cast(OverloadPart, item.accept(self)) for item in node.items]
    for newitem, olditem in zip(items, node.items):
        newitem.line = olditem.line
    new = OverloadedFuncDef(items)
    new._fullname = node._fullname
    new_type = self.optional_type(node.type)
    assert isinstance(new_type, ProperType)
    new.type = new_type
    new.info = node.info
    new.is_static = node.is_static
    new.is_class = node.is_class
    new.is_property = node.is_property
    new.is_final = node.is_final
    if node.impl:
        new.impl = cast(OverloadPart, node.impl.accept(self))
    return new

</t>
<t tx="ekr.20220525082935.1072">def visit_class_def(self, node: ClassDef) -&gt; ClassDef:
    new = ClassDef(node.name,
                   self.block(node.defs),
                   node.type_vars,
                   self.expressions(node.base_type_exprs),
                   self.optional_expr(node.metaclass))
    new.fullname = node.fullname
    new.info = node.info
    new.decorators = [self.expr(decorator)
                      for decorator in node.decorators]
    return new

</t>
<t tx="ekr.20220525082935.1073">def visit_global_decl(self, node: GlobalDecl) -&gt; GlobalDecl:
    return GlobalDecl(node.names[:])

</t>
<t tx="ekr.20220525082935.1074">def visit_nonlocal_decl(self, node: NonlocalDecl) -&gt; NonlocalDecl:
    return NonlocalDecl(node.names[:])

</t>
<t tx="ekr.20220525082935.1075">def visit_block(self, node: Block) -&gt; Block:
    return Block(self.statements(node.body))

</t>
<t tx="ekr.20220525082935.1076">def visit_decorator(self, node: Decorator) -&gt; Decorator:
    # Note that a Decorator must be transformed to a Decorator.
    func = self.visit_func_def(node.func)
    func.line = node.func.line
    new = Decorator(func, self.expressions(node.decorators),
                    self.visit_var(node.var))
    new.is_overload = node.is_overload
    return new

</t>
<t tx="ekr.20220525082935.1077">def visit_var(self, node: Var) -&gt; Var:
    # Note that a Var must be transformed to a Var.
    if node in self.var_map:
        return self.var_map[node]
    new = Var(node.name, self.optional_type(node.type))
    new.line = node.line
    new._fullname = node._fullname
    new.info = node.info
    new.is_self = node.is_self
    new.is_ready = node.is_ready
    new.is_initialized_in_class = node.is_initialized_in_class
    new.is_staticmethod = node.is_staticmethod
    new.is_classmethod = node.is_classmethod
    new.is_property = node.is_property
    new.is_final = node.is_final
    new.final_value = node.final_value
    new.final_unset_in_class = node.final_unset_in_class
    new.final_set_in_init = node.final_set_in_init
    new.set_line(node.line)
    self.var_map[node] = new
    return new

</t>
<t tx="ekr.20220525082935.1078">def visit_expression_stmt(self, node: ExpressionStmt) -&gt; ExpressionStmt:
    return ExpressionStmt(self.expr(node.expr))

</t>
<t tx="ekr.20220525082935.1079">def visit_assignment_stmt(self, node: AssignmentStmt) -&gt; AssignmentStmt:
    return self.duplicate_assignment(node)

</t>
<t tx="ekr.20220525082935.108">def is_self_member_ref(self, memberexpr: MemberExpr) -&gt; bool:
    """Does memberexpr to refer to an attribute of self?"""
    if not isinstance(memberexpr.expr, NameExpr):
        return False
    node = memberexpr.expr.node
    return isinstance(node, Var) and node.is_self

</t>
<t tx="ekr.20220525082935.1080">def duplicate_assignment(self, node: AssignmentStmt) -&gt; AssignmentStmt:
    new = AssignmentStmt(self.expressions(node.lvalues),
                         self.expr(node.rvalue),
                         self.optional_type(node.unanalyzed_type))
    new.line = node.line
    new.is_final_def = node.is_final_def
    new.type = self.optional_type(node.type)
    return new

</t>
<t tx="ekr.20220525082935.1081">def visit_operator_assignment_stmt(self,
                                   node: OperatorAssignmentStmt) -&gt; OperatorAssignmentStmt:
    return OperatorAssignmentStmt(node.op,
                                  self.expr(node.lvalue),
                                  self.expr(node.rvalue))

</t>
<t tx="ekr.20220525082935.1082">def visit_while_stmt(self, node: WhileStmt) -&gt; WhileStmt:
    return WhileStmt(self.expr(node.expr),
                     self.block(node.body),
                     self.optional_block(node.else_body))

</t>
<t tx="ekr.20220525082935.1083">def visit_for_stmt(self, node: ForStmt) -&gt; ForStmt:
    new = ForStmt(self.expr(node.index),
                  self.expr(node.expr),
                  self.block(node.body),
                  self.optional_block(node.else_body),
                  self.optional_type(node.unanalyzed_index_type))
    new.is_async = node.is_async
    new.index_type = self.optional_type(node.index_type)
    return new

</t>
<t tx="ekr.20220525082935.1084">def visit_return_stmt(self, node: ReturnStmt) -&gt; ReturnStmt:
    return ReturnStmt(self.optional_expr(node.expr))

</t>
<t tx="ekr.20220525082935.1085">def visit_assert_stmt(self, node: AssertStmt) -&gt; AssertStmt:
    return AssertStmt(self.expr(node.expr), self.optional_expr(node.msg))

</t>
<t tx="ekr.20220525082935.1086">def visit_del_stmt(self, node: DelStmt) -&gt; DelStmt:
    return DelStmt(self.expr(node.expr))

</t>
<t tx="ekr.20220525082935.1087">def visit_if_stmt(self, node: IfStmt) -&gt; IfStmt:
    return IfStmt(self.expressions(node.expr),
                  self.blocks(node.body),
                  self.optional_block(node.else_body))

</t>
<t tx="ekr.20220525082935.1088">def visit_break_stmt(self, node: BreakStmt) -&gt; BreakStmt:
    return BreakStmt()

</t>
<t tx="ekr.20220525082935.1089">def visit_continue_stmt(self, node: ContinueStmt) -&gt; ContinueStmt:
    return ContinueStmt()

</t>
<t tx="ekr.20220525082935.109">def check_lvalue_validity(self, node: Union[Expression, SymbolNode, None],
                          ctx: Context) -&gt; None:
    if isinstance(node, TypeVarExpr):
        self.fail('Invalid assignment target', ctx)
    elif isinstance(node, TypeInfo):
        self.fail(message_registry.CANNOT_ASSIGN_TO_TYPE, ctx)

</t>
<t tx="ekr.20220525082935.1090">def visit_pass_stmt(self, node: PassStmt) -&gt; PassStmt:
    return PassStmt()

</t>
<t tx="ekr.20220525082935.1091">def visit_raise_stmt(self, node: RaiseStmt) -&gt; RaiseStmt:
    return RaiseStmt(self.optional_expr(node.expr),
                     self.optional_expr(node.from_expr))

</t>
<t tx="ekr.20220525082935.1092">def visit_try_stmt(self, node: TryStmt) -&gt; TryStmt:
    return TryStmt(self.block(node.body),
                   self.optional_names(node.vars),
                   self.optional_expressions(node.types),
                   self.blocks(node.handlers),
                   self.optional_block(node.else_body),
                   self.optional_block(node.finally_body))

</t>
<t tx="ekr.20220525082935.1093">def visit_with_stmt(self, node: WithStmt) -&gt; WithStmt:
    new = WithStmt(self.expressions(node.expr),
                   self.optional_expressions(node.target),
                   self.block(node.body),
                   self.optional_type(node.unanalyzed_type))
    new.is_async = node.is_async
    new.analyzed_types = [self.type(typ) for typ in node.analyzed_types]
    return new

</t>
<t tx="ekr.20220525082935.1094">def visit_print_stmt(self, node: PrintStmt) -&gt; PrintStmt:
    return PrintStmt(self.expressions(node.args),
                     node.newline,
                     self.optional_expr(node.target))

</t>
<t tx="ekr.20220525082935.1095">def visit_exec_stmt(self, node: ExecStmt) -&gt; ExecStmt:
    return ExecStmt(self.expr(node.expr),
                    self.optional_expr(node.globals),
                    self.optional_expr(node.locals))

</t>
<t tx="ekr.20220525082935.1096">def visit_star_expr(self, node: StarExpr) -&gt; StarExpr:
    return StarExpr(node.expr)

</t>
<t tx="ekr.20220525082935.1097">def visit_int_expr(self, node: IntExpr) -&gt; IntExpr:
    return IntExpr(node.value)

</t>
<t tx="ekr.20220525082935.1098">def visit_str_expr(self, node: StrExpr) -&gt; StrExpr:
    return StrExpr(node.value, node.from_python_3)

</t>
<t tx="ekr.20220525082935.1099">def visit_bytes_expr(self, node: BytesExpr) -&gt; BytesExpr:
    return BytesExpr(node.value)

</t>
<t tx="ekr.20220525082935.11">def refresh_partial(self,
                    node: Union[MypyFile, FuncDef, OverloadedFuncDef],
                    patches: List[Tuple[int, Callable[[], None]]],
                    final_iteration: bool,
                    file_node: MypyFile,
                    options: Options,
                    active_type: Optional[TypeInfo] = None) -&gt; None:
    """Refresh a stale target in fine-grained incremental mode."""
    self.patches = patches
    self.deferred = False
    self.incomplete = False
    self._final_iteration = final_iteration
    self.missing_names[-1] = set()

    with self.file_context(file_node, options, active_type):
        if isinstance(node, MypyFile):
            self.refresh_top_level(node)
        else:
            self.recurse_into_functions = True
            self.accept(node)
    del self.patches

</t>
<t tx="ekr.20220525082935.110">def store_declared_types(self, lvalue: Lvalue, typ: Type) -&gt; None:
    if isinstance(typ, StarType) and not isinstance(lvalue, StarExpr):
        self.fail('Star type only allowed for starred expressions', lvalue)
    if isinstance(lvalue, RefExpr):
        lvalue.is_inferred_def = False
        if isinstance(lvalue.node, Var):
            var = lvalue.node
            var.type = typ
            var.is_ready = True
        # If node is not a variable, we'll catch it elsewhere.
    elif isinstance(lvalue, TupleExpr):
        typ = get_proper_type(typ)
        if isinstance(typ, TupleType):
            if len(lvalue.items) != len(typ.items):
                self.fail('Incompatible number of tuple items', lvalue)
                return
            for item, itemtype in zip(lvalue.items, typ.items):
                self.store_declared_types(item, itemtype)
        else:
            self.fail('Tuple type expected for multiple variables',
                      lvalue)
    elif isinstance(lvalue, StarExpr):
        # Historical behavior for the old parser
        if isinstance(typ, StarType):
            self.store_declared_types(lvalue.expr, typ.type)
        else:
            self.store_declared_types(lvalue.expr, typ)
    else:
        # This has been flagged elsewhere as an error, so just ignore here.
        pass

</t>
<t tx="ekr.20220525082935.1100">def visit_unicode_expr(self, node: UnicodeExpr) -&gt; UnicodeExpr:
    return UnicodeExpr(node.value)

</t>
<t tx="ekr.20220525082935.1101">def visit_float_expr(self, node: FloatExpr) -&gt; FloatExpr:
    return FloatExpr(node.value)

</t>
<t tx="ekr.20220525082935.1102">def visit_complex_expr(self, node: ComplexExpr) -&gt; ComplexExpr:
    return ComplexExpr(node.value)

</t>
<t tx="ekr.20220525082935.1103">def visit_ellipsis(self, node: EllipsisExpr) -&gt; EllipsisExpr:
    return EllipsisExpr()

</t>
<t tx="ekr.20220525082935.1104">def visit_name_expr(self, node: NameExpr) -&gt; NameExpr:
    return self.duplicate_name(node)

</t>
<t tx="ekr.20220525082935.1105">def duplicate_name(self, node: NameExpr) -&gt; NameExpr:
    # This method is used when the transform result must be a NameExpr.
    # visit_name_expr() is used when there is no such restriction.
    new = NameExpr(node.name)
    self.copy_ref(new, node)
    new.is_special_form = node.is_special_form
    return new

</t>
<t tx="ekr.20220525082935.1106">def visit_member_expr(self, node: MemberExpr) -&gt; MemberExpr:
    member = MemberExpr(self.expr(node.expr),
                        node.name)
    if node.def_var:
        # This refers to an attribute and we don't transform attributes by default,
        # just normal variables.
        member.def_var = node.def_var
    self.copy_ref(member, node)
    return member

</t>
<t tx="ekr.20220525082935.1107">def copy_ref(self, new: RefExpr, original: RefExpr) -&gt; None:
    new.kind = original.kind
    new.fullname = original.fullname
    target = original.node
    if isinstance(target, Var):
        # Do not transform references to global variables. See
        # testGenericFunctionAliasExpand for an example where this is important.
        if original.kind != GDEF:
            target = self.visit_var(target)
    elif isinstance(target, Decorator):
        target = self.visit_var(target.var)
    elif isinstance(target, FuncDef):
        # Use a placeholder node for the function if it exists.
        target = self.func_placeholder_map.get(target, target)
    new.node = target
    new.is_new_def = original.is_new_def
    new.is_inferred_def = original.is_inferred_def

</t>
<t tx="ekr.20220525082935.1108">def visit_yield_from_expr(self, node: YieldFromExpr) -&gt; YieldFromExpr:
    return YieldFromExpr(self.expr(node.expr))

</t>
<t tx="ekr.20220525082935.1109">def visit_yield_expr(self, node: YieldExpr) -&gt; YieldExpr:
    return YieldExpr(self.optional_expr(node.expr))

</t>
<t tx="ekr.20220525082935.111">def process_typevar_declaration(self, s: AssignmentStmt) -&gt; bool:
    """Check if s declares a TypeVar; it yes, store it in symbol table.

    Return True if this looks like a type variable declaration (but maybe
    with errors), otherwise return False.
    """
    call = self.get_typevarlike_declaration(s, ("typing.TypeVar",))
    if not call:
        return False

    name = self.extract_typevarlike_name(s, call)
    if name is None:
        return False

    # Constraining types
    n_values = call.arg_kinds[1:].count(ARG_POS)
    values = self.analyze_value_types(call.args[1:1 + n_values])

    res = self.process_typevar_parameters(call.args[1 + n_values:],
                                          call.arg_names[1 + n_values:],
                                          call.arg_kinds[1 + n_values:],
                                          n_values,
                                          s)
    if res is None:
        return False
    variance, upper_bound = res

    existing = self.current_symbol_table().get(name)
    if existing and not (isinstance(existing.node, PlaceholderNode) or
                         # Also give error for another type variable with the same name.
                         (isinstance(existing.node, TypeVarExpr) and
                          existing.node is call.analyzed)):
        self.fail(f'Cannot redefine "{name}" as a type variable', s)
        return False

    if self.options.disallow_any_unimported:
        for idx, constraint in enumerate(values, start=1):
            if has_any_from_unimported_type(constraint):
                prefix = f"Constraint {idx}"
                self.msg.unimported_type_becomes_any(prefix, constraint, s)

        if has_any_from_unimported_type(upper_bound):
            prefix = "Upper bound of type variable"
            self.msg.unimported_type_becomes_any(prefix, upper_bound, s)

    for t in values + [upper_bound]:
        check_for_explicit_any(t, self.options, self.is_typeshed_stub_file, self.msg,
                               context=s)

    # mypyc suppresses making copies of a function to check each
    # possible type, so set the upper bound to Any to prevent that
    # from causing errors.
    if values and self.options.mypyc:
        upper_bound = AnyType(TypeOfAny.implementation_artifact)

    # Yes, it's a valid type variable definition! Add it to the symbol table.
    if not call.analyzed:
        type_var = TypeVarExpr(name, self.qualified_name(name),
                               values, upper_bound, variance)
        type_var.line = call.line
        call.analyzed = type_var
    else:
        assert isinstance(call.analyzed, TypeVarExpr)
        if call.analyzed.values != values or call.analyzed.upper_bound != upper_bound:
            self.progress = True
        call.analyzed.upper_bound = upper_bound
        call.analyzed.values = values

    self.add_symbol(name, call.analyzed, s)
    return True

</t>
<t tx="ekr.20220525082935.1110">def visit_await_expr(self, node: AwaitExpr) -&gt; AwaitExpr:
    return AwaitExpr(self.expr(node.expr))

</t>
<t tx="ekr.20220525082935.1111">def visit_call_expr(self, node: CallExpr) -&gt; CallExpr:
    return CallExpr(self.expr(node.callee),
                    self.expressions(node.args),
                    node.arg_kinds[:],
                    node.arg_names[:],
                    self.optional_expr(node.analyzed))

</t>
<t tx="ekr.20220525082935.1112">def visit_op_expr(self, node: OpExpr) -&gt; OpExpr:
    new = OpExpr(node.op, self.expr(node.left), self.expr(node.right))
    new.method_type = self.optional_type(node.method_type)
    return new

</t>
<t tx="ekr.20220525082935.1113">def visit_comparison_expr(self, node: ComparisonExpr) -&gt; ComparisonExpr:
    new = ComparisonExpr(node.operators, self.expressions(node.operands))
    new.method_types = [self.optional_type(t) for t in node.method_types]
    return new

</t>
<t tx="ekr.20220525082935.1114">def visit_cast_expr(self, node: CastExpr) -&gt; CastExpr:
    return CastExpr(self.expr(node.expr),
                    self.type(node.type))

</t>
<t tx="ekr.20220525082935.1115">def visit_assert_type_expr(self, node: AssertTypeExpr) -&gt; AssertTypeExpr:
    return AssertTypeExpr(self.expr(node.expr), self.type(node.type))

</t>
<t tx="ekr.20220525082935.1116">def visit_reveal_expr(self, node: RevealExpr) -&gt; RevealExpr:
    if node.kind == REVEAL_TYPE:
        assert node.expr is not None
        return RevealExpr(kind=REVEAL_TYPE, expr=self.expr(node.expr))
    else:
        # Reveal locals expressions don't have any sub expressions
        return node

</t>
<t tx="ekr.20220525082935.1117">def visit_super_expr(self, node: SuperExpr) -&gt; SuperExpr:
    call = self.expr(node.call)
    assert isinstance(call, CallExpr)
    new = SuperExpr(node.name, call)
    new.info = node.info
    return new

</t>
<t tx="ekr.20220525082935.1118">def visit_assignment_expr(self, node: AssignmentExpr) -&gt; AssignmentExpr:
    return AssignmentExpr(node.target, node.value)

</t>
<t tx="ekr.20220525082935.1119">def visit_unary_expr(self, node: UnaryExpr) -&gt; UnaryExpr:
    new = UnaryExpr(node.op, self.expr(node.expr))
    new.method_type = self.optional_type(node.method_type)
    return new

</t>
<t tx="ekr.20220525082935.112">def check_typevarlike_name(self, call: CallExpr, name: str, context: Context) -&gt; bool:
    """Checks that the name of a TypeVar or ParamSpec matches its variable."""
    name = unmangle(name)
    assert isinstance(call.callee, RefExpr)
    typevarlike_type = (
        call.callee.name if isinstance(call.callee, NameExpr) else call.callee.fullname
    )
    if len(call.args) &lt; 1:
        self.fail(f"Too few arguments for {typevarlike_type}()", context)
        return False
    if (not isinstance(call.args[0], (StrExpr, BytesExpr, UnicodeExpr))
            or not call.arg_kinds[0] == ARG_POS):
        self.fail(f"{typevarlike_type}() expects a string literal as first argument",
                  context)
        return False
    elif call.args[0].value != name:
        msg = 'String argument 1 "{}" to {}(...) does not match variable name "{}"'
        self.fail(msg.format(call.args[0].value, typevarlike_type, name), context)
        return False
    return True

</t>
<t tx="ekr.20220525082935.1120">def visit_list_expr(self, node: ListExpr) -&gt; ListExpr:
    return ListExpr(self.expressions(node.items))

</t>
<t tx="ekr.20220525082935.1121">def visit_dict_expr(self, node: DictExpr) -&gt; DictExpr:
    return DictExpr([(self.expr(key) if key else None, self.expr(value))
                     for key, value in node.items])

</t>
<t tx="ekr.20220525082935.1122">def visit_tuple_expr(self, node: TupleExpr) -&gt; TupleExpr:
    return TupleExpr(self.expressions(node.items))

</t>
<t tx="ekr.20220525082935.1123">def visit_set_expr(self, node: SetExpr) -&gt; SetExpr:
    return SetExpr(self.expressions(node.items))

</t>
<t tx="ekr.20220525082935.1124">def visit_index_expr(self, node: IndexExpr) -&gt; IndexExpr:
    new = IndexExpr(self.expr(node.base), self.expr(node.index))
    if node.method_type:
        new.method_type = self.type(node.method_type)
    if node.analyzed:
        if isinstance(node.analyzed, TypeApplication):
            new.analyzed = self.visit_type_application(node.analyzed)
        else:
            new.analyzed = self.visit_type_alias_expr(node.analyzed)
        new.analyzed.set_line(node.analyzed.line)
    return new

</t>
<t tx="ekr.20220525082935.1125">def visit_type_application(self, node: TypeApplication) -&gt; TypeApplication:
    return TypeApplication(self.expr(node.expr),
                           self.types(node.types))

</t>
<t tx="ekr.20220525082935.1126">def visit_list_comprehension(self, node: ListComprehension) -&gt; ListComprehension:
    generator = self.duplicate_generator(node.generator)
    generator.set_line(node.generator.line, node.generator.column)
    return ListComprehension(generator)

</t>
<t tx="ekr.20220525082935.1127">def visit_set_comprehension(self, node: SetComprehension) -&gt; SetComprehension:
    generator = self.duplicate_generator(node.generator)
    generator.set_line(node.generator.line, node.generator.column)
    return SetComprehension(generator)

</t>
<t tx="ekr.20220525082935.1128">def visit_dictionary_comprehension(self, node: DictionaryComprehension
                                   ) -&gt; DictionaryComprehension:
    return DictionaryComprehension(self.expr(node.key), self.expr(node.value),
                                   [self.expr(index) for index in node.indices],
                                   [self.expr(s) for s in node.sequences],
                                   [[self.expr(cond) for cond in conditions]
                                    for conditions in node.condlists],
                                   node.is_async)

</t>
<t tx="ekr.20220525082935.1129">def visit_generator_expr(self, node: GeneratorExpr) -&gt; GeneratorExpr:
    return self.duplicate_generator(node)

</t>
<t tx="ekr.20220525082935.113">def get_typevarlike_declaration(self, s: AssignmentStmt,
                                typevarlike_types: Tuple[str, ...]) -&gt; Optional[CallExpr]:
    """Returns the call expression if `s` is a declaration of `typevarlike_type`
    (TypeVar or ParamSpec), or None otherwise.
    """
    if len(s.lvalues) != 1 or not isinstance(s.lvalues[0], NameExpr):
        return None
    if not isinstance(s.rvalue, CallExpr):
        return None
    call = s.rvalue
    callee = call.callee
    if not isinstance(callee, RefExpr):
        return None
    if callee.fullname not in typevarlike_types:
        return None
    return call

</t>
<t tx="ekr.20220525082935.1130">def duplicate_generator(self, node: GeneratorExpr) -&gt; GeneratorExpr:
    return GeneratorExpr(self.expr(node.left_expr),
                         [self.expr(index) for index in node.indices],
                         [self.expr(s) for s in node.sequences],
                         [[self.expr(cond) for cond in conditions]
                          for conditions in node.condlists],
                         node.is_async)

</t>
<t tx="ekr.20220525082935.1131">def visit_slice_expr(self, node: SliceExpr) -&gt; SliceExpr:
    return SliceExpr(self.optional_expr(node.begin_index),
                     self.optional_expr(node.end_index),
                     self.optional_expr(node.stride))

</t>
<t tx="ekr.20220525082935.1132">def visit_conditional_expr(self, node: ConditionalExpr) -&gt; ConditionalExpr:
    return ConditionalExpr(self.expr(node.cond),
                           self.expr(node.if_expr),
                           self.expr(node.else_expr))

</t>
<t tx="ekr.20220525082935.1133">def visit_backquote_expr(self, node: BackquoteExpr) -&gt; BackquoteExpr:
    return BackquoteExpr(self.expr(node.expr))

</t>
<t tx="ekr.20220525082935.1134">def visit_type_var_expr(self, node: TypeVarExpr) -&gt; TypeVarExpr:
    return TypeVarExpr(node.name, node.fullname,
                       self.types(node.values),
                       self.type(node.upper_bound), variance=node.variance)

</t>
<t tx="ekr.20220525082935.1135">def visit_paramspec_expr(self, node: ParamSpecExpr) -&gt; ParamSpecExpr:
    return ParamSpecExpr(
        node.name, node.fullname, self.type(node.upper_bound), variance=node.variance
    )

</t>
<t tx="ekr.20220525082935.1136">def visit_type_var_tuple_expr(self, node: TypeVarTupleExpr) -&gt; TypeVarTupleExpr:
    return TypeVarTupleExpr(
        node.name, node.fullname, self.type(node.upper_bound), variance=node.variance
    )

</t>
<t tx="ekr.20220525082935.1137">def visit_type_alias_expr(self, node: TypeAliasExpr) -&gt; TypeAliasExpr:
    return TypeAliasExpr(node.node)

</t>
<t tx="ekr.20220525082935.1138">def visit_newtype_expr(self, node: NewTypeExpr) -&gt; NewTypeExpr:
    res = NewTypeExpr(node.name, node.old_type, line=node.line, column=node.column)
    res.info = node.info
    return res

</t>
<t tx="ekr.20220525082935.1139">def visit_namedtuple_expr(self, node: NamedTupleExpr) -&gt; NamedTupleExpr:
    return NamedTupleExpr(node.info)

</t>
<t tx="ekr.20220525082935.114">def process_typevar_parameters(self, args: List[Expression],
                               names: List[Optional[str]],
                               kinds: List[ArgKind],
                               num_values: int,
                               context: Context) -&gt; Optional[Tuple[int, Type]]:
    has_values = (num_values &gt; 0)
    covariant = False
    contravariant = False
    upper_bound: Type = self.object_type()
    for param_value, param_name, param_kind in zip(args, names, kinds):
        if not param_kind.is_named():
            self.fail(message_registry.TYPEVAR_UNEXPECTED_ARGUMENT, context)
            return None
        if param_name == 'covariant':
            if (isinstance(param_value, NameExpr)
                    and param_value.name in ('True', 'False')):
                covariant = param_value.name == 'True'
            else:
                self.fail(message_registry.TYPEVAR_VARIANCE_DEF.format(
                    'covariant'), context)
                return None
        elif param_name == 'contravariant':
            if (isinstance(param_value, NameExpr)
                    and param_value.name in ('True', 'False')):
                contravariant = param_value.name == 'True'
            else:
                self.fail(message_registry.TYPEVAR_VARIANCE_DEF.format(
                    'contravariant'), context)
                return None
        elif param_name == 'bound':
            if has_values:
                self.fail("TypeVar cannot have both values and an upper bound", context)
                return None
            try:
                # We want to use our custom error message below, so we suppress
                # the default error message for invalid types here.
                analyzed = self.expr_to_analyzed_type(param_value,
                                                      allow_placeholder=True,
                                                      report_invalid_types=False)
                if analyzed is None:
                    # Type variables are special: we need to place them in the symbol table
                    # soon, even if upper bound is not ready yet. Otherwise avoiding
                    # a "deadlock" in this common pattern would be tricky:
                    #     T = TypeVar('T', bound=Custom[Any])
                    #     class Custom(Generic[T]):
                    #         ...
                    analyzed = PlaceholderType(None, [], context.line)
                upper_bound = get_proper_type(analyzed)
                if isinstance(upper_bound, AnyType) and upper_bound.is_from_error:
                    self.fail(message_registry.TYPEVAR_BOUND_MUST_BE_TYPE, param_value)
                    # Note: we do not return 'None' here -- we want to continue
                    # using the AnyType as the upper bound.
            except TypeTranslationError:
                self.fail(message_registry.TYPEVAR_BOUND_MUST_BE_TYPE, param_value)
                return None
        elif param_name == 'values':
            # Probably using obsolete syntax with values=(...). Explain the current syntax.
            self.fail('TypeVar "values" argument not supported', context)
            self.fail("Use TypeVar('T', t, ...) instead of TypeVar('T', values=(t, ...))",
                      context)
            return None
        else:
            self.fail('{}: "{}"'.format(
                message_registry.TYPEVAR_UNEXPECTED_ARGUMENT, param_name,
            ), context)
            return None

    if covariant and contravariant:
        self.fail("TypeVar cannot be both covariant and contravariant", context)
        return None
    elif num_values == 1:
        self.fail("TypeVar cannot have only a single constraint", context)
        return None
    elif covariant:
        variance = COVARIANT
    elif contravariant:
        variance = CONTRAVARIANT
    else:
        variance = INVARIANT
    return variance, upper_bound

</t>
<t tx="ekr.20220525082935.1140">def visit_enum_call_expr(self, node: EnumCallExpr) -&gt; EnumCallExpr:
    return EnumCallExpr(node.info, node.items, node.values)

</t>
<t tx="ekr.20220525082935.1141">def visit_typeddict_expr(self, node: TypedDictExpr) -&gt; Node:
    return TypedDictExpr(node.info)

</t>
<t tx="ekr.20220525082935.1142">def visit__promote_expr(self, node: PromoteExpr) -&gt; PromoteExpr:
    return PromoteExpr(node.type)

</t>
<t tx="ekr.20220525082935.1143">def visit_temp_node(self, node: TempNode) -&gt; TempNode:
    return TempNode(self.type(node.type))

</t>
<t tx="ekr.20220525082935.1144">def node(self, node: Node) -&gt; Node:
    new = node.accept(self)
    new.set_line(node.line)
    return new

</t>
<t tx="ekr.20220525082935.1145">def mypyfile(self, node: MypyFile) -&gt; MypyFile:
    new = node.accept(self)
    assert isinstance(new, MypyFile)
    new.set_line(node.line)
    return new

</t>
<t tx="ekr.20220525082935.1146">def expr(self, expr: Expression) -&gt; Expression:
    new = expr.accept(self)
    assert isinstance(new, Expression)
    new.set_line(expr.line, expr.column)
    return new

</t>
<t tx="ekr.20220525082935.1147">def stmt(self, stmt: Statement) -&gt; Statement:
    new = stmt.accept(self)
    assert isinstance(new, Statement)
    new.set_line(stmt.line, stmt.column)
    return new

</t>
<t tx="ekr.20220525082935.1148"># Helpers
#
# All the node helpers also propagate line numbers.

</t>
<t tx="ekr.20220525082935.1149">def optional_expr(self, expr: Optional[Expression]) -&gt; Optional[Expression]:
    if expr:
        return self.expr(expr)
    else:
        return None

</t>
<t tx="ekr.20220525082935.115">def extract_typevarlike_name(self, s: AssignmentStmt, call: CallExpr) -&gt; Optional[str]:
    if not call:
        return None

    lvalue = s.lvalues[0]
    assert isinstance(lvalue, NameExpr)
    if s.type:
        self.fail("Cannot declare the type of a TypeVar or similar construct", s)
        return None

    if not self.check_typevarlike_name(call, lvalue.name, s):
        return None
    return lvalue.name

</t>
<t tx="ekr.20220525082935.1150">def block(self, block: Block) -&gt; Block:
    new = self.visit_block(block)
    new.line = block.line
    return new

</t>
<t tx="ekr.20220525082935.1151">def optional_block(self, block: Optional[Block]) -&gt; Optional[Block]:
    if block:
        return self.block(block)
    else:
        return None

</t>
<t tx="ekr.20220525082935.1152">def statements(self, statements: List[Statement]) -&gt; List[Statement]:
    return [self.stmt(stmt) for stmt in statements]

</t>
<t tx="ekr.20220525082935.1153">def expressions(self, expressions: List[Expression]) -&gt; List[Expression]:
    return [self.expr(expr) for expr in expressions]

</t>
<t tx="ekr.20220525082935.1154">def optional_expressions(self, expressions: Iterable[Optional[Expression]]
                         ) -&gt; List[Optional[Expression]]:
    return [self.optional_expr(expr) for expr in expressions]

</t>
<t tx="ekr.20220525082935.1155">def blocks(self, blocks: List[Block]) -&gt; List[Block]:
    return [self.block(block) for block in blocks]

</t>
<t tx="ekr.20220525082935.1156">def names(self, names: List[NameExpr]) -&gt; List[NameExpr]:
    return [self.duplicate_name(name) for name in names]

</t>
<t tx="ekr.20220525082935.1157">def optional_names(self, names: Iterable[Optional[NameExpr]]) -&gt; List[Optional[NameExpr]]:
    result: List[Optional[NameExpr]] = []
    for name in names:
        if name:
            result.append(self.duplicate_name(name))
        else:
            result.append(None)
    return result

</t>
<t tx="ekr.20220525082935.1158">def type(self, type: Type) -&gt; Type:
    # Override this method to transform types.
    return type

</t>
<t tx="ekr.20220525082935.1159">def optional_type(self, type: Optional[Type]) -&gt; Optional[Type]:
    if type:
        return self.type(type)
    else:
        return None

</t>
<t tx="ekr.20220525082935.116">def process_paramspec_declaration(self, s: AssignmentStmt) -&gt; bool:
    """Checks if s declares a ParamSpec; if yes, store it in symbol table.

    Return True if this looks like a ParamSpec (maybe with errors), otherwise return False.

    In the future, ParamSpec may accept bounds and variance arguments, in which
    case more aggressive sharing of code with process_typevar_declaration should be pursued.
    """
    call = self.get_typevarlike_declaration(
        s, ("typing_extensions.ParamSpec", "typing.ParamSpec")
    )
    if not call:
        return False

    name = self.extract_typevarlike_name(s, call)
    if name is None:
        return False

    # ParamSpec is different from a regular TypeVar:
    # arguments are not semantically valid. But, allowed in runtime.
    # So, we need to warn users about possible invalid usage.
    if len(call.args) &gt; 1:
        self.fail(
            "Only the first argument to ParamSpec has defined semantics",
            s,
        )

    # PEP 612 reserves the right to define bound, covariant and contravariant arguments to
    # ParamSpec in a later PEP. If and when that happens, we should do something
    # on the lines of process_typevar_parameters

    if not call.analyzed:
        paramspec_var = ParamSpecExpr(
            name, self.qualified_name(name), self.object_type(), INVARIANT
        )
        paramspec_var.line = call.line
        call.analyzed = paramspec_var
    else:
        assert isinstance(call.analyzed, ParamSpecExpr)
    self.add_symbol(name, call.analyzed, s)
    return True

</t>
<t tx="ekr.20220525082935.1160">def types(self, types: List[Type]) -&gt; List[Type]:
    return [self.type(type) for type in types]


</t>
<t tx="ekr.20220525082935.1161">class FuncMapInitializer(TraverserVisitor):
    """This traverser creates mappings from nested FuncDefs to placeholder FuncDefs.

    The placeholders will later be replaced with transformed nodes.
    """

    @others
</t>
<t tx="ekr.20220525082935.1162">def __init__(self, transformer: TransformVisitor) -&gt; None:
    self.transformer = transformer

</t>
<t tx="ekr.20220525082935.1163">def visit_func_def(self, node: FuncDef) -&gt; None:
    if node not in self.transformer.func_placeholder_map:
        # Haven't seen this FuncDef before, so create a placeholder node.
        self.transformer.func_placeholder_map[node] = FuncDef(
            node.name, node.arguments, node.body, None)
    super().visit_func_def(node)
</t>
<t tx="ekr.20220525082935.1164">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from typing import Optional, Dict, Union
from mypy.types import (
    TypeVarLikeType, TypeVarType, ParamSpecType, ParamSpecFlavor, TypeVarId, TypeVarTupleType,
)
from mypy.nodes import (
    ParamSpecExpr, TypeVarExpr, TypeVarLikeExpr, SymbolTableNode, TypeVarTupleExpr,
)


@others
</t>
<t tx="ekr.20220525082935.1165">class TypeVarLikeScope:
    """Scope that holds bindings for type variables and parameter specifications.

    Node fullname -&gt; TypeVarLikeType.
    """

    @others
</t>
<t tx="ekr.20220525082935.1166">def __init__(self,
             parent: 'Optional[TypeVarLikeScope]' = None,
             is_class_scope: bool = False,
             prohibited: 'Optional[TypeVarLikeScope]' = None,
             namespace: str = '') -&gt; None:
    """Initializer for TypeVarLikeScope

    Parameters:
      parent: the outer scope for this scope
      is_class_scope: True if this represents a generic class
      prohibited: Type variables that aren't strictly in scope exactly,
                  but can't be bound because they're part of an outer class's scope.
    """
    self.scope: Dict[str, TypeVarLikeType] = {}
    self.parent = parent
    self.func_id = 0
    self.class_id = 0
    self.is_class_scope = is_class_scope
    self.prohibited = prohibited
    self.namespace = namespace
    if parent is not None:
        self.func_id = parent.func_id
        self.class_id = parent.class_id

</t>
<t tx="ekr.20220525082935.1167">def get_function_scope(self) -&gt; 'Optional[TypeVarLikeScope]':
    """Get the nearest parent that's a function scope, not a class scope"""
    it: Optional[TypeVarLikeScope] = self
    while it is not None and it.is_class_scope:
        it = it.parent
    return it

</t>
<t tx="ekr.20220525082935.1168">def allow_binding(self, fullname: str) -&gt; bool:
    if fullname in self.scope:
        return False
    elif self.parent and not self.parent.allow_binding(fullname):
        return False
    elif self.prohibited and not self.prohibited.allow_binding(fullname):
        return False
    return True

</t>
<t tx="ekr.20220525082935.1169">def method_frame(self) -&gt; 'TypeVarLikeScope':
    """A new scope frame for binding a method"""
    return TypeVarLikeScope(self, False, None)

</t>
<t tx="ekr.20220525082935.117">def process_typevartuple_declaration(self, s: AssignmentStmt) -&gt; bool:
    """Checks if s declares a TypeVarTuple; if yes, store it in symbol table.

    Return True if this looks like a TypeVarTuple (maybe with errors), otherwise return False.
    """
    call = self.get_typevarlike_declaration(
        s, ("typing_extensions.TypeVarTuple", "typing.TypeVarTuple")
    )
    if not call:
        return False

    if len(call.args) &gt; 1:
        self.fail(
            "Only the first argument to TypeVarTuple has defined semantics",
            s,
        )

    if not self.options.enable_incomplete_features:
        self.fail('"TypeVarTuple" is not supported by mypy yet', s)
        return False

    name = self.extract_typevarlike_name(s, call)
    if name is None:
        return False

    # PEP 646 does not specify the behavior of variance, constraints, or bounds.
    if not call.analyzed:
        typevartuple_var = TypeVarTupleExpr(
            name, self.qualified_name(name), self.object_type(), INVARIANT
        )
        typevartuple_var.line = call.line
        call.analyzed = typevartuple_var
    else:
        assert isinstance(call.analyzed, TypeVarTupleExpr)
    self.add_symbol(name, call.analyzed, s)
    return True

</t>
<t tx="ekr.20220525082935.1170">def class_frame(self, namespace: str) -&gt; 'TypeVarLikeScope':
    """A new scope frame for binding a class. Prohibits *this* class's tvars"""
    return TypeVarLikeScope(self.get_function_scope(), True, self, namespace=namespace)

</t>
<t tx="ekr.20220525082935.1171">def bind_new(self, name: str, tvar_expr: TypeVarLikeExpr) -&gt; TypeVarLikeType:
    if self.is_class_scope:
        self.class_id += 1
        i = self.class_id
        namespace = self.namespace
    else:
        self.func_id -= 1
        i = self.func_id
        # TODO: Consider also using namespaces for functions
        namespace = ''
    if isinstance(tvar_expr, TypeVarExpr):
        tvar_def: TypeVarLikeType = TypeVarType(
            name,
            tvar_expr.fullname,
            TypeVarId(i, namespace=namespace),
            values=tvar_expr.values,
            upper_bound=tvar_expr.upper_bound,
            variance=tvar_expr.variance,
            line=tvar_expr.line,
            column=tvar_expr.column
        )
    elif isinstance(tvar_expr, ParamSpecExpr):
        tvar_def = ParamSpecType(
            name,
            tvar_expr.fullname,
            i,
            flavor=ParamSpecFlavor.BARE,
            upper_bound=tvar_expr.upper_bound,
            line=tvar_expr.line,
            column=tvar_expr.column
        )
    elif isinstance(tvar_expr, TypeVarTupleExpr):
        tvar_def = TypeVarTupleType(
            name,
            tvar_expr.fullname,
            i,
            upper_bound=tvar_expr.upper_bound,
            line=tvar_expr.line,
            column=tvar_expr.column
        )
    else:
        assert False
    self.scope[tvar_expr.fullname] = tvar_def
    return tvar_def

</t>
<t tx="ekr.20220525082935.1172">def bind_existing(self, tvar_def: TypeVarLikeType) -&gt; None:
    self.scope[tvar_def.fullname] = tvar_def

</t>
<t tx="ekr.20220525082935.1173">def get_binding(self, item: Union[str, SymbolTableNode]) -&gt; Optional[TypeVarLikeType]:
    fullname = item.fullname if isinstance(item, SymbolTableNode) else item
    assert fullname is not None
    if fullname in self.scope:
        return self.scope[fullname]
    elif self.parent is not None:
        return self.parent.get_binding(fullname)
    else:
        return None

</t>
<t tx="ekr.20220525082935.1174">def __str__(self) -&gt; str:
    me = ", ".join(f'{k}: {v.name}`{v.id}' for k, v in self.scope.items())
    if self.parent is None:
        return me
    return f"{self.parent} &lt;- {me}"
</t>
<t tx="ekr.20220525082935.1175">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Semantic analysis of types"""

import itertools
from itertools import chain
from contextlib import contextmanager
from mypy.backports import OrderedDict

from typing import Callable, List, Optional, Set, Tuple, Iterator, TypeVar, Iterable, Sequence
from typing_extensions import Final, Protocol

from mypy.messages import MessageBuilder, quote_type_string, format_type_bare
from mypy.options import Options
from mypy.types import (
    NEVER_NAMES, Type, UnboundType, TupleType, TypedDictType, UnionType, Instance, AnyType,
    CallableType, NoneType, ErasedType, DeletedType, TypeList, TypeVarType, SyntheticTypeVisitor,
    StarType, PartialType, EllipsisType, UninhabitedType, TypeType, CallableArgument,
    Parameters, TypeQuery, union_items, TypeOfAny, LiteralType, RawExpressionType,
    PlaceholderType, Overloaded, get_proper_type, TypeAliasType, RequiredType,
    TypeVarLikeType, ParamSpecType, ParamSpecFlavor, UnpackType, TypeVarTupleType,
    callable_with_ellipsis, TYPE_ALIAS_NAMES, FINAL_TYPE_NAMES,
    LITERAL_TYPE_NAMES, ANNOTATED_TYPE_NAMES,
)

from mypy.nodes import (
    TypeInfo, Context, SymbolTableNode, Var, Expression,
    get_nongen_builtins, check_arg_names, check_arg_kinds, ArgKind, ARG_POS, ARG_NAMED,
    ARG_OPT, ARG_NAMED_OPT, ARG_STAR, ARG_STAR2, TypeVarExpr, TypeVarLikeExpr, ParamSpecExpr,
    TypeAlias, PlaceholderNode, SYMBOL_FUNCBASE_TYPES, Decorator, MypyFile,
    TypeVarTupleExpr
)
from mypy.typetraverser import TypeTraverserVisitor
from mypy.tvar_scope import TypeVarLikeScope
from mypy.exprtotype import expr_to_unanalyzed_type, TypeTranslationError
from mypy.plugin import Plugin, TypeAnalyzerPluginInterface, AnalyzeTypeContext
from mypy.semanal_shared import SemanticAnalyzerCoreInterface, paramspec_args, paramspec_kwargs
from mypy.errorcodes import ErrorCode
from mypy import nodes, message_registry, errorcodes as codes

T = TypeVar('T')

type_constructors: Final = {
    'typing.Callable',
    'typing.Optional',
    'typing.Tuple',
    'typing.Type',
    'typing.Union',
    *LITERAL_TYPE_NAMES,
    *ANNOTATED_TYPE_NAMES,
}

ARG_KINDS_BY_CONSTRUCTOR: Final = {
    'mypy_extensions.Arg': ARG_POS,
    'mypy_extensions.DefaultArg': ARG_OPT,
    'mypy_extensions.NamedArg': ARG_NAMED,
    'mypy_extensions.DefaultNamedArg': ARG_NAMED_OPT,
    'mypy_extensions.VarArg': ARG_STAR,
    'mypy_extensions.KwArg': ARG_STAR2,
}

GENERIC_STUB_NOT_AT_RUNTIME_TYPES: Final = {
    'queue.Queue',
    'builtins._PathLike',
    'asyncio.futures.Future',
}


@others
</t>
<t tx="ekr.20220525082935.1176">def analyze_type_alias(node: Expression,
                       api: SemanticAnalyzerCoreInterface,
                       tvar_scope: TypeVarLikeScope,
                       plugin: Plugin,
                       options: Options,
                       is_typeshed_stub: bool,
                       allow_placeholder: bool = False,
                       in_dynamic_func: bool = False,
                       global_scope: bool = True) -&gt; Optional[Tuple[Type, Set[str]]]:
    """Analyze r.h.s. of a (potential) type alias definition.

    If `node` is valid as a type alias rvalue, return the resulting type and a set of
    full names of type aliases it depends on (directly or indirectly).
    Return None otherwise. 'node' must have been semantically analyzed.
    """
    try:
        type = expr_to_unanalyzed_type(node, options, api.is_stub_file)
    except TypeTranslationError:
        api.fail('Invalid type alias: expression is not a valid type', node)
        return None
    analyzer = TypeAnalyser(api, tvar_scope, plugin, options, is_typeshed_stub,
                            defining_alias=True,
                            allow_placeholder=allow_placeholder)
    analyzer.in_dynamic_func = in_dynamic_func
    analyzer.global_scope = global_scope
    res = type.accept(analyzer)
    return res, analyzer.aliases_used


</t>
<t tx="ekr.20220525082935.1177">def no_subscript_builtin_alias(name: str, propose_alt: bool = True) -&gt; str:
    class_name = name.split('.')[-1]
    msg = f'"{class_name}" is not subscriptable'
    # This should never be called if the python_version is 3.9 or newer
    nongen_builtins = get_nongen_builtins((3, 8))
    replacement = nongen_builtins[name]
    if replacement and propose_alt:
        msg += f', use "{replacement}" instead'
    return msg


</t>
<t tx="ekr.20220525082935.1178">class TypeAnalyser(SyntheticTypeVisitor[Type], TypeAnalyzerPluginInterface):
    """Semantic analyzer for types.

    Converts unbound types into bound types. This is a no-op for already
    bound types.

    If an incomplete reference is encountered, this does a defer. The
    caller never needs to defer.
    """

    # Is this called from an untyped function definition?
    in_dynamic_func: bool = False
    # Is this called from global scope?
    global_scope: bool = True

    @others
</t>
<t tx="ekr.20220525082935.1179">def __init__(self,
             api: SemanticAnalyzerCoreInterface,
             tvar_scope: TypeVarLikeScope,
             plugin: Plugin,
             options: Options,
             is_typeshed_stub: bool, *,
             defining_alias: bool = False,
             allow_tuple_literal: bool = False,
             allow_unbound_tvars: bool = False,
             allow_placeholder: bool = False,
             allow_required: bool = False,
             allow_param_spec_literals: bool = False,
             report_invalid_types: bool = True) -&gt; None:
    self.api = api
    self.lookup_qualified = api.lookup_qualified
    self.lookup_fqn_func = api.lookup_fully_qualified
    self.fail_func = api.fail
    self.note_func = api.note
    self.tvar_scope = tvar_scope
    # Are we analysing a type alias definition rvalue?
    self.defining_alias = defining_alias
    self.allow_tuple_literal = allow_tuple_literal
    # Positive if we are analyzing arguments of another (outer) type
    self.nesting_level = 0
    # Should we allow new type syntax when targeting older Python versions
    # like 'list[int]' or 'X | Y' (allowed in stubs and with `__future__` import)?
    self.always_allow_new_syntax = (
        self.api.is_stub_file
        or self.api.is_future_flag_set('annotations')
    )
    # Should we accept unbound type variables (always OK in aliases)?
    self.allow_unbound_tvars = allow_unbound_tvars or defining_alias
    # If false, record incomplete ref if we generate PlaceholderType.
    self.allow_placeholder = allow_placeholder
    # Are we in a context where Required[] is allowed?
    self.allow_required = allow_required
    # Are we in a context where ParamSpec literals are allowed?
    self.allow_param_spec_literals = allow_param_spec_literals
    # Should we report an error whenever we encounter a RawExpressionType outside
    # of a Literal context: e.g. whenever we encounter an invalid type? Normally,
    # we want to report an error, but the caller may want to do more specialized
    # error handling.
    self.report_invalid_types = report_invalid_types
    self.plugin = plugin
    self.options = options
    self.is_typeshed_stub = is_typeshed_stub
    # Names of type aliases encountered while analysing a type will be collected here.
    self.aliases_used: Set[str] = set()

</t>
<t tx="ekr.20220525082935.118">def basic_new_typeinfo(self, name: str,
                       basetype_or_fallback: Instance,
                       line: int) -&gt; TypeInfo:
    if self.is_func_scope() and not self.type and '@' not in name:
        name += '@' + str(line)
    class_def = ClassDef(name, Block([]))
    if self.is_func_scope() and not self.type:
        # Full names of generated classes should always be prefixed with the module names
        # even if they are nested in a function, since these classes will be (de-)serialized.
        # (Note that the caller should append @line to the name to avoid collisions.)
        # TODO: clean this up, see #6422.
        class_def.fullname = self.cur_mod_id + '.' + self.qualified_name(name)
    else:
        class_def.fullname = self.qualified_name(name)

    info = TypeInfo(SymbolTable(), class_def, self.cur_mod_id)
    class_def.info = info
    mro = basetype_or_fallback.type.mro
    if not mro:
        # Forward reference, MRO should be recalculated in third pass.
        mro = [basetype_or_fallback.type, self.object_type().type]
    info.mro = [info] + mro
    info.bases = [basetype_or_fallback]
    return info

</t>
<t tx="ekr.20220525082935.1180">def visit_unbound_type(self, t: UnboundType, defining_literal: bool = False) -&gt; Type:
    typ = self.visit_unbound_type_nonoptional(t, defining_literal)
    if t.optional:
        # We don't need to worry about double-wrapping Optionals or
        # wrapping Anys: Union simplification will take care of that.
        return make_optional_type(typ)
    return typ

</t>
<t tx="ekr.20220525082935.1181">def visit_unbound_type_nonoptional(self, t: UnboundType, defining_literal: bool) -&gt; Type:
    sym = self.lookup_qualified(t.name, t)
    if sym is not None:
        node = sym.node
        if isinstance(node, PlaceholderNode):
            if node.becomes_typeinfo:
                # Reference to placeholder type.
                if self.api.final_iteration:
                    self.cannot_resolve_type(t)
                    return AnyType(TypeOfAny.from_error)
                elif self.allow_placeholder:
                    self.api.defer()
                else:
                    self.api.record_incomplete_ref()
                return PlaceholderType(node.fullname, self.anal_array(t.args), t.line)
            else:
                if self.api.final_iteration:
                    self.cannot_resolve_type(t)
                    return AnyType(TypeOfAny.from_error)
                else:
                    # Reference to an unknown placeholder node.
                    self.api.record_incomplete_ref()
                    return AnyType(TypeOfAny.special_form)
        if node is None:
            self.fail(f'Internal error (node is None, kind={sym.kind})', t)
            return AnyType(TypeOfAny.special_form)
        fullname = node.fullname
        hook = self.plugin.get_type_analyze_hook(fullname)
        if hook is not None:
            return hook(AnalyzeTypeContext(t, t, self))
        if (fullname in get_nongen_builtins(self.options.python_version)
                and t.args
                and not self.always_allow_new_syntax):
            self.fail(no_subscript_builtin_alias(fullname,
                                                 propose_alt=not self.defining_alias), t)
        tvar_def = self.tvar_scope.get_binding(sym)
        if isinstance(sym.node, ParamSpecExpr):
            if tvar_def is None:
                self.fail(f'ParamSpec "{t.name}" is unbound', t)
                return AnyType(TypeOfAny.from_error)
            assert isinstance(tvar_def, ParamSpecType)
            if len(t.args) &gt; 0:
                self.fail(f'ParamSpec "{t.name}" used with arguments', t)
            # Change the line number
            return ParamSpecType(
                tvar_def.name, tvar_def.fullname, tvar_def.id, tvar_def.flavor,
                tvar_def.upper_bound, line=t.line, column=t.column,
            )
        if isinstance(sym.node, TypeVarExpr) and tvar_def is not None and self.defining_alias:
            self.fail('Can\'t use bound type variable "{}"'
                      ' to define generic alias'.format(t.name), t)
            return AnyType(TypeOfAny.from_error)
        if isinstance(sym.node, TypeVarExpr) and tvar_def is not None:
            assert isinstance(tvar_def, TypeVarType)
            if len(t.args) &gt; 0:
                self.fail(f'Type variable "{t.name}" used with arguments', t)
            # Change the line number
            return TypeVarType(
                tvar_def.name, tvar_def.fullname, tvar_def.id, tvar_def.values,
                tvar_def.upper_bound, tvar_def.variance, line=t.line, column=t.column,
            )
        if isinstance(sym.node, TypeVarTupleExpr) and (
            tvar_def is not None and self.defining_alias
        ):
            self.fail('Can\'t use bound type variable "{}"'
                      ' to define generic alias'.format(t.name), t)
            return AnyType(TypeOfAny.from_error)
        if isinstance(sym.node, TypeVarTupleExpr):
            if tvar_def is None:
                self.fail(f'TypeVarTuple "{t.name}" is unbound', t)
                return AnyType(TypeOfAny.from_error)
            assert isinstance(tvar_def, TypeVarTupleType)
            if len(t.args) &gt; 0:
                self.fail(f'Type variable "{t.name}" used with arguments', t)
            # Change the line number
            return TypeVarTupleType(
                tvar_def.name, tvar_def.fullname, tvar_def.id,
                tvar_def.upper_bound, line=t.line, column=t.column,
            )
        special = self.try_analyze_special_unbound_type(t, fullname)
        if special is not None:
            return special
        if isinstance(node, TypeAlias):
            self.aliases_used.add(fullname)
            an_args = self.anal_array(t.args)
            disallow_any = self.options.disallow_any_generics and not self.is_typeshed_stub
            res = expand_type_alias(node, an_args, self.fail, node.no_args, t,
                                    unexpanded_type=t,
                                    disallow_any=disallow_any)
            # The only case where expand_type_alias() can return an incorrect instance is
            # when it is top-level instance, so no need to recurse.
            if (isinstance(res, Instance) and  # type: ignore[misc]
                    len(res.args) != len(res.type.type_vars) and
                    not self.defining_alias):
                fix_instance(
                    res,
                    self.fail,
                    self.note,
                    disallow_any=disallow_any,
                    python_version=self.options.python_version,
                    use_generic_error=True,
                    unexpanded_type=t)
            if node.eager:
                # TODO: Generate error if recursive (once we have recursive types)
                res = get_proper_type(res)
            return res
        elif isinstance(node, TypeInfo):
            return self.analyze_type_with_type_info(node, t.args, t)
        elif node.fullname in TYPE_ALIAS_NAMES:
            return AnyType(TypeOfAny.special_form)
        # Concatenate is an operator, no need for a proper type
        elif node.fullname in ('typing_extensions.Concatenate', 'typing.Concatenate'):
            # We check the return type further up the stack for valid use locations
            return self.apply_concatenate_operator(t)
        else:
            return self.analyze_unbound_type_without_type_info(t, sym, defining_literal)
    else:  # sym is None
        return AnyType(TypeOfAny.special_form)

</t>
<t tx="ekr.20220525082935.1182">def cannot_resolve_type(self, t: UnboundType) -&gt; None:
    # TODO: Move error message generation to messages.py. We'd first
    #       need access to MessageBuilder here. Also move the similar
    #       message generation logic in semanal.py.
    self.api.fail(
        f'Cannot resolve name "{t.name}" (possible cyclic definition)',
        t)

</t>
<t tx="ekr.20220525082935.1183">def apply_concatenate_operator(self, t: UnboundType) -&gt; Type:
    if len(t.args) == 0:
        self.api.fail('Concatenate needs type arguments', t)
        return AnyType(TypeOfAny.from_error)

    # last argument has to be ParamSpec
    ps = self.anal_type(t.args[-1], allow_param_spec=True)
    if not isinstance(ps, ParamSpecType):
        self.api.fail('The last parameter to Concatenate needs to be a ParamSpec', t)
        return AnyType(TypeOfAny.from_error)

    # TODO: this may not work well with aliases, if those worked.
    #   Those should be special-cased.
    elif ps.prefix.arg_types:
        self.api.fail('Nested Concatenates are invalid', t)

    args = self.anal_array(t.args[:-1])
    pre = ps.prefix

    # mypy can't infer this :(
    names: List[Optional[str]] = [None] * len(args)

    pre = Parameters(args + pre.arg_types,
                     [ARG_POS] * len(args) + pre.arg_kinds,
                     names + pre.arg_names)
    return ps.copy_modified(prefix=pre)

</t>
<t tx="ekr.20220525082935.1184">def try_analyze_special_unbound_type(self, t: UnboundType, fullname: str) -&gt; Optional[Type]:
    """Bind special type that is recognized through magic name such as 'typing.Any'.

    Return the bound type if successful, and return None if the type is a normal type.
    """
    if fullname == 'builtins.None':
        return NoneType()
    elif fullname == 'typing.Any' or fullname == 'builtins.Any':
        return AnyType(TypeOfAny.explicit)
    elif fullname in FINAL_TYPE_NAMES:
        self.fail("Final can be only used as an outermost qualifier"
                  " in a variable annotation", t)
        return AnyType(TypeOfAny.from_error)
    elif (fullname == 'typing.Tuple' or
         (fullname == 'builtins.tuple'
            and (self.always_allow_new_syntax or self.options.python_version &gt;= (3, 9)))):
        # Tuple is special because it is involved in builtin import cycle
        # and may be not ready when used.
        sym = self.api.lookup_fully_qualified_or_none('builtins.tuple')
        if not sym or isinstance(sym.node, PlaceholderNode):
            if self.api.is_incomplete_namespace('builtins'):
                self.api.record_incomplete_ref()
            else:
                self.fail('Name "tuple" is not defined', t)
            return AnyType(TypeOfAny.special_form)
        if len(t.args) == 0 and not t.empty_tuple_index:
            # Bare 'Tuple' is same as 'tuple'
            any_type = self.get_omitted_any(t)
            return self.named_type('builtins.tuple', [any_type],
                                   line=t.line, column=t.column)
        if len(t.args) == 2 and isinstance(t.args[1], EllipsisType):
            # Tuple[T, ...] (uniform, variable-length tuple)
            instance = self.named_type('builtins.tuple', [self.anal_type(t.args[0])])
            instance.line = t.line
            return instance
        return self.tuple_type(self.anal_array(t.args))
    elif fullname == 'typing.Union':
        items = self.anal_array(t.args)
        return UnionType.make_union(items)
    elif fullname == 'typing.Optional':
        if len(t.args) != 1:
            self.fail('Optional[...] must have exactly one type argument', t)
            return AnyType(TypeOfAny.from_error)
        item = self.anal_type(t.args[0])
        return make_optional_type(item)
    elif fullname == 'typing.Callable':
        return self.analyze_callable_type(t)
    elif (fullname == 'typing.Type' or
         (fullname == 'builtins.type'
            and (self.always_allow_new_syntax or self.options.python_version &gt;= (3, 9)))):
        if len(t.args) == 0:
            if fullname == 'typing.Type':
                any_type = self.get_omitted_any(t)
                return TypeType(any_type, line=t.line, column=t.column)
            else:
                # To prevent assignment of 'builtins.type' inferred as 'builtins.object'
                # See https://github.com/python/mypy/issues/9476 for more information
                return None
        if len(t.args) != 1:
            type_str = 'Type[...]' if fullname == 'typing.Type' else 'type[...]'
            self.fail(type_str + ' must have exactly one type argument', t)
        item = self.anal_type(t.args[0])
        return TypeType.make_normalized(item, line=t.line)
    elif fullname == 'typing.ClassVar':
        if self.nesting_level &gt; 0:
            self.fail('Invalid type: ClassVar nested inside other type', t)
        if len(t.args) == 0:
            return AnyType(TypeOfAny.from_omitted_generics, line=t.line, column=t.column)
        if len(t.args) != 1:
            self.fail('ClassVar[...] must have at most one type argument', t)
            return AnyType(TypeOfAny.from_error)
        return self.anal_type(t.args[0])
    elif fullname in NEVER_NAMES:
        return UninhabitedType(is_noreturn=True)
    elif fullname in LITERAL_TYPE_NAMES:
        return self.analyze_literal_type(t)
    elif fullname in ANNOTATED_TYPE_NAMES:
        if len(t.args) &lt; 2:
            self.fail("Annotated[...] must have exactly one type argument"
                      " and at least one annotation", t)
            return AnyType(TypeOfAny.from_error)
        return self.anal_type(t.args[0])
    elif fullname in ('typing_extensions.Required', 'typing.Required'):
        if not self.allow_required:
            self.fail("Required[] can be only used in a TypedDict definition", t)
            return AnyType(TypeOfAny.from_error)
        if len(t.args) != 1:
            self.fail("Required[] must have exactly one type argument", t)
            return AnyType(TypeOfAny.from_error)
        return RequiredType(self.anal_type(t.args[0]), required=True)
    elif fullname in ('typing_extensions.NotRequired', 'typing.NotRequired'):
        if not self.allow_required:
            self.fail("NotRequired[] can be only used in a TypedDict definition", t)
            return AnyType(TypeOfAny.from_error)
        if len(t.args) != 1:
            self.fail("NotRequired[] must have exactly one type argument", t)
            return AnyType(TypeOfAny.from_error)
        return RequiredType(self.anal_type(t.args[0]), required=False)
    elif self.anal_type_guard_arg(t, fullname) is not None:
        # In most contexts, TypeGuard[...] acts as an alias for bool (ignoring its args)
        return self.named_type('builtins.bool')
    elif fullname in ('typing.Unpack', 'typing_extensions.Unpack'):
        # We don't want people to try to use this yet.
        if not self.options.enable_incomplete_features:
            self.fail('"Unpack" is not supported by mypy yet', t)
            return AnyType(TypeOfAny.from_error)
        return UnpackType(
            self.anal_type(t.args[0]), line=t.line, column=t.column,
        )
    return None

</t>
<t tx="ekr.20220525082935.1185">def get_omitted_any(self, typ: Type, fullname: Optional[str] = None) -&gt; AnyType:
    disallow_any = not self.is_typeshed_stub and self.options.disallow_any_generics
    return get_omitted_any(disallow_any, self.fail, self.note, typ,
                           self.options.python_version, fullname)

</t>
<t tx="ekr.20220525082935.1186">def analyze_type_with_type_info(
        self, info: TypeInfo, args: Sequence[Type], ctx: Context) -&gt; Type:
    """Bind unbound type when were able to find target TypeInfo.

    This handles simple cases like 'int', 'modname.UserClass[str]', etc.
    """

    if len(args) &gt; 0 and info.fullname == 'builtins.tuple':
        fallback = Instance(info, [AnyType(TypeOfAny.special_form)], ctx.line)
        return TupleType(self.anal_array(args), fallback, ctx.line)

    # This is a heuristic: it will be checked later anyways but the error
    # message may be worse.
    with self.set_allow_param_spec_literals(info.has_param_spec_type):
        # Analyze arguments and (usually) construct Instance type. The
        # number of type arguments and their values are
        # checked only later, since we do not always know the
        # valid count at this point. Thus we may construct an
        # Instance with an invalid number of type arguments.
        instance = Instance(info, self.anal_array(args, allow_param_spec=True),
                            ctx.line, ctx.column)

    # "aesthetic" paramspec literals
    # these do not support mypy_extensions VarArgs, etc. as they were already analyzed
    #   TODO: should these be re-analyzed to get rid of this inconsistency?
    # another inconsistency is with empty type args (Z[] is more possibly an error imo)
    if len(info.type_vars) == 1 and info.has_param_spec_type and len(instance.args) &gt; 0:
        first_arg = get_proper_type(instance.args[0])

        # TODO: can I use tuple syntax to isinstance multiple in 3.6?
        if not (len(instance.args) == 1 and (isinstance(first_arg, Parameters) or
                                             isinstance(first_arg, ParamSpecType) or
                                             isinstance(first_arg, AnyType))):
            args = instance.args
            instance.args = (Parameters(args, [ARG_POS] * len(args), [None] * len(args)),)

    # Check type argument count.
    if len(instance.args) != len(info.type_vars) and not self.defining_alias:
        fix_instance(instance, self.fail, self.note,
                     disallow_any=self.options.disallow_any_generics and
                     not self.is_typeshed_stub,
                     python_version=self.options.python_version)

    tup = info.tuple_type
    if tup is not None:
        # The class has a Tuple[...] base class so it will be
        # represented as a tuple type.
        if args:
            self.fail('Generic tuple types not supported', ctx)
            return AnyType(TypeOfAny.from_error)
        return tup.copy_modified(items=self.anal_array(tup.items),
                                 fallback=instance)
    td = info.typeddict_type
    if td is not None:
        # The class has a TypedDict[...] base class so it will be
        # represented as a typeddict type.
        if args:
            self.fail('Generic TypedDict types not supported', ctx)
            return AnyType(TypeOfAny.from_error)
        # Create a named TypedDictType
        return td.copy_modified(item_types=self.anal_array(list(td.items.values())),
                                fallback=instance)
    return instance

</t>
<t tx="ekr.20220525082935.1187">def analyze_unbound_type_without_type_info(self, t: UnboundType, sym: SymbolTableNode,
                                           defining_literal: bool) -&gt; Type:
    """Figure out what an unbound type that doesn't refer to a TypeInfo node means.

    This is something unusual. We try our best to find out what it is.
    """
    name = sym.fullname
    if name is None:
        assert sym.node is not None
        name = sym.node.name
    # Option 1:
    # Something with an Any type -- make it an alias for Any in a type
    # context. This is slightly problematic as it allows using the type 'Any'
    # as a base class -- however, this will fail soon at runtime so the problem
    # is pretty minor.
    if isinstance(sym.node, Var):
        typ = get_proper_type(sym.node.type)
        if isinstance(typ, AnyType):
            return AnyType(TypeOfAny.from_unimported_type,
                           missing_import_name=typ.missing_import_name)
    # Option 2:
    # Unbound type variable. Currently these may be still valid,
    # for example when defining a generic type alias.
    unbound_tvar = (isinstance(sym.node, (TypeVarExpr, TypeVarTupleExpr)) and
                    self.tvar_scope.get_binding(sym) is None)
    if self.allow_unbound_tvars and unbound_tvar:
        return t

    # Option 3:
    # Enum value. Note: we only want to return a LiteralType when
    # we're using this enum value specifically within context of
    # a "Literal[...]" type. So, if `defining_literal` is not set,
    # we bail out early with an error.
    #
    # If, in the distant future, we decide to permit things like
    # `def foo(x: Color.RED) -&gt; None: ...`, we can remove that
    # check entirely.
    if isinstance(sym.node, Var) and sym.node.info and sym.node.info.is_enum:
        value = sym.node.name
        base_enum_short_name = sym.node.info.name
        if not defining_literal:
            msg = message_registry.INVALID_TYPE_RAW_ENUM_VALUE.format(
                base_enum_short_name, value)
            self.fail(msg, t)
            return AnyType(TypeOfAny.from_error)
        return LiteralType(
            value=value,
            fallback=Instance(sym.node.info, [], line=t.line, column=t.column),
            line=t.line,
            column=t.column,
        )

    # None of the above options worked. We parse the args (if there are any)
    # to make sure there are no remaining semanal-only types, then give up.
    t = t.copy_modified(args=self.anal_array(t.args))
    # TODO: Move this message building logic to messages.py.
    notes: List[str] = []
    if isinstance(sym.node, Var):
        notes.append('See https://mypy.readthedocs.io/en/'
                     'stable/common_issues.html#variables-vs-type-aliases')
        message = 'Variable "{}" is not valid as a type'
    elif isinstance(sym.node, (SYMBOL_FUNCBASE_TYPES, Decorator)):
        message = 'Function "{}" is not valid as a type'
        if name == 'builtins.any':
            notes.append('Perhaps you meant "typing.Any" instead of "any"?')
        elif name == 'builtins.callable':
            notes.append('Perhaps you meant "typing.Callable" instead of "callable"?')
        else:
            notes.append('Perhaps you need "Callable[...]" or a callback protocol?')
    elif isinstance(sym.node, MypyFile):
        # TODO: suggest a protocol when supported.
        message = 'Module "{}" is not valid as a type'
    elif unbound_tvar:
        message = 'Type variable "{}" is unbound'
        short = name.split('.')[-1]
        notes.append(('(Hint: Use "Generic[{}]" or "Protocol[{}]" base class'
                      ' to bind "{}" inside a class)').format(short, short, short))
        notes.append('(Hint: Use "{}" in function signature to bind "{}"'
                     ' inside a function)'.format(short, short))
    else:
        message = 'Cannot interpret reference "{}" as a type'
    self.fail(message.format(name), t, code=codes.VALID_TYPE)
    for note in notes:
        self.note(note, t, code=codes.VALID_TYPE)

    # TODO: Would it be better to always return Any instead of UnboundType
    # in case of an error? On one hand, UnboundType has a name so error messages
    # are more detailed, on the other hand, some of them may be bogus,
    # see https://github.com/python/mypy/issues/4987.
    return t

</t>
<t tx="ekr.20220525082935.1188">def visit_any(self, t: AnyType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082935.1189">def visit_none_type(self, t: NoneType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082935.119">def analyze_value_types(self, items: List[Expression]) -&gt; List[Type]:
    """Analyze types from values expressions in type variable definition."""
    result: List[Type] = []
    for node in items:
        try:
            analyzed = self.anal_type(self.expr_to_unanalyzed_type(node),
                                      allow_placeholder=True)
            if analyzed is None:
                # Type variables are special: we need to place them in the symbol table
                # soon, even if some value is not ready yet, see process_typevar_parameters()
                # for an example.
                analyzed = PlaceholderType(None, [], node.line)
            result.append(analyzed)
        except TypeTranslationError:
            self.fail('Type expected', node)
            result.append(AnyType(TypeOfAny.from_error))
    return result

</t>
<t tx="ekr.20220525082935.1190">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082935.1191">def visit_erased_type(self, t: ErasedType) -&gt; Type:
    # This type should exist only temporarily during type inference
    assert False, "Internal error: Unexpected erased type"

</t>
<t tx="ekr.20220525082935.1192">def visit_deleted_type(self, t: DeletedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082935.1193">def visit_type_list(self, t: TypeList) -&gt; Type:
    # paramspec literal (Z[[int, str, Whatever]])
    if self.allow_param_spec_literals:
        params = self.analyze_callable_args(t)
        if params:
            ts, kinds, names = params
            # bind these types
            return Parameters(self.anal_array(ts), kinds, names)
        else:
            return AnyType(TypeOfAny.from_error)
    else:
        self.fail('Bracketed expression "[...]" is not valid as a type', t)
        self.note('Did you mean "List[...]"?', t)
        return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20220525082935.1194">def visit_callable_argument(self, t: CallableArgument) -&gt; Type:
    self.fail('Invalid type', t)
    return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20220525082935.1195">def visit_instance(self, t: Instance) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082935.1196">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # TODO: should we do something here?
    return t

</t>
<t tx="ekr.20220525082935.1197">def visit_type_var(self, t: TypeVarType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082935.1198">def visit_param_spec(self, t: ParamSpecType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082935.1199">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082935.12">def refresh_top_level(self, file_node: MypyFile) -&gt; None:
    """Reanalyze a stale module top-level in fine-grained incremental mode."""
    self.recurse_into_functions = False
    self.add_implicit_module_attrs(file_node)
    for d in file_node.defs:
        self.accept(d)
    if file_node.fullname == 'typing':
        self.add_builtin_aliases(file_node)
    if file_node.fullname == 'typing_extensions':
        self.add_typing_extension_aliases(file_node)
    self.adjust_public_exports()
    self.export_map[self.cur_mod_id] = self.all_exports
    self.all_exports = []

</t>
<t tx="ekr.20220525082935.120">def check_classvar(self, s: AssignmentStmt) -&gt; None:
    """Check if assignment defines a class variable."""
    lvalue = s.lvalues[0]
    if len(s.lvalues) != 1 or not isinstance(lvalue, RefExpr):
        return
    if not s.type or not self.is_classvar(s.type):
        return
    if self.is_class_scope() and isinstance(lvalue, NameExpr):
        node = lvalue.node
        if isinstance(node, Var):
            node.is_classvar = True
        analyzed = self.anal_type(s.type)
        if analyzed is not None and get_type_vars(analyzed):
            # This means that we have a type var defined inside of a ClassVar.
            # This is not allowed by PEP526.
            # See https://github.com/python/mypy/issues/11538
            self.fail(message_registry.CLASS_VAR_WITH_TYPEVARS, s)
    elif not isinstance(lvalue, MemberExpr) or self.is_self_member_ref(lvalue):
        # In case of member access, report error only when assigning to self
        # Other kinds of member assignments should be already reported
        self.fail_invalid_classvar(lvalue)

</t>
<t tx="ekr.20220525082935.1200">def visit_unpack_type(self, t: UnpackType) -&gt; Type:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.1201">def visit_parameters(self, t: Parameters) -&gt; Type:
    raise NotImplementedError("ParamSpec literals cannot have unbound TypeVars")

</t>
<t tx="ekr.20220525082935.1202">def visit_callable_type(self, t: CallableType, nested: bool = True) -&gt; Type:
    # Every Callable can bind its own type variables, if they're not in the outer scope
    with self.tvar_scope_frame():
        if self.defining_alias:
            variables = t.variables
        else:
            variables = self.bind_function_type_variables(t, t)
        special = self.anal_type_guard(t.ret_type)
        arg_kinds = t.arg_kinds
        if len(arg_kinds) &gt;= 2 and arg_kinds[-2] == ARG_STAR and arg_kinds[-1] == ARG_STAR2:
            arg_types = self.anal_array(t.arg_types[:-2], nested=nested) + [
                self.anal_star_arg_type(t.arg_types[-2], ARG_STAR, nested=nested),
                self.anal_star_arg_type(t.arg_types[-1], ARG_STAR2, nested=nested),
            ]
        else:
            arg_types = self.anal_array(t.arg_types, nested=nested)
        ret = t.copy_modified(arg_types=arg_types,
                              ret_type=self.anal_type(t.ret_type, nested=nested),
                              # If the fallback isn't filled in yet,
                              # its type will be the falsey FakeInfo
                              fallback=(t.fallback if t.fallback.type
                                        else self.named_type('builtins.function')),
                              variables=self.anal_var_defs(variables),
                              type_guard=special,
                              )
    return ret

</t>
<t tx="ekr.20220525082935.1203">def anal_type_guard(self, t: Type) -&gt; Optional[Type]:
    if isinstance(t, UnboundType):
        sym = self.lookup_qualified(t.name, t)
        if sym is not None and sym.node is not None:
            return self.anal_type_guard_arg(t, sym.node.fullname)
    # TODO: What if it's an Instance? Then use t.type.fullname?
    return None

</t>
<t tx="ekr.20220525082935.1204">def anal_type_guard_arg(self, t: UnboundType, fullname: str) -&gt; Optional[Type]:
    if fullname in ('typing_extensions.TypeGuard', 'typing.TypeGuard'):
        if len(t.args) != 1:
            self.fail("TypeGuard must have exactly one type argument", t)
            return AnyType(TypeOfAny.from_error)
        return self.anal_type(t.args[0])
    return None

</t>
<t tx="ekr.20220525082935.1205">def anal_star_arg_type(self, t: Type, kind: ArgKind, nested: bool) -&gt; Type:
    """Analyze signature argument type for *args and **kwargs argument."""
    # TODO: Check that suffix and kind match
    if isinstance(t, UnboundType) and t.name and '.' in t.name and not t.args:
        components = t.name.split('.')
        sym = self.lookup_qualified('.'.join(components[:-1]), t)
        if sym is not None and isinstance(sym.node, ParamSpecExpr):
            tvar_def = self.tvar_scope.get_binding(sym)
            if isinstance(tvar_def, ParamSpecType):
                if kind == ARG_STAR:
                    make_paramspec = paramspec_args
                elif kind == ARG_STAR2:
                    make_paramspec = paramspec_kwargs
                else:
                    assert False, kind
                return make_paramspec(tvar_def.name, tvar_def.fullname, tvar_def.id,
                                     named_type_func=self.named_type,
                                     line=t.line, column=t.column)
    return self.anal_type(t, nested=nested)

</t>
<t tx="ekr.20220525082935.1206">def visit_overloaded(self, t: Overloaded) -&gt; Type:
    # Overloaded types are manually constructed in semanal.py by analyzing the
    # AST and combining together the Callable types this visitor converts.
    #
    # So if we're ever asked to reanalyze an Overloaded type, we know it's
    # fine to just return it as-is.
    return t

</t>
<t tx="ekr.20220525082935.1207">def visit_tuple_type(self, t: TupleType) -&gt; Type:
    # Types such as (t1, t2, ...) only allowed in assignment statements. They'll
    # generate errors elsewhere, and Tuple[t1, t2, ...] must be used instead.
    if t.implicit and not self.allow_tuple_literal:
        self.fail('Syntax error in type annotation', t, code=codes.SYNTAX)
        if len(t.items) == 0:
            self.note('Suggestion: Use Tuple[()] instead of () for an empty tuple, or '
            'None for a function without a return value', t, code=codes.SYNTAX)
        elif len(t.items) == 1:
            self.note('Suggestion: Is there a spurious trailing comma?', t, code=codes.SYNTAX)
        else:
            self.note('Suggestion: Use Tuple[T1, ..., Tn] instead of (T1, ..., Tn)', t,
                      code=codes.SYNTAX)
        return AnyType(TypeOfAny.from_error)
    star_count = sum(1 for item in t.items if isinstance(item, StarType))
    if star_count &gt; 1:
        self.fail('At most one star type allowed in a tuple', t)
        if t.implicit:
            return TupleType([AnyType(TypeOfAny.from_error) for _ in t.items],
                             self.named_type('builtins.tuple'),
                             t.line)
        else:
            return AnyType(TypeOfAny.from_error)
    any_type = AnyType(TypeOfAny.special_form)
    # If the fallback isn't filled in yet, its type will be the falsey FakeInfo
    fallback = (t.partial_fallback if t.partial_fallback.type
                else self.named_type('builtins.tuple', [any_type]))
    return TupleType(self.anal_array(t.items), fallback, t.line)

</t>
<t tx="ekr.20220525082935.1208">def visit_typeddict_type(self, t: TypedDictType) -&gt; Type:
    items = OrderedDict([
        (item_name, self.anal_type(item_type))
        for (item_name, item_type) in t.items.items()
    ])
    return TypedDictType(items, set(t.required_keys), t.fallback)

</t>
<t tx="ekr.20220525082935.1209">def visit_raw_expression_type(self, t: RawExpressionType) -&gt; Type:
    # We should never see a bare Literal. We synthesize these raw literals
    # in the earlier stages of semantic analysis, but those
    # "fake literals" should always be wrapped in an UnboundType
    # corresponding to 'Literal'.
    #
    # Note: if at some point in the distant future, we decide to
    # make signatures like "foo(x: 20) -&gt; None" legal, we can change
    # this method so it generates and returns an actual LiteralType
    # instead.

    if self.report_invalid_types:
        if t.base_type_name in ('builtins.int', 'builtins.bool'):
            # The only time it makes sense to use an int or bool is inside of
            # a literal type.
            msg = f"Invalid type: try using Literal[{repr(t.literal_value)}] instead?"
        elif t.base_type_name in ('builtins.float', 'builtins.complex'):
            # We special-case warnings for floats and complex numbers.
            msg = f"Invalid type: {t.simple_name()} literals cannot be used as a type"
        else:
            # And in all other cases, we default to a generic error message.
            # Note: the reason why we use a generic error message for strings
            # but not ints or bools is because whenever we see an out-of-place
            # string, it's unclear if the user meant to construct a literal type
            # or just misspelled a regular type. So we avoid guessing.
            msg = 'Invalid type comment or annotation'

        self.fail(msg, t, code=codes.VALID_TYPE)
        if t.note is not None:
            self.note(t.note, t, code=codes.VALID_TYPE)

    return AnyType(TypeOfAny.from_error, line=t.line, column=t.column)

</t>
<t tx="ekr.20220525082935.121">def is_classvar(self, typ: Type) -&gt; bool:
    if not isinstance(typ, UnboundType):
        return False
    sym = self.lookup_qualified(typ.name, typ)
    if not sym or not sym.node:
        return False
    return sym.node.fullname == 'typing.ClassVar'

</t>
<t tx="ekr.20220525082935.1210">def visit_literal_type(self, t: LiteralType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082935.1211">def visit_star_type(self, t: StarType) -&gt; Type:
    return StarType(self.anal_type(t.type), t.line)

</t>
<t tx="ekr.20220525082935.1212">def visit_union_type(self, t: UnionType) -&gt; Type:
    if (t.uses_pep604_syntax is True
            and t.is_evaluated is True
            and not self.always_allow_new_syntax
            and not self.options.python_version &gt;= (3, 10)):
        self.fail("X | Y syntax for unions requires Python 3.10", t)
    return UnionType(self.anal_array(t.items), t.line)

</t>
<t tx="ekr.20220525082935.1213">def visit_partial_type(self, t: PartialType) -&gt; Type:
    assert False, "Internal error: Unexpected partial type"

</t>
<t tx="ekr.20220525082935.1214">def visit_ellipsis_type(self, t: EllipsisType) -&gt; Type:
    if self.allow_param_spec_literals:
        any_type = AnyType(TypeOfAny.explicit)
        return Parameters([any_type, any_type],
                          [ARG_STAR, ARG_STAR2],
                          [None, None],
                          is_ellipsis_args=True)
    else:
        self.fail('Unexpected "..."', t)
        return AnyType(TypeOfAny.from_error)

</t>
<t tx="ekr.20220525082935.1215">def visit_type_type(self, t: TypeType) -&gt; Type:
    return TypeType.make_normalized(self.anal_type(t.item), line=t.line)

</t>
<t tx="ekr.20220525082935.1216">def visit_placeholder_type(self, t: PlaceholderType) -&gt; Type:
    n = None if t.fullname is None else self.api.lookup_fully_qualified(t.fullname)
    if not n or isinstance(n.node, PlaceholderNode):
        self.api.defer()  # Still incomplete
        return t
    else:
        # TODO: Handle non-TypeInfo
        assert isinstance(n.node, TypeInfo)
        return self.analyze_type_with_type_info(n.node, t.args, t)

</t>
<t tx="ekr.20220525082935.1217">def analyze_callable_args_for_paramspec(
    self,
    callable_args: Type,
    ret_type: Type,
    fallback: Instance,
) -&gt; Optional[CallableType]:
    """Construct a 'Callable[P, RET]', where P is ParamSpec, return None if we cannot."""
    if not isinstance(callable_args, UnboundType):
        return None
    sym = self.lookup_qualified(callable_args.name, callable_args)
    if sym is None:
        return None
    tvar_def = self.tvar_scope.get_binding(sym)
    if not isinstance(tvar_def, ParamSpecType):
        return None

    return CallableType(
        [paramspec_args(tvar_def.name, tvar_def.fullname, tvar_def.id,
                       named_type_func=self.named_type),
         paramspec_kwargs(tvar_def.name, tvar_def.fullname, tvar_def.id,
                       named_type_func=self.named_type)],
        [nodes.ARG_STAR, nodes.ARG_STAR2],
        [None, None],
        ret_type=ret_type,
        fallback=fallback,
    )

</t>
<t tx="ekr.20220525082935.1218">def analyze_callable_args_for_concatenate(
    self,
    callable_args: Type,
    ret_type: Type,
    fallback: Instance,
) -&gt; Optional[CallableType]:
    """Construct a 'Callable[C, RET]', where C is Concatenate[..., P], returning None if we
    cannot.
    """
    if not isinstance(callable_args, UnboundType):
        return None
    sym = self.lookup_qualified(callable_args.name, callable_args)
    if sym is None:
        return None
    if sym.node is None:
        return None
    if sym.node.fullname not in ('typing_extensions.Concatenate', 'typing.Concatenate'):
        return None

    tvar_def = self.anal_type(callable_args, allow_param_spec=True)
    if not isinstance(tvar_def, ParamSpecType):
        return None

    # ick, CallableType should take ParamSpecType
    prefix = tvar_def.prefix
    # we don't set the prefix here as generic arguments will get updated at some point
    # in the future. CallableType.param_spec() accounts for this.
    return CallableType(
        [*prefix.arg_types,
         paramspec_args(tvar_def.name, tvar_def.fullname, tvar_def.id,
                       named_type_func=self.named_type),
         paramspec_kwargs(tvar_def.name, tvar_def.fullname, tvar_def.id,
                       named_type_func=self.named_type)],
        [*prefix.arg_kinds, nodes.ARG_STAR, nodes.ARG_STAR2],
        [*prefix.arg_names, None, None],
        ret_type=ret_type,
        fallback=fallback,
        from_concatenate=True,
    )

</t>
<t tx="ekr.20220525082935.1219">def analyze_callable_type(self, t: UnboundType) -&gt; Type:
    fallback = self.named_type('builtins.function')
    if len(t.args) == 0:
        # Callable (bare). Treat as Callable[..., Any].
        any_type = self.get_omitted_any(t)
        ret = callable_with_ellipsis(any_type, any_type, fallback)
    elif len(t.args) == 2:
        callable_args = t.args[0]
        ret_type = t.args[1]
        if isinstance(callable_args, TypeList):
            # Callable[[ARG, ...], RET] (ordinary callable type)
            analyzed_args = self.analyze_callable_args(callable_args)
            if analyzed_args is None:
                return AnyType(TypeOfAny.from_error)
            args, kinds, names = analyzed_args
            ret = CallableType(args,
                               kinds,
                               names,
                               ret_type=ret_type,
                               fallback=fallback)
        elif isinstance(callable_args, EllipsisType):
            # Callable[..., RET] (with literal ellipsis; accept arbitrary arguments)
            ret = callable_with_ellipsis(AnyType(TypeOfAny.explicit),
                                         ret_type=ret_type,
                                         fallback=fallback)
        else:
            # Callable[P, RET] (where P is ParamSpec)
            maybe_ret = self.analyze_callable_args_for_paramspec(
                callable_args,
                ret_type,
                fallback
            ) or self.analyze_callable_args_for_concatenate(
                callable_args,
                ret_type,
                fallback
            )
            if maybe_ret is None:
                # Callable[?, RET] (where ? is something invalid)
                self.fail(
                    'The first argument to Callable must be a '
                    'list of types, parameter specification, or "..."', t)
                self.note(
                    'See https://mypy.readthedocs.io/en/stable/kinds_of_types.html#callable-types-and-lambdas',  # noqa: E501
                    t
                )
                return AnyType(TypeOfAny.from_error)
            ret = maybe_ret
    else:
        if self.options.disallow_any_generics:
            self.fail('Please use "Callable[[&lt;parameters&gt;], &lt;return type&gt;]"', t)
        else:
            self.fail('Please use "Callable[[&lt;parameters&gt;], &lt;return type&gt;]" or "Callable"', t)
        return AnyType(TypeOfAny.from_error)
    assert isinstance(ret, CallableType)
    return ret.accept(self)

</t>
<t tx="ekr.20220525082935.122">def is_final_type(self, typ: Optional[Type]) -&gt; bool:
    if not isinstance(typ, UnboundType):
        return False
    sym = self.lookup_qualified(typ.name, typ)
    if not sym or not sym.node:
        return False
    return sym.node.fullname in FINAL_TYPE_NAMES

</t>
<t tx="ekr.20220525082935.1220">def analyze_callable_args(self, arglist: TypeList) -&gt; Optional[Tuple[List[Type],
                                                                     List[ArgKind],
                                                                     List[Optional[str]]]]:
    args: List[Type] = []
    kinds: List[ArgKind] = []
    names: List[Optional[str]] = []
    for arg in arglist.items:
        if isinstance(arg, CallableArgument):
            args.append(arg.typ)
            names.append(arg.name)
            if arg.constructor is None:
                return None
            found = self.lookup_qualified(arg.constructor, arg)
            if found is None:
                # Looking it up already put an error message in
                return None
            elif found.fullname not in ARG_KINDS_BY_CONSTRUCTOR:
                self.fail(f'Invalid argument constructor "{found.fullname}"', arg)
                return None
            else:
                assert found.fullname is not None
                kind = ARG_KINDS_BY_CONSTRUCTOR[found.fullname]
                kinds.append(kind)
                if arg.name is not None and kind.is_star():
                    self.fail("{} arguments should not have names".format(
                        arg.constructor), arg)
                    return None
        else:
            args.append(arg)
            kinds.append(ARG_POS)
            names.append(None)
    # Note that arglist below is only used for error context.
    check_arg_names(names, [arglist] * len(args), self.fail, "Callable")
    check_arg_kinds(kinds, [arglist] * len(args), self.fail)
    return args, kinds, names

</t>
<t tx="ekr.20220525082935.1221">def analyze_literal_type(self, t: UnboundType) -&gt; Type:
    if len(t.args) == 0:
        self.fail('Literal[...] must have at least one parameter', t)
        return AnyType(TypeOfAny.from_error)

    output: List[Type] = []
    for i, arg in enumerate(t.args):
        analyzed_types = self.analyze_literal_param(i + 1, arg, t)
        if analyzed_types is None:
            return AnyType(TypeOfAny.from_error)
        else:
            output.extend(analyzed_types)
    return UnionType.make_union(output, line=t.line)

</t>
<t tx="ekr.20220525082935.1222">def analyze_literal_param(self, idx: int, arg: Type, ctx: Context) -&gt; Optional[List[Type]]:
    # This UnboundType was originally defined as a string.
    if isinstance(arg, UnboundType) and arg.original_str_expr is not None:
        assert arg.original_str_fallback is not None
        return [LiteralType(
            value=arg.original_str_expr,
            fallback=self.named_type_with_normalized_str(arg.original_str_fallback),
            line=arg.line,
            column=arg.column,
        )]

    # If arg is an UnboundType that was *not* originally defined as
    # a string, try expanding it in case it's a type alias or something.
    if isinstance(arg, UnboundType):
        self.nesting_level += 1
        try:
            arg = self.visit_unbound_type(arg, defining_literal=True)
        finally:
            self.nesting_level -= 1

    # Literal[...] cannot contain Any. Give up and add an error message
    # (if we haven't already).
    arg = get_proper_type(arg)
    if isinstance(arg, AnyType):
        # Note: We can encounter Literals containing 'Any' under three circumstances:
        #
        # 1. If the user attempts use an explicit Any as a parameter
        # 2. If the user is trying to use an enum value imported from a module with
        #    no type hints, giving it an implicit type of 'Any'
        # 3. If there's some other underlying problem with the parameter.
        #
        # We report an error in only the first two cases. In the third case, we assume
        # some other region of the code has already reported a more relevant error.
        #
        # TODO: Once we start adding support for enums, make sure we report a custom
        # error for case 2 as well.
        if arg.type_of_any not in (TypeOfAny.from_error, TypeOfAny.special_form):
            self.fail(f'Parameter {idx} of Literal[...] cannot be of type "Any"', ctx)
        return None
    elif isinstance(arg, RawExpressionType):
        # A raw literal. Convert it directly into a literal if we can.
        if arg.literal_value is None:
            name = arg.simple_name()
            if name in ('float', 'complex'):
                msg = f'Parameter {idx} of Literal[...] cannot be of type "{name}"'
            else:
                msg = 'Invalid type: Literal[...] cannot contain arbitrary expressions'
            self.fail(msg, ctx)
            # Note: we deliberately ignore arg.note here: the extra info might normally be
            # helpful, but it generally won't make sense in the context of a Literal[...].
            return None

        # Remap bytes and unicode into the appropriate type for the correct Python version
        fallback = self.named_type_with_normalized_str(arg.base_type_name)
        assert isinstance(fallback, Instance)
        return [LiteralType(arg.literal_value, fallback, line=arg.line, column=arg.column)]
    elif isinstance(arg, (NoneType, LiteralType)):
        # Types that we can just add directly to the literal/potential union of literals.
        return [arg]
    elif isinstance(arg, Instance) and arg.last_known_value is not None:
        # Types generated from declarations like "var: Final = 4".
        return [arg.last_known_value]
    elif isinstance(arg, UnionType):
        out = []
        for union_arg in arg.items:
            union_result = self.analyze_literal_param(idx, union_arg, ctx)
            if union_result is None:
                return None
            out.extend(union_result)
        return out
    else:
        self.fail(f'Parameter {idx} of Literal[...] is invalid', ctx)
        return None

</t>
<t tx="ekr.20220525082935.1223">def analyze_type(self, t: Type) -&gt; Type:
    return t.accept(self)

</t>
<t tx="ekr.20220525082935.1224">def fail(self, msg: str, ctx: Context, *, code: Optional[ErrorCode] = None) -&gt; None:
    self.fail_func(msg, ctx, code=code)

</t>
<t tx="ekr.20220525082935.1225">def note(self, msg: str, ctx: Context, *, code: Optional[ErrorCode] = None) -&gt; None:
    self.note_func(msg, ctx, code=code)

</t>
<t tx="ekr.20220525082935.1226">@contextmanager
def tvar_scope_frame(self) -&gt; Iterator[None]:
    old_scope = self.tvar_scope
    self.tvar_scope = self.tvar_scope.method_frame()
    yield
    self.tvar_scope = old_scope

</t>
<t tx="ekr.20220525082935.1227">def infer_type_variables(self,
                         type: CallableType) -&gt; List[Tuple[str, TypeVarLikeExpr]]:
    """Return list of unique type variables referred to in a callable."""
    names: List[str] = []
    tvars: List[TypeVarLikeExpr] = []
    for arg in type.arg_types:
        for name, tvar_expr in arg.accept(
            TypeVarLikeQuery(self.lookup_qualified, self.tvar_scope)
        ):
            if name not in names:
                names.append(name)
                tvars.append(tvar_expr)
    # When finding type variables in the return type of a function, don't
    # look inside Callable types.  Type variables only appearing in
    # functions in the return type belong to those functions, not the
    # function we're currently analyzing.
    for name, tvar_expr in type.ret_type.accept(
        TypeVarLikeQuery(self.lookup_qualified, self.tvar_scope, include_callables=False)
    ):
        if name not in names:
            names.append(name)
            tvars.append(tvar_expr)
    return list(zip(names, tvars))

</t>
<t tx="ekr.20220525082935.1228">def bind_function_type_variables(
    self, fun_type: CallableType, defn: Context
) -&gt; Sequence[TypeVarLikeType]:
    """Find the type variables of the function type and bind them in our tvar_scope"""
    if fun_type.variables:
        for var in fun_type.variables:
            var_node = self.lookup_qualified(var.name, defn)
            assert var_node, "Binding for function type variable not found within function"
            var_expr = var_node.node
            assert isinstance(var_expr, TypeVarLikeExpr)
            self.tvar_scope.bind_new(var.name, var_expr)
        return fun_type.variables
    typevars = self.infer_type_variables(fun_type)
    # Do not define a new type variable if already defined in scope.
    typevars = [(name, tvar) for name, tvar in typevars
                if not self.is_defined_type_var(name, defn)]
    defs: List[TypeVarLikeType] = []
    for name, tvar in typevars:
        if not self.tvar_scope.allow_binding(tvar.fullname):
            self.fail(f'Type variable "{name}" is bound by an outer class', defn)
        self.tvar_scope.bind_new(name, tvar)
        binding = self.tvar_scope.get_binding(tvar.fullname)
        assert binding is not None
        defs.append(binding)

    return defs

</t>
<t tx="ekr.20220525082935.1229">def is_defined_type_var(self, tvar: str, context: Context) -&gt; bool:
    tvar_node = self.lookup_qualified(tvar, context)
    if not tvar_node:
        return False
    return self.tvar_scope.get_binding(tvar_node) is not None

</t>
<t tx="ekr.20220525082935.123">def fail_invalid_classvar(self, context: Context) -&gt; None:
    self.fail(message_registry.CLASS_VAR_OUTSIDE_OF_CLASS, context)

</t>
<t tx="ekr.20220525082935.1230">def anal_array(self,
               a: Iterable[Type],
               nested: bool = True, *,
               allow_param_spec: bool = False) -&gt; List[Type]:
    res: List[Type] = []
    for t in a:
        res.append(self.anal_type(t, nested, allow_param_spec=allow_param_spec))
    return res

</t>
<t tx="ekr.20220525082935.1231">def anal_type(self, t: Type, nested: bool = True, *, allow_param_spec: bool = False) -&gt; Type:
    if nested:
        self.nesting_level += 1
    old_allow_required = self.allow_required
    self.allow_required = False
    try:
        analyzed = t.accept(self)
    finally:
        if nested:
            self.nesting_level -= 1
        self.allow_required = old_allow_required
    if (not allow_param_spec
            and isinstance(analyzed, ParamSpecType)
            and analyzed.flavor == ParamSpecFlavor.BARE):
        if analyzed.prefix.arg_types:
            self.fail('Invalid location for Concatenate', t)
            self.note(
                'You can use Concatenate as the first argument to Callable',
                t
            )
        else:
            self.fail(f'Invalid location for ParamSpec "{analyzed.name}"', t)
            self.note(
                'You can use ParamSpec as the first argument to Callable, e.g., '
                "'Callable[{}, int]'".format(analyzed.name),
                t
            )
    return analyzed

</t>
<t tx="ekr.20220525082935.1232">def anal_var_def(self, var_def: TypeVarLikeType) -&gt; TypeVarLikeType:
    if isinstance(var_def, TypeVarType):
        return TypeVarType(
            var_def.name,
            var_def.fullname,
            var_def.id.raw_id,
            self.anal_array(var_def.values),
            var_def.upper_bound.accept(self),
            var_def.variance,
            var_def.line
        )
    else:
        return var_def

</t>
<t tx="ekr.20220525082935.1233">def anal_var_defs(self, var_defs: Sequence[TypeVarLikeType]) -&gt; List[TypeVarLikeType]:
    return [self.anal_var_def(vd) for vd in var_defs]

</t>
<t tx="ekr.20220525082935.1234">def named_type_with_normalized_str(self, fully_qualified_name: str) -&gt; Instance:
    """Does almost the same thing as `named_type`, except that we immediately
    unalias `builtins.bytes` and `builtins.unicode` to `builtins.str` as appropriate.
    """
    python_version = self.options.python_version
    if python_version[0] == 2 and fully_qualified_name == 'builtins.bytes':
        fully_qualified_name = 'builtins.str'
    if python_version[0] &gt;= 3 and fully_qualified_name == 'builtins.unicode':
        fully_qualified_name = 'builtins.str'
    return self.named_type(fully_qualified_name)

</t>
<t tx="ekr.20220525082935.1235">def named_type(self, fully_qualified_name: str,
               args: Optional[List[Type]] = None,
               line: int = -1,
               column: int = -1) -&gt; Instance:
    node = self.lookup_fqn_func(fully_qualified_name)
    assert isinstance(node.node, TypeInfo)
    any_type = AnyType(TypeOfAny.special_form)
    return Instance(node.node, args or [any_type] * len(node.node.defn.type_vars),
                    line=line, column=column)

</t>
<t tx="ekr.20220525082935.1236">def tuple_type(self, items: List[Type]) -&gt; TupleType:
    any_type = AnyType(TypeOfAny.special_form)
    return TupleType(items, fallback=self.named_type('builtins.tuple', [any_type]))

</t>
<t tx="ekr.20220525082935.1237">@contextmanager
def set_allow_param_spec_literals(self, to: bool) -&gt; Iterator[None]:
    old = self.allow_param_spec_literals
    try:
        self.allow_param_spec_literals = to
        yield
    finally:
        self.allow_param_spec_literals = old


</t>
<t tx="ekr.20220525082935.1238">TypeVarLikeList = List[Tuple[str, TypeVarLikeExpr]]


</t>
<t tx="ekr.20220525082935.1239">class MsgCallback(Protocol):
    @others
</t>
<t tx="ekr.20220525082935.124">def process_module_assignment(self, lvals: List[Lvalue], rval: Expression,
                              ctx: AssignmentStmt) -&gt; None:
    """Propagate module references across assignments.

    Recursively handles the simple form of iterable unpacking; doesn't
    handle advanced unpacking with *rest, dictionary unpacking, etc.

    In an expression like x = y = z, z is the rval and lvals will be [x,
    y].

    """
    if (isinstance(rval, (TupleExpr, ListExpr))
            and all(isinstance(v, TupleExpr) for v in lvals)):
        # rval and all lvals are either list or tuple, so we are dealing
        # with unpacking assignment like `x, y = a, b`. Mypy didn't
        # understand our all(isinstance(...)), so cast them as TupleExpr
        # so mypy knows it is safe to access their .items attribute.
        seq_lvals = cast(List[TupleExpr], lvals)
        # given an assignment like:
        #     (x, y) = (m, n) = (a, b)
        # we now have:
        #     seq_lvals = [(x, y), (m, n)]
        #     seq_rval = (a, b)
        # We now zip this into:
        #     elementwise_assignments = [(a, x, m), (b, y, n)]
        # where each elementwise assignment includes one element of rval and the
        # corresponding element of each lval. Basically we unpack
        #     (x, y) = (m, n) = (a, b)
        # into elementwise assignments
        #     x = m = a
        #     y = n = b
        # and then we recursively call this method for each of those assignments.
        # If the rval and all lvals are not all of the same length, zip will just ignore
        # extra elements, so no error will be raised here; mypy will later complain
        # about the length mismatch in type-checking.
        elementwise_assignments = zip(rval.items, *[v.items for v in seq_lvals])
        for rv, *lvs in elementwise_assignments:
            self.process_module_assignment(lvs, rv, ctx)
    elif isinstance(rval, RefExpr):
        rnode = self.lookup_type_node(rval)
        if rnode and isinstance(rnode.node, MypyFile):
            for lval in lvals:
                if not isinstance(lval, RefExpr):
                    continue
                # respect explicitly annotated type
                if (isinstance(lval.node, Var) and lval.node.type is not None):
                    continue

                # We can handle these assignments to locals and to self
                if isinstance(lval, NameExpr):
                    lnode = self.current_symbol_table().get(lval.name)
                elif isinstance(lval, MemberExpr) and self.is_self_member_ref(lval):
                    assert self.type is not None
                    lnode = self.type.names.get(lval.name)
                else:
                    continue

                if lnode:
                    if isinstance(lnode.node, MypyFile) and lnode.node is not rnode.node:
                        assert isinstance(lval, (NameExpr, MemberExpr))
                        self.fail(
                            'Cannot assign multiple modules to name "{}" '
                            'without explicit "types.ModuleType" annotation'.format(lval.name),
                            ctx)
                    # never create module alias except on initial var definition
                    elif lval.is_inferred_def:
                        assert rnode.node is not None
                        lnode.node = rnode.node

</t>
<t tx="ekr.20220525082935.1240">def __call__(
    self,
    __msg: str,
    __ctx: Context,
    *,
    code: Optional[ErrorCode] = None
) -&gt; None: ...


</t>
<t tx="ekr.20220525082935.1241">def get_omitted_any(disallow_any: bool, fail: MsgCallback, note: MsgCallback,
                    orig_type: Type, python_version: Tuple[int, int],
                    fullname: Optional[str] = None,
                    unexpanded_type: Optional[Type] = None) -&gt; AnyType:
    if disallow_any:
        nongen_builtins = get_nongen_builtins(python_version)
        if fullname in nongen_builtins:
            typ = orig_type
            # We use a dedicated error message for builtin generics (as the most common case).
            alternative = nongen_builtins[fullname]
            fail(message_registry.IMPLICIT_GENERIC_ANY_BUILTIN.format(alternative), typ,
                 code=codes.TYPE_ARG)
        else:
            typ = unexpanded_type or orig_type
            type_str = typ.name if isinstance(typ, UnboundType) else format_type_bare(typ)

            fail(
                message_registry.BARE_GENERIC.format(quote_type_string(type_str)),
                typ,
                code=codes.TYPE_ARG)
            base_type = get_proper_type(orig_type)
            base_fullname = (
                base_type.type.fullname if isinstance(base_type, Instance) else fullname
            )
            # Ideally, we'd check whether the type is quoted or `from __future__ annotations`
            # is set before issuing this note
            if python_version &lt; (3, 9) and base_fullname in GENERIC_STUB_NOT_AT_RUNTIME_TYPES:
                # Recommend `from __future__ import annotations` or to put type in quotes
                # (string literal escaping) for classes not generic at runtime
                note(
                    "Subscripting classes that are not generic at runtime may require "
                    "escaping, see https://mypy.readthedocs.io/en/stable/runtime_troubles.html"
                    "#not-generic-runtime",
                    typ,
                    code=codes.TYPE_ARG)

        any_type = AnyType(TypeOfAny.from_error, line=typ.line, column=typ.column)
    else:
        any_type = AnyType(
            TypeOfAny.from_omitted_generics, line=orig_type.line, column=orig_type.column
        )
    return any_type


</t>
<t tx="ekr.20220525082935.1242">def fix_instance(t: Instance, fail: MsgCallback, note: MsgCallback,
                 disallow_any: bool, python_version: Tuple[int, int],
                 use_generic_error: bool = False,
                 unexpanded_type: Optional[Type] = None,) -&gt; None:
    """Fix a malformed instance by replacing all type arguments with Any.

    Also emit a suitable error if this is not due to implicit Any's.
    """
    if len(t.args) == 0:
        if use_generic_error:
            fullname: Optional[str] = None
        else:
            fullname = t.type.fullname
        any_type = get_omitted_any(disallow_any, fail, note, t, python_version, fullname,
                                   unexpanded_type)
        t.args = (any_type,) * len(t.type.type_vars)
        return
    # Invalid number of type parameters.
    n = len(t.type.type_vars)
    s = f'{n} type arguments'
    if n == 0:
        s = 'no type arguments'
    elif n == 1:
        s = '1 type argument'
    act = str(len(t.args))
    if act == '0':
        act = 'none'
    fail(f'"{t.type.name}" expects {s}, but {act} given', t, code=codes.TYPE_ARG)
    # Construct the correct number of type arguments, as
    # otherwise the type checker may crash as it expects
    # things to be right.
    t.args = tuple(AnyType(TypeOfAny.from_error) for _ in t.type.type_vars)
    t.invalid = True


</t>
<t tx="ekr.20220525082935.1243">def expand_type_alias(node: TypeAlias, args: List[Type],
                      fail: MsgCallback, no_args: bool, ctx: Context, *,
                      unexpanded_type: Optional[Type] = None,
                      disallow_any: bool = False) -&gt; Type:
    """Expand a (generic) type alias target following the rules outlined in TypeAlias docstring.

    Here:
        target: original target type (contains unbound type variables)
        alias_tvars: type variable names
        args: types to be substituted in place of type variables
        fail: error reporter callback
        no_args: whether original definition used a bare generic `A = List`
        ctx: context where expansion happens
    """
    exp_len = len(node.alias_tvars)
    act_len = len(args)
    if exp_len &gt; 0 and act_len == 0:
        # Interpret bare Alias same as normal generic, i.e., Alias[Any, Any, ...]
        return set_any_tvars(node, ctx.line, ctx.column,
                             disallow_any=disallow_any, fail=fail,
                             unexpanded_type=unexpanded_type)
    if exp_len == 0 and act_len == 0:
        if no_args:
            assert isinstance(node.target, Instance)  # type: ignore[misc]
            # Note: this is the only case where we use an eager expansion. See more info about
            # no_args aliases like L = List in the docstring for TypeAlias class.
            return Instance(node.target.type, [], line=ctx.line, column=ctx.column)
        return TypeAliasType(node, [], line=ctx.line, column=ctx.column)
    if (exp_len == 0 and act_len &gt; 0
            and isinstance(node.target, Instance)  # type: ignore[misc]
            and no_args):
        tp = Instance(node.target.type, args)
        tp.line = ctx.line
        tp.column = ctx.column
        return tp
    if act_len != exp_len:
        fail('Bad number of arguments for type alias, expected: %s, given: %s'
             % (exp_len, act_len), ctx)
        return set_any_tvars(node, ctx.line, ctx.column, from_error=True)
    typ = TypeAliasType(node, args, ctx.line, ctx.column)
    assert typ.alias is not None
    # HACK: Implement FlexibleAlias[T, typ] by expanding it to typ here.
    if (isinstance(typ.alias.target, Instance)  # type: ignore
            and typ.alias.target.type.fullname == 'mypy_extensions.FlexibleAlias'):
        exp = get_proper_type(typ)
        assert isinstance(exp, Instance)
        return exp.args[-1]
    return typ


</t>
<t tx="ekr.20220525082935.1244">def set_any_tvars(node: TypeAlias,
                  newline: int, newcolumn: int, *,
                  from_error: bool = False,
                  disallow_any: bool = False,
                  fail: Optional[MsgCallback] = None,
                  unexpanded_type: Optional[Type] = None) -&gt; Type:
    if from_error or disallow_any:
        type_of_any = TypeOfAny.from_error
    else:
        type_of_any = TypeOfAny.from_omitted_generics
    if disallow_any:
        assert fail is not None
        otype = unexpanded_type or node.target
        type_str = otype.name if isinstance(otype, UnboundType) else format_type_bare(otype)

        fail(message_registry.BARE_GENERIC.format(quote_type_string(type_str)),
             Context(newline, newcolumn), code=codes.TYPE_ARG)
    any_type = AnyType(type_of_any, line=newline, column=newcolumn)
    return TypeAliasType(node, [any_type] * len(node.alias_tvars), newline, newcolumn)


</t>
<t tx="ekr.20220525082935.1245">def remove_dups(tvars: Iterable[T]) -&gt; List[T]:
    # Get unique elements in order of appearance
    all_tvars: Set[T] = set()
    new_tvars: List[T] = []
    for t in tvars:
        if t not in all_tvars:
            new_tvars.append(t)
            all_tvars.add(t)
    return new_tvars


</t>
<t tx="ekr.20220525082935.1246">def flatten_tvars(ll: Iterable[List[T]]) -&gt; List[T]:
    return remove_dups(chain.from_iterable(ll))


</t>
<t tx="ekr.20220525082935.1247">class TypeVarLikeQuery(TypeQuery[TypeVarLikeList]):
    """Find TypeVar and ParamSpec references in an unbound type."""

    @others
</t>
<t tx="ekr.20220525082935.1248">def __init__(self,
             lookup: Callable[[str, Context], Optional[SymbolTableNode]],
             scope: 'TypeVarLikeScope',
             *,
             include_callables: bool = True,
             include_bound_tvars: bool = False) -&gt; None:
    self.include_callables = include_callables
    self.lookup = lookup
    self.scope = scope
    self.include_bound_tvars = include_bound_tvars
    super().__init__(flatten_tvars)

</t>
<t tx="ekr.20220525082935.1249">def _seems_like_callable(self, type: UnboundType) -&gt; bool:
    if not type.args:
        return False
    if isinstance(type.args[0], (EllipsisType, TypeList, ParamSpecType)):
        return True
    return False

</t>
<t tx="ekr.20220525082935.125">def process__all__(self, s: AssignmentStmt) -&gt; None:
    """Export names if argument is a __all__ assignment."""
    if (len(s.lvalues) == 1 and isinstance(s.lvalues[0], NameExpr) and
            s.lvalues[0].name == '__all__' and s.lvalues[0].kind == GDEF and
            isinstance(s.rvalue, (ListExpr, TupleExpr))):
        self.add_exports(s.rvalue.items)

</t>
<t tx="ekr.20220525082935.1250">def visit_unbound_type(self, t: UnboundType) -&gt; TypeVarLikeList:
    name = t.name
    node = None
    # Special case P.args and P.kwargs for ParamSpecs only.
    if name.endswith('args'):
        if name.endswith('.args') or name.endswith('.kwargs'):
            base = '.'.join(name.split('.')[:-1])
            n = self.lookup(base, t)
            if n is not None and isinstance(n.node, ParamSpecExpr):
                node = n
                name = base
    if node is None:
        node = self.lookup(name, t)
    if node and isinstance(node.node, TypeVarLikeExpr) and (
            self.include_bound_tvars or self.scope.get_binding(node) is None):
        assert isinstance(node.node, TypeVarLikeExpr)
        return [(name, node.node)]
    elif not self.include_callables and self._seems_like_callable(t):
        return []
    elif node and node.fullname in LITERAL_TYPE_NAMES:
        return []
    elif node and node.fullname in ANNOTATED_TYPE_NAMES and t.args:
        # Don't query the second argument to Annotated for TypeVars
        return self.query_types([t.args[0]])
    else:
        return super().visit_unbound_type(t)

</t>
<t tx="ekr.20220525082935.1251">def visit_callable_type(self, t: CallableType) -&gt; TypeVarLikeList:
    if self.include_callables:
        return super().visit_callable_type(t)
    else:
        return []


</t>
<t tx="ekr.20220525082935.1252">def check_for_explicit_any(typ: Optional[Type],
                           options: Options,
                           is_typeshed_stub: bool,
                           msg: MessageBuilder,
                           context: Context) -&gt; None:
    if (options.disallow_any_explicit and
            not is_typeshed_stub and
            typ and
            has_explicit_any(typ)):
        msg.explicit_any(context)


</t>
<t tx="ekr.20220525082935.1253">def has_explicit_any(t: Type) -&gt; bool:
    """
    Whether this type is or type it contains is an Any coming from explicit type annotation
    """
    return t.accept(HasExplicitAny())


</t>
<t tx="ekr.20220525082935.1254">class HasExplicitAny(TypeQuery[bool]):
    @others
</t>
<t tx="ekr.20220525082935.1255">def __init__(self) -&gt; None:
    super().__init__(any)

</t>
<t tx="ekr.20220525082935.1256">def visit_any(self, t: AnyType) -&gt; bool:
    return t.type_of_any == TypeOfAny.explicit

</t>
<t tx="ekr.20220525082935.1257">def visit_typeddict_type(self, t: TypedDictType) -&gt; bool:
    # typeddict is checked during TypedDict declaration, so don't typecheck it here.
    return False


</t>
<t tx="ekr.20220525082935.1258">def has_any_from_unimported_type(t: Type) -&gt; bool:
    """Return true if this type is Any because an import was not followed.

    If type t is such Any type or has type arguments that contain such Any type
    this function will return true.
    """
    return t.accept(HasAnyFromUnimportedType())


</t>
<t tx="ekr.20220525082935.1259">class HasAnyFromUnimportedType(TypeQuery[bool]):
    @others
</t>
<t tx="ekr.20220525082935.126">def process__deletable__(self, s: AssignmentStmt) -&gt; None:
    if not self.options.mypyc:
        return
    if (len(s.lvalues) == 1 and isinstance(s.lvalues[0], NameExpr) and
            s.lvalues[0].name == '__deletable__' and s.lvalues[0].kind == MDEF):
        rvalue = s.rvalue
        if not isinstance(rvalue, (ListExpr, TupleExpr)):
            self.fail('"__deletable__" must be initialized with a list or tuple expression', s)
            return
        items = rvalue.items
        attrs = []
        for item in items:
            if not isinstance(item, StrExpr):
                self.fail('Invalid "__deletable__" item; string literal expected', item)
            else:
                attrs.append(item.value)
        assert self.type
        self.type.deletable_attributes = attrs

</t>
<t tx="ekr.20220525082935.1260">def __init__(self) -&gt; None:
    super().__init__(any)

</t>
<t tx="ekr.20220525082935.1261">def visit_any(self, t: AnyType) -&gt; bool:
    return t.type_of_any == TypeOfAny.from_unimported_type

</t>
<t tx="ekr.20220525082935.1262">def visit_typeddict_type(self, t: TypedDictType) -&gt; bool:
    # typeddict is checked during TypedDict declaration, so don't typecheck it here
    return False


</t>
<t tx="ekr.20220525082935.1263">def collect_all_inner_types(t: Type) -&gt; List[Type]:
    """
    Return all types that `t` contains
    """
    return t.accept(CollectAllInnerTypesQuery())


</t>
<t tx="ekr.20220525082935.1264">class CollectAllInnerTypesQuery(TypeQuery[List[Type]]):
    @others
</t>
<t tx="ekr.20220525082935.1265">def __init__(self) -&gt; None:
    super().__init__(self.combine_lists_strategy)

</t>
<t tx="ekr.20220525082935.1266">def query_types(self, types: Iterable[Type]) -&gt; List[Type]:
    return self.strategy([t.accept(self) for t in types]) + list(types)

</t>
<t tx="ekr.20220525082935.1267">@classmethod
def combine_lists_strategy(cls, it: Iterable[List[Type]]) -&gt; List[Type]:
    return list(itertools.chain.from_iterable(it))


</t>
<t tx="ekr.20220525082935.1268">def make_optional_type(t: Type) -&gt; Type:
    """Return the type corresponding to Optional[t].

    Note that we can't use normal union simplification, since this function
    is called during semantic analysis and simplification only works during
    type checking.
    """
    t = get_proper_type(t)
    if isinstance(t, NoneType):
        return t
    elif isinstance(t, UnionType):
        items = [item for item in union_items(t)
                 if not isinstance(item, NoneType)]
        return UnionType(items + [NoneType()], t.line, t.column)
    else:
        return UnionType([t, NoneType()], t.line, t.column)


</t>
<t tx="ekr.20220525082935.1269">def fix_instance_types(t: Type, fail: MsgCallback, note: MsgCallback,
                       python_version: Tuple[int, int]) -&gt; None:
    """Recursively fix all instance types (type argument count) in a given type.

    For example 'Union[Dict, List[str, int]]' will be transformed into
    'Union[Dict[Any, Any], List[Any]]' in place.
    """
    t.accept(InstanceFixer(fail, note, python_version))


</t>
<t tx="ekr.20220525082935.127">def process__slots__(self, s: AssignmentStmt) -&gt; None:
    """
    Processing ``__slots__`` if defined in type.

    See: https://docs.python.org/3/reference/datamodel.html#slots
    """
    # Later we can support `__slots__` defined as `__slots__ = other = ('a', 'b')`
    if (isinstance(self.type, TypeInfo) and
            len(s.lvalues) == 1 and isinstance(s.lvalues[0], NameExpr) and
            s.lvalues[0].name == '__slots__' and s.lvalues[0].kind == MDEF):

        # We understand `__slots__` defined as string, tuple, list, set, and dict:
        if not isinstance(s.rvalue, (StrExpr, ListExpr, TupleExpr, SetExpr, DictExpr)):
            # For example, `__slots__` can be defined as a variable,
            # we don't support it for now.
            return

        if any(p.slots is None for p in self.type.mro[1:-1]):
            # At least one type in mro (excluding `self` and `object`)
            # does not have concrete `__slots__` defined. Ignoring.
            return

        concrete_slots = True
        rvalue: List[Expression] = []
        if isinstance(s.rvalue, StrExpr):
            rvalue.append(s.rvalue)
        elif isinstance(s.rvalue, (ListExpr, TupleExpr, SetExpr)):
            rvalue.extend(s.rvalue.items)
        else:
            # We have a special treatment of `dict` with possible `{**kwargs}` usage.
            # In this case we consider all `__slots__` to be non-concrete.
            for key, _ in s.rvalue.items:
                if concrete_slots and key is not None:
                    rvalue.append(key)
                else:
                    concrete_slots = False

        slots = []
        for item in rvalue:
            # Special case for `'__dict__'` value:
            # when specified it will still allow any attribute assignment.
            if isinstance(item, StrExpr) and item.value != '__dict__':
                slots.append(item.value)
            else:
                concrete_slots = False
        if not concrete_slots:
            # Some slot items are dynamic, we don't want any false positives,
            # so, we just pretend that this type does not have any slots at all.
            return

        # We need to copy all slots from super types:
        for super_type in self.type.mro[1:-1]:
            assert super_type.slots is not None
            slots.extend(super_type.slots)
        self.type.slots = set(slots)

</t>
<t tx="ekr.20220525082935.1270">class InstanceFixer(TypeTraverserVisitor):
    @others
</t>
<t tx="ekr.20220525082935.1271">def __init__(
    self, fail: MsgCallback, note: MsgCallback, python_version: Tuple[int, int]
) -&gt; None:
    self.fail = fail
    self.note = note
    self.python_version = python_version

</t>
<t tx="ekr.20220525082935.1272">def visit_instance(self, typ: Instance) -&gt; None:
    super().visit_instance(typ)
    if len(typ.args) != len(typ.type.type_vars):
        fix_instance(typ, self.fail, self.note, disallow_any=False,
                     python_version=self.python_version, use_generic_error=True)
</t>
<t tx="ekr.20220525082935.1273">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Miscellaneous type operations and helpers for use during type checking.

NOTE: These must not be accessed from mypy.nodes or mypy.types to avoid import
      cycles. These must not be called from the semantic analysis main pass
      since these may assume that MROs are ready.
"""

from typing import cast, Optional, List, Sequence, Set, Iterable, TypeVar, Dict, Tuple, Any, Union
from typing_extensions import Type as TypingType
import itertools
import sys

from mypy.types import (
    TupleType, Instance, FunctionLike, Type, CallableType, TypeVarLikeType, Overloaded,
    TypeVarType, UninhabitedType, FormalArgument, UnionType, NoneType,
    AnyType, TypeOfAny, TypeType, ProperType, LiteralType, get_proper_type, get_proper_types,
    TypeAliasType, TypeQuery, ParamSpecType, Parameters, ENUM_REMOVED_PROPS
)
from mypy.nodes import (
    FuncBase, FuncItem, FuncDef, OverloadedFuncDef, TypeInfo, ARG_STAR, ARG_STAR2, ARG_POS,
    Expression, StrExpr, Var, Decorator, SYMBOL_FUNCBASE_TYPES
)
from mypy.maptype import map_instance_to_supertype
from mypy.expandtype import expand_type_by_instance, expand_type
from mypy.copytype import copy_type

from mypy.typevars import fill_typevars

from mypy.state import state


@others
</t>
<t tx="ekr.20220525082935.1274">def is_recursive_pair(s: Type, t: Type) -&gt; bool:
    """Is this a pair of recursive type aliases?"""
    return (isinstance(s, TypeAliasType) and isinstance(t, TypeAliasType) and
            s.is_recursive and t.is_recursive)


</t>
<t tx="ekr.20220525082935.1275">def tuple_fallback(typ: TupleType) -&gt; Instance:
    """Return fallback type for a tuple."""
    from mypy.join import join_type_list

    info = typ.partial_fallback.type
    if info.fullname != 'builtins.tuple':
        return typ.partial_fallback
    return Instance(info, [join_type_list(typ.items)])


</t>
<t tx="ekr.20220525082935.1276">def type_object_type_from_function(signature: FunctionLike,
                                   info: TypeInfo,
                                   def_info: TypeInfo,
                                   fallback: Instance,
                                   is_new: bool) -&gt; FunctionLike:
    # We first need to record all non-trivial (explicit) self types in __init__,
    # since they will not be available after we bind them. Note, we use explicit
    # self-types only in the defining class, similar to __new__ (but not exactly the same,
    # see comment in class_callable below). This is mostly useful for annotating library
    # classes such as subprocess.Popen.
    default_self = fill_typevars(info)
    if not is_new and not info.is_newtype:
        orig_self_types = [(it.arg_types[0] if it.arg_types and it.arg_types[0] != default_self
                            and it.arg_kinds[0] == ARG_POS else None) for it in signature.items]
    else:
        orig_self_types = [None] * len(signature.items)

    # The __init__ method might come from a generic superclass 'def_info'
    # with type variables that do not map identically to the type variables of
    # the class 'info' being constructed. For example:
    #
    #   class A(Generic[T]):
    #       def __init__(self, x: T) -&gt; None: ...
    #   class B(A[List[T]]):
    #      ...
    #
    # We need to map B's __init__ to the type (List[T]) -&gt; None.
    signature = bind_self(signature, original_type=default_self, is_classmethod=is_new)
    signature = cast(FunctionLike, map_type_from_supertype(signature, info, def_info))

    special_sig: Optional[str] = None
    if def_info.fullname == 'builtins.dict':
        # Special signature!
        special_sig = 'dict'

    if isinstance(signature, CallableType):
        return class_callable(signature, info, fallback, special_sig, is_new, orig_self_types[0])
    else:
        # Overloaded __init__/__new__.
        assert isinstance(signature, Overloaded)
        items: List[CallableType] = []
        for item, orig_self in zip(signature.items, orig_self_types):
            items.append(class_callable(item, info, fallback, special_sig, is_new, orig_self))
        return Overloaded(items)


</t>
<t tx="ekr.20220525082935.1277">def class_callable(init_type: CallableType, info: TypeInfo, type_type: Instance,
                   special_sig: Optional[str],
                   is_new: bool, orig_self_type: Optional[Type] = None) -&gt; CallableType:
    """Create a type object type based on the signature of __init__."""
    variables: List[TypeVarLikeType] = []
    variables.extend(info.defn.type_vars)
    variables.extend(init_type.variables)

    from mypy.subtypes import is_subtype

    init_ret_type = get_proper_type(init_type.ret_type)
    orig_self_type = get_proper_type(orig_self_type)
    default_ret_type = fill_typevars(info)
    explicit_type = init_ret_type if is_new else orig_self_type
    if (
        isinstance(explicit_type, (Instance, TupleType))
        # We have to skip protocols, because it can be a subtype of a return type
        # by accident. Like `Hashable` is a subtype of `object`. See #11799
        and isinstance(default_ret_type, Instance)
        and not default_ret_type.type.is_protocol
        # Only use the declared return type from __new__ or declared self in __init__
        # if it is actually returning a subtype of what we would return otherwise.
        and is_subtype(explicit_type, default_ret_type, ignore_type_params=True)
    ):
        ret_type: Type = explicit_type
    else:
        ret_type = default_ret_type

    callable_type = init_type.copy_modified(
        ret_type=ret_type, fallback=type_type, name=None, variables=variables,
        special_sig=special_sig)
    c = callable_type.with_name(info.name)
    return c


</t>
<t tx="ekr.20220525082935.1278">def map_type_from_supertype(typ: Type,
                            sub_info: TypeInfo,
                            super_info: TypeInfo) -&gt; Type:
    """Map type variables in a type defined in a supertype context to be valid
    in the subtype context. Assume that the result is unique; if more than
    one type is possible, return one of the alternatives.

    For example, assume

      class D(Generic[S]): ...
      class C(D[E[T]], Generic[T]): ...

    Now S in the context of D would be mapped to E[T] in the context of C.
    """
    # Create the type of self in subtype, of form t[a1, ...].
    inst_type = fill_typevars(sub_info)
    if isinstance(inst_type, TupleType):
        inst_type = tuple_fallback(inst_type)
    # Map the type of self to supertype. This gets us a description of the
    # supertype type variables in terms of subtype variables, i.e. t[t1, ...]
    # so that any type variables in tN are to be interpreted in subtype
    # context.
    inst_type = map_instance_to_supertype(inst_type, super_info)
    # Finally expand the type variables in type with those in the previously
    # constructed type. Note that both type and inst_type may have type
    # variables, but in type they are interpreted in supertype context while
    # in inst_type they are interpreted in subtype context. This works even if
    # the names of type variables in supertype and subtype overlap.
    return expand_type_by_instance(typ, inst_type)


</t>
<t tx="ekr.20220525082935.1279">def supported_self_type(typ: ProperType) -&gt; bool:
    """Is this a supported kind of explicit self-types?

    Currently, this means a X or Type[X], where X is an instance or
    a type variable with an instance upper bound.
    """
    if isinstance(typ, TypeType):
        return supported_self_type(typ.item)
    return (isinstance(typ, TypeVarType) or
            (isinstance(typ, Instance) and typ != fill_typevars(typ.type)))


</t>
<t tx="ekr.20220525082935.128">#
# Misc statements
#

</t>
<t tx="ekr.20220525082935.1280">F = TypeVar('F', bound=FunctionLike)


</t>
<t tx="ekr.20220525082935.1281">def bind_self(method: F, original_type: Optional[Type] = None, is_classmethod: bool = False) -&gt; F:
    """Return a copy of `method`, with the type of its first parameter (usually
    self or cls) bound to original_type.

    If the type of `self` is a generic type (T, or Type[T] for classmethods),
    instantiate every occurrence of type with original_type in the rest of the
    signature and in the return type.

    original_type is the type of E in the expression E.copy(). It is None in
    compatibility checks. In this case we treat it as the erasure of the
    declared type of self.

    This way we can express "the type of self". For example:

    T = TypeVar('T', bound='A')
    class A:
        def copy(self: T) -&gt; T: ...

    class B(A): pass

    b = B().copy()  # type: B

    """
    if isinstance(method, Overloaded):
        return cast(F, Overloaded([bind_self(c, original_type, is_classmethod)
                                   for c in method.items]))
    assert isinstance(method, CallableType)
    func = method
    if not func.arg_types:
        # Invalid method, return something.
        return cast(F, func)
    if func.arg_kinds[0] == ARG_STAR:
        # The signature is of the form 'def foo(*args, ...)'.
        # In this case we shouldn't drop the first arg,
        # since func will be absorbed by the *args.

        # TODO: infer bounds on the type of *args?
        return cast(F, func)
    self_param_type = get_proper_type(func.arg_types[0])

    variables: Sequence[TypeVarLikeType] = []
    if func.variables and supported_self_type(self_param_type):
        from mypy.infer import infer_type_arguments

        if original_type is None:
            # TODO: type check method override (see #7861).
            original_type = erase_to_bound(self_param_type)
        original_type = get_proper_type(original_type)

        all_ids = func.type_var_ids()
        typeargs = infer_type_arguments(all_ids, self_param_type, original_type,
                                        is_supertype=True)
        if (is_classmethod
                # TODO: why do we need the extra guards here?
                and any(isinstance(get_proper_type(t), UninhabitedType) for t in typeargs)
                and isinstance(original_type, (Instance, TypeVarType, TupleType))):
            # In case we call a classmethod through an instance x, fallback to type(x)
            typeargs = infer_type_arguments(all_ids, self_param_type, TypeType(original_type),
                                            is_supertype=True)

        ids = [tid for tid in all_ids
               if any(tid == t.id for t in get_type_vars(self_param_type))]

        # Technically, some constrains might be unsolvable, make them &lt;nothing&gt;.
        to_apply = [t if t is not None else UninhabitedType() for t in typeargs]

        def expand(target: Type) -&gt; Type:
            return expand_type(target, {id: to_apply[all_ids.index(id)] for id in ids})

        arg_types = [expand(x) for x in func.arg_types[1:]]
        ret_type = expand(func.ret_type)
        variables = [v for v in func.variables if v.id not in ids]
    else:
        arg_types = func.arg_types[1:]
        ret_type = func.ret_type
        variables = func.variables

    original_type = get_proper_type(original_type)
    if isinstance(original_type, CallableType) and original_type.is_type_obj():
        original_type = TypeType.make_normalized(original_type.ret_type)
    res = func.copy_modified(arg_types=arg_types,
                             arg_kinds=func.arg_kinds[1:],
                             arg_names=func.arg_names[1:],
                             variables=variables,
                             ret_type=ret_type,
                             bound_args=[original_type])
    return cast(F, res)


</t>
<t tx="ekr.20220525082935.1282">def erase_to_bound(t: Type) -&gt; Type:
    # TODO: use value restrictions to produce a union?
    t = get_proper_type(t)
    if isinstance(t, TypeVarType):
        return t.upper_bound
    if isinstance(t, TypeType):
        if isinstance(t.item, TypeVarType):
            return TypeType.make_normalized(t.item.upper_bound)
    return t


</t>
<t tx="ekr.20220525082935.1283">def callable_corresponding_argument(typ: Union[CallableType, Parameters],
                                    model: FormalArgument) -&gt; Optional[FormalArgument]:
    """Return the argument a function that corresponds to `model`"""

    by_name = typ.argument_by_name(model.name)
    by_pos = typ.argument_by_position(model.pos)
    if by_name is None and by_pos is None:
        return None
    if by_name is not None and by_pos is not None:
        if by_name == by_pos:
            return by_name
        # If we're dealing with an optional pos-only and an optional
        # name-only arg, merge them.  This is the case for all functions
        # taking both *args and **args, or a pair of functions like so:

        # def right(a: int = ...) -&gt; None: ...
        # def left(__a: int = ..., *, a: int = ...) -&gt; None: ...
        from mypy.subtypes import is_equivalent

        if (not (by_name.required or by_pos.required)
                and by_pos.name is None
                and by_name.pos is None
                and is_equivalent(by_name.typ, by_pos.typ)):
            return FormalArgument(by_name.name, by_pos.pos, by_name.typ, False)
    return by_name if by_name is not None else by_pos


</t>
<t tx="ekr.20220525082935.1284">def simple_literal_value_key(t: ProperType) -&gt; Optional[Tuple[str, ...]]:
    """Return a hashable description of simple literal type.

    Return None if not a simple literal type.

    The return value can be used to simplify away duplicate types in
    unions by comparing keys for equality. For now enum, string or
    Instance with string last_known_value are supported.
    """
    if isinstance(t, LiteralType):
        if t.fallback.type.is_enum or t.fallback.type.fullname == 'builtins.str':
            assert isinstance(t.value, str)
            return 'literal', t.value, t.fallback.type.fullname
    if isinstance(t, Instance):
        if t.last_known_value is not None and isinstance(t.last_known_value.value, str):
            return 'instance', t.last_known_value.value, t.type.fullname
    return None


</t>
<t tx="ekr.20220525082935.1285">def simple_literal_type(t: Optional[ProperType]) -&gt; Optional[Instance]:
    """Extract the underlying fallback Instance type for a simple Literal"""
    if isinstance(t, Instance) and t.last_known_value is not None:
        t = t.last_known_value
    if isinstance(t, LiteralType):
        return t.fallback
    return None


</t>
<t tx="ekr.20220525082935.1286">def is_simple_literal(t: ProperType) -&gt; bool:
    """Fast way to check if simple_literal_value_key() would return a non-None value."""
    if isinstance(t, LiteralType):
        return t.fallback.type.is_enum or t.fallback.type.fullname == 'builtins.str'
    if isinstance(t, Instance):
        return t.last_known_value is not None and isinstance(t.last_known_value.value, str)
    return False


</t>
<t tx="ekr.20220525082935.1287">def make_simplified_union(items: Sequence[Type],
                          line: int = -1, column: int = -1,
                          *, keep_erased: bool = False,
                          contract_literals: bool = True) -&gt; ProperType:
    """Build union type with redundant union items removed.

    If only a single item remains, this may return a non-union type.

    Examples:

    * [int, str] -&gt; Union[int, str]
    * [int, object] -&gt; object
    * [int, int] -&gt; int
    * [int, Any] -&gt; Union[int, Any] (Any types are not simplified away!)
    * [Any, Any] -&gt; Any

    Note: This must NOT be used during semantic analysis, since TypeInfos may not
          be fully initialized.

    The keep_erased flag is used for type inference against union types
    containing type variables. If set to True, keep all ErasedType items.

    The contract_literals flag indicates whether we need to contract literal types
    back into a sum type. Set it to False when called by try_expanding_sum_type_
    to_union().
    """
    items = get_proper_types(items)

    # Step 1: expand all nested unions
    while any(isinstance(typ, UnionType) for typ in items):
        all_items: List[ProperType] = []
        for typ in items:
            if isinstance(typ, UnionType):
                all_items.extend(get_proper_types(typ.items))
            else:
                all_items.append(typ)
        items = all_items

    # Step 2: remove redundant unions
    simplified_set = _remove_redundant_union_items(items, keep_erased)

    # Step 3: If more than one literal exists in the union, try to simplify
    if contract_literals and sum(isinstance(item, LiteralType) for item in simplified_set) &gt; 1:
        simplified_set = try_contracting_literals_in_union(simplified_set)

    return UnionType.make_union(simplified_set, line, column)


</t>
<t tx="ekr.20220525082935.1288">def _remove_redundant_union_items(items: List[ProperType], keep_erased: bool) -&gt; List[ProperType]:
    from mypy.subtypes import is_proper_subtype

    removed: Set[int] = set()
    seen: Set[Tuple[str, ...]] = set()

    # NB: having a separate fast path for Union of Literal and slow path for other things
    # would arguably be cleaner, however it breaks down when simplifying the Union of two
    # different enum types as try_expanding_sum_type_to_union works recursively and will
    # trigger intermediate simplifications that would render the fast path useless
    for i, item in enumerate(items):
        if i in removed:
            continue
        # Avoid slow nested for loop for Union of Literal of strings/enums (issue #9169)
        k = simple_literal_value_key(item)
        if k is not None:
            if k in seen:
                removed.add(i)
                continue

            # NB: one would naively expect that it would be safe to skip the slow path
            # always for literals. One would be sorely mistaken. Indeed, some simplifications
            # such as that of None/Optional when strict optional is false, do require that we
            # proceed with the slow path. Thankfully, all literals will have the same subtype
            # relationship to non-literal types, so we only need to do that walk for the first
            # literal, which keeps the fast path fast even in the presence of a mixture of
            # literals and other types.
            safe_skip = len(seen) &gt; 0
            seen.add(k)
            if safe_skip:
                continue

        # Keep track of the truthiness info for deleted subtypes which can be relevant
        cbt = cbf = False
        for j, tj in enumerate(items):
            if (
                i == j
                # avoid further checks if this item was already marked redundant.
                or j in removed
                # if the current item is a simple literal then this simplification loop can
                # safely skip all other simple literals as two literals will only ever be
                # subtypes of each other if they are equal, which is already handled above.
                # However, if the current item is not a literal, it might plausibly be a
                # supertype of other literals in the union, so we must check them again.
                # This is an important optimization as is_proper_subtype is pretty expensive.
                or (k is not None and is_simple_literal(tj))
            ):
                continue
            # actual redundancy checks
            if (
                is_redundant_literal_instance(item, tj)  # XXX?
                and is_proper_subtype(tj, item, keep_erased_types=keep_erased)
            ):
                # We found a redundant item in the union.
                removed.add(j)
                cbt = cbt or tj.can_be_true
                cbf = cbf or tj.can_be_false
        # if deleted subtypes had more general truthiness, use that
        if not item.can_be_true and cbt:
            items[i] = true_or_false(item)
        elif not item.can_be_false and cbf:
            items[i] = true_or_false(item)

    return [items[i] for i in range(len(items)) if i not in removed]


</t>
<t tx="ekr.20220525082935.1289">def _get_type_special_method_bool_ret_type(t: Type) -&gt; Optional[Type]:
    t = get_proper_type(t)

    if isinstance(t, Instance):
        bool_method = t.type.get("__bool__")
        if bool_method:
            callee = get_proper_type(bool_method.type)
            if isinstance(callee, CallableType):
                return callee.ret_type

    return None


</t>
<t tx="ekr.20220525082935.129">def visit_block(self, b: Block) -&gt; None:
    if b.is_unreachable:
        return
    self.block_depth[-1] += 1
    for s in b.body:
        self.accept(s)
    self.block_depth[-1] -= 1

</t>
<t tx="ekr.20220525082935.1290">def true_only(t: Type) -&gt; ProperType:
    """
    Restricted version of t with only True-ish values
    """
    t = get_proper_type(t)

    if not t.can_be_true:
        # All values of t are False-ish, so there are no true values in it
        return UninhabitedType(line=t.line, column=t.column)
    elif not t.can_be_false:
        # All values of t are already True-ish, so true_only is idempotent in this case
        return t
    elif isinstance(t, UnionType):
        # The true version of a union type is the union of the true versions of its components
        new_items = [true_only(item) for item in t.items]
        can_be_true_items = [item for item in new_items if item.can_be_true]
        return make_simplified_union(can_be_true_items, line=t.line, column=t.column)
    else:
        ret_type = _get_type_special_method_bool_ret_type(t)

        if ret_type and ret_type.can_be_false and not ret_type.can_be_true:
            new_t = copy_type(t)
            new_t.can_be_true = False
            return new_t

        new_t = copy_type(t)
        new_t.can_be_false = False
        return new_t


</t>
<t tx="ekr.20220525082935.1291">def false_only(t: Type) -&gt; ProperType:
    """
    Restricted version of t with only False-ish values
    """
    t = get_proper_type(t)

    if not t.can_be_false:
        if state.strict_optional:
            # All values of t are True-ish, so there are no false values in it
            return UninhabitedType(line=t.line)
        else:
            # When strict optional checking is disabled, everything can be
            # False-ish since anything can be None
            return NoneType(line=t.line)
    elif not t.can_be_true:
        # All values of t are already False-ish, so false_only is idempotent in this case
        return t
    elif isinstance(t, UnionType):
        # The false version of a union type is the union of the false versions of its components
        new_items = [false_only(item) for item in t.items]
        can_be_false_items = [item for item in new_items if item.can_be_false]
        return make_simplified_union(can_be_false_items, line=t.line, column=t.column)
    else:
        ret_type = _get_type_special_method_bool_ret_type(t)

        if ret_type and ret_type.can_be_true and not ret_type.can_be_false:
            new_t = copy_type(t)
            new_t.can_be_false = False
            return new_t

        new_t = copy_type(t)
        new_t.can_be_true = False
        return new_t


</t>
<t tx="ekr.20220525082935.1292">def true_or_false(t: Type) -&gt; ProperType:
    """
    Unrestricted version of t with both True-ish and False-ish values
    """
    t = get_proper_type(t)

    if isinstance(t, UnionType):
        new_items = [true_or_false(item) for item in t.items]
        return make_simplified_union(new_items, line=t.line, column=t.column)

    new_t = copy_type(t)
    new_t.can_be_true = new_t.can_be_true_default()
    new_t.can_be_false = new_t.can_be_false_default()
    return new_t


</t>
<t tx="ekr.20220525082935.1293">def erase_def_to_union_or_bound(tdef: TypeVarLikeType) -&gt; Type:
    # TODO(PEP612): fix for ParamSpecType
    if isinstance(tdef, ParamSpecType):
        return AnyType(TypeOfAny.from_error)
    assert isinstance(tdef, TypeVarType)
    if tdef.values:
        return make_simplified_union(tdef.values)
    else:
        return tdef.upper_bound


</t>
<t tx="ekr.20220525082935.1294">def erase_to_union_or_bound(typ: TypeVarType) -&gt; ProperType:
    if typ.values:
        return make_simplified_union(typ.values)
    else:
        return get_proper_type(typ.upper_bound)


</t>
<t tx="ekr.20220525082935.1295">def function_type(func: FuncBase, fallback: Instance) -&gt; FunctionLike:
    if func.type:
        assert isinstance(func.type, FunctionLike)
        return func.type
    else:
        # Implicit type signature with dynamic types.
        if isinstance(func, FuncItem):
            return callable_type(func, fallback)
        else:
            # Broken overloads can have self.type set to None.
            # TODO: should we instead always set the type in semantic analyzer?
            assert isinstance(func, OverloadedFuncDef)
            any_type = AnyType(TypeOfAny.from_error)
            dummy = CallableType([any_type, any_type],
                                 [ARG_STAR, ARG_STAR2],
                                 [None, None], any_type,
                                 fallback,
                                 line=func.line, is_ellipsis_args=True)
            # Return an Overloaded, because some callers may expect that
            # an OverloadedFuncDef has an Overloaded type.
            return Overloaded([dummy])


</t>
<t tx="ekr.20220525082935.1296">def callable_type(fdef: FuncItem, fallback: Instance,
                  ret_type: Optional[Type] = None) -&gt; CallableType:
    # TODO: somewhat unfortunate duplication with prepare_method_signature in semanal
    if fdef.info and not fdef.is_static and fdef.arg_names:
        self_type: Type = fill_typevars(fdef.info)
        if fdef.is_class or fdef.name == '__new__':
            self_type = TypeType.make_normalized(self_type)
        args = [self_type] + [AnyType(TypeOfAny.unannotated)] * (len(fdef.arg_names)-1)
    else:
        args = [AnyType(TypeOfAny.unannotated)] * len(fdef.arg_names)

    return CallableType(
        args,
        fdef.arg_kinds,
        fdef.arg_names,
        ret_type or AnyType(TypeOfAny.unannotated),
        fallback,
        name=fdef.name,
        line=fdef.line,
        column=fdef.column,
        implicit=True,
        # We need this for better error messages, like missing `self` note:
        definition=fdef if isinstance(fdef, FuncDef) else None,
    )


</t>
<t tx="ekr.20220525082935.1297">def try_getting_str_literals(expr: Expression, typ: Type) -&gt; Optional[List[str]]:
    """If the given expression or type corresponds to a string literal
    or a union of string literals, returns a list of the underlying strings.
    Otherwise, returns None.

    Specifically, this function is guaranteed to return a list with
    one or more strings if one of the following is true:

    1. 'expr' is a StrExpr
    2. 'typ' is a LiteralType containing a string
    3. 'typ' is a UnionType containing only LiteralType of strings
    """
    if isinstance(expr, StrExpr):
        return [expr.value]

    # TODO: See if we can eliminate this function and call the below one directly
    return try_getting_str_literals_from_type(typ)


</t>
<t tx="ekr.20220525082935.1298">def try_getting_str_literals_from_type(typ: Type) -&gt; Optional[List[str]]:
    """If the given expression or type corresponds to a string Literal
    or a union of string Literals, returns a list of the underlying strings.
    Otherwise, returns None.

    For example, if we had the type 'Literal["foo", "bar"]' as input, this function
    would return a list of strings ["foo", "bar"].
    """
    return try_getting_literals_from_type(typ, str, "builtins.str")


</t>
<t tx="ekr.20220525082935.1299">def try_getting_int_literals_from_type(typ: Type) -&gt; Optional[List[int]]:
    """If the given expression or type corresponds to an int Literal
    or a union of int Literals, returns a list of the underlying ints.
    Otherwise, returns None.

    For example, if we had the type 'Literal[1, 2, 3]' as input, this function
    would return a list of ints [1, 2, 3].
    """
    return try_getting_literals_from_type(typ, int, "builtins.int")


</t>
<t tx="ekr.20220525082935.13">def add_implicit_module_attrs(self, file_node: MypyFile) -&gt; None:
    """Manually add implicit definitions of module '__name__' etc."""
    for name, t in implicit_module_attrs.items():
        # unicode docstrings should be accepted in Python 2
        if name == '__doc__':
            if self.options.python_version &gt;= (3, 0):
                typ: Type = UnboundType("__builtins__.str")
            else:
                typ = UnionType([UnboundType('__builtins__.str'),
                                 UnboundType('__builtins__.unicode')])
        elif name == '__path__':
            if not file_node.is_package_init_file():
                continue
            # Need to construct the type ourselves, to avoid issues with __builtins__.list
            # not being subscriptable or typing.List not getting bound
            sym = self.lookup_qualified("__builtins__.list", Context())
            if not sym:
                continue
            node = sym.node
            assert isinstance(node, TypeInfo)
            typ = Instance(node, [self.str_type()])
        elif name == '__annotations__':
            sym = self.lookup_qualified("__builtins__.dict", Context(), suppress_errors=True)
            if not sym:
                continue
            node = sym.node
            assert isinstance(node, TypeInfo)
            typ = Instance(node, [self.str_type(), AnyType(TypeOfAny.special_form)])
        else:
            assert t is not None, f'type should be specified for {name}'
            typ = UnboundType(t)

        existing = file_node.names.get(name)
        if existing is not None and not isinstance(existing.node, PlaceholderNode):
            # Already exists.
            continue

        an_type = self.anal_type(typ)
        if an_type:
            var = Var(name, an_type)
            var._fullname = self.qualified_name(name)
            var.is_ready = True
            self.add_symbol(name, var, dummy_context())
        else:
            self.add_symbol(name,
                            PlaceholderNode(self.qualified_name(name), file_node, -1),
                            dummy_context())

</t>
<t tx="ekr.20220525082935.130">def visit_block_maybe(self, b: Optional[Block]) -&gt; None:
    if b:
        self.visit_block(b)

</t>
<t tx="ekr.20220525082935.1300">T = TypeVar('T')


</t>
<t tx="ekr.20220525082935.1301">def try_getting_literals_from_type(typ: Type,
                                   target_literal_type: TypingType[T],
                                   target_fullname: str) -&gt; Optional[List[T]]:
    """If the given expression or type corresponds to a Literal or
    union of Literals where the underlying values correspond to the given
    target type, returns a list of those underlying values. Otherwise,
    returns None.
    """
    typ = get_proper_type(typ)

    if isinstance(typ, Instance) and typ.last_known_value is not None:
        possible_literals: List[Type] = [typ.last_known_value]
    elif isinstance(typ, UnionType):
        possible_literals = list(typ.items)
    else:
        possible_literals = [typ]

    literals: List[T] = []
    for lit in get_proper_types(possible_literals):
        if isinstance(lit, LiteralType) and lit.fallback.type.fullname == target_fullname:
            val = lit.value
            if isinstance(val, target_literal_type):
                literals.append(val)
            else:
                return None
        else:
            return None
    return literals


</t>
<t tx="ekr.20220525082935.1302">def is_literal_type_like(t: Optional[Type]) -&gt; bool:
    """Returns 'true' if the given type context is potentially either a LiteralType,
    a Union of LiteralType, or something similar.
    """
    t = get_proper_type(t)
    if t is None:
        return False
    elif isinstance(t, LiteralType):
        return True
    elif isinstance(t, UnionType):
        return any(is_literal_type_like(item) for item in t.items)
    elif isinstance(t, TypeVarType):
        return (is_literal_type_like(t.upper_bound)
                or any(is_literal_type_like(item) for item in t.values))
    else:
        return False


</t>
<t tx="ekr.20220525082935.1303">def get_enum_values(typ: Instance) -&gt; List[str]:
    """Return the list of values for an Enum."""
    return [name for name, sym in typ.type.names.items() if isinstance(sym.node, Var)]


</t>
<t tx="ekr.20220525082935.1304">def is_singleton_type(typ: Type) -&gt; bool:
    """Returns 'true' if this type is a "singleton type" -- if there exists
    exactly only one runtime value associated with this type.

    That is, given two values 'a' and 'b' that have the same type 't',
    'is_singleton_type(t)' returns True if and only if the expression 'a is b' is
    always true.

    Currently, this returns True when given NoneTypes, enum LiteralTypes and
    enum types with a single value.

    Note that other kinds of LiteralTypes cannot count as singleton types. For
    example, suppose we do 'a = 100000 + 1' and 'b = 100001'. It is not guaranteed
    that 'a is b' will always be true -- some implementations of Python will end up
    constructing two distinct instances of 100001.
    """
    typ = get_proper_type(typ)
    # TODO:
    # Also make this return True if the type corresponds to ... (ellipsis) or NotImplemented?
    return (
            isinstance(typ, NoneType)
            or (isinstance(typ, LiteralType)
                and (typ.is_enum_literal() or isinstance(typ.value, bool)))
            or (isinstance(typ, Instance) and typ.type.is_enum and len(get_enum_values(typ)) == 1)
    )


</t>
<t tx="ekr.20220525082935.1305">def try_expanding_sum_type_to_union(typ: Type, target_fullname: str) -&gt; ProperType:
    """Attempts to recursively expand any enum Instances with the given target_fullname
    into a Union of all of its component LiteralTypes.

    For example, if we have:

        class Color(Enum):
            RED = 1
            BLUE = 2
            YELLOW = 3

        class Status(Enum):
            SUCCESS = 1
            FAILURE = 2
            UNKNOWN = 3

    ...and if we call `try_expanding_enum_to_union(Union[Color, Status], 'module.Color')`,
    this function will return Literal[Color.RED, Color.BLUE, Color.YELLOW, Status].
    """
    typ = get_proper_type(typ)

    if isinstance(typ, UnionType):
        items = [
            try_expanding_sum_type_to_union(item, target_fullname)
            for item in typ.relevant_items()
        ]
        return make_simplified_union(items, contract_literals=False)
    elif isinstance(typ, Instance) and typ.type.fullname == target_fullname:
        if typ.type.is_enum:
            new_items = []
            for name, symbol in typ.type.names.items():
                if not isinstance(symbol.node, Var):
                    continue
                # Skip these since Enum will remove it
                if name in ENUM_REMOVED_PROPS:
                    continue
                new_items.append(LiteralType(name, typ))
            # SymbolTables are really just dicts, and dicts are guaranteed to preserve
            # insertion order only starting with Python 3.7. So, we sort these for older
            # versions of Python to help make tests deterministic.
            #
            # We could probably skip the sort for Python 3.6 since people probably run mypy
            # only using CPython, but we might as well for the sake of full correctness.
            if sys.version_info &lt; (3, 7):
                new_items.sort(key=lambda lit: lit.value)
            return make_simplified_union(new_items, contract_literals=False)
        elif typ.type.fullname == "builtins.bool":
            return make_simplified_union(
                [LiteralType(True, typ), LiteralType(False, typ)],
                contract_literals=False
            )

    return typ


</t>
<t tx="ekr.20220525082935.1306">def try_contracting_literals_in_union(types: Sequence[Type]) -&gt; List[ProperType]:
    """Contracts any literal types back into a sum type if possible.

    Will replace the first instance of the literal with the sum type and
    remove all others.

    If we call `try_contracting_union(Literal[Color.RED, Color.BLUE, Color.YELLOW])`,
    this function will return Color.

    We also treat `Literal[True, False]` as `bool`.
    """
    proper_types = [get_proper_type(typ) for typ in types]
    sum_types: Dict[str, Tuple[Set[Any], List[int]]] = {}
    marked_for_deletion = set()
    for idx, typ in enumerate(proper_types):
        if isinstance(typ, LiteralType):
            fullname = typ.fallback.type.fullname
            if typ.fallback.type.is_enum or isinstance(typ.value, bool):
                if fullname not in sum_types:
                    sum_types[fullname] = (set(get_enum_values(typ.fallback))
                                           if typ.fallback.type.is_enum
                                           else {True, False},
                                           [])
                literals, indexes = sum_types[fullname]
                literals.discard(typ.value)
                indexes.append(idx)
                if not literals:
                    first, *rest = indexes
                    proper_types[first] = typ.fallback
                    marked_for_deletion |= set(rest)
    return list(itertools.compress(proper_types, [(i not in marked_for_deletion)
                                                  for i in range(len(proper_types))]))


</t>
<t tx="ekr.20220525082935.1307">def coerce_to_literal(typ: Type) -&gt; Type:
    """Recursively converts any Instances that have a last_known_value or are
    instances of enum types with a single value into the corresponding LiteralType.
    """
    original_type = typ
    typ = get_proper_type(typ)
    if isinstance(typ, UnionType):
        new_items = [coerce_to_literal(item) for item in typ.items]
        return UnionType.make_union(new_items)
    elif isinstance(typ, Instance):
        if typ.last_known_value:
            return typ.last_known_value
        elif typ.type.is_enum:
            enum_values = get_enum_values(typ)
            if len(enum_values) == 1:
                return LiteralType(value=enum_values[0], fallback=typ)
    return original_type


</t>
<t tx="ekr.20220525082935.1308">def get_type_vars(tp: Type) -&gt; List[TypeVarType]:
    return tp.accept(TypeVarExtractor())


</t>
<t tx="ekr.20220525082935.1309">class TypeVarExtractor(TypeQuery[List[TypeVarType]]):
    @others
</t>
<t tx="ekr.20220525082935.131">def visit_expression_stmt(self, s: ExpressionStmt) -&gt; None:
    self.statement = s
    s.expr.accept(self)

</t>
<t tx="ekr.20220525082935.1310">def __init__(self) -&gt; None:
    super().__init__(self._merge)

</t>
<t tx="ekr.20220525082935.1311">def _merge(self, iter: Iterable[List[TypeVarType]]) -&gt; List[TypeVarType]:
    out = []
    for item in iter:
        out.extend(item)
    return out

</t>
<t tx="ekr.20220525082935.1312">def visit_type_var(self, t: TypeVarType) -&gt; List[TypeVarType]:
    return [t]


</t>
<t tx="ekr.20220525082935.1313">def custom_special_method(typ: Type, name: str, check_all: bool = False) -&gt; bool:
    """Does this type have a custom special method such as __format__() or __eq__()?

    If check_all is True ensure all items of a union have a custom method, not just some.
    """
    typ = get_proper_type(typ)
    if isinstance(typ, Instance):
        method = typ.type.get(name)
        if method and isinstance(method.node, (SYMBOL_FUNCBASE_TYPES, Decorator, Var)):
            if method.node.info:
                return not method.node.info.fullname.startswith('builtins.')
        return False
    if isinstance(typ, UnionType):
        if check_all:
            return all(custom_special_method(t, name, check_all) for t in typ.items)
        return any(custom_special_method(t, name) for t in typ.items)
    if isinstance(typ, TupleType):
        return custom_special_method(tuple_fallback(typ), name, check_all)
    if isinstance(typ, CallableType) and typ.is_type_obj():
        # Look up __method__ on the metaclass for class objects.
        return custom_special_method(typ.fallback, name, check_all)
    if isinstance(typ, AnyType):
        # Avoid false positives in uncertain cases.
        return True
    # TODO: support other types (see ExpressionChecker.has_member())?
    return False


</t>
<t tx="ekr.20220525082935.1314">def is_redundant_literal_instance(general: ProperType, specific: ProperType) -&gt; bool:
    if not isinstance(general, Instance) or general.last_known_value is None:
        return True
    if isinstance(specific, Instance) and specific.last_known_value == general.last_known_value:
        return True
    if isinstance(specific, UninhabitedType):
        return True

    return False


</t>
<t tx="ekr.20220525082935.1315">def separate_union_literals(t: UnionType) -&gt; Tuple[Sequence[LiteralType], Sequence[Type]]:
    """Separate literals from other members in a union type."""
    literal_items = []
    union_items = []

    for item in t.items:
        proper = get_proper_type(item)
        if isinstance(proper, LiteralType):
            literal_items.append(proper)
        else:
            union_items.append(item)

    return literal_items, union_items
</t>
<t tx="ekr.20220525082935.1316">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Classes for representing mypy types."""

import sys
from abc import abstractmethod

from typing import (
    Any, TypeVar, Dict, List, Tuple, cast, Set, Optional, Union, Iterable, NamedTuple,
    Sequence
)
from typing_extensions import ClassVar, Final, TYPE_CHECKING, overload, TypeAlias as _TypeAlias

from mypy.backports import OrderedDict
import mypy.nodes
from mypy.state import state
from mypy.nodes import (
    INVARIANT, SymbolNode, FuncDef, FakeInfo,
    ArgKind, ARG_POS, ARG_STAR, ARG_STAR2,
)
from mypy.util import IdMapper
from mypy.bogus_type import Bogus


T = TypeVar('T')

JsonDict: _TypeAlias = Dict[str, Any]

# The set of all valid expressions that can currently be contained
# inside of a Literal[...].
#
# Literals can contain bytes and enum-values: we special-case both of these
# and store the value as a string. We rely on the fallback type that's also
# stored with the Literal to determine how a string is being used.
#
# TODO: confirm that we're happy with representing enums (and the
# other types) in the manner described above.
#
# Note: if we change the set of types included below, we must also
# make sure to audit the following methods:
#
# 1. types.LiteralType's serialize and deserialize methods: this method
#    needs to make sure it can convert the below types into JSON and back.
#
# 2. types.LiteralType's 'alue_repr` method: this method is ultimately used
#    by TypeStrVisitor's visit_literal_type to generate a reasonable
#    repr-able output.
#
# 3. server.astdiff.SnapshotTypeVisitor's visit_literal_type_method: this
#    method assumes that the following types supports equality checks and
#    hashability.
#
# Note: Although "Literal[None]" is a valid type, we internally always convert
# such a type directly into "None". So, "None" is not a valid parameter of
# LiteralType and is omitted from this list.
LiteralValue: _TypeAlias = Union[int, str, bool]


# If we only import type_visitor in the middle of the file, mypy
# breaks, and if we do it at the top, it breaks at runtime because of
# import cycle issues, so we do it at the top while typechecking and
# then again in the middle at runtime.
# We should be able to remove this once we are switched to the new
# semantic analyzer!
if TYPE_CHECKING:
    from mypy.type_visitor import (
        TypeVisitor as TypeVisitor,
        SyntheticTypeVisitor as SyntheticTypeVisitor,
    )

# Supported names of TypedDict type constructors.
TPDICT_NAMES: Final = (
    "typing.TypedDict",
    "typing_extensions.TypedDict",
    "mypy_extensions.TypedDict",
)

# Supported fallback instance type names for TypedDict types.
TPDICT_FB_NAMES: Final = (
    "typing._TypedDict",
    "typing_extensions._TypedDict",
    "mypy_extensions._TypedDict",
)

# Supported names of Protocol base class.
PROTOCOL_NAMES: Final = (
    'typing.Protocol',
    'typing_extensions.Protocol',
)

# Supported TypeAlias names.
TYPE_ALIAS_NAMES: Final = (
    "typing.TypeAlias",
    "typing_extensions.TypeAlias",
)

# Supported Final type names.
FINAL_TYPE_NAMES: Final = (
    'typing.Final',
    'typing_extensions.Final',
)

# Supported @final decorator names.
FINAL_DECORATOR_NAMES: Final = (
    'typing.final',
    'typing_extensions.final',
)

# Supported Literal type names.
LITERAL_TYPE_NAMES: Final = (
    'typing.Literal',
    'typing_extensions.Literal',
)

# Supported Annotated type names.
ANNOTATED_TYPE_NAMES: Final = (
    'typing.Annotated',
    'typing_extensions.Annotated',
)

# We use this constant in various places when checking `tuple` subtyping:
TUPLE_LIKE_INSTANCE_NAMES: Final = (
    'builtins.tuple',
    'typing.Iterable',
    'typing.Container',
    'typing.Sequence',
    'typing.Reversible',
)

REVEAL_TYPE_NAMES: Final = (
    'builtins.reveal_type',
    'typing.reveal_type',
    'typing_extensions.reveal_type',
)

ASSERT_TYPE_NAMES: Final = (
    'typing.assert_type',
    'typing_extensions.assert_type',
)

OVERLOAD_NAMES: Final = (
    'typing.overload',
    'typing_extensions.overload',
)

# Attributes that can optionally be defined in the body of a subclass of
# enum.Enum but are removed from the class __dict__ by EnumMeta.
ENUM_REMOVED_PROPS: Final = (
    '_ignore_',
    '_order_',
    '__order__',
)

NEVER_NAMES: Final = (
    'typing.NoReturn',
    'typing_extensions.NoReturn',
    'mypy_extensions.NoReturn',
    'typing.Never',
    'typing_extensions.Never',
)

# A placeholder used for Bogus[...] parameters
_dummy: Final[Any] = object()


@others
</t>
<t tx="ekr.20220525082935.1317">class TypeOfAny:
    """
    This class describes different types of Any. Each 'Any' can be of only one type at a time.
    """

    __slots__ = ()

    # Was this Any type inferred without a type annotation?
    unannotated: Final = 1
    # Does this Any come from an explicit type annotation?
    explicit: Final = 2
    # Does this come from an unfollowed import? See --disallow-any-unimported option
    from_unimported_type: Final = 3
    # Does this Any type come from omitted generics?
    from_omitted_generics: Final = 4
    # Does this Any come from an error?
    from_error: Final = 5
    # Is this a type that can't be represented in mypy's type system? For instance, type of
    # call to NewType...). Even though these types aren't real Anys, we treat them as such.
    # Also used for variables named '_'.
    special_form: Final = 6
    # Does this Any come from interaction with another Any?
    from_another_any: Final = 7
    # Does this Any come from an implementation limitation/bug?
    implementation_artifact: Final = 8
    # Does this Any come from use in the suggestion engine?  This is
    # used to ignore Anys inserted by the suggestion engine when
    # generating constraints.
    suggestion_engine: Final = 9


</t>
<t tx="ekr.20220525082935.1318">def deserialize_type(data: Union[JsonDict, str]) -&gt; 'Type':
    if isinstance(data, str):
        return Instance.deserialize(data)
    classname = data['.class']
    method = deserialize_map.get(classname)
    if method is not None:
        return method(data)
    raise NotImplementedError(f'unexpected .class {classname}')


</t>
<t tx="ekr.20220525082935.1319">class Type(mypy.nodes.Context):
    """Abstract base class for all types."""

    __slots__ = ('can_be_true', 'can_be_false')
    # 'can_be_true' and 'can_be_false' mean whether the value of the
    # expression can be true or false in a boolean context. They are useful
    # when inferring the type of logic expressions like `x and y`.
    #
    # For example:
    #   * the literal `False` can't be true while `True` can.
    #   * a value with type `bool` can be true or false.
    #   * `None` can't be true
    #   * ...

    @others
</t>
<t tx="ekr.20220525082935.132">def visit_return_stmt(self, s: ReturnStmt) -&gt; None:
    self.statement = s
    if not self.is_func_scope():
        self.fail('"return" outside function', s)
    if s.expr:
        s.expr.accept(self)

</t>
<t tx="ekr.20220525082935.1320">def __init__(self, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.can_be_true = self.can_be_true_default()
    self.can_be_false = self.can_be_false_default()

</t>
<t tx="ekr.20220525082935.1321">def can_be_true_default(self) -&gt; bool:
    return True

</t>
<t tx="ekr.20220525082935.1322">def can_be_false_default(self) -&gt; bool:
    return True

</t>
<t tx="ekr.20220525082935.1323">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    raise RuntimeError('Not implemented')

</t>
<t tx="ekr.20220525082935.1324">def __repr__(self) -&gt; str:
    return self.accept(TypeStrVisitor())

</t>
<t tx="ekr.20220525082935.1325">def serialize(self) -&gt; Union[JsonDict, str]:
    raise NotImplementedError(f'Cannot serialize {self.__class__.__name__} instance')

</t>
<t tx="ekr.20220525082935.1326">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'Type':
    raise NotImplementedError(f'Cannot deserialize {cls.__name__} instance')


</t>
<t tx="ekr.20220525082935.1327">class TypeAliasType(Type):
    """A type alias to another type.

    NOTE: this is not being used yet, and the implementation is still incomplete.

    To support recursive type aliases we don't immediately expand a type alias
    during semantic analysis, but create an instance of this type that records the target alias
    definition node (mypy.nodes.TypeAlias) and type arguments (for generic aliases).

    This is very similar to how TypeInfo vs Instance interact, where a recursive class-based
    structure like
        class Node:
            value: int
            children: List[Node]
    can be represented in a tree-like manner.
    """

    __slots__ = ('alias', 'args', 'line', 'column', 'type_ref')

    @others
</t>
<t tx="ekr.20220525082935.1328">def __init__(self, alias: Optional[mypy.nodes.TypeAlias], args: List[Type],
             line: int = -1, column: int = -1) -&gt; None:
    self.alias = alias
    self.args = args
    self.type_ref: Optional[str] = None
    super().__init__(line, column)

</t>
<t tx="ekr.20220525082935.1329">def _expand_once(self) -&gt; Type:
    """Expand to the target type exactly once.

    This doesn't do full expansion, i.e. the result can contain another
    (or even this same) type alias. Use this internal helper only when really needed,
    its public wrapper mypy.types.get_proper_type() is preferred.
    """
    assert self.alias is not None
    if self.alias.no_args:
        # We know that no_args=True aliases like L = List must have an instance
        # as their target.
        assert isinstance(self.alias.target, Instance)  # type: ignore[misc]
        return self.alias.target.copy_modified(args=self.args)
    return replace_alias_tvars(self.alias.target, self.alias.alias_tvars, self.args,
                               self.line, self.column)

</t>
<t tx="ekr.20220525082935.133">def visit_raise_stmt(self, s: RaiseStmt) -&gt; None:
    self.statement = s
    if s.expr:
        s.expr.accept(self)
    if s.from_expr:
        s.from_expr.accept(self)

</t>
<t tx="ekr.20220525082935.1330">def _partial_expansion(self) -&gt; Tuple['ProperType', bool]:
    # Private method mostly for debugging and testing.
    unroller = UnrollAliasVisitor(set())
    unrolled = self.accept(unroller)
    assert isinstance(unrolled, ProperType)
    return unrolled, unroller.recursed

</t>
<t tx="ekr.20220525082935.1331">def expand_all_if_possible(self) -&gt; Optional['ProperType']:
    """Attempt a full expansion of the type alias (including nested aliases).

    If the expansion is not possible, i.e. the alias is (mutually-)recursive,
    return None.
    """
    unrolled, recursed = self._partial_expansion()
    if recursed:
        return None
    return unrolled

</t>
<t tx="ekr.20220525082935.1332">@property
def is_recursive(self) -&gt; bool:
    assert self.alias is not None, 'Unfixed type alias'
    is_recursive = self.alias._is_recursive
    if is_recursive is None:
        is_recursive = self.expand_all_if_possible() is None
        # We cache the value on the underlying TypeAlias node as an optimization,
        # since the value is the same for all instances of the same alias.
        self.alias._is_recursive = is_recursive
    return is_recursive

</t>
<t tx="ekr.20220525082935.1333">def can_be_true_default(self) -&gt; bool:
    if self.alias is not None:
        return self.alias.target.can_be_true
    return super().can_be_true_default()

</t>
<t tx="ekr.20220525082935.1334">def can_be_false_default(self) -&gt; bool:
    if self.alias is not None:
        return self.alias.target.can_be_false
    return super().can_be_false_default()

</t>
<t tx="ekr.20220525082935.1335">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_type_alias_type(self)

</t>
<t tx="ekr.20220525082935.1336">def __hash__(self) -&gt; int:
    return hash((self.alias, tuple(self.args)))

</t>
<t tx="ekr.20220525082935.1337">def __eq__(self, other: object) -&gt; bool:
    # Note: never use this to determine subtype relationships, use is_subtype().
    if not isinstance(other, TypeAliasType):
        return NotImplemented
    return (self.alias == other.alias
            and self.args == other.args)

</t>
<t tx="ekr.20220525082935.1338">def serialize(self) -&gt; JsonDict:
    assert self.alias is not None
    data: JsonDict = {
        ".class": "TypeAliasType",
        "type_ref": self.alias.fullname,
        "args": [arg.serialize() for arg in self.args],
    }
    return data

</t>
<t tx="ekr.20220525082935.1339">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'TypeAliasType':
    assert data['.class'] == 'TypeAliasType'
    args: List[Type] = []
    if 'args' in data:
        args_list = data['args']
        assert isinstance(args_list, list)
        args = [deserialize_type(arg) for arg in args_list]
    alias = TypeAliasType(None, args)
    alias.type_ref = data['type_ref']
    return alias

</t>
<t tx="ekr.20220525082935.134">def visit_assert_stmt(self, s: AssertStmt) -&gt; None:
    self.statement = s
    if s.expr:
        s.expr.accept(self)
    if s.msg:
        s.msg.accept(self)

</t>
<t tx="ekr.20220525082935.1340">def copy_modified(self, *,
                  args: Optional[List[Type]] = None) -&gt; 'TypeAliasType':
    return TypeAliasType(
        self.alias,
        args if args is not None else self.args.copy(),
        self.line, self.column)


</t>
<t tx="ekr.20220525082935.1341">class TypeGuardedType(Type):
    """Only used by find_isinstance_check() etc."""

    __slots__ = ('type_guard',)

    @others
</t>
<t tx="ekr.20220525082935.1342">def __init__(self, type_guard: Type):
    super().__init__(line=type_guard.line, column=type_guard.column)
    self.type_guard = type_guard

</t>
<t tx="ekr.20220525082935.1343">def __repr__(self) -&gt; str:
    return f"TypeGuard({self.type_guard})"


</t>
<t tx="ekr.20220525082935.1344">class RequiredType(Type):
    """Required[T] or NotRequired[T]. Only usable at top-level of a TypedDict definition."""

    @others
</t>
<t tx="ekr.20220525082935.1345">def __init__(self, item: Type, *, required: bool) -&gt; None:
    super().__init__(line=item.line, column=item.column)
    self.item = item
    self.required = required

</t>
<t tx="ekr.20220525082935.1346">def __repr__(self) -&gt; str:
    if self.required:
        return f"Required[{self.item}]"
    else:
        return f"NotRequired[{self.item}]"

</t>
<t tx="ekr.20220525082935.1347">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return self.item.accept(visitor)


</t>
<t tx="ekr.20220525082935.1348">class ProperType(Type):
    """Not a type alias.

    Every type except TypeAliasType must inherit from this type.
    """

    __slots__ = ()


</t>
<t tx="ekr.20220525082935.1349">class TypeVarId:
    # A type variable is uniquely identified by its raw id and meta level.

    # For plain variables (type parameters of generic classes and
    # functions) raw ids are allocated by semantic analysis, using
    # positive ids 1, 2, ... for generic class parameters and negative
    # ids -1, ... for generic function type arguments. This convention
    # is only used to keep type variable ids distinct when allocating
    # them; the type checker makes no distinction between class and
    # function type variables.

    # Metavariables are allocated unique ids starting from 1.
    raw_id: int = 0

    # Level of the variable in type inference. Currently either 0 for
    # declared types, or 1 for type inference metavariables.
    meta_level: int = 0

    # Class variable used for allocating fresh ids for metavariables.
    next_raw_id: ClassVar[int] = 1

    # Fullname of class (or potentially function in the future) which
    # declares this type variable (not the fullname of the TypeVar
    # definition!), or ''
    namespace: str

    @others
</t>
<t tx="ekr.20220525082935.135">def visit_operator_assignment_stmt(self,
                                   s: OperatorAssignmentStmt) -&gt; None:
    self.statement = s
    s.lvalue.accept(self)
    s.rvalue.accept(self)
    if (isinstance(s.lvalue, NameExpr) and s.lvalue.name == '__all__' and
            s.lvalue.kind == GDEF and isinstance(s.rvalue, (ListExpr, TupleExpr))):
        self.add_exports(s.rvalue.items)

</t>
<t tx="ekr.20220525082935.1350">def __init__(self, raw_id: int, meta_level: int = 0, *, namespace: str = '') -&gt; None:
    self.raw_id = raw_id
    self.meta_level = meta_level
    self.namespace = namespace

</t>
<t tx="ekr.20220525082935.1351">@staticmethod
def new(meta_level: int) -&gt; 'TypeVarId':
    raw_id = TypeVarId.next_raw_id
    TypeVarId.next_raw_id += 1
    return TypeVarId(raw_id, meta_level)

</t>
<t tx="ekr.20220525082935.1352">def __repr__(self) -&gt; str:
    return self.raw_id.__repr__()

</t>
<t tx="ekr.20220525082935.1353">def __eq__(self, other: object) -&gt; bool:
    if isinstance(other, TypeVarId):
        return (self.raw_id == other.raw_id and
                self.meta_level == other.meta_level and
                self.namespace == other.namespace)
    else:
        return False

</t>
<t tx="ekr.20220525082935.1354">def __ne__(self, other: object) -&gt; bool:
    return not (self == other)

</t>
<t tx="ekr.20220525082935.1355">def __hash__(self) -&gt; int:
    return hash((self.raw_id, self.meta_level, self.namespace))

</t>
<t tx="ekr.20220525082935.1356">def is_meta_var(self) -&gt; bool:
    return self.meta_level &gt; 0


</t>
<t tx="ekr.20220525082935.1357">class TypeVarLikeType(ProperType):

    __slots__ = ('name', 'fullname', 'id', 'upper_bound')

    name: str  # Name (may be qualified)
    fullname: str  # Fully qualified name
    id: TypeVarId
    upper_bound: Type

    @others
</t>
<t tx="ekr.20220525082935.1358">def __init__(
    self, name: str, fullname: str, id: Union[TypeVarId, int], upper_bound: Type,
    line: int = -1, column: int = -1
) -&gt; None:
    super().__init__(line, column)
    self.name = name
    self.fullname = fullname
    if isinstance(id, int):
        id = TypeVarId(id)
    self.id = id
    self.upper_bound = upper_bound

</t>
<t tx="ekr.20220525082935.1359">def serialize(self) -&gt; JsonDict:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.136">def visit_while_stmt(self, s: WhileStmt) -&gt; None:
    self.statement = s
    s.expr.accept(self)
    self.loop_depth += 1
    s.body.accept(self)
    self.loop_depth -= 1
    self.visit_block_maybe(s.else_body)

</t>
<t tx="ekr.20220525082935.1360">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'TypeVarLikeType':
    raise NotImplementedError


</t>
<t tx="ekr.20220525082935.1361">class TypeVarType(TypeVarLikeType):
    """Type that refers to a type variable."""

    __slots__ = ('values', 'variance')

    values: List[Type]  # Value restriction, empty list if no restriction
    variance: int

    @others
</t>
<t tx="ekr.20220525082935.1362">def __init__(self, name: str, fullname: str, id: Union[TypeVarId, int], values: List[Type],
             upper_bound: Type, variance: int = INVARIANT, line: int = -1,
             column: int = -1) -&gt; None:
    super().__init__(name, fullname, id, upper_bound, line, column)
    assert values is not None, "No restrictions must be represented by empty list"
    self.values = values
    self.variance = variance

</t>
<t tx="ekr.20220525082935.1363">@staticmethod
def new_unification_variable(old: 'TypeVarType') -&gt; 'TypeVarType':
    new_id = TypeVarId.new(meta_level=1)
    return TypeVarType(old.name, old.fullname, new_id, old.values,
                      old.upper_bound, old.variance, old.line, old.column)

</t>
<t tx="ekr.20220525082935.1364">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_type_var(self)

</t>
<t tx="ekr.20220525082935.1365">def __hash__(self) -&gt; int:
    return hash(self.id)

</t>
<t tx="ekr.20220525082935.1366">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, TypeVarType):
        return NotImplemented
    return self.id == other.id

</t>
<t tx="ekr.20220525082935.1367">def serialize(self) -&gt; JsonDict:
    assert not self.id.is_meta_var()
    return {'.class': 'TypeVarType',
            'name': self.name,
            'fullname': self.fullname,
            'id': self.id.raw_id,
            'namespace': self.id.namespace,
            'values': [v.serialize() for v in self.values],
            'upper_bound': self.upper_bound.serialize(),
            'variance': self.variance,
            }

</t>
<t tx="ekr.20220525082935.1368">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'TypeVarType':
    assert data['.class'] == 'TypeVarType'
    return TypeVarType(
        data['name'],
        data['fullname'],
        TypeVarId(data['id'], namespace=data['namespace']),
        [deserialize_type(v) for v in data['values']],
        deserialize_type(data['upper_bound']),
        data['variance'],
    )


</t>
<t tx="ekr.20220525082935.1369">class ParamSpecFlavor:
    # Simple ParamSpec reference such as "P"
    BARE: Final = 0
    # P.args
    ARGS: Final = 1
    # P.kwargs
    KWARGS: Final = 2


</t>
<t tx="ekr.20220525082935.137">def visit_for_stmt(self, s: ForStmt) -&gt; None:
    if s.is_async:
        if not self.is_func_scope() or not self.function_stack[-1].is_coroutine:
            self.fail(message_registry.ASYNC_FOR_OUTSIDE_COROUTINE, s, code=codes.SYNTAX)

    self.statement = s
    s.expr.accept(self)

    # Bind index variables and check if they define new names.
    self.analyze_lvalue(s.index, explicit_type=s.index_type is not None)
    if s.index_type:
        if self.is_classvar(s.index_type):
            self.fail_invalid_classvar(s.index)
        allow_tuple_literal = isinstance(s.index, TupleExpr)
        analyzed = self.anal_type(s.index_type, allow_tuple_literal=allow_tuple_literal)
        if analyzed is not None:
            self.store_declared_types(s.index, analyzed)
            s.index_type = analyzed

    self.loop_depth += 1
    self.visit_block(s.body)
    self.loop_depth -= 1

    self.visit_block_maybe(s.else_body)

</t>
<t tx="ekr.20220525082935.1370">class ParamSpecType(TypeVarLikeType):
    """Type that refers to a ParamSpec.

    A ParamSpec is a type variable that represents the parameter
    types, names and kinds of a callable (i.e., the signature without
    the return type).

    This can be one of these forms
     * P (ParamSpecFlavor.BARE)
     * P.args (ParamSpecFlavor.ARGS)
     * P.kwargs (ParamSpecFLavor.KWARGS)

    The upper_bound is really used as a fallback type -- it's shared
    with TypeVarType for simplicity. It can't be specified by the user
    and the value is directly derived from the flavor (currently
    always just 'object').
    """

    __slots__ = ('flavor', 'prefix')

    flavor: int
    prefix: 'Parameters'

    @others
</t>
<t tx="ekr.20220525082935.1371">def __init__(
     self, name: str, fullname: str, id: Union[TypeVarId, int], flavor: int,
     upper_bound: Type, *, line: int = -1, column: int = -1,
     prefix: Optional['Parameters'] = None
) -&gt; None:
    super().__init__(name, fullname, id, upper_bound, line=line, column=column)
    self.flavor = flavor
    self.prefix = prefix or Parameters([], [], [])

</t>
<t tx="ekr.20220525082935.1372">@staticmethod
def new_unification_variable(old: 'ParamSpecType') -&gt; 'ParamSpecType':
    new_id = TypeVarId.new(meta_level=1)
    return ParamSpecType(old.name, old.fullname, new_id, old.flavor, old.upper_bound,
                         line=old.line, column=old.column, prefix=old.prefix)

</t>
<t tx="ekr.20220525082935.1373">def with_flavor(self, flavor: int) -&gt; 'ParamSpecType':
    return ParamSpecType(self.name, self.fullname, self.id, flavor,
                         upper_bound=self.upper_bound, prefix=self.prefix)

</t>
<t tx="ekr.20220525082935.1374">def copy_modified(self, *,
                  id: Bogus[Union[TypeVarId, int]] = _dummy,
                  flavor: Bogus[int] = _dummy,
                  prefix: Bogus['Parameters'] = _dummy) -&gt; 'ParamSpecType':
    return ParamSpecType(
        self.name,
        self.fullname,
        id if id is not _dummy else self.id,
        flavor if flavor is not _dummy else self.flavor,
        self.upper_bound,
        line=self.line,
        column=self.column,
        prefix=prefix if prefix is not _dummy else self.prefix,
    )

</t>
<t tx="ekr.20220525082935.1375">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_param_spec(self)

</t>
<t tx="ekr.20220525082935.1376">def name_with_suffix(self) -&gt; str:
    n = self.name
    if self.flavor == ParamSpecFlavor.ARGS:
        return f'{n}.args'
    elif self.flavor == ParamSpecFlavor.KWARGS:
        return f'{n}.kwargs'
    return n

</t>
<t tx="ekr.20220525082935.1377">def __hash__(self) -&gt; int:
    return hash((self.id, self.flavor))

</t>
<t tx="ekr.20220525082935.1378">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, ParamSpecType):
        return NotImplemented
    # Upper bound can be ignored, since it's determined by flavor.
    return self.id == other.id and self.flavor == other.flavor

</t>
<t tx="ekr.20220525082935.1379">def serialize(self) -&gt; JsonDict:
    assert not self.id.is_meta_var()
    return {
        '.class': 'ParamSpecType',
        'name': self.name,
        'fullname': self.fullname,
        'id': self.id.raw_id,
        'flavor': self.flavor,
        'upper_bound': self.upper_bound.serialize(),
        'prefix': self.prefix.serialize()
    }

</t>
<t tx="ekr.20220525082935.138">def visit_break_stmt(self, s: BreakStmt) -&gt; None:
    self.statement = s
    if self.loop_depth == 0:
        self.fail('"break" outside loop', s, serious=True, blocker=True)

</t>
<t tx="ekr.20220525082935.1380">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'ParamSpecType':
    assert data['.class'] == 'ParamSpecType'
    return ParamSpecType(
        data['name'],
        data['fullname'],
        data['id'],
        data['flavor'],
        deserialize_type(data['upper_bound']),
        prefix=Parameters.deserialize(data['prefix'])
    )


</t>
<t tx="ekr.20220525082935.1381">class TypeVarTupleType(TypeVarLikeType):
    """Type that refers to a TypeVarTuple.

    See PEP646 for more information.
    """
    @others
</t>
<t tx="ekr.20220525082935.1382">def serialize(self) -&gt; JsonDict:
    assert not self.id.is_meta_var()
    return {'.class': 'TypeVarTupleType',
            'name': self.name,
            'fullname': self.fullname,
            'id': self.id.raw_id,
            'upper_bound': self.upper_bound.serialize(),
            }

</t>
<t tx="ekr.20220525082935.1383">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'TypeVarTupleType':
    assert data['.class'] == 'TypeVarTupleType'
    return TypeVarTupleType(
        data['name'],
        data['fullname'],
        data['id'],
        deserialize_type(data['upper_bound']),
    )

</t>
<t tx="ekr.20220525082935.1384">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_type_var_tuple(self)

</t>
<t tx="ekr.20220525082935.1385">def __hash__(self) -&gt; int:
    return hash(self.id)

</t>
<t tx="ekr.20220525082935.1386">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, TypeVarTupleType):
        return NotImplemented
    return self.id == other.id

</t>
<t tx="ekr.20220525082935.1387">@staticmethod
def new_unification_variable(old: 'TypeVarTupleType') -&gt; 'TypeVarTupleType':
    new_id = TypeVarId.new(meta_level=1)
    return TypeVarTupleType(old.name, old.fullname, new_id, old.upper_bound,
                         line=old.line, column=old.column)


</t>
<t tx="ekr.20220525082935.1388">class UnboundType(ProperType):
    """Instance type that has not been bound during semantic analysis."""

    __slots__ = ('name', 'args', 'optional', 'empty_tuple_index',
                 'original_str_expr', 'original_str_fallback')

    @others
</t>
<t tx="ekr.20220525082935.1389">def __init__(self,
             name: Optional[str],
             args: Optional[Sequence[Type]] = None,
             line: int = -1,
             column: int = -1,
             optional: bool = False,
             empty_tuple_index: bool = False,
             original_str_expr: Optional[str] = None,
             original_str_fallback: Optional[str] = None,
             ) -&gt; None:
    super().__init__(line, column)
    if not args:
        args = []
    assert name is not None
    self.name = name
    self.args = tuple(args)
    # Should this type be wrapped in an Optional?
    self.optional = optional
    # Special case for X[()]
    self.empty_tuple_index = empty_tuple_index
    # If this UnboundType was originally defined as a str or bytes, keep track of
    # the original contents of that string-like thing. This way, if this UnboundExpr
    # ever shows up inside of a LiteralType, we can determine whether that
    # Literal[...] is valid or not. E.g. Literal[foo] is most likely invalid
    # (unless 'foo' is an alias for another literal or something) and
    # Literal["foo"] most likely is.
    #
    # We keep track of the entire string instead of just using a boolean flag
    # so we can distinguish between things like Literal["foo"] vs
    # Literal["    foo   "].
    #
    # We also keep track of what the original base fallback type was supposed to be
    # so we don't have to try and recompute it later
    self.original_str_expr = original_str_expr
    self.original_str_fallback = original_str_fallback

</t>
<t tx="ekr.20220525082935.139">def visit_continue_stmt(self, s: ContinueStmt) -&gt; None:
    self.statement = s
    if self.loop_depth == 0:
        self.fail('"continue" outside loop', s, serious=True, blocker=True)

</t>
<t tx="ekr.20220525082935.1390">def copy_modified(self,
                  args: Bogus[Optional[Sequence[Type]]] = _dummy,
                  ) -&gt; 'UnboundType':
    if args is _dummy:
        args = self.args
    return UnboundType(
        name=self.name,
        args=args,
        line=self.line,
        column=self.column,
        optional=self.optional,
        empty_tuple_index=self.empty_tuple_index,
        original_str_expr=self.original_str_expr,
        original_str_fallback=self.original_str_fallback,
    )

</t>
<t tx="ekr.20220525082935.1391">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_unbound_type(self)

</t>
<t tx="ekr.20220525082935.1392">def __hash__(self) -&gt; int:
    return hash((self.name, self.optional, tuple(self.args), self.original_str_expr))

</t>
<t tx="ekr.20220525082935.1393">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, UnboundType):
        return NotImplemented
    return (self.name == other.name and self.optional == other.optional and
            self.args == other.args and self.original_str_expr == other.original_str_expr and
            self.original_str_fallback == other.original_str_fallback)

</t>
<t tx="ekr.20220525082935.1394">def serialize(self) -&gt; JsonDict:
    return {'.class': 'UnboundType',
            'name': self.name,
            'args': [a.serialize() for a in self.args],
            'expr': self.original_str_expr,
            'expr_fallback': self.original_str_fallback,
            }

</t>
<t tx="ekr.20220525082935.1395">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'UnboundType':
    assert data['.class'] == 'UnboundType'
    return UnboundType(data['name'],
                       [deserialize_type(a) for a in data['args']],
                       original_str_expr=data['expr'],
                       original_str_fallback=data['expr_fallback'],
                       )


</t>
<t tx="ekr.20220525082935.1396">class CallableArgument(ProperType):
    """Represents a Arg(type, 'name') inside a Callable's type list.

    Note that this is a synthetic type for helping parse ASTs, not a real type.
    """

    __slots__ = ('typ', 'name', 'constructor')

    typ: Type
    name: Optional[str]
    constructor: Optional[str]

    @others
</t>
<t tx="ekr.20220525082935.1397">def __init__(self, typ: Type, name: Optional[str], constructor: Optional[str],
             line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.typ = typ
    self.name = name
    self.constructor = constructor

</t>
<t tx="ekr.20220525082935.1398">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    assert isinstance(visitor, SyntheticTypeVisitor)
    return visitor.visit_callable_argument(self)

</t>
<t tx="ekr.20220525082935.1399">def serialize(self) -&gt; JsonDict:
    assert False, "Synthetic types don't serialize"


</t>
<t tx="ekr.20220525082935.14">def add_builtin_aliases(self, tree: MypyFile) -&gt; None:
    """Add builtin type aliases to typing module.

    For historical reasons, the aliases like `List = list` are not defined
    in typeshed stubs for typing module. Instead we need to manually add the
    corresponding nodes on the fly. We explicitly mark these aliases as normalized,
    so that a user can write `typing.List[int]`.
    """
    assert tree.fullname == 'typing'
    for alias, target_name in type_aliases.items():
        if type_aliases_source_versions[alias] &gt; self.options.python_version:
            # This alias is not available on this Python version.
            continue
        name = alias.split('.')[-1]
        if name in tree.names and not isinstance(tree.names[name].node, PlaceholderNode):
            continue
        self.create_alias(tree, target_name, alias, name)

</t>
<t tx="ekr.20220525082935.140">def visit_if_stmt(self, s: IfStmt) -&gt; None:
    self.statement = s
    infer_reachability_of_if_statement(s, self.options)
    for i in range(len(s.expr)):
        s.expr[i].accept(self)
        self.visit_block(s.body[i])
    self.visit_block_maybe(s.else_body)

</t>
<t tx="ekr.20220525082935.1400">class TypeList(ProperType):
    """Information about argument types and names [...].

    This is used for the arguments of a Callable type, i.e. for
    [arg, ...] in Callable[[arg, ...], ret]. This is not a real type
    but a syntactic AST construct. UnboundTypes can also have TypeList
    types before they are processed into Callable types.
    """

    __slots__ = ('items',)

    items: List[Type]

    @others
</t>
<t tx="ekr.20220525082935.1401">def __init__(self, items: List[Type], line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.items = items

</t>
<t tx="ekr.20220525082935.1402">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    assert isinstance(visitor, SyntheticTypeVisitor)
    return visitor.visit_type_list(self)

</t>
<t tx="ekr.20220525082935.1403">def serialize(self) -&gt; JsonDict:
    assert False, "Synthetic types don't serialize"


</t>
<t tx="ekr.20220525082935.1404">class UnpackType(ProperType):
    """Type operator Unpack from PEP646. Can be either with Unpack[]
    or unpacking * syntax.

    The inner type should be either a TypeVarTuple, a constant size
    tuple, or a variable length tuple, or a union of one of those.
    """
    __slots__ = ["type"]

    @others
</t>
<t tx="ekr.20220525082935.1405">def __init__(self, typ: Type, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.type = typ

</t>
<t tx="ekr.20220525082935.1406">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_unpack_type(self)

</t>
<t tx="ekr.20220525082935.1407">def serialize(self) -&gt; JsonDict:
    return {
        ".class": "UnpackType",
        "type": self.type.serialize(),
    }

</t>
<t tx="ekr.20220525082935.1408">@classmethod
def deserialize(cls, data: JsonDict) -&gt; "UnpackType":
    assert data[".class"] == "UnpackType"
    typ = data["type"]
    return UnpackType(deserialize_type(typ))


</t>
<t tx="ekr.20220525082935.1409">class AnyType(ProperType):
    """The type 'Any'."""

    __slots__ = ('type_of_any', 'source_any', 'missing_import_name')

    @others
</t>
<t tx="ekr.20220525082935.141">def visit_try_stmt(self, s: TryStmt) -&gt; None:
    self.statement = s
    self.analyze_try_stmt(s, self)

</t>
<t tx="ekr.20220525082935.1410">def __init__(self,
             type_of_any: int,
             source_any: Optional['AnyType'] = None,
             missing_import_name: Optional[str] = None,
             line: int = -1,
             column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.type_of_any = type_of_any
    # If this Any was created as a result of interacting with another 'Any', record the source
    # and use it in reports.
    self.source_any = source_any
    if source_any and source_any.source_any:
        self.source_any = source_any.source_any

    if source_any is None:
        self.missing_import_name = missing_import_name
    else:
        self.missing_import_name = source_any.missing_import_name

    # Only unimported type anys and anys from other anys should have an import name
    assert (missing_import_name is None or
            type_of_any in (TypeOfAny.from_unimported_type, TypeOfAny.from_another_any))
    # Only Anys that come from another Any can have source_any.
    assert type_of_any != TypeOfAny.from_another_any or source_any is not None
    # We should not have chains of Anys.
    assert not self.source_any or self.source_any.type_of_any != TypeOfAny.from_another_any

</t>
<t tx="ekr.20220525082935.1411">@property
def is_from_error(self) -&gt; bool:
    return self.type_of_any == TypeOfAny.from_error

</t>
<t tx="ekr.20220525082935.1412">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_any(self)

</t>
<t tx="ekr.20220525082935.1413">def copy_modified(self,
                  # Mark with Bogus because _dummy is just an object (with type Any)
                  type_of_any: Bogus[int] = _dummy,
                  original_any: Bogus[Optional['AnyType']] = _dummy,
                  ) -&gt; 'AnyType':
    if type_of_any is _dummy:
        type_of_any = self.type_of_any
    if original_any is _dummy:
        original_any = self.source_any
    return AnyType(type_of_any=type_of_any, source_any=original_any,
                   missing_import_name=self.missing_import_name,
                   line=self.line, column=self.column)

</t>
<t tx="ekr.20220525082935.1414">def __hash__(self) -&gt; int:
    return hash(AnyType)

</t>
<t tx="ekr.20220525082935.1415">def __eq__(self, other: object) -&gt; bool:
    return isinstance(other, AnyType)

</t>
<t tx="ekr.20220525082935.1416">def serialize(self) -&gt; JsonDict:
    return {'.class': 'AnyType', 'type_of_any': self.type_of_any,
            'source_any': self.source_any.serialize() if self.source_any is not None else None,
            'missing_import_name': self.missing_import_name}

</t>
<t tx="ekr.20220525082935.1417">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'AnyType':
    assert data['.class'] == 'AnyType'
    source = data['source_any']
    return AnyType(data['type_of_any'],
                   AnyType.deserialize(source) if source is not None else None,
                   data['missing_import_name'])


</t>
<t tx="ekr.20220525082935.1418">class UninhabitedType(ProperType):
    """This type has no members.

    This type is the bottom type.
    With strict Optional checking, it is the only common subtype between all
    other types, which allows `meet` to be well defined.  Without strict
    Optional checking, NoneType fills this role.

    In general, for any type T:
        join(UninhabitedType, T) = T
        meet(UninhabitedType, T) = UninhabitedType
        is_subtype(UninhabitedType, T) = True
    """

    __slots__ = ('ambiguous', 'is_noreturn',)

    is_noreturn: bool  # Does this come from a NoReturn?  Purely for error messages.
    # It is important to track whether this is an actual NoReturn type, or just a result
    # of ambiguous type inference, in the latter case we don't want to mark a branch as
    # unreachable in binder.
    ambiguous: bool  # Is this a result of inference for a variable without constraints?

    @others
</t>
<t tx="ekr.20220525082935.1419">def __init__(self, is_noreturn: bool = False, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.is_noreturn = is_noreturn
    self.ambiguous = False

</t>
<t tx="ekr.20220525082935.142">def analyze_try_stmt(self, s: TryStmt, visitor: NodeVisitor[None]) -&gt; None:
    s.body.accept(visitor)
    for type, var, handler in zip(s.types, s.vars, s.handlers):
        if type:
            type.accept(visitor)
        if var:
            self.analyze_lvalue(var)
        handler.accept(visitor)
    if s.else_body:
        s.else_body.accept(visitor)
    if s.finally_body:
        s.finally_body.accept(visitor)

</t>
<t tx="ekr.20220525082935.1420">def can_be_true_default(self) -&gt; bool:
    return False

</t>
<t tx="ekr.20220525082935.1421">def can_be_false_default(self) -&gt; bool:
    return False

</t>
<t tx="ekr.20220525082935.1422">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_uninhabited_type(self)

</t>
<t tx="ekr.20220525082935.1423">def __hash__(self) -&gt; int:
    return hash(UninhabitedType)

</t>
<t tx="ekr.20220525082935.1424">def __eq__(self, other: object) -&gt; bool:
    return isinstance(other, UninhabitedType)

</t>
<t tx="ekr.20220525082935.1425">def serialize(self) -&gt; JsonDict:
    return {'.class': 'UninhabitedType',
            'is_noreturn': self.is_noreturn}

</t>
<t tx="ekr.20220525082935.1426">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'UninhabitedType':
    assert data['.class'] == 'UninhabitedType'
    return UninhabitedType(is_noreturn=data['is_noreturn'])


</t>
<t tx="ekr.20220525082935.1427">class NoneType(ProperType):
    """The type of 'None'.

    This type can be written by users as 'None'.
    """

    __slots__ = ()

    @others
</t>
<t tx="ekr.20220525082935.1428">def __init__(self, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)

</t>
<t tx="ekr.20220525082935.1429">def can_be_true_default(self) -&gt; bool:
    return False

</t>
<t tx="ekr.20220525082935.143">def visit_with_stmt(self, s: WithStmt) -&gt; None:
    self.statement = s
    types: List[Type] = []

    if s.is_async:
        if not self.is_func_scope() or not self.function_stack[-1].is_coroutine:
            self.fail(message_registry.ASYNC_WITH_OUTSIDE_COROUTINE, s, code=codes.SYNTAX)

    if s.unanalyzed_type:
        assert isinstance(s.unanalyzed_type, ProperType)
        actual_targets = [t for t in s.target if t is not None]
        if len(actual_targets) == 0:
            # We have a type for no targets
            self.fail('Invalid type comment: "with" statement has no targets', s)
        elif len(actual_targets) == 1:
            # We have one target and one type
            types = [s.unanalyzed_type]
        elif isinstance(s.unanalyzed_type, TupleType):
            # We have multiple targets and multiple types
            if len(actual_targets) == len(s.unanalyzed_type.items):
                types = s.unanalyzed_type.items.copy()
            else:
                # But it's the wrong number of items
                self.fail('Incompatible number of types for "with" targets', s)
        else:
            # We have multiple targets and one type
            self.fail('Multiple types expected for multiple "with" targets', s)

    new_types: List[Type] = []
    for e, n in zip(s.expr, s.target):
        e.accept(self)
        if n:
            self.analyze_lvalue(n, explicit_type=s.unanalyzed_type is not None)

            # Since we have a target, pop the next type from types
            if types:
                t = types.pop(0)
                if self.is_classvar(t):
                    self.fail_invalid_classvar(n)
                allow_tuple_literal = isinstance(n, TupleExpr)
                analyzed = self.anal_type(t, allow_tuple_literal=allow_tuple_literal)
                if analyzed is not None:
                    # TODO: Deal with this better
                    new_types.append(analyzed)
                    self.store_declared_types(n, analyzed)

    s.analyzed_types = new_types

    self.visit_block(s.body)

</t>
<t tx="ekr.20220525082935.1430">def __hash__(self) -&gt; int:
    return hash(NoneType)

</t>
<t tx="ekr.20220525082935.1431">def __eq__(self, other: object) -&gt; bool:
    return isinstance(other, NoneType)

</t>
<t tx="ekr.20220525082935.1432">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_none_type(self)

</t>
<t tx="ekr.20220525082935.1433">def serialize(self) -&gt; JsonDict:
    return {'.class': 'NoneType'}

</t>
<t tx="ekr.20220525082935.1434">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'NoneType':
    assert data['.class'] == 'NoneType'
    return NoneType()


</t>
<t tx="ekr.20220525082935.1435"># NoneType used to be called NoneTyp so to avoid needlessly breaking
# external plugins we keep that alias here.
NoneTyp = NoneType


</t>
<t tx="ekr.20220525082935.1436">class ErasedType(ProperType):
    """Placeholder for an erased type.

    This is used during type inference. This has the special property that
    it is ignored during type inference.
    """

    __slots__ = ()

    @others
</t>
<t tx="ekr.20220525082935.1437">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_erased_type(self)


</t>
<t tx="ekr.20220525082935.1438">class DeletedType(ProperType):
    """Type of deleted variables.

    These can be used as lvalues but not rvalues.
    """

    __slots__ = ('source',)

    source: Optional[str]  # May be None; name that generated this value

    @others
</t>
<t tx="ekr.20220525082935.1439">def __init__(self, source: Optional[str] = None, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.source = source

</t>
<t tx="ekr.20220525082935.144">def visit_del_stmt(self, s: DelStmt) -&gt; None:
    self.statement = s
    s.expr.accept(self)
    if not self.is_valid_del_target(s.expr):
        self.fail('Invalid delete target', s)

</t>
<t tx="ekr.20220525082935.1440">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_deleted_type(self)

</t>
<t tx="ekr.20220525082935.1441">def serialize(self) -&gt; JsonDict:
    return {'.class': 'DeletedType',
            'source': self.source}

</t>
<t tx="ekr.20220525082935.1442">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'DeletedType':
    assert data['.class'] == 'DeletedType'
    return DeletedType(data['source'])


</t>
<t tx="ekr.20220525082935.1443"># Fake TypeInfo to be used as a placeholder during Instance de-serialization.
NOT_READY: Final = mypy.nodes.FakeInfo("De-serialization failure: TypeInfo not fixed")


</t>
<t tx="ekr.20220525082935.1444">class Instance(ProperType):
    """An instance type of form C[T1, ..., Tn].

    The list of type variables may be empty.

    Several types has fallbacks to `Instance`. Why?
    Because, for example `TupleTuple` is related to `builtins.tuple` instance.
    And `FunctionLike` has `builtins.function` fallback.
    This allows us to use types defined
    in typeshed for our "special" and more precise types.

    We used to have this helper function to get a fallback from different types.
    Note, that it might be incomplete, since it is not used and not updated.
    It just illustrates the concept:

        def try_getting_instance_fallback(typ: ProperType) -&gt; Optional[Instance]:
            '''Returns the Instance fallback for this type if one exists or None.'''
            if isinstance(typ, Instance):
                return typ
            elif isinstance(typ, TupleType):
                return tuple_fallback(typ)
            elif isinstance(typ, TypedDictType):
                return typ.fallback
            elif isinstance(typ, FunctionLike):
                return typ.fallback
            elif isinstance(typ, LiteralType):
                return typ.fallback
            return None

    """

    __slots__ = ('type', 'args', 'invalid', 'type_ref', 'last_known_value', '_hash')

    @others
</t>
<t tx="ekr.20220525082935.1445">def __init__(self, typ: mypy.nodes.TypeInfo, args: Sequence[Type],
             line: int = -1, column: int = -1, *,
             last_known_value: Optional['LiteralType'] = None) -&gt; None:
    super().__init__(line, column)
    self.type = typ
    self.args = tuple(args)
    self.type_ref: Optional[str] = None

    # True if recovered after incorrect number of type arguments error
    self.invalid = False

    # This field keeps track of the underlying Literal[...] value associated with
    # this instance, if one is known.
    #
    # This field is set whenever possible within expressions, but is erased upon
    # variable assignment (see erasetype.remove_instance_last_known_values) unless
    # the variable is declared to be final.
    #
    # For example, consider the following program:
    #
    #     a = 1
    #     b: Final[int] = 2
    #     c: Final = 3
    #     print(a + b + c + 4)
    #
    # The 'Instance' objects associated with the expressions '1', '2', '3', and '4' will
    # have last_known_values of type Literal[1], Literal[2], Literal[3], and Literal[4]
    # respectively. However, the Instance object assigned to 'a' and 'b' will have their
    # last_known_value erased: variable 'a' is mutable; variable 'b' was declared to be
    # specifically an int.
    #
    # Or more broadly, this field lets this Instance "remember" its original declaration
    # when applicable. We want this behavior because we want implicit Final declarations
    # to act pretty much identically with constants: we should be able to replace any
    # places where we use some Final variable with the original value and get the same
    # type-checking behavior. For example, we want this program:
    #
    #    def expects_literal(x: Literal[3]) -&gt; None: pass
    #    var: Final = 3
    #    expects_literal(var)
    #
    # ...to type-check in the exact same way as if we had written the program like this:
    #
    #    def expects_literal(x: Literal[3]) -&gt; None: pass
    #    expects_literal(3)
    #
    # In order to make this work (especially with literal types), we need var's type
    # (an Instance) to remember the "original" value.
    #
    # Preserving this value within expressions is useful for similar reasons.
    #
    # Currently most of mypy will ignore this field and will continue to treat this type like
    # a regular Instance. We end up using this field only when we are explicitly within a
    # Literal context.
    self.last_known_value = last_known_value

    # Cached hash value
    self._hash = -1

</t>
<t tx="ekr.20220525082935.1446">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_instance(self)

</t>
<t tx="ekr.20220525082935.1447">def __hash__(self) -&gt; int:
    if self._hash == -1:
        self._hash = hash((self.type, self.args, self.last_known_value))
    return self._hash

</t>
<t tx="ekr.20220525082935.1448">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, Instance):
        return NotImplemented
    return (self.type == other.type
            and self.args == other.args
            and self.last_known_value == other.last_known_value)

</t>
<t tx="ekr.20220525082935.1449">def serialize(self) -&gt; Union[JsonDict, str]:
    assert self.type is not None
    type_ref = self.type.fullname
    if not self.args and not self.last_known_value:
        return type_ref
    data: JsonDict = {
        ".class": "Instance",
    }
    data["type_ref"] = type_ref
    data["args"] = [arg.serialize() for arg in self.args]
    if self.last_known_value is not None:
        data['last_known_value'] = self.last_known_value.serialize()
    return data

</t>
<t tx="ekr.20220525082935.145">def is_valid_del_target(self, s: Expression) -&gt; bool:
    if isinstance(s, (IndexExpr, NameExpr, MemberExpr)):
        return True
    elif isinstance(s, (TupleExpr, ListExpr)):
        return all(self.is_valid_del_target(item) for item in s.items)
    else:
        return False

</t>
<t tx="ekr.20220525082935.1450">@classmethod
def deserialize(cls, data: Union[JsonDict, str]) -&gt; 'Instance':
    if isinstance(data, str):
        inst = Instance(NOT_READY, [])
        inst.type_ref = data
        return inst
    assert data['.class'] == 'Instance'
    args: List[Type] = []
    if 'args' in data:
        args_list = data['args']
        assert isinstance(args_list, list)
        args = [deserialize_type(arg) for arg in args_list]
    inst = Instance(NOT_READY, args)
    inst.type_ref = data['type_ref']  # Will be fixed up by fixup.py later.
    if 'last_known_value' in data:
        inst.last_known_value = LiteralType.deserialize(data['last_known_value'])
    return inst

</t>
<t tx="ekr.20220525082935.1451">def copy_modified(self, *,
                  args: Bogus[List[Type]] = _dummy,
                  last_known_value: Bogus[Optional['LiteralType']] = _dummy) -&gt; 'Instance':
    return Instance(
        self.type,
        args if args is not _dummy else self.args,
        self.line,
        self.column,
        last_known_value=last_known_value if last_known_value is not _dummy
        else self.last_known_value,
    )

</t>
<t tx="ekr.20220525082935.1452">def has_readable_member(self, name: str) -&gt; bool:
    return self.type.has_readable_member(name)


</t>
<t tx="ekr.20220525082935.1453">class FunctionLike(ProperType):
    """Abstract base class for function types."""

    __slots__ = ('fallback',)

    fallback: Instance

    @others
</t>
<t tx="ekr.20220525082935.1454">def __init__(self, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.can_be_false = False

</t>
<t tx="ekr.20220525082935.1455">@abstractmethod
def is_type_obj(self) -&gt; bool: pass

</t>
<t tx="ekr.20220525082935.1456">@abstractmethod
def type_object(self) -&gt; mypy.nodes.TypeInfo: pass

</t>
<t tx="ekr.20220525082935.1457">@property
@abstractmethod
def items(self) -&gt; List['CallableType']: pass

</t>
<t tx="ekr.20220525082935.1458">@abstractmethod
def with_name(self, name: str) -&gt; 'FunctionLike': pass

</t>
<t tx="ekr.20220525082935.1459">@abstractmethod
def get_name(self) -&gt; Optional[str]: pass


</t>
<t tx="ekr.20220525082935.146">def visit_global_decl(self, g: GlobalDecl) -&gt; None:
    self.statement = g
    for name in g.names:
        if name in self.nonlocal_decls[-1]:
            self.fail(f'Name "{name}" is nonlocal and global', g)
        self.global_decls[-1].add(name)

</t>
<t tx="ekr.20220525082935.1460">class FormalArgument(NamedTuple):
    name: Optional[str]
    pos: Optional[int]
    typ: Type
    required: bool


</t>
<t tx="ekr.20220525082935.1461"># TODO: should this take bound typevars too? what would this take?
#   ex: class Z(Generic[P, T]): ...; Z[[V], V]
# What does a typevar even mean in this context?
class Parameters(ProperType):
    """Type that represents the parameters to a function.

    Used for ParamSpec analysis."""
    __slots__ = ('arg_types',
                 'arg_kinds',
                 'arg_names',
                 'min_args',
                 'is_ellipsis_args',
                 'variables')

    @others
</t>
<t tx="ekr.20220525082935.1462">def __init__(self,
             arg_types: Sequence[Type],
             arg_kinds: List[ArgKind],
             arg_names: Sequence[Optional[str]],
             *,
             variables: Optional[Sequence[TypeVarLikeType]] = None,
             is_ellipsis_args: bool = False,
             line: int = -1,
             column: int = -1
             ) -&gt; None:
    super().__init__(line, column)
    self.arg_types = list(arg_types)
    self.arg_kinds = arg_kinds
    self.arg_names = list(arg_names)
    assert len(arg_types) == len(arg_kinds) == len(arg_names)
    self.min_args = arg_kinds.count(ARG_POS)
    self.is_ellipsis_args = is_ellipsis_args
    self.variables = variables or []

</t>
<t tx="ekr.20220525082935.1463">def copy_modified(self,
                  arg_types: Bogus[Sequence[Type]] = _dummy,
                  arg_kinds: Bogus[List[ArgKind]] = _dummy,
                  arg_names: Bogus[Sequence[Optional[str]]] = _dummy,
                  *,
                  variables: Bogus[Sequence[TypeVarLikeType]] = _dummy,
                  is_ellipsis_args: Bogus[bool] = _dummy
                  ) -&gt; 'Parameters':
    return Parameters(
        arg_types=arg_types if arg_types is not _dummy else self.arg_types,
        arg_kinds=arg_kinds if arg_kinds is not _dummy else self.arg_kinds,
        arg_names=arg_names if arg_names is not _dummy else self.arg_names,
        is_ellipsis_args=(is_ellipsis_args if is_ellipsis_args is not _dummy
                          else self.is_ellipsis_args),
        variables=variables if variables is not _dummy else self.variables
    )

</t>
<t tx="ekr.20220525082935.1464"># the following are copied from CallableType. Is there a way to decrease code duplication?
def var_arg(self) -&gt; Optional[FormalArgument]:
    """The formal argument for *args."""
    for position, (type, kind) in enumerate(zip(self.arg_types, self.arg_kinds)):
        if kind == ARG_STAR:
            return FormalArgument(None, position, type, False)
    return None

</t>
<t tx="ekr.20220525082935.1465">def kw_arg(self) -&gt; Optional[FormalArgument]:
    """The formal argument for **kwargs."""
    for position, (type, kind) in enumerate(zip(self.arg_types, self.arg_kinds)):
        if kind == ARG_STAR2:
            return FormalArgument(None, position, type, False)
    return None

</t>
<t tx="ekr.20220525082935.1466">def formal_arguments(self, include_star_args: bool = False) -&gt; List[FormalArgument]:
    """Yields the formal arguments corresponding to this callable, ignoring *arg and **kwargs.

    To handle *args and **kwargs, use the 'callable.var_args' and 'callable.kw_args' fields,
    if they are not None.

    If you really want to include star args in the yielded output, set the
    'include_star_args' parameter to 'True'."""
    args = []
    done_with_positional = False
    for i in range(len(self.arg_types)):
        kind = self.arg_kinds[i]
        if kind.is_named() or kind.is_star():
            done_with_positional = True
        if not include_star_args and kind.is_star():
            continue

        required = kind.is_required()
        pos = None if done_with_positional else i
        arg = FormalArgument(
            self.arg_names[i],
            pos,
            self.arg_types[i],
            required
        )
        args.append(arg)
    return args

</t>
<t tx="ekr.20220525082935.1467">def argument_by_name(self, name: Optional[str]) -&gt; Optional[FormalArgument]:
    if name is None:
        return None
    seen_star = False
    for i, (arg_name, kind, typ) in enumerate(
            zip(self.arg_names, self.arg_kinds, self.arg_types)):
        # No more positional arguments after these.
        if kind.is_named() or kind.is_star():
            seen_star = True
        if kind.is_star():
            continue
        if arg_name == name:
            position = None if seen_star else i
            return FormalArgument(name, position, typ, kind.is_required())
    return self.try_synthesizing_arg_from_kwarg(name)

</t>
<t tx="ekr.20220525082935.1468">def argument_by_position(self, position: Optional[int]) -&gt; Optional[FormalArgument]:
    if position is None:
        return None
    if position &gt;= len(self.arg_names):
        return self.try_synthesizing_arg_from_vararg(position)
    name, kind, typ = (
        self.arg_names[position],
        self.arg_kinds[position],
        self.arg_types[position],
    )
    if kind.is_positional():
        return FormalArgument(name, position, typ, kind == ARG_POS)
    else:
        return self.try_synthesizing_arg_from_vararg(position)

</t>
<t tx="ekr.20220525082935.1469">def try_synthesizing_arg_from_kwarg(self,
                                    name: Optional[str]) -&gt; Optional[FormalArgument]:
    kw_arg = self.kw_arg()
    if kw_arg is not None:
        return FormalArgument(name, None, kw_arg.typ, False)
    else:
        return None

</t>
<t tx="ekr.20220525082935.147">def visit_nonlocal_decl(self, d: NonlocalDecl) -&gt; None:
    self.statement = d
    if not self.is_func_scope():
        self.fail("nonlocal declaration not allowed at module level", d)
    else:
        for name in d.names:
            for table in reversed(self.locals[:-1]):
                if table is not None and name in table:
                    break
            else:
                self.fail(f'No binding for nonlocal "{name}" found', d)

            if self.locals[-1] is not None and name in self.locals[-1]:
                self.fail('Name "{}" is already defined in local '
                          'scope before nonlocal declaration'.format(name), d)

            if name in self.global_decls[-1]:
                self.fail(f'Name "{name}" is nonlocal and global', d)
            self.nonlocal_decls[-1].add(name)

</t>
<t tx="ekr.20220525082935.1470">def try_synthesizing_arg_from_vararg(self,
                                     position: Optional[int]) -&gt; Optional[FormalArgument]:
    var_arg = self.var_arg()
    if var_arg is not None:
        return FormalArgument(None, position, var_arg.typ, False)
    else:
        return None

</t>
<t tx="ekr.20220525082935.1471">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_parameters(self)

</t>
<t tx="ekr.20220525082935.1472">def serialize(self) -&gt; JsonDict:
    return {'.class': 'Parameters',
            'arg_types': [t.serialize() for t in self.arg_types],
            'arg_kinds': [int(x.value) for x in self.arg_kinds],
            'arg_names': self.arg_names,
            'variables': [tv.serialize() for tv in self.variables],
            }

</t>
<t tx="ekr.20220525082935.1473">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'Parameters':
    assert data['.class'] == 'Parameters'
    return Parameters(
        [deserialize_type(t) for t in data['arg_types']],
        [ArgKind(x) for x in data['arg_kinds']],
        data['arg_names'],
        variables=[cast(TypeVarLikeType, deserialize_type(v)) for v in data['variables']],
    )

</t>
<t tx="ekr.20220525082935.1474">def __hash__(self) -&gt; int:
    return hash((self.is_ellipsis_args, tuple(self.arg_types),
                 tuple(self.arg_names), tuple(self.arg_kinds)))

</t>
<t tx="ekr.20220525082935.1475">def __eq__(self, other: object) -&gt; bool:
    if isinstance(other, Parameters) or isinstance(other, CallableType):
        return (
            self.arg_types == other.arg_types and
            self.arg_names == other.arg_names and
            self.arg_kinds == other.arg_kinds and
            self.is_ellipsis_args == other.is_ellipsis_args
        )
    else:
        return NotImplemented


</t>
<t tx="ekr.20220525082935.1476">class CallableType(FunctionLike):
    """Type of a non-overloaded callable object (such as function)."""

    __slots__ = ('arg_types',  # Types of function arguments
                 'arg_kinds',  # ARG_ constants
                 'arg_names',  # Argument names; None if not a keyword argument
                 'min_args',  # Minimum number of arguments; derived from arg_kinds
                 'ret_type',  # Return value type
                 'name',  # Name (may be None; for error messages and plugins)
                 'definition',  # For error messages.  May be None.
                 'variables',  # Type variables for a generic function
                 'is_ellipsis_args',  # Is this Callable[..., t] (with literal '...')?
                 'is_classmethod_class',  # Is this callable constructed for the benefit
                                          # of a classmethod's 'cls' argument?
                 'implicit',  # Was this type implicitly generated instead of explicitly
                              # specified by the user?
                 'special_sig',  # Non-None for signatures that require special handling
                                 # (currently only value is 'dict' for a signature similar to
                                 # 'dict')
                 'from_type_type',  # Was this callable generated by analyzing Type[...]
                                    # instantiation?
                 'bound_args',  # Bound type args, mostly unused but may be useful for
                                # tools that consume mypy ASTs
                 'def_extras',  # Information about original definition we want to serialize.
                                # This is used for more detailed error messages.
                 'type_guard',  # T, if -&gt; TypeGuard[T] (ret_type is bool in this case).
                 'from_concatenate',  # whether this callable is from a concatenate object
                                      # (this is used for error messages)
                 )

    @others
</t>
<t tx="ekr.20220525082935.1477">def __init__(self,
             # maybe this should be refactored to take a Parameters object
             arg_types: Sequence[Type],
             arg_kinds: List[ArgKind],
             arg_names: Sequence[Optional[str]],
             ret_type: Type,
             fallback: Instance,
             name: Optional[str] = None,
             definition: Optional[SymbolNode] = None,
             variables: Optional[Sequence[TypeVarLikeType]] = None,
             line: int = -1,
             column: int = -1,
             is_ellipsis_args: bool = False,
             implicit: bool = False,
             special_sig: Optional[str] = None,
             from_type_type: bool = False,
             bound_args: Sequence[Optional[Type]] = (),
             def_extras: Optional[Dict[str, Any]] = None,
             type_guard: Optional[Type] = None,
             from_concatenate: bool = False
             ) -&gt; None:
    super().__init__(line, column)
    assert len(arg_types) == len(arg_kinds) == len(arg_names)
    if variables is None:
        variables = []
    self.arg_types = list(arg_types)
    self.arg_kinds = arg_kinds
    self.arg_names = list(arg_names)
    self.min_args = arg_kinds.count(ARG_POS)
    self.ret_type = ret_type
    self.fallback = fallback
    assert not name or '&lt;bound method' not in name
    self.name = name
    self.definition = definition
    self.variables = variables
    self.is_ellipsis_args = is_ellipsis_args
    self.implicit = implicit
    self.special_sig = special_sig
    self.from_type_type = from_type_type
    self.from_concatenate = from_concatenate
    if not bound_args:
        bound_args = ()
    self.bound_args = bound_args
    if def_extras:
        self.def_extras = def_extras
    elif isinstance(definition, FuncDef):
        # This information would be lost if we don't have definition
        # after serialization, but it is useful in error messages.
        # TODO: decide how to add more info here (file, line, column)
        # without changing interface hash.
        self.def_extras = {
            'first_arg': (
                definition.arguments[0].variable.name
                if (getattr(definition, 'arguments', None)
                    and definition.arg_names
                    and definition.info
                    and not definition.is_static)
                else None
            ),
        }
    else:
        self.def_extras = {}
    self.type_guard = type_guard

</t>
<t tx="ekr.20220525082935.1478">def copy_modified(self,
                  arg_types: Bogus[Sequence[Type]] = _dummy,
                  arg_kinds: Bogus[List[ArgKind]] = _dummy,
                  arg_names: Bogus[List[Optional[str]]] = _dummy,
                  ret_type: Bogus[Type] = _dummy,
                  fallback: Bogus[Instance] = _dummy,
                  name: Bogus[Optional[str]] = _dummy,
                  definition: Bogus[SymbolNode] = _dummy,
                  variables: Bogus[Sequence[TypeVarLikeType]] = _dummy,
                  line: Bogus[int] = _dummy,
                  column: Bogus[int] = _dummy,
                  is_ellipsis_args: Bogus[bool] = _dummy,
                  implicit: Bogus[bool] = _dummy,
                  special_sig: Bogus[Optional[str]] = _dummy,
                  from_type_type: Bogus[bool] = _dummy,
                  bound_args: Bogus[List[Optional[Type]]] = _dummy,
                  def_extras: Bogus[Dict[str, Any]] = _dummy,
                  type_guard: Bogus[Optional[Type]] = _dummy,
                  from_concatenate: Bogus[bool] = _dummy,
                  ) -&gt; 'CallableType':
    return CallableType(
        arg_types=arg_types if arg_types is not _dummy else self.arg_types,
        arg_kinds=arg_kinds if arg_kinds is not _dummy else self.arg_kinds,
        arg_names=arg_names if arg_names is not _dummy else self.arg_names,
        ret_type=ret_type if ret_type is not _dummy else self.ret_type,
        fallback=fallback if fallback is not _dummy else self.fallback,
        name=name if name is not _dummy else self.name,
        definition=definition if definition is not _dummy else self.definition,
        variables=variables if variables is not _dummy else self.variables,
        line=line if line is not _dummy else self.line,
        column=column if column is not _dummy else self.column,
        is_ellipsis_args=(
            is_ellipsis_args if is_ellipsis_args is not _dummy else self.is_ellipsis_args),
        implicit=implicit if implicit is not _dummy else self.implicit,
        special_sig=special_sig if special_sig is not _dummy else self.special_sig,
        from_type_type=from_type_type if from_type_type is not _dummy else self.from_type_type,
        bound_args=bound_args if bound_args is not _dummy else self.bound_args,
        def_extras=def_extras if def_extras is not _dummy else dict(self.def_extras),
        type_guard=type_guard if type_guard is not _dummy else self.type_guard,
        from_concatenate=(from_concatenate if from_concatenate is not _dummy
                          else self.from_concatenate),
    )

</t>
<t tx="ekr.20220525082935.1479">def var_arg(self) -&gt; Optional[FormalArgument]:
    """The formal argument for *args."""
    for position, (type, kind) in enumerate(zip(self.arg_types, self.arg_kinds)):
        if kind == ARG_STAR:
            return FormalArgument(None, position, type, False)
    return None

</t>
<t tx="ekr.20220525082935.148">def visit_print_stmt(self, s: PrintStmt) -&gt; None:
    self.statement = s
    for arg in s.args:
        arg.accept(self)
    if s.target:
        s.target.accept(self)

</t>
<t tx="ekr.20220525082935.1480">def kw_arg(self) -&gt; Optional[FormalArgument]:
    """The formal argument for **kwargs."""
    for position, (type, kind) in enumerate(zip(self.arg_types, self.arg_kinds)):
        if kind == ARG_STAR2:
            return FormalArgument(None, position, type, False)
    return None

</t>
<t tx="ekr.20220525082935.1481">@property
def is_var_arg(self) -&gt; bool:
    """Does this callable have a *args argument?"""
    return ARG_STAR in self.arg_kinds

</t>
<t tx="ekr.20220525082935.1482">@property
def is_kw_arg(self) -&gt; bool:
    """Does this callable have a **kwargs argument?"""
    return ARG_STAR2 in self.arg_kinds

</t>
<t tx="ekr.20220525082935.1483">def is_type_obj(self) -&gt; bool:
    return self.fallback.type.is_metaclass()

</t>
<t tx="ekr.20220525082935.1484">def type_object(self) -&gt; mypy.nodes.TypeInfo:
    assert self.is_type_obj()
    ret = get_proper_type(self.ret_type)
    if isinstance(ret, TypeVarType):
        ret = get_proper_type(ret.upper_bound)
    if isinstance(ret, TupleType):
        ret = ret.partial_fallback
    assert isinstance(ret, Instance)
    return ret.type

</t>
<t tx="ekr.20220525082935.1485">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_callable_type(self)

</t>
<t tx="ekr.20220525082935.1486">def with_name(self, name: str) -&gt; 'CallableType':
    """Return a copy of this type with the specified name."""
    return self.copy_modified(ret_type=self.ret_type, name=name)

</t>
<t tx="ekr.20220525082935.1487">def get_name(self) -&gt; Optional[str]:
    return self.name

</t>
<t tx="ekr.20220525082935.1488">def max_possible_positional_args(self) -&gt; int:
    """Returns maximum number of positional arguments this method could possibly accept.

    This takes into account *arg and **kwargs but excludes keyword-only args."""
    if self.is_var_arg or self.is_kw_arg:
        return sys.maxsize
    return sum(kind.is_positional() for kind in self.arg_kinds)

</t>
<t tx="ekr.20220525082935.1489">def formal_arguments(self, include_star_args: bool = False) -&gt; List[FormalArgument]:
    """Return a list of the formal arguments of this callable, ignoring *arg and **kwargs.

    To handle *args and **kwargs, use the 'callable.var_args' and 'callable.kw_args' fields,
    if they are not None.

    If you really want to include star args in the yielded output, set the
    'include_star_args' parameter to 'True'."""
    args = []
    done_with_positional = False
    for i in range(len(self.arg_types)):
        kind = self.arg_kinds[i]
        if kind.is_named() or kind.is_star():
            done_with_positional = True
        if not include_star_args and kind.is_star():
            continue

        required = kind.is_required()
        pos = None if done_with_positional else i
        arg = FormalArgument(
            self.arg_names[i],
            pos,
            self.arg_types[i],
            required
        )
        args.append(arg)
    return args

</t>
<t tx="ekr.20220525082935.149">def visit_exec_stmt(self, s: ExecStmt) -&gt; None:
    self.statement = s
    s.expr.accept(self)
    if s.globals:
        s.globals.accept(self)
    if s.locals:
        s.locals.accept(self)

</t>
<t tx="ekr.20220525082935.1490">def argument_by_name(self, name: Optional[str]) -&gt; Optional[FormalArgument]:
    if name is None:
        return None
    seen_star = False
    for i, (arg_name, kind, typ) in enumerate(
            zip(self.arg_names, self.arg_kinds, self.arg_types)):
        # No more positional arguments after these.
        if kind.is_named() or kind.is_star():
            seen_star = True
        if kind.is_star():
            continue
        if arg_name == name:
            position = None if seen_star else i
            return FormalArgument(name, position, typ, kind.is_required())
    return self.try_synthesizing_arg_from_kwarg(name)

</t>
<t tx="ekr.20220525082935.1491">def argument_by_position(self, position: Optional[int]) -&gt; Optional[FormalArgument]:
    if position is None:
        return None
    if position &gt;= len(self.arg_names):
        return self.try_synthesizing_arg_from_vararg(position)
    name, kind, typ = (
        self.arg_names[position],
        self.arg_kinds[position],
        self.arg_types[position],
    )
    if kind.is_positional():
        return FormalArgument(name, position, typ, kind == ARG_POS)
    else:
        return self.try_synthesizing_arg_from_vararg(position)

</t>
<t tx="ekr.20220525082935.1492">def try_synthesizing_arg_from_kwarg(self,
                                    name: Optional[str]) -&gt; Optional[FormalArgument]:
    kw_arg = self.kw_arg()
    if kw_arg is not None:
        return FormalArgument(name, None, kw_arg.typ, False)
    else:
        return None

</t>
<t tx="ekr.20220525082935.1493">def try_synthesizing_arg_from_vararg(self,
                                     position: Optional[int]) -&gt; Optional[FormalArgument]:
    var_arg = self.var_arg()
    if var_arg is not None:
        return FormalArgument(None, position, var_arg.typ, False)
    else:
        return None

</t>
<t tx="ekr.20220525082935.1494">@property
def items(self) -&gt; List['CallableType']:
    return [self]

</t>
<t tx="ekr.20220525082935.1495">def is_generic(self) -&gt; bool:
    return bool(self.variables)

</t>
<t tx="ekr.20220525082935.1496">def type_var_ids(self) -&gt; List[TypeVarId]:
    a: List[TypeVarId] = []
    for tv in self.variables:
        a.append(tv.id)
    return a

</t>
<t tx="ekr.20220525082935.1497">def param_spec(self) -&gt; Optional[ParamSpecType]:
    """Return ParamSpec if callable can be called with one.

    A Callable accepting ParamSpec P args (*args, **kwargs) must have the
    two final parameters like this: *args: P.args, **kwargs: P.kwargs.
    """
    if len(self.arg_types) &lt; 2:
        return None
    if self.arg_kinds[-2] != ARG_STAR or self.arg_kinds[-1] != ARG_STAR2:
        return None
    arg_type = self.arg_types[-2]
    if not isinstance(arg_type, ParamSpecType):
        return None
    # sometimes paramspectypes are analyzed in from mysterious places,
    # e.g. def f(prefix..., *args: P.args, **kwargs: P.kwargs) -&gt; ...: ...
    prefix = arg_type.prefix
    if not prefix.arg_types:
        # TODO: confirm that all arg kinds are positional
        prefix = Parameters(self.arg_types[:-2], self.arg_kinds[:-2], self.arg_names[:-2])
    return ParamSpecType(arg_type.name, arg_type.fullname, arg_type.id, ParamSpecFlavor.BARE,
                         arg_type.upper_bound, prefix=prefix)

</t>
<t tx="ekr.20220525082935.1498">def expand_param_spec(self,
                      c: Union['CallableType', Parameters],
                      no_prefix: bool = False) -&gt; 'CallableType':
    variables = c.variables

    if no_prefix:
        return self.copy_modified(arg_types=c.arg_types,
                                  arg_kinds=c.arg_kinds,
                                  arg_names=c.arg_names,
                                  is_ellipsis_args=c.is_ellipsis_args,
                                  variables=[*variables, *self.variables])
    else:
        return self.copy_modified(arg_types=self.arg_types[:-2] + c.arg_types,
                                  arg_kinds=self.arg_kinds[:-2] + c.arg_kinds,
                                  arg_names=self.arg_names[:-2] + c.arg_names,
                                  is_ellipsis_args=c.is_ellipsis_args,
                                  variables=[*variables, *self.variables])

</t>
<t tx="ekr.20220525082935.1499">def __hash__(self) -&gt; int:
    # self.is_type_obj() will fail if self.fallback.type is a FakeInfo
    if isinstance(self.fallback.type, FakeInfo):
        is_type_obj = 2
    else:
        is_type_obj = self.is_type_obj()
    return hash((self.ret_type, is_type_obj,
                 self.is_ellipsis_args, self.name,
                tuple(self.arg_types), tuple(self.arg_names), tuple(self.arg_kinds)))

</t>
<t tx="ekr.20220525082935.15">def add_typing_extension_aliases(self, tree: MypyFile) -&gt; None:
    """Typing extensions module does contain some type aliases.

    We need to analyze them as such, because in typeshed
    they are just defined as `_Alias()` call.
    Which is not supported natively.
    """
    assert tree.fullname == 'typing_extensions'

    for alias, target_name in typing_extensions_aliases.items():
        name = alias.split('.')[-1]
        if name in tree.names and isinstance(tree.names[name].node, TypeAlias):
            continue  # Do not reset TypeAliases on the second pass.

        # We need to remove any node that is there at the moment. It is invalid.
        tree.names.pop(name, None)

        # Now, create a new alias.
        self.create_alias(tree, target_name, alias, name)

</t>
<t tx="ekr.20220525082935.150">def visit_match_stmt(self, s: MatchStmt) -&gt; None:
    self.statement = s
    infer_reachability_of_match_statement(s, self.options)
    s.subject.accept(self)
    for i in range(len(s.patterns)):
        s.patterns[i].accept(self)
        guard = s.guards[i]
        if guard is not None:
            guard.accept(self)
        self.visit_block(s.bodies[i])

</t>
<t tx="ekr.20220525082935.1500">def __eq__(self, other: object) -&gt; bool:
    if isinstance(other, CallableType):
        return (self.ret_type == other.ret_type and
                self.arg_types == other.arg_types and
                self.arg_names == other.arg_names and
                self.arg_kinds == other.arg_kinds and
                self.name == other.name and
                self.is_type_obj() == other.is_type_obj() and
                self.is_ellipsis_args == other.is_ellipsis_args)
    else:
        return NotImplemented

</t>
<t tx="ekr.20220525082935.1501">def serialize(self) -&gt; JsonDict:
    # TODO: As an optimization, leave out everything related to
    # generic functions for non-generic functions.
    return {'.class': 'CallableType',
            'arg_types': [t.serialize() for t in self.arg_types],
            'arg_kinds': [int(x.value) for x in self.arg_kinds],
            'arg_names': self.arg_names,
            'ret_type': self.ret_type.serialize(),
            'fallback': self.fallback.serialize(),
            'name': self.name,
            # We don't serialize the definition (only used for error messages).
            'variables': [v.serialize() for v in self.variables],
            'is_ellipsis_args': self.is_ellipsis_args,
            'implicit': self.implicit,
            'bound_args': [(None if t is None else t.serialize())
                           for t in self.bound_args],
            'def_extras': dict(self.def_extras),
            'type_guard': self.type_guard.serialize() if self.type_guard is not None else None,
            'from_concatenate': self.from_concatenate,
            }

</t>
<t tx="ekr.20220525082935.1502">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'CallableType':
    assert data['.class'] == 'CallableType'
    # TODO: Set definition to the containing SymbolNode?
    return CallableType(
        [deserialize_type(t) for t in data['arg_types']],
        [ArgKind(x) for x in data['arg_kinds']],
        data['arg_names'],
        deserialize_type(data['ret_type']),
        Instance.deserialize(data['fallback']),
        name=data['name'],
        variables=[cast(TypeVarLikeType, deserialize_type(v)) for v in data['variables']],
        is_ellipsis_args=data['is_ellipsis_args'],
        implicit=data['implicit'],
        bound_args=[(None if t is None else deserialize_type(t)) for t in data['bound_args']],
        def_extras=data['def_extras'],
        type_guard=(deserialize_type(data['type_guard'])
                    if data['type_guard'] is not None else None),
        from_concatenate=data['from_concatenate'],
    )


</t>
<t tx="ekr.20220525082935.1503">class Overloaded(FunctionLike):
    """Overloaded function type T1, ... Tn, where each Ti is CallableType.

    The variant to call is chosen based on static argument
    types. Overloaded function types can only be defined in stub
    files, and thus there is no explicit runtime dispatch
    implementation.
    """

    __slots__ = ('_items', 'fallback')

    _items: List[CallableType]  # Must not be empty

    @others
</t>
<t tx="ekr.20220525082935.1504">def __init__(self, items: List[CallableType]) -&gt; None:
    super().__init__(items[0].line, items[0].column)
    self._items = items
    self.fallback = items[0].fallback

</t>
<t tx="ekr.20220525082935.1505">@property
def items(self) -&gt; List[CallableType]:
    return self._items

</t>
<t tx="ekr.20220525082935.1506">def name(self) -&gt; Optional[str]:
    return self.get_name()

</t>
<t tx="ekr.20220525082935.1507">def is_type_obj(self) -&gt; bool:
    # All the items must have the same type object status, so it's
    # sufficient to query only (any) one of them.
    return self._items[0].is_type_obj()

</t>
<t tx="ekr.20220525082935.1508">def type_object(self) -&gt; mypy.nodes.TypeInfo:
    # All the items must have the same type object, so it's sufficient to
    # query only (any) one of them.
    return self._items[0].type_object()

</t>
<t tx="ekr.20220525082935.1509">def with_name(self, name: str) -&gt; 'Overloaded':
    ni: List[CallableType] = []
    for it in self._items:
        ni.append(it.with_name(name))
    return Overloaded(ni)

</t>
<t tx="ekr.20220525082935.151">#
# Expressions
#

</t>
<t tx="ekr.20220525082935.1510">def get_name(self) -&gt; Optional[str]:
    return self._items[0].name

</t>
<t tx="ekr.20220525082935.1511">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_overloaded(self)

</t>
<t tx="ekr.20220525082935.1512">def __hash__(self) -&gt; int:
    return hash(tuple(self.items))

</t>
<t tx="ekr.20220525082935.1513">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, Overloaded):
        return NotImplemented
    return self.items == other.items

</t>
<t tx="ekr.20220525082935.1514">def serialize(self) -&gt; JsonDict:
    return {'.class': 'Overloaded',
            'items': [t.serialize() for t in self.items],
            }

</t>
<t tx="ekr.20220525082935.1515">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'Overloaded':
    assert data['.class'] == 'Overloaded'
    return Overloaded([CallableType.deserialize(t) for t in data['items']])


</t>
<t tx="ekr.20220525082935.1516">class TupleType(ProperType):
    """The tuple type Tuple[T1, ..., Tn] (at least one type argument).

    Instance variables:
        items: Tuple item types
        partial_fallback: The (imprecise) underlying instance type that is used
            for non-tuple methods. This is generally builtins.tuple[Any, ...] for
            regular tuples, but it's different for named tuples and classes with
            a tuple base class. Use mypy.typeops.tuple_fallback to calculate the
            precise fallback type derived from item types.
        implicit: If True, derived from a tuple expression (t,....) instead of Tuple[t, ...]
    """

    __slots__ = ('items', 'partial_fallback', 'implicit')

    items: List[Type]
    partial_fallback: Instance
    implicit: bool

    @others
</t>
<t tx="ekr.20220525082935.1517">def __init__(self, items: List[Type], fallback: Instance, line: int = -1,
             column: int = -1, implicit: bool = False) -&gt; None:
    self.partial_fallback = fallback
    self.items = items
    self.implicit = implicit
    super().__init__(line, column)

</t>
<t tx="ekr.20220525082935.1518">def can_be_true_default(self) -&gt; bool:
    if self.can_be_any_bool():
        # Corner case: it is a `NamedTuple` with `__bool__` method defined.
        # It can be anything: both `True` and `False`.
        return True
    return self.length() &gt; 0

</t>
<t tx="ekr.20220525082935.1519">def can_be_false_default(self) -&gt; bool:
    if self.can_be_any_bool():
        # Corner case: it is a `NamedTuple` with `__bool__` method defined.
        # It can be anything: both `True` and `False`.
        return True
    return self.length() == 0

</t>
<t tx="ekr.20220525082935.152">def visit_name_expr(self, expr: NameExpr) -&gt; None:
    n = self.lookup(expr.name, expr)
    if n:
        self.bind_name_expr(expr, n)

</t>
<t tx="ekr.20220525082935.1520">def can_be_any_bool(self) -&gt; bool:
    return bool(
        self.partial_fallback.type
        and self.partial_fallback.type.fullname != 'builtins.tuple'
        and self.partial_fallback.type.names.get('__bool__')
    )

</t>
<t tx="ekr.20220525082935.1521">def length(self) -&gt; int:
    return len(self.items)

</t>
<t tx="ekr.20220525082935.1522">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_tuple_type(self)

</t>
<t tx="ekr.20220525082935.1523">def __hash__(self) -&gt; int:
    return hash((tuple(self.items), self.partial_fallback))

</t>
<t tx="ekr.20220525082935.1524">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, TupleType):
        return NotImplemented
    return self.items == other.items and self.partial_fallback == other.partial_fallback

</t>
<t tx="ekr.20220525082935.1525">def serialize(self) -&gt; JsonDict:
    return {'.class': 'TupleType',
            'items': [t.serialize() for t in self.items],
            'partial_fallback': self.partial_fallback.serialize(),
            'implicit': self.implicit,
            }

</t>
<t tx="ekr.20220525082935.1526">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'TupleType':
    assert data['.class'] == 'TupleType'
    return TupleType([deserialize_type(t) for t in data['items']],
                     Instance.deserialize(data['partial_fallback']),
                     implicit=data['implicit'])

</t>
<t tx="ekr.20220525082935.1527">def copy_modified(self, *, fallback: Optional[Instance] = None,
                  items: Optional[List[Type]] = None) -&gt; 'TupleType':
    if fallback is None:
        fallback = self.partial_fallback
    if items is None:
        items = self.items
    return TupleType(items, fallback, self.line, self.column)

</t>
<t tx="ekr.20220525082935.1528">def slice(self, begin: Optional[int], end: Optional[int],
          stride: Optional[int]) -&gt; 'TupleType':
    return TupleType(self.items[begin:end:stride], self.partial_fallback,
                     self.line, self.column, self.implicit)


</t>
<t tx="ekr.20220525082935.1529">class TypedDictType(ProperType):
    """Type of TypedDict object {'k1': v1, ..., 'kn': vn}.

    A TypedDict object is a dictionary with specific string (literal) keys. Each
    key has a value with a distinct type that depends on the key. TypedDict objects
    are normal dict objects at runtime.

    A TypedDictType can be either named or anonymous. If it's anonymous, its
    fallback will be typing_extensions._TypedDict (Instance). _TypedDict is a subclass
    of Mapping[str, object] and defines all non-mapping dict methods that TypedDict
    supports. Some dict methods are unsafe and not supported. _TypedDict isn't defined
    at runtime.

    If a TypedDict is named, its fallback will be an Instance of the named type
    (ex: "Point") whose TypeInfo has a typeddict_type that is anonymous. This
    is similar to how named tuples work.

    TODO: The fallback structure is perhaps overly complicated.
    """

    __slots__ = ('items', 'required_keys', 'fallback')

    items: "OrderedDict[str, Type]"  # item_name -&gt; item_type
    required_keys: Set[str]
    fallback: Instance

    @others
</t>
<t tx="ekr.20220525082935.153">def bind_name_expr(self, expr: NameExpr, sym: SymbolTableNode) -&gt; None:
    """Bind name expression to a symbol table node."""
    if isinstance(sym.node, TypeVarExpr) and self.tvar_scope.get_binding(sym):
        self.fail('"{}" is a type variable and only valid in type '
                  'context'.format(expr.name), expr)
    elif isinstance(sym.node, PlaceholderNode):
        self.process_placeholder(expr.name, 'name', expr)
    else:
        expr.kind = sym.kind
        expr.node = sym.node
        expr.fullname = sym.fullname

</t>
<t tx="ekr.20220525082935.1530">def __init__(self, items: 'OrderedDict[str, Type]', required_keys: Set[str],
             fallback: Instance, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.items = items
    self.required_keys = required_keys
    self.fallback = fallback
    self.can_be_true = len(self.items) &gt; 0
    self.can_be_false = len(self.required_keys) == 0

</t>
<t tx="ekr.20220525082935.1531">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_typeddict_type(self)

</t>
<t tx="ekr.20220525082935.1532">def __hash__(self) -&gt; int:
    return hash((frozenset(self.items.items()), self.fallback,
                 frozenset(self.required_keys)))

</t>
<t tx="ekr.20220525082935.1533">def __eq__(self, other: object) -&gt; bool:
    if isinstance(other, TypedDictType):
        if frozenset(self.items.keys()) != frozenset(other.items.keys()):
            return False
        for (_, left_item_type, right_item_type) in self.zip(other):
            if not left_item_type == right_item_type:
                return False
        return self.fallback == other.fallback and self.required_keys == other.required_keys
    else:
        return NotImplemented

</t>
<t tx="ekr.20220525082935.1534">def serialize(self) -&gt; JsonDict:
    return {'.class': 'TypedDictType',
            'items': [[n, t.serialize()] for (n, t) in self.items.items()],
            'required_keys': sorted(self.required_keys),
            'fallback': self.fallback.serialize(),
            }

</t>
<t tx="ekr.20220525082935.1535">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'TypedDictType':
    assert data['.class'] == 'TypedDictType'
    return TypedDictType(OrderedDict([(n, deserialize_type(t))
                                      for (n, t) in data['items']]),
                         set(data['required_keys']),
                         Instance.deserialize(data['fallback']))

</t>
<t tx="ekr.20220525082935.1536">def is_anonymous(self) -&gt; bool:
    return self.fallback.type.fullname in TPDICT_FB_NAMES

</t>
<t tx="ekr.20220525082935.1537">def as_anonymous(self) -&gt; 'TypedDictType':
    if self.is_anonymous():
        return self
    assert self.fallback.type.typeddict_type is not None
    return self.fallback.type.typeddict_type.as_anonymous()

</t>
<t tx="ekr.20220525082935.1538">def copy_modified(self, *, fallback: Optional[Instance] = None,
                  item_types: Optional[List[Type]] = None,
                  required_keys: Optional[Set[str]] = None) -&gt; 'TypedDictType':
    if fallback is None:
        fallback = self.fallback
    if item_types is None:
        items = self.items
    else:
        items = OrderedDict(zip(self.items, item_types))
    if required_keys is None:
        required_keys = self.required_keys
    return TypedDictType(items, required_keys, fallback, self.line, self.column)

</t>
<t tx="ekr.20220525082935.1539">def create_anonymous_fallback(self) -&gt; Instance:
    anonymous = self.as_anonymous()
    return anonymous.fallback

</t>
<t tx="ekr.20220525082935.154">def visit_super_expr(self, expr: SuperExpr) -&gt; None:
    if not self.type and not expr.call.args:
        self.fail('"super" used outside class', expr)
        return
    expr.info = self.type
    for arg in expr.call.args:
        arg.accept(self)

</t>
<t tx="ekr.20220525082935.1540">def names_are_wider_than(self, other: 'TypedDictType') -&gt; bool:
    return len(other.items.keys() - self.items.keys()) == 0

</t>
<t tx="ekr.20220525082935.1541">def zip(self, right: 'TypedDictType') -&gt; Iterable[Tuple[str, Type, Type]]:
    left = self
    for (item_name, left_item_type) in left.items.items():
        right_item_type = right.items.get(item_name)
        if right_item_type is not None:
            yield (item_name, left_item_type, right_item_type)

</t>
<t tx="ekr.20220525082935.1542">def zipall(self, right: 'TypedDictType') \
        -&gt; Iterable[Tuple[str, Optional[Type], Optional[Type]]]:
    left = self
    for (item_name, left_item_type) in left.items.items():
        right_item_type = right.items.get(item_name)
        yield (item_name, left_item_type, right_item_type)
    for (item_name, right_item_type) in right.items.items():
        if item_name in left.items:
            continue
        yield (item_name, None, right_item_type)


</t>
<t tx="ekr.20220525082935.1543">class RawExpressionType(ProperType):
    """A synthetic type representing some arbitrary expression that does not cleanly
    translate into a type.

    This synthetic type is only used at the beginning stages of semantic analysis
    and should be completely removing during the process for mapping UnboundTypes to
    actual types: we either turn it into a LiteralType or an AnyType.

    For example, suppose `Foo[1]` is initially represented as the following:

        UnboundType(
            name='Foo',
            args=[
                RawExpressionType(value=1, base_type_name='builtins.int'),
            ],
        )

    As we perform semantic analysis, this type will transform into one of two
    possible forms.

    If 'Foo' was an alias for 'Literal' all along, this type is transformed into:

        LiteralType(value=1, fallback=int_instance_here)

    Alternatively, if 'Foo' is an unrelated class, we report an error and instead
    produce something like this:

        Instance(type=typeinfo_for_foo, args=[AnyType(TypeOfAny.from_error))

    If the "note" field is not None, the provided note will be reported alongside the
    error at this point.

    Note: if "literal_value" is None, that means this object is representing some
    expression that cannot possibly be a parameter of Literal[...]. For example,
    "Foo[3j]" would be represented as:

        UnboundType(
            name='Foo',
            args=[
                RawExpressionType(value=None, base_type_name='builtins.complex'),
            ],
        )
    """

    __slots__ = ('literal_value', 'base_type_name', 'note')

    @others
</t>
<t tx="ekr.20220525082935.1544">def __init__(self,
             literal_value: Optional[LiteralValue],
             base_type_name: str,
             line: int = -1,
             column: int = -1,
             note: Optional[str] = None,
             ) -&gt; None:
    super().__init__(line, column)
    self.literal_value = literal_value
    self.base_type_name = base_type_name
    self.note = note

</t>
<t tx="ekr.20220525082935.1545">def simple_name(self) -&gt; str:
    return self.base_type_name.replace("builtins.", "")

</t>
<t tx="ekr.20220525082935.1546">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    assert isinstance(visitor, SyntheticTypeVisitor)
    return visitor.visit_raw_expression_type(self)

</t>
<t tx="ekr.20220525082935.1547">def serialize(self) -&gt; JsonDict:
    assert False, "Synthetic types don't serialize"

</t>
<t tx="ekr.20220525082935.1548">def __hash__(self) -&gt; int:
    return hash((self.literal_value, self.base_type_name))

</t>
<t tx="ekr.20220525082935.1549">def __eq__(self, other: object) -&gt; bool:
    if isinstance(other, RawExpressionType):
        return (self.base_type_name == other.base_type_name
                and self.literal_value == other.literal_value)
    else:
        return NotImplemented


</t>
<t tx="ekr.20220525082935.155">def visit_tuple_expr(self, expr: TupleExpr) -&gt; None:
    for item in expr.items:
        if isinstance(item, StarExpr):
            item.valid = True
        item.accept(self)

</t>
<t tx="ekr.20220525082935.1550">class LiteralType(ProperType):
    """The type of a Literal instance. Literal[Value]

    A Literal always consists of:

    1. A native Python object corresponding to the contained inner value
    2. A fallback for this Literal. The fallback also corresponds to the
       parent type this Literal subtypes.

    For example, 'Literal[42]' is represented as
    'LiteralType(value=42, fallback=instance_of_int)'

    As another example, `Literal[Color.RED]` (where Color is an enum) is
    represented as `LiteralType(value="RED", fallback=instance_of_color)'.
    """
    __slots__ = ('value', 'fallback', '_hash')

    @others
</t>
<t tx="ekr.20220525082935.1551">def __init__(self, value: LiteralValue, fallback: Instance,
             line: int = -1, column: int = -1) -&gt; None:
    self.value = value
    super().__init__(line, column)
    self.fallback = fallback
    self._hash = -1  # Cached hash value

</t>
<t tx="ekr.20220525082935.1552">def can_be_false_default(self) -&gt; bool:
    return not self.value

</t>
<t tx="ekr.20220525082935.1553">def can_be_true_default(self) -&gt; bool:
    return bool(self.value)

</t>
<t tx="ekr.20220525082935.1554">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_literal_type(self)

</t>
<t tx="ekr.20220525082935.1555">def __hash__(self) -&gt; int:
    if self._hash == -1:
        self._hash = hash((self.value, self.fallback))
    return self._hash

</t>
<t tx="ekr.20220525082935.1556">def __eq__(self, other: object) -&gt; bool:
    if isinstance(other, LiteralType):
        return self.fallback == other.fallback and self.value == other.value
    else:
        return NotImplemented

</t>
<t tx="ekr.20220525082935.1557">def is_enum_literal(self) -&gt; bool:
    return self.fallback.type.is_enum

</t>
<t tx="ekr.20220525082935.1558">def value_repr(self) -&gt; str:
    """Returns the string representation of the underlying type.

    This function is almost equivalent to running `repr(self.value)`,
    except it includes some additional logic to correctly handle cases
    where the value is a string, byte string, a unicode string, or an enum.
    """
    raw = repr(self.value)
    fallback_name = self.fallback.type.fullname

    # If this is backed by an enum,
    if self.is_enum_literal():
        return f'{fallback_name}.{self.value}'

    if fallback_name == 'builtins.bytes':
        # Note: 'builtins.bytes' only appears in Python 3, so we want to
        # explicitly prefix with a "b"
        return 'b' + raw
    elif fallback_name == 'builtins.unicode':
        # Similarly, 'builtins.unicode' only appears in Python 2, where we also
        # want to explicitly prefix
        return 'u' + raw
    else:
        # 'builtins.str' could mean either depending on context, but either way
        # we don't prefix: it's the "native" string. And of course, if value is
        # some other type, we just return that string repr directly.
        return raw

</t>
<t tx="ekr.20220525082935.1559">def serialize(self) -&gt; Union[JsonDict, str]:
    return {
        '.class': 'LiteralType',
        'value': self.value,
        'fallback': self.fallback.serialize(),
    }

</t>
<t tx="ekr.20220525082935.156">def visit_list_expr(self, expr: ListExpr) -&gt; None:
    for item in expr.items:
        if isinstance(item, StarExpr):
            item.valid = True
        item.accept(self)

</t>
<t tx="ekr.20220525082935.1560">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'LiteralType':
    assert data['.class'] == 'LiteralType'
    return LiteralType(
        value=data['value'],
        fallback=Instance.deserialize(data['fallback']),
    )


</t>
<t tx="ekr.20220525082935.1561">class StarType(ProperType):
    """The star type *type_parameter.

    This is not a real type but a syntactic AST construct.
    """

    __slots__ = ('type',)

    type: Type

    @others
</t>
<t tx="ekr.20220525082935.1562">def __init__(self, type: Type, line: int = -1, column: int = -1) -&gt; None:
    super().__init__(line, column)
    self.type = type

</t>
<t tx="ekr.20220525082935.1563">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    assert isinstance(visitor, SyntheticTypeVisitor)
    return visitor.visit_star_type(self)

</t>
<t tx="ekr.20220525082935.1564">def serialize(self) -&gt; JsonDict:
    assert False, "Synthetic types don't serialize"


</t>
<t tx="ekr.20220525082935.1565">class UnionType(ProperType):
    """The union type Union[T1, ..., Tn] (at least one type argument)."""

    __slots__ = ('items', 'is_evaluated', 'uses_pep604_syntax')

    @others
</t>
<t tx="ekr.20220525082935.1566">def __init__(self, items: Sequence[Type], line: int = -1, column: int = -1,
             is_evaluated: bool = True, uses_pep604_syntax: bool = False) -&gt; None:
    super().__init__(line, column)
    self.items = flatten_nested_unions(items)
    self.can_be_true = any(item.can_be_true for item in items)
    self.can_be_false = any(item.can_be_false for item in items)
    # is_evaluated should be set to false for type comments and string literals
    self.is_evaluated = is_evaluated
    # uses_pep604_syntax is True if Union uses OR syntax (X | Y)
    self.uses_pep604_syntax = uses_pep604_syntax

</t>
<t tx="ekr.20220525082935.1567">def __hash__(self) -&gt; int:
    return hash(frozenset(self.items))

</t>
<t tx="ekr.20220525082935.1568">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, UnionType):
        return NotImplemented
    return frozenset(self.items) == frozenset(other.items)

</t>
<t tx="ekr.20220525082935.1569">@overload
@staticmethod
def make_union(items: Sequence[ProperType],
               line: int = -1, column: int = -1) -&gt; ProperType: ...

</t>
<t tx="ekr.20220525082935.157">def visit_set_expr(self, expr: SetExpr) -&gt; None:
    for item in expr.items:
        if isinstance(item, StarExpr):
            item.valid = True
        item.accept(self)

</t>
<t tx="ekr.20220525082935.1570">@overload
@staticmethod
def make_union(items: Sequence[Type], line: int = -1, column: int = -1) -&gt; Type: ...

</t>
<t tx="ekr.20220525082935.1571">@staticmethod
def make_union(items: Sequence[Type], line: int = -1, column: int = -1) -&gt; Type:
    if len(items) &gt; 1:
        return UnionType(items, line, column)
    elif len(items) == 1:
        return items[0]
    else:
        return UninhabitedType()

</t>
<t tx="ekr.20220525082935.1572">def length(self) -&gt; int:
    return len(self.items)

</t>
<t tx="ekr.20220525082935.1573">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_union_type(self)

</t>
<t tx="ekr.20220525082935.1574">def has_readable_member(self, name: str) -&gt; bool:
    """For a tree of unions of instances, check whether all instances have a given member.

    TODO: Deal with attributes of TupleType etc.
    TODO: This should probably be refactored to go elsewhere.
    """
    return all((isinstance(x, UnionType) and x.has_readable_member(name)) or
               (isinstance(x, Instance) and x.type.has_readable_member(name))
               for x in get_proper_types(self.relevant_items()))

</t>
<t tx="ekr.20220525082935.1575">def relevant_items(self) -&gt; List[Type]:
    """Removes NoneTypes from Unions when strict Optional checking is off."""
    if state.strict_optional:
        return self.items
    else:
        return [i for i in get_proper_types(self.items) if not isinstance(i, NoneType)]

</t>
<t tx="ekr.20220525082935.1576">def serialize(self) -&gt; JsonDict:
    return {'.class': 'UnionType',
            'items': [t.serialize() for t in self.items],
            }

</t>
<t tx="ekr.20220525082935.1577">@classmethod
def deserialize(cls, data: JsonDict) -&gt; 'UnionType':
    assert data['.class'] == 'UnionType'
    return UnionType([deserialize_type(t) for t in data['items']])


</t>
<t tx="ekr.20220525082935.1578">class PartialType(ProperType):
    """Type such as List[?] where type arguments are unknown, or partial None type.

    These are used for inferring types in multiphase initialization such as this:

      x = []       # x gets a partial type List[?], as item type is unknown
      x.append(1)  # partial type gets replaced with normal type List[int]

    Or with None:

      x = None  # x gets a partial type None
      if c:
          x = 1  # Infer actual type int for x
    """

    __slots__ = ('type', 'var', 'value_type')

    # None for the 'None' partial type; otherwise a generic class
    type: Optional[mypy.nodes.TypeInfo]
    var: mypy.nodes.Var
    # For partial defaultdict[K, V], the type V (K is unknown). If V is generic,
    # the type argument is Any and will be replaced later.
    value_type: Optional[Instance]

    @others
</t>
<t tx="ekr.20220525082935.1579">def __init__(self,
             type: 'Optional[mypy.nodes.TypeInfo]',
             var: 'mypy.nodes.Var',
             value_type: 'Optional[Instance]' = None) -&gt; None:
    super().__init__()
    self.type = type
    self.var = var
    self.value_type = value_type

</t>
<t tx="ekr.20220525082935.158">def visit_dict_expr(self, expr: DictExpr) -&gt; None:
    for key, value in expr.items:
        if key is not None:
            key.accept(self)
        value.accept(self)

</t>
<t tx="ekr.20220525082935.1580">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_partial_type(self)


</t>
<t tx="ekr.20220525082935.1581">class EllipsisType(ProperType):
    """The type ... (ellipsis).

    This is not a real type but a syntactic AST construct, used in Callable[..., T], for example.

    A semantically analyzed type will never have ellipsis types.
    """

    __slots__ = ()

    @others
</t>
<t tx="ekr.20220525082935.1582">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    assert isinstance(visitor, SyntheticTypeVisitor)
    return visitor.visit_ellipsis_type(self)

</t>
<t tx="ekr.20220525082935.1583">def serialize(self) -&gt; JsonDict:
    assert False, "Synthetic types don't serialize"


</t>
<t tx="ekr.20220525082935.1584">class TypeType(ProperType):
    """For types like Type[User].

    This annotates variables that are class objects, constrained by
    the type argument.  See PEP 484 for more details.

    We may encounter expressions whose values are specific classes;
    those are represented as callables (possibly overloaded)
    corresponding to the class's constructor's signature and returning
    an instance of that class.  The difference with Type[C] is that
    those callables always represent the exact class given as the
    return type; Type[C] represents any class that's a subclass of C,
    and C may also be a type variable or a union (or Any).

    Many questions around subtype relationships between Type[C1] and
    def(...) -&gt; C2 are answered by looking at the subtype
    relationships between C1 and C2, since Type[] is considered
    covariant.

    There's an unsolved problem with constructor signatures (also
    unsolved in PEP 484): calling a variable whose type is Type[C]
    assumes the constructor signature for C, even though a subclass of
    C might completely change the constructor signature.  For now we
    just assume that users of Type[C] are careful not to do that (in
    the future we might detect when they are violating that
    assumption).
    """

    __slots__ = ('item',)

    # This can't be everything, but it can be a class reference,
    # a generic class instance, a union, Any, a type variable...
    item: ProperType

    @others
</t>
<t tx="ekr.20220525082935.1585">def __init__(self, item: Bogus[Union[Instance, AnyType, TypeVarType, TupleType, NoneType,
                                     CallableType]], *,
             line: int = -1, column: int = -1) -&gt; None:
    """To ensure Type[Union[A, B]] is always represented as Union[Type[A], Type[B]], item of
    type UnionType must be handled through make_normalized static method.
    """
    super().__init__(line, column)
    self.item = item

</t>
<t tx="ekr.20220525082935.1586">@staticmethod
def make_normalized(item: Type, *, line: int = -1, column: int = -1) -&gt; ProperType:
    item = get_proper_type(item)
    if isinstance(item, UnionType):
        return UnionType.make_union(
            [TypeType.make_normalized(union_item) for union_item in item.items],
            line=line, column=column
        )
    return TypeType(item, line=line, column=column)  # type: ignore[arg-type]

</t>
<t tx="ekr.20220525082935.1587">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    return visitor.visit_type_type(self)

</t>
<t tx="ekr.20220525082935.1588">def __hash__(self) -&gt; int:
    return hash(self.item)

</t>
<t tx="ekr.20220525082935.1589">def __eq__(self, other: object) -&gt; bool:
    if not isinstance(other, TypeType):
        return NotImplemented
    return self.item == other.item

</t>
<t tx="ekr.20220525082935.159">def visit_star_expr(self, expr: StarExpr) -&gt; None:
    if not expr.valid:
        # XXX TODO Change this error message
        self.fail('Can use starred expression only as assignment target', expr)
    else:
        expr.expr.accept(self)

</t>
<t tx="ekr.20220525082935.1590">def serialize(self) -&gt; JsonDict:
    return {'.class': 'TypeType', 'item': self.item.serialize()}

</t>
<t tx="ekr.20220525082935.1591">@classmethod
def deserialize(cls, data: JsonDict) -&gt; Type:
    assert data['.class'] == 'TypeType'
    return TypeType.make_normalized(deserialize_type(data['item']))


</t>
<t tx="ekr.20220525082935.1592">class PlaceholderType(ProperType):
    """Temporary, yet-unknown type during semantic analysis.

    This is needed when there's a reference to a type before the real symbol
    table entry of the target type is available (specifically, we use a
    temporary PlaceholderNode symbol node). Consider this example:

      class str(Sequence[str]): ...

    We use a PlaceholderType for the 'str' in 'Sequence[str]' since we can't create
    a TypeInfo for 'str' until all base classes have been resolved. We'll soon
    perform another analysis iteration which replaces the base class with a complete
    type without any placeholders. After semantic analysis, no placeholder types must
    exist.
    """

    __slots__ = ('fullname', 'args')

    @others
</t>
<t tx="ekr.20220525082935.1593">def __init__(self, fullname: Optional[str], args: List[Type], line: int) -&gt; None:
    super().__init__(line)
    self.fullname = fullname  # Must be a valid full name of an actual node (or None).
    self.args = args

</t>
<t tx="ekr.20220525082935.1594">def accept(self, visitor: 'TypeVisitor[T]') -&gt; T:
    assert isinstance(visitor, SyntheticTypeVisitor)
    return visitor.visit_placeholder_type(self)

</t>
<t tx="ekr.20220525082935.1595">def serialize(self) -&gt; str:
    # We should never get here since all placeholders should be replaced
    # during semantic analysis.
    assert False, f"Internal error: unresolved placeholder type {self.fullname}"


</t>
<t tx="ekr.20220525082935.1596">@overload
def get_proper_type(typ: None) -&gt; None: ...
</t>
<t tx="ekr.20220525082935.1597">@overload
def get_proper_type(typ: Type) -&gt; ProperType: ...


</t>
<t tx="ekr.20220525082935.1598">def get_proper_type(typ: Optional[Type]) -&gt; Optional[ProperType]:
    """Get the expansion of a type alias type.

    If the type is already a proper type, this is a no-op. Use this function
    wherever a decision is made on a call like e.g. 'if isinstance(typ, UnionType): ...',
    because 'typ' in this case may be an alias to union. Note: if after making the decision
    on the isinstance() call you pass on the original type (and not one of its components)
    it is recommended to *always* pass on the unexpanded alias.
    """
    if typ is None:
        return None
    if isinstance(typ, TypeGuardedType):  # type: ignore[misc]
        typ = typ.type_guard
    while isinstance(typ, TypeAliasType):
        typ = typ._expand_once()
    assert isinstance(typ, ProperType), typ
    # TODO: store the name of original type alias on this type, so we can show it in errors.
    return typ


</t>
<t tx="ekr.20220525082935.1599">@overload
def get_proper_types(it: Iterable[Type]) -&gt; List[ProperType]: ...  # type: ignore[misc]
</t>
<t tx="ekr.20220525082935.16">def create_alias(self, tree: MypyFile, target_name: str, alias: str, name: str) -&gt; None:
    tag = self.track_incomplete_refs()
    n = self.lookup_fully_qualified_or_none(target_name)
    if n:
        if isinstance(n.node, PlaceholderNode):
            self.mark_incomplete(name, tree)
        else:
            # Found built-in class target. Create alias.
            target = self.named_type_or_none(target_name, [])
            assert target is not None
            # Transform List to List[Any], etc.
            fix_instance_types(target, self.fail, self.note, self.options.python_version)
            alias_node = TypeAlias(target, alias,
                                   line=-1, column=-1,  # there is no context
                                   no_args=True, normalized=True)
            self.add_symbol(name, alias_node, tree)
    elif self.found_incomplete_ref(tag):
        # Built-in class target may not ready yet -- defer.
        self.mark_incomplete(name, tree)
    else:
        # Test fixtures may be missing some builtin classes, which is okay.
        # Kill the placeholder if there is one.
        if name in tree.names:
            assert isinstance(tree.names[name].node, PlaceholderNode)
            del tree.names[name]

</t>
<t tx="ekr.20220525082935.160">def visit_yield_from_expr(self, e: YieldFromExpr) -&gt; None:
    if not self.is_func_scope():
        self.fail('"yield from" outside function', e, serious=True, blocker=True)
    elif self.is_comprehension_stack[-1]:
        self.fail('"yield from" inside comprehension or generator expression',
                  e, serious=True, blocker=True)
    elif self.function_stack[-1].is_coroutine:
        self.fail('"yield from" in async function', e, serious=True, blocker=True)
    else:
        self.function_stack[-1].is_generator = True
    if e.expr:
        e.expr.accept(self)

</t>
<t tx="ekr.20220525082935.1600">@overload
def get_proper_types(it: Iterable[Optional[Type]]) -&gt; List[Optional[ProperType]]: ...


</t>
<t tx="ekr.20220525082935.1601">def get_proper_types(it: Iterable[Optional[Type]]
                     ) -&gt; Union[List[ProperType], List[Optional[ProperType]]]:
    return [get_proper_type(t) for t in it]


</t>
<t tx="ekr.20220525082935.1602"># We split off the type visitor base classes to another module
# to make it easier to gradually get modules working with mypyc.
# Import them here, after the types are defined.
# This is intended as a re-export also.
from mypy.type_visitor import (  # noqa
    TypeVisitor as TypeVisitor,
    SyntheticTypeVisitor as SyntheticTypeVisitor,
    TypeTranslator as TypeTranslator,
    TypeQuery as TypeQuery,
)


</t>
<t tx="ekr.20220525082935.1603">class TypeStrVisitor(SyntheticTypeVisitor[str]):
    """Visitor for pretty-printing types into strings.

    This is mostly for debugging/testing.

    Do not preserve original formatting.

    Notes:
     - Represent unbound types as Foo? or Foo?[...].
     - Represent the NoneType type as None.
    """

    @others
</t>
<t tx="ekr.20220525082935.1604">def __init__(self, id_mapper: Optional[IdMapper] = None) -&gt; None:
    self.id_mapper = id_mapper
    self.any_as_dots = False

</t>
<t tx="ekr.20220525082935.1605">def visit_unbound_type(self, t: UnboundType) -&gt; str:
    s = t.name + '?'
    if t.args:
        s += f'[{self.list_str(t.args)}]'
    return s

</t>
<t tx="ekr.20220525082935.1606">def visit_type_list(self, t: TypeList) -&gt; str:
    return f'&lt;TypeList {self.list_str(t.items)}&gt;'

</t>
<t tx="ekr.20220525082935.1607">def visit_callable_argument(self, t: CallableArgument) -&gt; str:
    typ = t.typ.accept(self)
    if t.name is None:
        return f"{t.constructor}({typ})"
    else:
        return f"{t.constructor}({typ}, {t.name})"

</t>
<t tx="ekr.20220525082935.1608">def visit_any(self, t: AnyType) -&gt; str:
    if self.any_as_dots and t.type_of_any == TypeOfAny.special_form:
        return '...'
    return 'Any'

</t>
<t tx="ekr.20220525082935.1609">def visit_none_type(self, t: NoneType) -&gt; str:
    return "None"

</t>
<t tx="ekr.20220525082935.161">def visit_call_expr(self, expr: CallExpr) -&gt; None:
    """Analyze a call expression.

    Some call expressions are recognized as special forms, including
    cast(...).
    """
    expr.callee.accept(self)
    if refers_to_fullname(expr.callee, 'typing.cast'):
        # Special form cast(...).
        if not self.check_fixed_args(expr, 2, 'cast'):
            return
        # Translate first argument to an unanalyzed type.
        try:
            target = self.expr_to_unanalyzed_type(expr.args[0])
        except TypeTranslationError:
            self.fail('Cast target is not a type', expr)
            return
        # Piggyback CastExpr object to the CallExpr object; it takes
        # precedence over the CallExpr semantics.
        expr.analyzed = CastExpr(expr.args[1], target)
        expr.analyzed.line = expr.line
        expr.analyzed.column = expr.column
        expr.analyzed.accept(self)
    elif refers_to_fullname(expr.callee, ASSERT_TYPE_NAMES):
        if not self.check_fixed_args(expr, 2, 'assert_type'):
            return
        # Translate second argument to an unanalyzed type.
        try:
            target = self.expr_to_unanalyzed_type(expr.args[1])
        except TypeTranslationError:
            self.fail('assert_type() type is not a type', expr)
            return
        expr.analyzed = AssertTypeExpr(expr.args[0], target)
        expr.analyzed.line = expr.line
        expr.analyzed.column = expr.column
        expr.analyzed.accept(self)
    elif refers_to_fullname(expr.callee, REVEAL_TYPE_NAMES):
        if not self.check_fixed_args(expr, 1, 'reveal_type'):
            return
        expr.analyzed = RevealExpr(kind=REVEAL_TYPE, expr=expr.args[0])
        expr.analyzed.line = expr.line
        expr.analyzed.column = expr.column
        expr.analyzed.accept(self)
    elif refers_to_fullname(expr.callee, 'builtins.reveal_locals'):
        # Store the local variable names into the RevealExpr for use in the
        # type checking pass
        local_nodes: List[Var] = []
        if self.is_module_scope():
            # try to determine just the variable declarations in module scope
            # self.globals.values() contains SymbolTableNode's
            # Each SymbolTableNode has an attribute node that is nodes.Var
            # look for variable nodes that marked as is_inferred
            # Each symboltable node has a Var node as .node
            local_nodes = [n.node
                           for name, n in self.globals.items()
                           if getattr(n.node, 'is_inferred', False)
                           and isinstance(n.node, Var)]
        elif self.is_class_scope():
            # type = None  # type: Optional[TypeInfo]
            if self.type is not None:
                local_nodes = [st.node
                               for st in self.type.names.values()
                               if isinstance(st.node, Var)]
        elif self.is_func_scope():
            # locals = None  # type: List[Optional[SymbolTable]]
            if self.locals is not None:
                symbol_table = self.locals[-1]
                if symbol_table is not None:
                    local_nodes = [st.node
                                   for st in symbol_table.values()
                                   if isinstance(st.node, Var)]
        expr.analyzed = RevealExpr(kind=REVEAL_LOCALS, local_nodes=local_nodes)
        expr.analyzed.line = expr.line
        expr.analyzed.column = expr.column
        expr.analyzed.accept(self)
    elif refers_to_fullname(expr.callee, 'typing.Any'):
        # Special form Any(...) no longer supported.
        self.fail('Any(...) is no longer supported. Use cast(Any, ...) instead', expr)
    elif refers_to_fullname(expr.callee, 'typing._promote'):
        # Special form _promote(...).
        if not self.check_fixed_args(expr, 1, '_promote'):
            return
        # Translate first argument to an unanalyzed type.
        try:
            target = self.expr_to_unanalyzed_type(expr.args[0])
        except TypeTranslationError:
            self.fail('Argument 1 to _promote is not a type', expr)
            return
        expr.analyzed = PromoteExpr(target)
        expr.analyzed.line = expr.line
        expr.analyzed.accept(self)
    elif refers_to_fullname(expr.callee, 'builtins.dict'):
        expr.analyzed = self.translate_dict_call(expr)
    elif refers_to_fullname(expr.callee, 'builtins.divmod'):
        if not self.check_fixed_args(expr, 2, 'divmod'):
            return
        expr.analyzed = OpExpr('divmod', expr.args[0], expr.args[1])
        expr.analyzed.line = expr.line
        expr.analyzed.accept(self)
    else:
        # Normal call expression.
        for a in expr.args:
            a.accept(self)

        if (isinstance(expr.callee, MemberExpr) and
                isinstance(expr.callee.expr, NameExpr) and
                expr.callee.expr.name == '__all__' and
                expr.callee.expr.kind == GDEF and
                expr.callee.name in ('append', 'extend')):
            if expr.callee.name == 'append' and expr.args:
                self.add_exports(expr.args[0])
            elif (expr.callee.name == 'extend' and expr.args and
                    isinstance(expr.args[0], (ListExpr, TupleExpr))):
                self.add_exports(expr.args[0].items)

</t>
<t tx="ekr.20220525082935.1610">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; str:
    return "&lt;nothing&gt;"

</t>
<t tx="ekr.20220525082935.1611">def visit_erased_type(self, t: ErasedType) -&gt; str:
    return "&lt;Erased&gt;"

</t>
<t tx="ekr.20220525082935.1612">def visit_deleted_type(self, t: DeletedType) -&gt; str:
    if t.source is None:
        return "&lt;Deleted&gt;"
    else:
        return f"&lt;Deleted '{t.source}'&gt;"

</t>
<t tx="ekr.20220525082935.1613">def visit_instance(self, t: Instance) -&gt; str:
    if t.last_known_value and not t.args:
        # Instances with a literal fallback should never be generic. If they are,
        # something went wrong so we fall back to showing the full Instance repr.
        s = f'{t.last_known_value}?'
    else:
        s = t.type.fullname or t.type.name or '&lt;???&gt;'

    if t.args:
        if t.type.fullname == 'builtins.tuple':
            assert len(t.args) == 1
            s += f'[{self.list_str(t.args)}, ...]'
        else:
            s += f'[{self.list_str(t.args)}]'
    if self.id_mapper:
        s += f'&lt;{self.id_mapper.id(t.type)}&gt;'
    return s

</t>
<t tx="ekr.20220525082935.1614">def visit_type_var(self, t: TypeVarType) -&gt; str:
    if t.name is None:
        # Anonymous type variable type (only numeric id).
        s = f'`{t.id}'
    else:
        # Named type variable type.
        s = f'{t.name}`{t.id}'
    if self.id_mapper and t.upper_bound:
        s += f'(upper_bound={t.upper_bound.accept(self)})'
    return s

</t>
<t tx="ekr.20220525082935.1615">def visit_param_spec(self, t: ParamSpecType) -&gt; str:
    # prefixes are displayed as Concatenate
    s = ''
    if t.prefix.arg_types:
        s += f'[{self.list_str(t.prefix.arg_types)}, **'
    if t.name is None:
        # Anonymous type variable type (only numeric id).
        s += f'`{t.id}'
    else:
        # Named type variable type.
        s += f'{t.name_with_suffix()}`{t.id}'
    if t.prefix.arg_types:
        s += ']'
    return s

</t>
<t tx="ekr.20220525082935.1616">def visit_parameters(self, t: Parameters) -&gt; str:
    # This is copied from visit_callable -- is there a way to decrease duplication?
    if t.is_ellipsis_args:
        return '...'

    s = ''
    bare_asterisk = False
    for i in range(len(t.arg_types)):
        if s != '':
            s += ', '
        if t.arg_kinds[i].is_named() and not bare_asterisk:
            s += '*, '
            bare_asterisk = True
        if t.arg_kinds[i] == ARG_STAR:
            s += '*'
        if t.arg_kinds[i] == ARG_STAR2:
            s += '**'
        name = t.arg_names[i]
        if name:
            s += f'{name}: '
        r = t.arg_types[i].accept(self)

        s += r

        if t.arg_kinds[i].is_optional():
            s += ' ='

    return f'[{s}]'

</t>
<t tx="ekr.20220525082935.1617">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; str:
    if t.name is None:
        # Anonymous type variable type (only numeric id).
        s = f'`{t.id}'
    else:
        # Named type variable type.
        s = f'{t.name}`{t.id}'
    return s

</t>
<t tx="ekr.20220525082935.1618">def visit_callable_type(self, t: CallableType) -&gt; str:
    param_spec = t.param_spec()
    if param_spec is not None:
        num_skip = 2
    else:
        num_skip = 0

    s = ''
    bare_asterisk = False
    for i in range(len(t.arg_types) - num_skip):
        if s != '':
            s += ', '
        if t.arg_kinds[i].is_named() and not bare_asterisk:
            s += '*, '
            bare_asterisk = True
        if t.arg_kinds[i] == ARG_STAR:
            s += '*'
        if t.arg_kinds[i] == ARG_STAR2:
            s += '**'
        name = t.arg_names[i]
        if name:
            s += name + ': '
        s += t.arg_types[i].accept(self)
        if t.arg_kinds[i].is_optional():
            s += ' ='

    if param_spec is not None:
        n = param_spec.name
        if s:
            s += ', '
        s += f'*{n}.args, **{n}.kwargs'

    s = f'({s})'

    if not isinstance(get_proper_type(t.ret_type), NoneType):
        if t.type_guard is not None:
            s += f' -&gt; TypeGuard[{t.type_guard.accept(self)}]'
        else:
            s += f' -&gt; {t.ret_type.accept(self)}'

    if t.variables:
        vs = []
        for var in t.variables:
            if isinstance(var, TypeVarType):
                # We reimplement TypeVarType.__repr__ here in order to support id_mapper.
                if var.values:
                    vals = f"({', '.join(val.accept(self) for val in var.values)})"
                    vs.append(f'{var.name} in {vals}')
                elif not is_named_instance(var.upper_bound, 'builtins.object'):
                    vs.append(f'{var.name} &lt;: {var.upper_bound.accept(self)}')
                else:
                    vs.append(var.name)
            else:
                # For other TypeVarLikeTypes, just use the name
                vs.append(var.name)
        s = f"[{', '.join(vs)}] {s}"

    return f'def {s}'

</t>
<t tx="ekr.20220525082935.1619">def visit_overloaded(self, t: Overloaded) -&gt; str:
    a = []
    for i in t.items:
        a.append(i.accept(self))
    return f"Overload({', '.join(a)})"

</t>
<t tx="ekr.20220525082935.162">def translate_dict_call(self, call: CallExpr) -&gt; Optional[DictExpr]:
    """Translate 'dict(x=y, ...)' to {'x': y, ...} and 'dict()' to {}.

    For other variants of dict(...), return None.
    """
    if not all(kind == ARG_NAMED for kind in call.arg_kinds):
        # Must still accept those args.
        for a in call.args:
            a.accept(self)
        return None
    expr = DictExpr([(StrExpr(cast(str, key)), value)  # since they are all ARG_NAMED
                     for key, value in zip(call.arg_names, call.args)])
    expr.set_line(call)
    expr.accept(self)
    return expr

</t>
<t tx="ekr.20220525082935.1620">def visit_tuple_type(self, t: TupleType) -&gt; str:
    s = self.list_str(t.items)
    if t.partial_fallback and t.partial_fallback.type:
        fallback_name = t.partial_fallback.type.fullname
        if fallback_name != 'builtins.tuple':
            return f'Tuple[{s}, fallback={t.partial_fallback.accept(self)}]'
    return f'Tuple[{s}]'

</t>
<t tx="ekr.20220525082935.1621">def visit_typeddict_type(self, t: TypedDictType) -&gt; str:
    def item_str(name: str, typ: str) -&gt; str:
        if name in t.required_keys:
            return f'{name!r}: {typ}'
        else:
            return f'{name!r}?: {typ}'

    s = '{' + ', '.join(item_str(name, typ.accept(self))
                        for name, typ in t.items.items()) + '}'
    prefix = ''
    if t.fallback and t.fallback.type:
        if t.fallback.type.fullname not in TPDICT_FB_NAMES:
            prefix = repr(t.fallback.type.fullname) + ', '
    return f'TypedDict({prefix}{s})'

</t>
<t tx="ekr.20220525082935.1622">def visit_raw_expression_type(self, t: RawExpressionType) -&gt; str:
    return repr(t.literal_value)

</t>
<t tx="ekr.20220525082935.1623">def visit_literal_type(self, t: LiteralType) -&gt; str:
    return f'Literal[{t.value_repr()}]'

</t>
<t tx="ekr.20220525082935.1624">def visit_star_type(self, t: StarType) -&gt; str:
    s = t.type.accept(self)
    return f'*{s}'

</t>
<t tx="ekr.20220525082935.1625">def visit_union_type(self, t: UnionType) -&gt; str:
    s = self.list_str(t.items)
    return f'Union[{s}]'

</t>
<t tx="ekr.20220525082935.1626">def visit_partial_type(self, t: PartialType) -&gt; str:
    if t.type is None:
        return '&lt;partial None&gt;'
    else:
        return '&lt;partial {}[{}]&gt;'.format(t.type.name,
                                         ', '.join(['?'] * len(t.type.type_vars)))

</t>
<t tx="ekr.20220525082935.1627">def visit_ellipsis_type(self, t: EllipsisType) -&gt; str:
    return '...'

</t>
<t tx="ekr.20220525082935.1628">def visit_type_type(self, t: TypeType) -&gt; str:
    return f'Type[{t.item.accept(self)}]'

</t>
<t tx="ekr.20220525082935.1629">def visit_placeholder_type(self, t: PlaceholderType) -&gt; str:
    return f'&lt;placeholder {t.fullname}&gt;'

</t>
<t tx="ekr.20220525082935.163">def check_fixed_args(self, expr: CallExpr, numargs: int,
                     name: str) -&gt; bool:
    """Verify that expr has specified number of positional args.

    Return True if the arguments are valid.
    """
    s = 's'
    if numargs == 1:
        s = ''
    if len(expr.args) != numargs:
        self.fail('"%s" expects %d argument%s' % (name, numargs, s),
                  expr)
        return False
    if expr.arg_kinds != [ARG_POS] * numargs:
        self.fail('"%s" must be called with %s positional argument%s' %
                  (name, numargs, s), expr)
        return False
    return True

</t>
<t tx="ekr.20220525082935.1630">def visit_type_alias_type(self, t: TypeAliasType) -&gt; str:
    if t.alias is not None:
        unrolled, recursed = t._partial_expansion()
        self.any_as_dots = recursed
        type_str = unrolled.accept(self)
        self.any_as_dots = False
        return type_str
    return '&lt;alias (unfixed)&gt;'

</t>
<t tx="ekr.20220525082935.1631">def visit_unpack_type(self, t: UnpackType) -&gt; str:
    return f'Unpack[{t.type.accept(self)}]'

</t>
<t tx="ekr.20220525082935.1632">def list_str(self, a: Iterable[Type]) -&gt; str:
    """Convert items of an array to strings (pretty-print types)
    and join the results with commas.
    """
    res = []
    for t in a:
        res.append(t.accept(self))
    return ', '.join(res)


</t>
<t tx="ekr.20220525082935.1633">class UnrollAliasVisitor(TypeTranslator):
    @others
</t>
<t tx="ekr.20220525082935.1634">def __init__(self, initial_aliases: Set[TypeAliasType]) -&gt; None:
    self.recursed = False
    self.initial_aliases = initial_aliases

</t>
<t tx="ekr.20220525082935.1635">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    if t in self.initial_aliases:
        self.recursed = True
        return AnyType(TypeOfAny.special_form)
    # Create a new visitor on encountering a new type alias, so that an alias like
    #     A = Tuple[B, B]
    #     B = int
    # will not be detected as recursive on the second encounter of B.
    subvisitor = UnrollAliasVisitor(self.initial_aliases | {t})
    result = get_proper_type(t).accept(subvisitor)
    if subvisitor.recursed:
        self.recursed = True
    return result


</t>
<t tx="ekr.20220525082935.1636">def strip_type(typ: Type) -&gt; ProperType:
    """Make a copy of type without 'debugging info' (function name)."""
    typ = get_proper_type(typ)
    if isinstance(typ, CallableType):
        return typ.copy_modified(name=None)
    elif isinstance(typ, Overloaded):
        return Overloaded([cast(CallableType, strip_type(item))
                           for item in typ.items])
    else:
        return typ


</t>
<t tx="ekr.20220525082935.1637">def is_named_instance(t: Type, fullnames: Union[str, Tuple[str, ...]]) -&gt; bool:
    if not isinstance(fullnames, tuple):
        fullnames = (fullnames,)

    t = get_proper_type(t)
    return isinstance(t, Instance) and t.type.fullname in fullnames


</t>
<t tx="ekr.20220525082935.1638">class InstantiateAliasVisitor(TypeTranslator):
    @others
</t>
<t tx="ekr.20220525082935.1639">def __init__(self, vars: List[str], subs: List[Type]) -&gt; None:
    self.replacements = {v: s for (v, s) in zip(vars, subs)}

</t>
<t tx="ekr.20220525082935.164">def visit_member_expr(self, expr: MemberExpr) -&gt; None:
    base = expr.expr
    base.accept(self)
    if isinstance(base, RefExpr) and isinstance(base.node, MypyFile):
        # Handle module attribute.
        sym = self.get_module_symbol(base.node, expr.name)
        if sym:
            if isinstance(sym.node, PlaceholderNode):
                self.process_placeholder(expr.name, 'attribute', expr)
                return
            expr.kind = sym.kind
            expr.fullname = sym.fullname
            expr.node = sym.node
    elif isinstance(base, RefExpr):
        # This branch handles the case C.bar (or cls.bar or self.bar inside
        # a classmethod/method), where C is a class and bar is a type
        # definition or a module resulting from `import bar` (or a module
        # assignment) inside class C. We look up bar in the class' TypeInfo
        # namespace.  This is done only when bar is a module or a type;
        # other things (e.g. methods) are handled by other code in
        # checkmember.
        type_info = None
        if isinstance(base.node, TypeInfo):
            # C.bar where C is a class
            type_info = base.node
        elif isinstance(base.node, Var) and self.type and self.function_stack:
            # check for self.bar or cls.bar in method/classmethod
            func_def = self.function_stack[-1]
            if not func_def.is_static and isinstance(func_def.type, CallableType):
                formal_arg = func_def.type.argument_by_name(base.node.name)
                if formal_arg and formal_arg.pos == 0:
                    type_info = self.type
        elif isinstance(base.node, TypeAlias) and base.node.no_args:
            assert isinstance(base.node.target, ProperType)
            if isinstance(base.node.target, Instance):
                type_info = base.node.target.type

        if type_info:
            n = type_info.names.get(expr.name)
            if n is not None and isinstance(n.node, (MypyFile, TypeInfo, TypeAlias)):
                if not n:
                    return
                expr.kind = n.kind
                expr.fullname = n.fullname
                expr.node = n.node

</t>
<t tx="ekr.20220525082935.1640">def visit_type_alias_type(self, typ: TypeAliasType) -&gt; Type:
    return typ.copy_modified(args=[t.accept(self) for t in typ.args])

</t>
<t tx="ekr.20220525082935.1641">def visit_unbound_type(self, typ: UnboundType) -&gt; Type:
    # TODO: stop using unbound type variables for type aliases.
    # Now that type aliases are very similar to TypeInfos we should
    # make type variable tracking similar as well. Maybe we can even support
    # upper bounds etc. for generic type aliases.
    if typ.name in self.replacements:
        return self.replacements[typ.name]
    return typ

</t>
<t tx="ekr.20220525082935.1642">def visit_type_var(self, typ: TypeVarType) -&gt; Type:
    if typ.name in self.replacements:
        return self.replacements[typ.name]
    return typ


</t>
<t tx="ekr.20220525082935.1643">def replace_alias_tvars(tp: Type, vars: List[str], subs: List[Type],
                        newline: int, newcolumn: int) -&gt; Type:
    """Replace type variables in a generic type alias tp with substitutions subs
    resetting context. Length of subs should be already checked.
    """
    replacer = InstantiateAliasVisitor(vars, subs)
    new_tp = tp.accept(replacer)
    new_tp.line = newline
    new_tp.column = newcolumn
    return new_tp


</t>
<t tx="ekr.20220525082935.1644">class HasTypeVars(TypeQuery[bool]):
    def __init__(self) -&gt; None:
        super().__init__(any)

    def visit_type_var(self, t: TypeVarType) -&gt; bool:
        return True


</t>
<t tx="ekr.20220525082935.1645">def has_type_vars(typ: Type) -&gt; bool:
    """Check if a type contains any type variables (recursively)."""
    return typ.accept(HasTypeVars())


</t>
<t tx="ekr.20220525082935.1646">def flatten_nested_unions(types: Iterable[Type],
                          handle_type_alias_type: bool = False) -&gt; List[Type]:
    """Flatten nested unions in a type list."""
    # This and similar functions on unions can cause infinite recursion
    # if passed a "pathological" alias like A = Union[int, A] or similar.
    # TODO: ban such aliases in semantic analyzer.
    flat_items: List[Type] = []
    if handle_type_alias_type:
        types = get_proper_types(types)
    # TODO: avoid duplicate types in unions (e.g. using hash)
    for tp in types:
        if isinstance(tp, ProperType) and isinstance(tp, UnionType):
            flat_items.extend(flatten_nested_unions(tp.items,
                              handle_type_alias_type=handle_type_alias_type))
        else:
            flat_items.append(tp)
    return flat_items


</t>
<t tx="ekr.20220525082935.1647">def union_items(typ: Type) -&gt; List[ProperType]:
    """Return the flattened items of a union type.

    For non-union types, return a list containing just the argument.
    """
    typ = get_proper_type(typ)
    if isinstance(typ, UnionType):
        items = []
        for item in typ.items:
            items.extend(union_items(item))
        return items
    else:
        return [typ]


</t>
<t tx="ekr.20220525082935.1648">def is_union_with_any(tp: Type) -&gt; bool:
    """Is this a union with Any or a plain Any type?"""
    tp = get_proper_type(tp)
    if isinstance(tp, AnyType):
        return True
    if not isinstance(tp, UnionType):
        return False
    return any(is_union_with_any(t) for t in get_proper_types(tp.items))


</t>
<t tx="ekr.20220525082935.1649">def is_generic_instance(tp: Type) -&gt; bool:
    tp = get_proper_type(tp)
    return isinstance(tp, Instance) and bool(tp.args)


</t>
<t tx="ekr.20220525082935.165">def visit_op_expr(self, expr: OpExpr) -&gt; None:
    expr.left.accept(self)

    if expr.op in ('and', 'or'):
        inferred = infer_condition_value(expr.left, self.options)
        if ((inferred in (ALWAYS_FALSE, MYPY_FALSE) and expr.op == 'and') or
                (inferred in (ALWAYS_TRUE, MYPY_TRUE) and expr.op == 'or')):
            expr.right_unreachable = True
            return
        elif ((inferred in (ALWAYS_TRUE, MYPY_TRUE) and expr.op == 'and') or
                (inferred in (ALWAYS_FALSE, MYPY_FALSE) and expr.op == 'or')):
            expr.right_always = True

    expr.right.accept(self)

</t>
<t tx="ekr.20220525082935.1650">def is_optional(t: Type) -&gt; bool:
    t = get_proper_type(t)
    return isinstance(t, UnionType) and any(isinstance(get_proper_type(e), NoneType)
                                            for e in t.items)


</t>
<t tx="ekr.20220525082935.1651">def remove_optional(typ: Type) -&gt; Type:
    typ = get_proper_type(typ)
    if isinstance(typ, UnionType):
        return UnionType.make_union([t for t in typ.items
                                     if not isinstance(get_proper_type(t), NoneType)])
    else:
        return typ


</t>
<t tx="ekr.20220525082935.1652">def is_literal_type(typ: ProperType, fallback_fullname: str, value: LiteralValue) -&gt; bool:
    """Check if this type is a LiteralType with the given fallback type and value."""
    if isinstance(typ, Instance) and typ.last_known_value:
        typ = typ.last_known_value
    if not isinstance(typ, LiteralType):
        return False
    if typ.fallback.type.fullname != fallback_fullname:
        return False
    return typ.value == value


</t>
<t tx="ekr.20220525082935.1653">names: Final = globals().copy()
names.pop('NOT_READY', None)
deserialize_map: Final = {
    key: obj.deserialize
    for key, obj in names.items()
    if isinstance(obj, type) and issubclass(obj, Type) and obj is not Type
}


</t>
<t tx="ekr.20220525082935.1654">def callable_with_ellipsis(any_type: AnyType,
                           ret_type: Type,
                           fallback: Instance) -&gt; CallableType:
    """Construct type Callable[..., ret_type]."""
    return CallableType([any_type, any_type],
                        [ARG_STAR, ARG_STAR2],
                        [None, None],
                        ret_type=ret_type,
                        fallback=fallback,
                        is_ellipsis_args=True)
</t>
<t tx="ekr.20220525082935.1655">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""
A shared state for all TypeInfos that holds global cache and dependency information,
and potentially other mutable TypeInfo state. This module contains mutable global state.
"""

from typing import Dict, Set, Tuple, Optional, List
from typing_extensions import ClassVar, Final, TypeAlias as _TypeAlias

from mypy.nodes import TypeInfo
from mypy.types import Instance, TypeAliasType, get_proper_type, Type
from mypy.server.trigger import make_trigger

# Represents that the 'left' instance is a subtype of the 'right' instance
SubtypeRelationship: _TypeAlias = Tuple[Instance, Instance]

# A tuple encoding the specific conditions under which we performed the subtype check.
# (e.g. did we want a proper subtype? A regular subtype while ignoring variance?)
SubtypeKind: _TypeAlias = Tuple[bool, ...]

# A cache that keeps track of whether the given TypeInfo is a part of a particular
# subtype relationship
SubtypeCache: _TypeAlias = Dict[TypeInfo, Dict[SubtypeKind, Set[SubtypeRelationship]]]


@others
</t>
<t tx="ekr.20220525082935.1656">class TypeState:
    """This class provides subtype caching to improve performance of subtype checks.
    It also holds protocol fine grained dependencies.

    Note: to avoid leaking global state, 'reset_all_subtype_caches()' should be called
    after a build has finished and after a daemon shutdown. This subtype cache only exists for
    performance reasons, resetting subtype caches for a class has no semantic effect.
    The protocol dependencies however are only stored here, and shouldn't be deleted unless
    not needed any more (e.g. during daemon shutdown).
    """
    # '_subtype_caches' keeps track of (subtype, supertype) pairs where supertypes are
    # instances of the given TypeInfo. The cache also keeps track of whether the check
    # was done in strict optional mode and of the specific *kind* of subtyping relationship,
    # which we represent as an arbitrary hashable tuple.
    # We need the caches, since subtype checks for structural types are very slow.
    _subtype_caches: Final[SubtypeCache] = {}

    # This contains protocol dependencies generated after running a full build,
    # or after an update. These dependencies are special because:
    #   * They are a global property of the program; i.e. some dependencies for imported
    #     classes can be generated in the importing modules.
    #   * Because of the above, they are serialized separately, after a full run,
    #     or a full update.
    # `proto_deps` can be None if after deserialization it turns out that they are
    # inconsistent with the other cache files (or an error occurred during deserialization).
    # A blocking error will be generated in this case, since we can't proceed safely.
    # For the description of kinds of protocol dependencies and corresponding examples,
    # see _snapshot_protocol_deps.
    proto_deps: ClassVar[Optional[Dict[str, Set[str]]]] = {}

    # Protocols (full names) a given class attempted to implement.
    # Used to calculate fine grained protocol dependencies and optimize protocol
    # subtype cache invalidation in fine grained mode. For example, if we pass a value
    # of type a.A to a function expecting something compatible with protocol p.P,
    # we'd have 'a.A' -&gt; {'p.P', ...} in the map. This map is flushed after every incremental
    # update.
    _attempted_protocols: Final[Dict[str, Set[str]]] = {}
    # We also snapshot protocol members of the above protocols. For example, if we pass
    # a value of type a.A to a function expecting something compatible with Iterable, we'd have
    # 'a.A' -&gt; {'__iter__', ...} in the map. This map is also flushed after every incremental
    # update. This map is needed to only generate dependencies like &lt;a.A.__iter__&gt; -&gt; &lt;a.A&gt;
    # instead of a wildcard to avoid unnecessarily invalidating classes.
    _checked_against_members: Final[Dict[str, Set[str]]] = {}
    # TypeInfos that appeared as a left type (subtype) in a subtype check since latest
    # dependency snapshot update. This is an optimisation for fine grained mode; during a full
    # run we only take a dependency snapshot at the very end, so this set will contain all
    # subtype-checked TypeInfos. After a fine grained update however, we can gather only new
    # dependencies generated from (typically) few TypeInfos that were subtype-checked
    # (i.e. appeared as r.h.s. in an assignment or an argument in a function call in
    # a re-checked target) during the update.
    _rechecked_types: Final[Set[TypeInfo]] = set()

    # The two attributes below are assumption stacks for subtyping relationships between
    # recursive type aliases. Normally, one would pass type assumptions as an additional
    # arguments to is_subtype(), but this would mean updating dozens of related functions
    # threading this through all callsites (see also comment for TypeInfo.assuming).
    _assuming: Final[List[Tuple[TypeAliasType, TypeAliasType]]] = []
    _assuming_proper: Final[List[Tuple[TypeAliasType, TypeAliasType]]] = []
    # Ditto for inference of generic constraints against recursive type aliases.
    _inferring: Final[List[TypeAliasType]] = []

    # N.B: We do all of the accesses to these properties through
    # TypeState, instead of making these classmethods and accessing
    # via the cls parameter, since mypyc can optimize accesses to
    # Final attributes of a directly referenced type.

    @others
</t>
<t tx="ekr.20220525082935.1657">@staticmethod
def is_assumed_subtype(left: Type, right: Type) -&gt; bool:
    for (l, r) in reversed(TypeState._assuming):
        if (get_proper_type(l) == get_proper_type(left)
                and get_proper_type(r) == get_proper_type(right)):
            return True
    return False

</t>
<t tx="ekr.20220525082935.1658">@staticmethod
def is_assumed_proper_subtype(left: Type, right: Type) -&gt; bool:
    for (l, r) in reversed(TypeState._assuming_proper):
        if (get_proper_type(l) == get_proper_type(left)
                and get_proper_type(r) == get_proper_type(right)):
            return True
    return False

</t>
<t tx="ekr.20220525082935.1659">@staticmethod
def reset_all_subtype_caches() -&gt; None:
    """Completely reset all known subtype caches."""
    TypeState._subtype_caches.clear()

</t>
<t tx="ekr.20220525082935.166">def visit_comparison_expr(self, expr: ComparisonExpr) -&gt; None:
    for operand in expr.operands:
        operand.accept(self)

</t>
<t tx="ekr.20220525082935.1660">@staticmethod
def reset_subtype_caches_for(info: TypeInfo) -&gt; None:
    """Reset subtype caches (if any) for a given supertype TypeInfo."""
    if info in TypeState._subtype_caches:
        TypeState._subtype_caches[info].clear()

</t>
<t tx="ekr.20220525082935.1661">@staticmethod
def reset_all_subtype_caches_for(info: TypeInfo) -&gt; None:
    """Reset subtype caches (if any) for a given supertype TypeInfo and its MRO."""
    for item in info.mro:
        TypeState.reset_subtype_caches_for(item)

</t>
<t tx="ekr.20220525082935.1662">@staticmethod
def is_cached_subtype_check(kind: SubtypeKind, left: Instance, right: Instance) -&gt; bool:
    if left.last_known_value is not None or right.last_known_value is not None:
        # If there is a literal last known value, give up. There
        # will be an unbounded number of potential types to cache,
        # making caching less effective.
        return False
    info = right.type
    cache = TypeState._subtype_caches.get(info)
    if cache is None:
        return False
    subcache = cache.get(kind)
    if subcache is None:
        return False
    return (left, right) in subcache

</t>
<t tx="ekr.20220525082935.1663">@staticmethod
def record_subtype_cache_entry(kind: SubtypeKind,
                               left: Instance, right: Instance) -&gt; None:
    if left.last_known_value is not None or right.last_known_value is not None:
        # These are unlikely to match, due to the large space of
        # possible values.  Avoid uselessly increasing cache sizes.
        return
    cache = TypeState._subtype_caches.setdefault(right.type, dict())
    cache.setdefault(kind, set()).add((left, right))

</t>
<t tx="ekr.20220525082935.1664">@staticmethod
def reset_protocol_deps() -&gt; None:
    """Reset dependencies after a full run or before a daemon shutdown."""
    TypeState.proto_deps = {}
    TypeState._attempted_protocols.clear()
    TypeState._checked_against_members.clear()
    TypeState._rechecked_types.clear()

</t>
<t tx="ekr.20220525082935.1665">@staticmethod
def record_protocol_subtype_check(left_type: TypeInfo, right_type: TypeInfo) -&gt; None:
    assert right_type.is_protocol
    TypeState._rechecked_types.add(left_type)
    TypeState._attempted_protocols.setdefault(
        left_type.fullname, set()).add(right_type.fullname)
    TypeState._checked_against_members.setdefault(
        left_type.fullname,
        set()).update(right_type.protocol_members)

</t>
<t tx="ekr.20220525082935.1666">@staticmethod
def _snapshot_protocol_deps() -&gt; Dict[str, Set[str]]:
    """Collect protocol attribute dependencies found so far from registered subtype checks.

    There are three kinds of protocol dependencies. For example, after a subtype check:

        x: Proto = C()

    the following dependencies will be generated:
        1. ..., &lt;SuperProto[wildcard]&gt;, &lt;Proto[wildcard]&gt; -&gt; &lt;Proto&gt;
        2. ..., &lt;B.attr&gt;, &lt;C.attr&gt; -&gt; &lt;C&gt; [for every attr in Proto members]
        3. &lt;C&gt; -&gt; Proto  # this one to invalidate the subtype cache

    The first kind is generated immediately per-module in deps.py (see also an example there
    for motivation why it is needed). While two other kinds are generated here after all
    modules are type checked and we have recorded all the subtype checks. To understand these
    two kinds, consider a simple example:

        class A:
            def __iter__(self) -&gt; Iterator[int]:
                ...

        it: Iterable[int] = A()

    We add &lt;a.A.__iter__&gt; -&gt; &lt;a.A&gt; to invalidate the assignment (module target in this case),
    whenever the signature of a.A.__iter__ changes. We also add &lt;a.A&gt; -&gt; typing.Iterable,
    to invalidate the subtype caches of the latter. (Note that the same logic applies to
    proper subtype checks, and calculating meets and joins, if this involves calling
    'subtypes.is_protocol_implementation').
    """
    deps: Dict[str, Set[str]] = {}
    for info in TypeState._rechecked_types:
        for attr in TypeState._checked_against_members[info.fullname]:
            # The need for full MRO here is subtle, during an update, base classes of
            # a concrete class may not be reprocessed, so not all &lt;B.x&gt; -&gt; &lt;C.x&gt; deps
            # are added.
            for base_info in info.mro[:-1]:
                trigger = make_trigger(f'{base_info.fullname}.{attr}')
                if 'typing' in trigger or 'builtins' in trigger:
                    # TODO: avoid everything from typeshed
                    continue
                deps.setdefault(trigger, set()).add(make_trigger(info.fullname))
        for proto in TypeState._attempted_protocols[info.fullname]:
            trigger = make_trigger(info.fullname)
            if 'typing' in trigger or 'builtins' in trigger:
                continue
            # If any class that was checked against a protocol changes,
            # we need to reset the subtype cache for the protocol.
            #
            # Note: strictly speaking, the protocol doesn't need to be
            # re-checked, we only need to reset the cache, and its uses
            # elsewhere are still valid (unless invalidated by other deps).
            deps.setdefault(trigger, set()).add(proto)
    return deps

</t>
<t tx="ekr.20220525082935.1667">@staticmethod
def update_protocol_deps(second_map: Optional[Dict[str, Set[str]]] = None) -&gt; None:
    """Update global protocol dependency map.

    We update the global map incrementally, using a snapshot only from recently
    type checked types. If second_map is given, update it as well. This is currently used
    by FineGrainedBuildManager that maintains normal (non-protocol) dependencies.
    """
    assert TypeState.proto_deps is not None, (
        "This should not be called after failed cache load")
    new_deps = TypeState._snapshot_protocol_deps()
    for trigger, targets in new_deps.items():
        TypeState.proto_deps.setdefault(trigger, set()).update(targets)
    if second_map is not None:
        for trigger, targets in new_deps.items():
            second_map.setdefault(trigger, set()).update(targets)
    TypeState._rechecked_types.clear()
    TypeState._attempted_protocols.clear()
    TypeState._checked_against_members.clear()

</t>
<t tx="ekr.20220525082935.1668">@staticmethod
def add_all_protocol_deps(deps: Dict[str, Set[str]]) -&gt; None:
    """Add all known protocol dependencies to deps.

    This is used by tests and debug output, and also when collecting
    all collected or loaded dependencies as part of build.
    """
    TypeState.update_protocol_deps()  # just in case
    if TypeState.proto_deps is not None:
        for trigger, targets in TypeState.proto_deps.items():
            deps.setdefault(trigger, set()).update(targets)


</t>
<t tx="ekr.20220525082935.1669">def reset_global_state() -&gt; None:
    """Reset most existing global state.

    Currently most of it is in this module. Few exceptions are strict optional status and
    and functools.lru_cache.
    """
    TypeState.reset_all_subtype_caches()
    TypeState.reset_protocol_deps()
</t>
<t tx="ekr.20220525082935.167">def visit_unary_expr(self, expr: UnaryExpr) -&gt; None:
    expr.expr.accept(self)

</t>
<t tx="ekr.20220525082935.1670">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from typing import Iterable

from mypy_extensions import trait

from mypy.types import (
    Type, SyntheticTypeVisitor, AnyType, UninhabitedType, NoneType, ErasedType, DeletedType,
    TypeVarType, LiteralType, Instance, CallableType, TupleType, TypedDictType, UnionType,
    Overloaded, TypeType, CallableArgument, UnboundType, TypeList, StarType, EllipsisType,
    PlaceholderType, PartialType, RawExpressionType, TypeAliasType, ParamSpecType, Parameters,
    UnpackType, TypeVarTupleType,
)


@others
</t>
<t tx="ekr.20220525082935.1671">@trait
class TypeTraverserVisitor(SyntheticTypeVisitor[None]):
    """Visitor that traverses all components of a type"""

    # Atomic types

    @others
</t>
<t tx="ekr.20220525082935.1672">def visit_any(self, t: AnyType) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082935.1673">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082935.1674">def visit_none_type(self, t: NoneType) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082935.1675">def visit_erased_type(self, t: ErasedType) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082935.1676">def visit_deleted_type(self, t: DeletedType) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082935.1677">def visit_type_var(self, t: TypeVarType) -&gt; None:
    # Note that type variable values and upper bound aren't treated as
    # components, since they are components of the type variable
    # definition. We want to traverse everything just once.
    pass

</t>
<t tx="ekr.20220525082935.1678">def visit_param_spec(self, t: ParamSpecType) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082935.1679">def visit_parameters(self, t: Parameters) -&gt; None:
    self.traverse_types(t.arg_types)

</t>
<t tx="ekr.20220525082935.168">def visit_index_expr(self, expr: IndexExpr) -&gt; None:
    base = expr.base
    base.accept(self)
    if (isinstance(base, RefExpr)
            and isinstance(base.node, TypeInfo)
            and not base.node.is_generic()):
        expr.index.accept(self)
    elif ((isinstance(base, RefExpr) and isinstance(base.node, TypeAlias))
          or refers_to_class_or_function(base)):
        # We need to do full processing on every iteration, since some type
        # arguments may contain placeholder types.
        self.analyze_type_application(expr)
    else:
        expr.index.accept(self)

</t>
<t tx="ekr.20220525082935.1680">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082935.1681">def visit_literal_type(self, t: LiteralType) -&gt; None:
    t.fallback.accept(self)

</t>
<t tx="ekr.20220525082935.1682"># Composite types

</t>
<t tx="ekr.20220525082935.1683">def visit_instance(self, t: Instance) -&gt; None:
    self.traverse_types(t.args)

</t>
<t tx="ekr.20220525082935.1684">def visit_callable_type(self, t: CallableType) -&gt; None:
    # FIX generics
    self.traverse_types(t.arg_types)
    t.ret_type.accept(self)
    t.fallback.accept(self)

</t>
<t tx="ekr.20220525082935.1685">def visit_tuple_type(self, t: TupleType) -&gt; None:
    self.traverse_types(t.items)
    t.partial_fallback.accept(self)

</t>
<t tx="ekr.20220525082935.1686">def visit_typeddict_type(self, t: TypedDictType) -&gt; None:
    self.traverse_types(t.items.values())
    t.fallback.accept(self)

</t>
<t tx="ekr.20220525082935.1687">def visit_union_type(self, t: UnionType) -&gt; None:
    self.traverse_types(t.items)

</t>
<t tx="ekr.20220525082935.1688">def visit_overloaded(self, t: Overloaded) -&gt; None:
    self.traverse_types(t.items)

</t>
<t tx="ekr.20220525082935.1689">def visit_type_type(self, t: TypeType) -&gt; None:
    t.item.accept(self)

</t>
<t tx="ekr.20220525082935.169">def analyze_type_application(self, expr: IndexExpr) -&gt; None:
    """Analyze special form -- type application (either direct or via type aliasing)."""
    types = self.analyze_type_application_args(expr)
    if types is None:
        return
    base = expr.base
    expr.analyzed = TypeApplication(base, types)
    expr.analyzed.line = expr.line
    expr.analyzed.column = expr.column
    # Types list, dict, set are not subscriptable, prohibit this if
    # subscripted either via type alias...
    if isinstance(base, RefExpr) and isinstance(base.node, TypeAlias):
        alias = base.node
        target = get_proper_type(alias.target)
        if isinstance(target, Instance):
            name = target.type.fullname
            if (alias.no_args and  # this avoids bogus errors for already reported aliases
                    name in get_nongen_builtins(self.options.python_version) and
                    not self.is_stub_file and
                    not alias.normalized):
                self.fail(no_subscript_builtin_alias(name, propose_alt=False), expr)
    # ...or directly.
    else:
        n = self.lookup_type_node(base)
        if (n and n.fullname in get_nongen_builtins(self.options.python_version) and
                not self.is_stub_file):
            self.fail(no_subscript_builtin_alias(n.fullname, propose_alt=False), expr)

</t>
<t tx="ekr.20220525082935.1690"># Special types (not real types)

</t>
<t tx="ekr.20220525082935.1691">def visit_callable_argument(self, t: CallableArgument) -&gt; None:
    t.typ.accept(self)

</t>
<t tx="ekr.20220525082935.1692">def visit_unbound_type(self, t: UnboundType) -&gt; None:
    self.traverse_types(t.args)

</t>
<t tx="ekr.20220525082935.1693">def visit_type_list(self, t: TypeList) -&gt; None:
    self.traverse_types(t.items)

</t>
<t tx="ekr.20220525082935.1694">def visit_star_type(self, t: StarType) -&gt; None:
    t.type.accept(self)

</t>
<t tx="ekr.20220525082935.1695">def visit_ellipsis_type(self, t: EllipsisType) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082935.1696">def visit_placeholder_type(self, t: PlaceholderType) -&gt; None:
    self.traverse_types(t.args)

</t>
<t tx="ekr.20220525082935.1697">def visit_partial_type(self, t: PartialType) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082935.1698">def visit_raw_expression_type(self, t: RawExpressionType) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082935.1699">def visit_type_alias_type(self, t: TypeAliasType) -&gt; None:
    self.traverse_types(t.args)

</t>
<t tx="ekr.20220525082935.17">def adjust_public_exports(self) -&gt; None:
    """Adjust the module visibility of globals due to __all__."""
    if '__all__' in self.globals:
        for name, g in self.globals.items():
            # Being included in __all__ explicitly exports and makes public.
            if name in self.all_exports:
                g.module_public = True
                g.module_hidden = False
            # But when __all__ is defined, and a symbol is not included in it,
            # it cannot be public.
            else:
                g.module_public = False

</t>
<t tx="ekr.20220525082935.170">def analyze_type_application_args(self, expr: IndexExpr) -&gt; Optional[List[Type]]:
    """Analyze type arguments (index) in a type application.

    Return None if anything was incomplete.
    """
    index = expr.index
    tag = self.track_incomplete_refs()
    self.analyze_type_expr(index)
    if self.found_incomplete_ref(tag):
        return None
    types: List[Type] = []
    if isinstance(index, TupleExpr):
        items = index.items
        is_tuple = isinstance(expr.base, RefExpr) and expr.base.fullname == 'builtins.tuple'
        if is_tuple and len(items) == 2 and isinstance(items[-1], EllipsisExpr):
            items = items[:-1]
    else:
        items = [index]

    # whether param spec literals be allowed here
    # TODO: should this be computed once and passed in?
    #   or is there a better way to do this?
    base = expr.base
    if isinstance(base, RefExpr) and isinstance(base.node, TypeAlias):
        alias = base.node
        target = get_proper_type(alias.target)
        if isinstance(target, Instance):
            has_param_spec = target.type.has_param_spec_type
            num_args = len(target.type.type_vars)
        else:
            has_param_spec = False
            num_args = -1
    elif isinstance(base, NameExpr) and isinstance(base.node, TypeInfo):
        has_param_spec = base.node.has_param_spec_type
        num_args = len(base.node.type_vars)
    else:
        has_param_spec = False
        num_args = -1

    for item in items:
        try:
            typearg = self.expr_to_unanalyzed_type(item)
        except TypeTranslationError:
            self.fail('Type expected within [...]', expr)
            return None
        # We always allow unbound type variables in IndexExpr, since we
        # may be analysing a type alias definition rvalue. The error will be
        # reported elsewhere if it is not the case.
        analyzed = self.anal_type(typearg, allow_unbound_tvars=True,
                                  allow_placeholder=True,
                                  allow_param_spec_literals=has_param_spec)
        if analyzed is None:
            return None
        types.append(analyzed)

    if has_param_spec and num_args == 1 and len(types) &gt; 0:
        first_arg = get_proper_type(types[0])
        if not (len(types) == 1 and (isinstance(first_arg, Parameters) or
                                     isinstance(first_arg, ParamSpecType) or
                                     isinstance(first_arg, AnyType))):
            types = [Parameters(types, [ARG_POS] * len(types), [None] * len(types))]

    return types

</t>
<t tx="ekr.20220525082935.1700">def visit_unpack_type(self, t: UnpackType) -&gt; None:
    t.type.accept(self)

</t>
<t tx="ekr.20220525082935.1701"># Helpers

</t>
<t tx="ekr.20220525082935.1702">def traverse_types(self, types: Iterable[Type]) -&gt; None:
    for typ in types:
        typ.accept(self)
</t>
<t tx="ekr.20220525082935.1703">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from typing import Union, List

from mypy.nodes import TypeInfo

from mypy.erasetype import erase_typevars
from mypy.types import Instance, TypeVarType, TupleType, Type, TypeOfAny, AnyType, ParamSpecType


@others
</t>
<t tx="ekr.20220525082935.171">def visit_slice_expr(self, expr: SliceExpr) -&gt; None:
    if expr.begin_index:
        expr.begin_index.accept(self)
    if expr.end_index:
        expr.end_index.accept(self)
    if expr.stride:
        expr.stride.accept(self)

</t>
<t tx="ekr.20220525082935.172">def visit_cast_expr(self, expr: CastExpr) -&gt; None:
    expr.expr.accept(self)
    analyzed = self.anal_type(expr.type)
    if analyzed is not None:
        expr.type = analyzed

</t>
<t tx="ekr.20220525082935.173">def visit_assert_type_expr(self, expr: AssertTypeExpr) -&gt; None:
    expr.expr.accept(self)
    analyzed = self.anal_type(expr.type)
    if analyzed is not None:
        expr.type = analyzed

</t>
<t tx="ekr.20220525082935.174">def visit_reveal_expr(self, expr: RevealExpr) -&gt; None:
    if expr.kind == REVEAL_TYPE:
        if expr.expr is not None:
            expr.expr.accept(self)
    else:
        # Reveal locals doesn't have an inner expression, there's no
        # need to traverse inside it
        pass

</t>
<t tx="ekr.20220525082935.175">def visit_type_application(self, expr: TypeApplication) -&gt; None:
    expr.expr.accept(self)
    for i in range(len(expr.types)):
        analyzed = self.anal_type(expr.types[i])
        if analyzed is not None:
            expr.types[i] = analyzed

</t>
<t tx="ekr.20220525082935.176">def visit_list_comprehension(self, expr: ListComprehension) -&gt; None:
    if any(expr.generator.is_async):
        if not self.is_func_scope() or not self.function_stack[-1].is_coroutine:
            self.fail(message_registry.ASYNC_FOR_OUTSIDE_COROUTINE, expr, code=codes.SYNTAX)

    expr.generator.accept(self)

</t>
<t tx="ekr.20220525082935.177">def visit_set_comprehension(self, expr: SetComprehension) -&gt; None:
    if any(expr.generator.is_async):
        if not self.is_func_scope() or not self.function_stack[-1].is_coroutine:
            self.fail(message_registry.ASYNC_FOR_OUTSIDE_COROUTINE, expr, code=codes.SYNTAX)

    expr.generator.accept(self)

</t>
<t tx="ekr.20220525082935.178">def visit_dictionary_comprehension(self, expr: DictionaryComprehension) -&gt; None:
    if any(expr.is_async):
        if not self.is_func_scope() or not self.function_stack[-1].is_coroutine:
            self.fail(message_registry.ASYNC_FOR_OUTSIDE_COROUTINE, expr, code=codes.SYNTAX)

    with self.enter(expr):
        self.analyze_comp_for(expr)
        expr.key.accept(self)
        expr.value.accept(self)
    self.analyze_comp_for_2(expr)

</t>
<t tx="ekr.20220525082935.179">def visit_generator_expr(self, expr: GeneratorExpr) -&gt; None:
    with self.enter(expr):
        self.analyze_comp_for(expr)
        expr.left_expr.accept(self)
    self.analyze_comp_for_2(expr)

</t>
<t tx="ekr.20220525082935.18">@contextmanager
def file_context(self,
                 file_node: MypyFile,
                 options: Options,
                 active_type: Optional[TypeInfo] = None) -&gt; Iterator[None]:
    """Configure analyzer for analyzing targets within a file/class.

    Args:
        file_node: target file
        options: options specific to the file
        active_type: must be the surrounding class to analyze method targets
    """
    scope = self.scope
    self.options = options
    self.errors.set_file(file_node.path, file_node.fullname, scope=scope)
    self.cur_mod_node = file_node
    self.cur_mod_id = file_node.fullname
    with scope.module_scope(self.cur_mod_id):
        self._is_stub_file = file_node.path.lower().endswith('.pyi')
        self._is_typeshed_stub_file = is_typeshed_file(file_node.path)
        self.globals = file_node.names
        self.tvar_scope = TypeVarLikeScope()

        self.named_tuple_analyzer = NamedTupleAnalyzer(options, self)
        self.typed_dict_analyzer = TypedDictAnalyzer(options, self, self.msg)
        self.enum_call_analyzer = EnumCallAnalyzer(options, self)
        self.newtype_analyzer = NewTypeAnalyzer(options, self, self.msg)

        # Counter that keeps track of references to undefined things potentially caused by
        # incomplete namespaces.
        self.num_incomplete_refs = 0

        if active_type:
            self.incomplete_type_stack.append(False)
            scope.enter_class(active_type)
            self.enter_class(active_type.defn.info)
            for tvar in active_type.defn.type_vars:
                self.tvar_scope.bind_existing(tvar)

        yield

        if active_type:
            scope.leave_class()
            self.leave_class()
            self.type = None
            self.incomplete_type_stack.pop()
    del self.options

</t>
<t tx="ekr.20220525082935.180">def analyze_comp_for(self, expr: Union[GeneratorExpr,
                                       DictionaryComprehension]) -&gt; None:
    """Analyses the 'comp_for' part of comprehensions (part 1).

    That is the part after 'for' in (x for x in l if p). This analyzes
    variables and conditions which are analyzed in a local scope.
    """
    for i, (index, sequence, conditions) in enumerate(zip(expr.indices,
                                                          expr.sequences,
                                                          expr.condlists)):
        if i &gt; 0:
            sequence.accept(self)
        # Bind index variables.
        self.analyze_lvalue(index)
        for cond in conditions:
            cond.accept(self)

</t>
<t tx="ekr.20220525082935.181">def analyze_comp_for_2(self, expr: Union[GeneratorExpr,
                                         DictionaryComprehension]) -&gt; None:
    """Analyses the 'comp_for' part of comprehensions (part 2).

    That is the part after 'for' in (x for x in l if p). This analyzes
    the 'l' part which is analyzed in the surrounding scope.
    """
    expr.sequences[0].accept(self)

</t>
<t tx="ekr.20220525082935.182">def visit_lambda_expr(self, expr: LambdaExpr) -&gt; None:
    self.analyze_arg_initializers(expr)
    self.analyze_function_body(expr)

</t>
<t tx="ekr.20220525082935.183">def visit_conditional_expr(self, expr: ConditionalExpr) -&gt; None:
    expr.if_expr.accept(self)
    expr.cond.accept(self)
    expr.else_expr.accept(self)

</t>
<t tx="ekr.20220525082935.184">def visit_backquote_expr(self, expr: BackquoteExpr) -&gt; None:
    expr.expr.accept(self)

</t>
<t tx="ekr.20220525082935.185">def visit__promote_expr(self, expr: PromoteExpr) -&gt; None:
    analyzed = self.anal_type(expr.type)
    if analyzed is not None:
        expr.type = analyzed

</t>
<t tx="ekr.20220525082935.186">def visit_yield_expr(self, e: YieldExpr) -&gt; None:
    if not self.is_func_scope():
        self.fail('"yield" outside function', e, serious=True, blocker=True)
    elif self.is_comprehension_stack[-1]:
        self.fail('"yield" inside comprehension or generator expression',
                  e, serious=True, blocker=True)
    elif self.function_stack[-1].is_coroutine:
        if self.options.python_version &lt; (3, 6):
            self.fail('"yield" in async function', e, serious=True, blocker=True)
        else:
            self.function_stack[-1].is_generator = True
            self.function_stack[-1].is_async_generator = True
    else:
        self.function_stack[-1].is_generator = True
    if e.expr:
        e.expr.accept(self)

</t>
<t tx="ekr.20220525082935.187">def visit_await_expr(self, expr: AwaitExpr) -&gt; None:
    if not self.is_func_scope():
        self.fail('"await" outside function', expr)
    elif not self.function_stack[-1].is_coroutine:
        self.fail('"await" outside coroutine ("async def")', expr)
    expr.expr.accept(self)

</t>
<t tx="ekr.20220525082935.188">#
# Patterns
#

</t>
<t tx="ekr.20220525082935.189">def visit_as_pattern(self, p: AsPattern) -&gt; None:
    if p.pattern is not None:
        p.pattern.accept(self)
    if p.name is not None:
        self.analyze_lvalue(p.name)

</t>
<t tx="ekr.20220525082935.19">#
# Functions
#

</t>
<t tx="ekr.20220525082935.190">def visit_or_pattern(self, p: OrPattern) -&gt; None:
    for pattern in p.patterns:
        pattern.accept(self)

</t>
<t tx="ekr.20220525082935.191">def visit_value_pattern(self, p: ValuePattern) -&gt; None:
    p.expr.accept(self)

</t>
<t tx="ekr.20220525082935.192">def visit_sequence_pattern(self, p: SequencePattern) -&gt; None:
    for pattern in p.patterns:
        pattern.accept(self)

</t>
<t tx="ekr.20220525082935.193">def visit_starred_pattern(self, p: StarredPattern) -&gt; None:
    if p.capture is not None:
        self.analyze_lvalue(p.capture)

</t>
<t tx="ekr.20220525082935.194">def visit_mapping_pattern(self, p: MappingPattern) -&gt; None:
    for key in p.keys:
        key.accept(self)
    for value in p.values:
        value.accept(self)
    if p.rest is not None:
        self.analyze_lvalue(p.rest)

</t>
<t tx="ekr.20220525082935.195">def visit_class_pattern(self, p: ClassPattern) -&gt; None:
    p.class_ref.accept(self)
    for pos in p.positionals:
        pos.accept(self)
    for v in p.keyword_values:
        v.accept(self)

</t>
<t tx="ekr.20220525082935.196">#
# Lookup functions
#

</t>
<t tx="ekr.20220525082935.197">def lookup(self, name: str, ctx: Context,
           suppress_errors: bool = False) -&gt; Optional[SymbolTableNode]:
    """Look up an unqualified (no dots) name in all active namespaces.

    Note that the result may contain a PlaceholderNode. The caller may
    want to defer in that case.

    Generate an error if the name is not defined unless suppress_errors
    is true or the current namespace is incomplete. In the latter case
    defer.
    """
    implicit_name = False
    # 1a. Name declared using 'global x' takes precedence
    if name in self.global_decls[-1]:
        if name in self.globals:
            return self.globals[name]
        if not suppress_errors:
            self.name_not_defined(name, ctx)
        return None
    # 1b. Name declared using 'nonlocal x' takes precedence
    if name in self.nonlocal_decls[-1]:
        for table in reversed(self.locals[:-1]):
            if table is not None and name in table:
                return table[name]
        else:
            if not suppress_errors:
                self.name_not_defined(name, ctx)
            return None
    # 2. Class attributes (if within class definition)
    if self.type and not self.is_func_scope() and name in self.type.names:
        node = self.type.names[name]
        if not node.implicit:
            if self.is_active_symbol_in_class_body(node.node):
                return node
        else:
            # Defined through self.x assignment
            implicit_name = True
            implicit_node = node
    # 3. Local (function) scopes
    for table in reversed(self.locals):
        if table is not None and name in table:
            return table[name]
    # 4. Current file global scope
    if name in self.globals:
        return self.globals[name]
    # 5. Builtins
    b = self.globals.get('__builtins__', None)
    if b:
        assert isinstance(b.node, MypyFile)
        table = b.node.names
        if name in table:
            if len(name) &gt; 1 and name[0] == "_" and name[1] != "_":
                if not suppress_errors:
                    self.name_not_defined(name, ctx)
                return None
            node = table[name]
            return node
    # Give up.
    if not implicit_name and not suppress_errors:
        self.name_not_defined(name, ctx)
    else:
        if implicit_name:
            return implicit_node
    return None

</t>
<t tx="ekr.20220525082935.198">def is_active_symbol_in_class_body(self, node: Optional[SymbolNode]) -&gt; bool:
    """Can a symbol defined in class body accessed at current statement?

    Only allow access to class attributes textually after
    the definition, so that it's possible to fall back to the
    outer scope. Example:

        class X: ...

        class C:
            X = X  # Initializer refers to outer scope

    Nested classes are an exception, since we want to support
    arbitrary forward references in type annotations.
    """
    # TODO: Forward reference to name imported in class body is not
    #       caught.
    if self.statement is None:
        # Assume it's fine -- don't have enough context to check
        return True
    return (node is None
            or self.is_textually_before_statement(node)
            or not self.is_defined_in_current_module(node.fullname)
            or isinstance(node, TypeInfo)
            or (isinstance(node, PlaceholderNode) and node.becomes_typeinfo))

</t>
<t tx="ekr.20220525082935.199">def is_textually_before_statement(self, node: SymbolNode) -&gt; bool:
    """Check if a node is defined textually before the current statement

    Note that decorated functions' line number are the same as
    the top decorator.
    """
    assert self.statement
    line_diff = self.statement.line - node.line

    # The first branch handles reference an overloaded function variant inside itself,
    # this is a corner case where mypy technically deviates from runtime name resolution,
    # but it is fine because we want an overloaded function to be treated as a single unit.
    if self.is_overloaded_item(node, self.statement):
        return False
    elif isinstance(node, Decorator) and not node.is_overload:
        return line_diff &gt; len(node.original_decorators)
    else:
        return line_diff &gt; 0

</t>
<t tx="ekr.20220525082935.2">def __init__(self,
             modules: Dict[str, MypyFile],
             missing_modules: Set[str],
             incomplete_namespaces: Set[str],
             errors: Errors,
             plugin: Plugin) -&gt; None:
    """Construct semantic analyzer.

    We reuse the same semantic analyzer instance across multiple modules.

    Args:
        modules: Global modules dictionary
        missing_modules: Modules that could not be imported encountered so far
        incomplete_namespaces: Namespaces that are being populated during semantic analysis
            (can contain modules and classes within the current SCC; mutated by the caller)
        errors: Report analysis errors using this instance
    """
    self.locals = [None]
    self.is_comprehension_stack = [False]
    # Saved namespaces from previous iteration. Every top-level function/method body is
    # analyzed in several iterations until all names are resolved. We need to save
    # the local namespaces for the top level function and all nested functions between
    # these iterations. See also semanal_main.process_top_level_function().
    self.saved_locals: Dict[
        Union[FuncItem, GeneratorExpr, DictionaryComprehension], SymbolTable
    ] = {}
    self.imports = set()
    self.type = None
    self.type_stack = []
    # Are the namespaces of classes being processed complete?
    self.incomplete_type_stack: List[bool] = []
    self.tvar_scope = TypeVarLikeScope()
    self.function_stack = []
    self.block_depth = [0]
    self.loop_depth = 0
    self.errors = errors
    self.modules = modules
    self.msg = MessageBuilder(errors, modules)
    self.missing_modules = missing_modules
    self.missing_names = [set()]
    # These namespaces are still in process of being populated. If we encounter a
    # missing name in these namespaces, we need to defer the current analysis target,
    # since it's possible that the name will be there once the namespace is complete.
    self.incomplete_namespaces = incomplete_namespaces
    self.all_exports: List[str] = []
    # Map from module id to list of explicitly exported names (i.e. names in __all__).
    self.export_map: Dict[str, List[str]] = {}
    self.plugin = plugin
    # If True, process function definitions. If False, don't. This is used
    # for processing module top levels in fine-grained incremental mode.
    self.recurse_into_functions = True
    self.scope = Scope()

    # Trace line numbers for every file where deferral happened during analysis of
    # current SCC or top-level function.
    self.deferral_debug_context: List[Tuple[str, int]] = []

</t>
<t tx="ekr.20220525082935.20">def visit_func_def(self, defn: FuncDef) -&gt; None:
    self.statement = defn

    # Visit default values because they may contain assignment expressions.
    for arg in defn.arguments:
        if arg.initializer:
            arg.initializer.accept(self)

    defn.is_conditional = self.block_depth[-1] &gt; 0

    # Set full names even for those definitions that aren't added
    # to a symbol table. For example, for overload items.
    defn._fullname = self.qualified_name(defn.name)

    # We don't add module top-level functions to symbol tables
    # when we analyze their bodies in the second phase on analysis,
    # since they were added in the first phase. Nested functions
    # get always added, since they aren't separate targets.
    if not self.recurse_into_functions or len(self.function_stack) &gt; 0:
        if not defn.is_decorated and not defn.is_overload:
            self.add_function_to_symbol_table(defn)

    if not self.recurse_into_functions:
        return

    with self.scope.function_scope(defn):
        self.analyze_func_def(defn)

</t>
<t tx="ekr.20220525082935.200">def is_overloaded_item(self, node: SymbolNode, statement: Statement) -&gt; bool:
    """Check whether the function belongs to the overloaded variants"""
    if isinstance(node, OverloadedFuncDef) and isinstance(statement, FuncDef):
        in_items = statement in {item.func if isinstance(item, Decorator)
                                 else item for item in node.items}
        in_impl = (node.impl is not None and
                  ((isinstance(node.impl, Decorator) and statement is node.impl.func)
                   or statement is node.impl))
        return in_items or in_impl
    return False

</t>
<t tx="ekr.20220525082935.201">def is_defined_in_current_module(self, fullname: Optional[str]) -&gt; bool:
    if fullname is None:
        return False
    return module_prefix(self.modules, fullname) == self.cur_mod_id

</t>
<t tx="ekr.20220525082935.202">def lookup_qualified(self, name: str, ctx: Context,
                     suppress_errors: bool = False) -&gt; Optional[SymbolTableNode]:
    """Lookup a qualified name in all activate namespaces.

    Note that the result may contain a PlaceholderNode. The caller may
    want to defer in that case.

    Generate an error if the name is not defined unless suppress_errors
    is true or the current namespace is incomplete. In the latter case
    defer.
    """
    if '.' not in name:
        # Simple case: look up a short name.
        return self.lookup(name, ctx, suppress_errors=suppress_errors)
    parts = name.split('.')
    namespace = self.cur_mod_id
    sym = self.lookup(parts[0], ctx, suppress_errors=suppress_errors)
    if sym:
        for i in range(1, len(parts)):
            node = sym.node
            part = parts[i]
            if isinstance(node, TypeInfo):
                nextsym = node.get(part)
            elif isinstance(node, MypyFile):
                nextsym = self.get_module_symbol(node, part)
                namespace = node.fullname
            elif isinstance(node, PlaceholderNode):
                return sym
            elif isinstance(node, TypeAlias) and node.no_args:
                assert isinstance(node.target, ProperType)
                if isinstance(node.target, Instance):
                    nextsym = node.target.type.get(part)
                else:
                    nextsym = None
            else:
                if isinstance(node, Var):
                    typ = get_proper_type(node.type)
                    if isinstance(typ, AnyType):
                        # Allow access through Var with Any type without error.
                        return self.implicit_symbol(sym, name, parts[i:], typ)
                # Lookup through invalid node, such as variable or function
                nextsym = None
            if not nextsym or nextsym.module_hidden:
                if not suppress_errors:
                    self.name_not_defined(name, ctx, namespace=namespace)
                return None
            sym = nextsym
    return sym

</t>
<t tx="ekr.20220525082935.203">def lookup_type_node(self, expr: Expression) -&gt; Optional[SymbolTableNode]:
    try:
        t = self.expr_to_unanalyzed_type(expr)
    except TypeTranslationError:
        return None
    if isinstance(t, UnboundType):
        n = self.lookup_qualified(t.name, expr, suppress_errors=True)
        return n
    return None

</t>
<t tx="ekr.20220525082935.204">def get_module_symbol(self, node: MypyFile, name: str) -&gt; Optional[SymbolTableNode]:
    """Look up a symbol from a module.

    Return None if no matching symbol could be bound.
    """
    module = node.fullname
    names = node.names
    sym = names.get(name)
    if not sym:
        fullname = module + '.' + name
        if fullname in self.modules:
            sym = SymbolTableNode(GDEF, self.modules[fullname])
        elif self.is_incomplete_namespace(module):
            self.record_incomplete_ref()
        elif ('__getattr__' in names
                and (node.is_stub
                     or self.options.python_version &gt;= (3, 7))):
            gvar = self.create_getattr_var(names['__getattr__'], name, fullname)
            if gvar:
                sym = SymbolTableNode(GDEF, gvar)
        elif self.is_missing_module(fullname):
            # We use the fullname of the original definition so that we can
            # detect whether two names refer to the same thing.
            var_type = AnyType(TypeOfAny.from_unimported_type)
            v = Var(name, type=var_type)
            v._fullname = fullname
            sym = SymbolTableNode(GDEF, v)
    elif sym.module_hidden:
        sym = None
    return sym

</t>
<t tx="ekr.20220525082935.205">def is_missing_module(self, module: str) -&gt; bool:
    return module in self.missing_modules

</t>
<t tx="ekr.20220525082935.206">def implicit_symbol(self, sym: SymbolTableNode, name: str, parts: List[str],
                    source_type: AnyType) -&gt; SymbolTableNode:
    """Create symbol for a qualified name reference through Any type."""
    if sym.node is None:
        basename = None
    else:
        basename = sym.node.fullname
    if basename is None:
        fullname = name
    else:
        fullname = basename + '.' + '.'.join(parts)
    var_type = AnyType(TypeOfAny.from_another_any, source_type)
    var = Var(parts[-1], var_type)
    var._fullname = fullname
    return SymbolTableNode(GDEF, var)

</t>
<t tx="ekr.20220525082935.207">def create_getattr_var(self, getattr_defn: SymbolTableNode,
                       name: str, fullname: str) -&gt; Optional[Var]:
    """Create a dummy variable using module-level __getattr__ return type.

    If not possible, return None.

    Note that multiple Var nodes can be created for a single name. We
    can use the from_module_getattr and the fullname attributes to
    check if two dummy Var nodes refer to the same thing. Reusing Var
    nodes would require non-local mutable state, which we prefer to
    avoid.
    """
    if isinstance(getattr_defn.node, (FuncDef, Var)):
        node_type = get_proper_type(getattr_defn.node.type)
        if isinstance(node_type, CallableType):
            typ = node_type.ret_type
        else:
            typ = AnyType(TypeOfAny.from_error)
        v = Var(name, type=typ)
        v._fullname = fullname
        v.from_module_getattr = True
        return v
    return None

</t>
<t tx="ekr.20220525082935.208">def lookup_fully_qualified(self, fullname: str) -&gt; SymbolTableNode:
    ret = self.lookup_fully_qualified_or_none(fullname)
    assert ret is not None, fullname
    return ret

</t>
<t tx="ekr.20220525082935.209">def lookup_fully_qualified_or_none(self, fullname: str) -&gt; Optional[SymbolTableNode]:
    """Lookup a fully qualified name that refers to a module-level definition.

    Don't assume that the name is defined. This happens in the global namespace --
    the local module namespace is ignored. This does not dereference indirect
    refs.

    Note that this can't be used for names nested in class namespaces.
    """
    # TODO: unify/clean-up/simplify lookup methods, see #4157.
    # TODO: support nested classes (but consider performance impact,
    #       we might keep the module level only lookup for thing like 'builtins.int').
    assert '.' in fullname
    module, name = fullname.rsplit('.', maxsplit=1)
    if module not in self.modules:
        return None
    filenode = self.modules[module]
    result = filenode.names.get(name)
    if result is None and self.is_incomplete_namespace(module):
        # TODO: More explicit handling of incomplete refs?
        self.record_incomplete_ref()
    return result

</t>
<t tx="ekr.20220525082935.21">def analyze_func_def(self, defn: FuncDef) -&gt; None:
    self.function_stack.append(defn)

    if defn.type:
        assert isinstance(defn.type, CallableType)
        self.update_function_type_variables(defn.type, defn)
    self.function_stack.pop()

    if self.is_class_scope():
        # Method definition
        assert self.type is not None
        defn.info = self.type
        if defn.type is not None and defn.name in ('__init__', '__init_subclass__'):
            assert isinstance(defn.type, CallableType)
            if isinstance(get_proper_type(defn.type.ret_type), AnyType):
                defn.type = defn.type.copy_modified(ret_type=NoneType())
        self.prepare_method_signature(defn, self.type)

    # Analyze function signature
    with self.tvar_scope_frame(self.tvar_scope.method_frame()):
        if defn.type:
            self.check_classvar_in_signature(defn.type)
            assert isinstance(defn.type, CallableType)
            # Signature must be analyzed in the surrounding scope so that
            # class-level imported names and type variables are in scope.
            analyzer = self.type_analyzer()
            tag = self.track_incomplete_refs()
            result = analyzer.visit_callable_type(defn.type, nested=False)
            # Don't store not ready types (including placeholders).
            if self.found_incomplete_ref(tag) or has_placeholder(result):
                self.defer(defn)
                return
            assert isinstance(result, ProperType)
            defn.type = result
            self.add_type_alias_deps(analyzer.aliases_used)
            self.check_function_signature(defn)
            if isinstance(defn, FuncDef):
                assert isinstance(defn.type, CallableType)
                defn.type = set_callable_name(defn.type, defn)

    self.analyze_arg_initializers(defn)
    self.analyze_function_body(defn)
    if (defn.is_coroutine and
            isinstance(defn.type, CallableType) and
            self.wrapped_coro_return_types.get(defn) != defn.type):
        if defn.is_async_generator:
            # Async generator types are handled elsewhere
            pass
        else:
            # A coroutine defined as `async def foo(...) -&gt; T: ...`
            # has external return type `Coroutine[Any, Any, T]`.
            any_type = AnyType(TypeOfAny.special_form)
            ret_type = self.named_type_or_none('typing.Coroutine',
                                               [any_type, any_type, defn.type.ret_type])
            assert ret_type is not None, "Internal error: typing.Coroutine not found"
            defn.type = defn.type.copy_modified(ret_type=ret_type)
            self.wrapped_coro_return_types[defn] = defn.type

</t>
<t tx="ekr.20220525082935.210">def object_type(self) -&gt; Instance:
    return self.named_type('builtins.object')

</t>
<t tx="ekr.20220525082935.211">def str_type(self) -&gt; Instance:
    return self.named_type('builtins.str')

</t>
<t tx="ekr.20220525082935.212">def named_type(self, fullname: str, args: Optional[List[Type]] = None) -&gt; Instance:
    sym = self.lookup_fully_qualified(fullname)
    assert sym, "Internal error: attempted to construct unknown type"
    node = sym.node
    assert isinstance(node, TypeInfo)
    if args:
        # TODO: assert len(args) == len(node.defn.type_vars)
        return Instance(node, args)
    return Instance(node, [AnyType(TypeOfAny.special_form)] * len(node.defn.type_vars))

</t>
<t tx="ekr.20220525082935.213">def named_type_or_none(self, fullname: str,
                       args: Optional[List[Type]] = None) -&gt; Optional[Instance]:
    sym = self.lookup_fully_qualified_or_none(fullname)
    if not sym or isinstance(sym.node, PlaceholderNode):
        return None
    node = sym.node
    if isinstance(node, TypeAlias):
        assert isinstance(node.target, Instance)  # type: ignore
        node = node.target.type
    assert isinstance(node, TypeInfo), node
    if args is not None:
        # TODO: assert len(args) == len(node.defn.type_vars)
        return Instance(node, args)
    return Instance(node, [AnyType(TypeOfAny.unannotated)] * len(node.defn.type_vars))

</t>
<t tx="ekr.20220525082935.214">def builtin_type(self, fully_qualified_name: str) -&gt; Instance:
    """Legacy function -- use named_type() instead."""
    return self.named_type(fully_qualified_name)

</t>
<t tx="ekr.20220525082935.215">def lookup_current_scope(self, name: str) -&gt; Optional[SymbolTableNode]:
    if self.locals[-1] is not None:
        return self.locals[-1].get(name)
    elif self.type is not None:
        return self.type.names.get(name)
    else:
        return self.globals.get(name)

</t>
<t tx="ekr.20220525082935.216">#
# Adding symbols
#

</t>
<t tx="ekr.20220525082935.217">def add_symbol(self,
               name: str,
               node: SymbolNode,
               context: Context,
               module_public: bool = True,
               module_hidden: bool = False,
               can_defer: bool = True,
               escape_comprehensions: bool = False) -&gt; bool:
    """Add symbol to the currently active symbol table.

    Generally additions to symbol table should go through this method or
    one of the methods below so that kinds, redefinitions, conditional
    definitions, and skipped names are handled consistently.

    Return True if we actually added the symbol, or False if we refused to do so
    (because something is not ready).

    If can_defer is True, defer current target if adding a placeholder.
    """
    if self.is_func_scope():
        kind = LDEF
    elif self.type is not None:
        kind = MDEF
    else:
        kind = GDEF
    symbol = SymbolTableNode(kind,
                             node,
                             module_public=module_public,
                             module_hidden=module_hidden)
    return self.add_symbol_table_node(name, symbol, context, can_defer, escape_comprehensions)

</t>
<t tx="ekr.20220525082935.218">def add_symbol_skip_local(self, name: str, node: SymbolNode) -&gt; None:
    """Same as above, but skipping the local namespace.

    This doesn't check for previous definition and is only used
    for serialization of method-level classes.

    Classes defined within methods can be exposed through an
    attribute type, but method-level symbol tables aren't serialized.
    This method can be used to add such classes to an enclosing,
    serialized symbol table.
    """
    # TODO: currently this is only used by named tuples. Use this method
    # also by typed dicts and normal classes, see issue #6422.
    if self.type is not None:
        names = self.type.names
        kind = MDEF
    else:
        names = self.globals
        kind = GDEF
    symbol = SymbolTableNode(kind, node)
    names[name] = symbol

</t>
<t tx="ekr.20220525082935.219">def add_symbol_table_node(self,
                          name: str,
                          symbol: SymbolTableNode,
                          context: Optional[Context] = None,
                          can_defer: bool = True,
                          escape_comprehensions: bool = False) -&gt; bool:
    """Add symbol table node to the currently active symbol table.

    Return True if we actually added the symbol, or False if we refused
    to do so (because something is not ready or it was a no-op).

    Generate an error if there is an invalid redefinition.

    If context is None, unconditionally add node, since we can't report
    an error. Note that this is used by plugins to forcibly replace nodes!

    TODO: Prevent plugins from replacing nodes, as it could cause problems?

    Args:
        name: short name of symbol
        symbol: Node to add
        can_defer: if True, defer current target if adding a placeholder
        context: error context (see above about None value)
    """
    names = self.current_symbol_table(escape_comprehensions=escape_comprehensions)
    existing = names.get(name)
    if isinstance(symbol.node, PlaceholderNode) and can_defer:
        if context is not None:
            self.process_placeholder(name, 'name', context)
        else:
            # see note in docstring describing None contexts
            self.defer()
    if (existing is not None
            and context is not None
            and not is_valid_replacement(existing, symbol)):
        # There is an existing node, so this may be a redefinition.
        # If the new node points to the same node as the old one,
        # or if both old and new nodes are placeholders, we don't
        # need to do anything.
        old = existing.node
        new = symbol.node
        if isinstance(new, PlaceholderNode):
            # We don't know whether this is okay. Let's wait until the next iteration.
            return False
        if not is_same_symbol(old, new):
            if isinstance(new, (FuncDef, Decorator, OverloadedFuncDef, TypeInfo)):
                self.add_redefinition(names, name, symbol)
            if not (isinstance(new, (FuncDef, Decorator))
                    and self.set_original_def(old, new)):
                self.name_already_defined(name, context, existing)
    elif (name not in self.missing_names[-1] and '*' not in self.missing_names[-1]):
        names[name] = symbol
        self.progress = True
        return True
    return False

</t>
<t tx="ekr.20220525082935.22">def prepare_method_signature(self, func: FuncDef, info: TypeInfo) -&gt; None:
    """Check basic signature validity and tweak annotation of self/cls argument."""
    # Only non-static methods are special.
    functype = func.type
    if not func.is_static:
        if func.name in ['__init_subclass__', '__class_getitem__']:
            func.is_class = True
        if not func.arguments:
            self.fail('Method must have at least one argument', func)
        elif isinstance(functype, CallableType):
            self_type = get_proper_type(functype.arg_types[0])
            if isinstance(self_type, AnyType):
                leading_type: Type = fill_typevars(info)
                if func.is_class or func.name == '__new__':
                    leading_type = self.class_type(leading_type)
                func.type = replace_implicit_first_type(functype, leading_type)

</t>
<t tx="ekr.20220525082935.220">def add_redefinition(self,
                     names: SymbolTable,
                     name: str,
                     symbol: SymbolTableNode) -&gt; None:
    """Add a symbol table node that reflects a redefinition as a function or a class.

    Redefinitions need to be added to the symbol table so that they can be found
    through AST traversal, but they have dummy names of form 'name-redefinition[N]',
    where N ranges over 2, 3, ... (omitted for the first redefinition).

    Note: we always store redefinitions independently of whether they are valid or not
    (so they will be semantically analyzed), the caller should give an error for invalid
    redefinitions (such as e.g. variable redefined as a class).
    """
    i = 1
    # Don't serialize redefined nodes. They are likely to have
    # busted internal references which can cause problems with
    # serialization and they can't have any external references to
    # them.
    symbol.no_serialize = True
    while True:
        if i == 1:
            new_name = f'{name}-redefinition'
        else:
            new_name = f'{name}-redefinition{i}'
        existing = names.get(new_name)
        if existing is None:
            names[new_name] = symbol
            return
        elif existing.node is symbol.node:
            # Already there
            return
        i += 1

</t>
<t tx="ekr.20220525082935.221">def add_local(self, node: Union[Var, FuncDef, OverloadedFuncDef], context: Context) -&gt; None:
    """Add local variable or function."""
    assert self.is_func_scope()
    name = node.name
    node._fullname = name
    self.add_symbol(name, node, context)

</t>
<t tx="ekr.20220525082935.222">def add_module_symbol(self,
                      id: str,
                      as_id: str,
                      context: Context,
                      module_public: bool,
                      module_hidden: bool) -&gt; None:
    """Add symbol that is a reference to a module object."""
    if id in self.modules:
        node = self.modules[id]
        self.add_symbol(as_id, node, context,
                        module_public=module_public,
                        module_hidden=module_hidden)
    else:
        self.add_unknown_imported_symbol(
            as_id, context, target_name=id, module_public=module_public,
            module_hidden=module_hidden
        )

</t>
<t tx="ekr.20220525082935.223">def _get_node_for_class_scoped_import(
    self, name: str, symbol_node: Optional[SymbolNode], context: Context
) -&gt; Optional[SymbolNode]:
    if symbol_node is None:
        return None
    # I promise this type checks; I'm just making mypyc issues go away.
    # mypyc is absolutely convinced that `symbol_node` narrows to a Var in the following,
    # when it can also be a FuncBase. Once fixed, `f` in the following can be removed.
    # See also https://github.com/mypyc/mypyc/issues/892
    f = cast(Any, lambda x: x)
    if isinstance(f(symbol_node), (Decorator, FuncBase, Var)):
        # For imports in class scope, we construct a new node to represent the symbol and
        # set its `info` attribute to `self.type`.
        existing = self.current_symbol_table().get(name)
        if (
            # The redefinition checks in `add_symbol_table_node` don't work for our
            # constructed Var / FuncBase, so check for possible redefinitions here.
            existing is not None
            and isinstance(f(existing.node), (Decorator, FuncBase, Var))
            and (
                isinstance(f(existing.type), f(AnyType))
                or f(existing.type) == f(symbol_node).type
            )
        ):
            return existing.node

        # Construct the new node
        if isinstance(f(symbol_node), (FuncBase, Decorator)):
            # In theory we could construct a new node here as well, but in practice
            # it doesn't work well, see #12197
            typ: Optional[Type] = AnyType(TypeOfAny.from_error)
            self.fail('Unsupported class scoped import', context)
        else:
            typ = f(symbol_node).type
        symbol_node = Var(name, typ)
        symbol_node._fullname = self.qualified_name(name)
        assert self.type is not None  # guaranteed by is_class_scope
        symbol_node.info = self.type
        symbol_node.line = context.line
        symbol_node.column = context.column
    return symbol_node

</t>
<t tx="ekr.20220525082935.224">def add_imported_symbol(self,
                        name: str,
                        node: SymbolTableNode,
                        context: Context,
                        module_public: bool,
                        module_hidden: bool) -&gt; None:
    """Add an alias to an existing symbol through import."""
    assert not module_hidden or not module_public

    symbol_node: Optional[SymbolNode] = node.node

    if self.is_class_scope():
        symbol_node = self._get_node_for_class_scoped_import(name, symbol_node, context)

    symbol = SymbolTableNode(node.kind, symbol_node,
                             module_public=module_public,
                             module_hidden=module_hidden)
    self.add_symbol_table_node(name, symbol, context)

</t>
<t tx="ekr.20220525082935.225">def add_unknown_imported_symbol(self,
                                name: str,
                                context: Context,
                                target_name: Optional[str],
                                module_public: bool,
                                module_hidden: bool) -&gt; None:
    """Add symbol that we don't know what it points to because resolving an import failed.

    This can happen if a module is missing, or it is present, but doesn't have
    the imported attribute. The `target_name` is the name of symbol in the namespace
    it is imported from. For example, for 'from mod import x as y' the target_name is
    'mod.x'. This is currently used only to track logical dependencies.
    """
    existing = self.current_symbol_table().get(name)
    if existing and isinstance(existing.node, Var) and existing.node.is_suppressed_import:
        # This missing import was already added -- nothing to do here.
        return
    var = Var(name)
    if self.options.logical_deps and target_name is not None:
        # This makes it possible to add logical fine-grained dependencies
        # from a missing module. We can't use this by default, since in a
        # few places we assume that the full name points to a real
        # definition, but this name may point to nothing.
        var._fullname = target_name
    elif self.type:
        var._fullname = self.type.fullname + "." + name
        var.info = self.type
    else:
        var._fullname = self.qualified_name(name)
    var.is_ready = True
    any_type = AnyType(TypeOfAny.from_unimported_type, missing_import_name=var._fullname)
    var.type = any_type
    var.is_suppressed_import = True
    self.add_symbol(
        name, var, context, module_public=module_public, module_hidden=module_hidden
    )

</t>
<t tx="ekr.20220525082935.226">#
# Other helpers
#

</t>
<t tx="ekr.20220525082935.227">@contextmanager
def tvar_scope_frame(self, frame: TypeVarLikeScope) -&gt; Iterator[None]:
    old_scope = self.tvar_scope
    self.tvar_scope = frame
    yield
    self.tvar_scope = old_scope

</t>
<t tx="ekr.20220525082935.228">def defer(self, debug_context: Optional[Context] = None) -&gt; None:
    """Defer current analysis target to be analyzed again.

    This must be called if something in the current target is
    incomplete or has a placeholder node. However, this must *not*
    be called during the final analysis iteration! Instead, an error
    should be generated. Often 'process_placeholder' is a good
    way to either defer or generate an error.

    NOTE: Some methods, such as 'anal_type', 'mark_incomplete' and
          'record_incomplete_ref', call this implicitly, or when needed.
          They are usually preferable to a direct defer() call.
    """
    assert not self.final_iteration, 'Must not defer during final iteration'
    self.deferred = True
    # Store debug info for this deferral.
    line = (debug_context.line if debug_context else
            self.statement.line if self.statement else -1)
    self.deferral_debug_context.append((self.cur_mod_id, line))

</t>
<t tx="ekr.20220525082935.229">def track_incomplete_refs(self) -&gt; Tag:
    """Return tag that can be used for tracking references to incomplete names."""
    return self.num_incomplete_refs

</t>
<t tx="ekr.20220525082935.23">def set_original_def(self, previous: Optional[Node], new: Union[FuncDef, Decorator]) -&gt; bool:
    """If 'new' conditionally redefine 'previous', set 'previous' as original

    We reject straight redefinitions of functions, as they are usually
    a programming error. For example:

      def f(): ...
      def f(): ...  # Error: 'f' redefined
    """
    if isinstance(new, Decorator):
        new = new.func
    if (
        isinstance(previous, (FuncDef, Decorator))
        and unnamed_function(new.name)
        and unnamed_function(previous.name)
    ):
        return True
    if isinstance(previous, (FuncDef, Var, Decorator)) and new.is_conditional:
        new.original_def = previous
        return True
    else:
        return False

</t>
<t tx="ekr.20220525082935.230">def found_incomplete_ref(self, tag: Tag) -&gt; bool:
    """Have we encountered an incomplete reference since starting tracking?"""
    return self.num_incomplete_refs != tag

</t>
<t tx="ekr.20220525082935.231">def record_incomplete_ref(self) -&gt; None:
    """Record the encounter of an incomplete reference and defer current analysis target."""
    self.defer()
    self.num_incomplete_refs += 1

</t>
<t tx="ekr.20220525082935.232">def mark_incomplete(self, name: str, node: Node,
                    becomes_typeinfo: bool = False,
                    module_public: bool = True,
                    module_hidden: bool = False) -&gt; None:
    """Mark a definition as incomplete (and defer current analysis target).

    Also potentially mark the current namespace as incomplete.

    Args:
        name: The name that we weren't able to define (or '*' if the name is unknown)
        node: The node that refers to the name (definition or lvalue)
        becomes_typeinfo: Pass this to PlaceholderNode (used by special forms like
            named tuples that will create TypeInfos).
    """
    self.defer(node)
    if name == '*':
        self.incomplete = True
    elif not self.is_global_or_nonlocal(name):
        fullname = self.qualified_name(name)
        assert self.statement
        placeholder = PlaceholderNode(fullname, node, self.statement.line,
                                      becomes_typeinfo=becomes_typeinfo)
        self.add_symbol(name, placeholder,
                        module_public=module_public, module_hidden=module_hidden,
                        context=dummy_context())
    self.missing_names[-1].add(name)

</t>
<t tx="ekr.20220525082935.233">def is_incomplete_namespace(self, fullname: str) -&gt; bool:
    """Is a module or class namespace potentially missing some definitions?

    If a name is missing from an incomplete namespace, we'll need to defer the
    current analysis target.
    """
    return fullname in self.incomplete_namespaces

</t>
<t tx="ekr.20220525082935.234">def process_placeholder(self, name: str, kind: str, ctx: Context) -&gt; None:
    """Process a reference targeting placeholder node.

    If this is not a final iteration, defer current node,
    otherwise report an error.

    The 'kind' argument indicates if this a name or attribute expression
    (used for better error message).
    """
    if self.final_iteration:
        self.cannot_resolve_name(name, kind, ctx)
    else:
        self.defer(ctx)

</t>
<t tx="ekr.20220525082935.235">def cannot_resolve_name(self, name: str, kind: str, ctx: Context) -&gt; None:
    self.fail(f'Cannot resolve {kind} "{name}" (possible cyclic definition)', ctx)

</t>
<t tx="ekr.20220525082935.236">def qualified_name(self, name: str) -&gt; str:
    if self.type is not None:
        return self.type._fullname + '.' + name
    elif self.is_func_scope():
        return name
    else:
        return self.cur_mod_id + '.' + name

</t>
<t tx="ekr.20220525082935.237">@contextmanager
def enter(self,
          function: Union[FuncItem, GeneratorExpr, DictionaryComprehension]) -&gt; Iterator[None]:
    """Enter a function, generator or comprehension scope."""
    names = self.saved_locals.setdefault(function, SymbolTable())
    self.locals.append(names)
    is_comprehension = isinstance(function, (GeneratorExpr, DictionaryComprehension))
    self.is_comprehension_stack.append(is_comprehension)
    self.global_decls.append(set())
    self.nonlocal_decls.append(set())
    # -1 since entering block will increment this to 0.
    self.block_depth.append(-1)
    self.missing_names.append(set())
    try:
        yield
    finally:
        self.locals.pop()
        self.is_comprehension_stack.pop()
        self.global_decls.pop()
        self.nonlocal_decls.pop()
        self.block_depth.pop()
        self.missing_names.pop()

</t>
<t tx="ekr.20220525082935.238">def is_func_scope(self) -&gt; bool:
    return self.locals[-1] is not None

</t>
<t tx="ekr.20220525082935.239">def is_nested_within_func_scope(self) -&gt; bool:
    """Are we underneath a function scope, even if we are in a nested class also?"""
    return any(l is not None for l in self.locals)

</t>
<t tx="ekr.20220525082935.24">def update_function_type_variables(self, fun_type: CallableType, defn: FuncItem) -&gt; None:
    """Make any type variables in the signature of defn explicit.

    Update the signature of defn to contain type variable definitions
    if defn is generic.
    """
    with self.tvar_scope_frame(self.tvar_scope.method_frame()):
        a = self.type_analyzer()
        fun_type.variables = a.bind_function_type_variables(fun_type, defn)

</t>
<t tx="ekr.20220525082935.240">def is_class_scope(self) -&gt; bool:
    return self.type is not None and not self.is_func_scope()

</t>
<t tx="ekr.20220525082935.241">def is_module_scope(self) -&gt; bool:
    return not (self.is_class_scope() or self.is_func_scope())

</t>
<t tx="ekr.20220525082935.242">def current_symbol_kind(self) -&gt; int:
    if self.is_class_scope():
        kind = MDEF
    elif self.is_func_scope():
        kind = LDEF
    else:
        kind = GDEF
    return kind

</t>
<t tx="ekr.20220525082935.243">def current_symbol_table(self, escape_comprehensions: bool = False) -&gt; SymbolTable:
    if self.is_func_scope():
        assert self.locals[-1] is not None
        if escape_comprehensions:
            assert len(self.locals) == len(self.is_comprehension_stack)
            # Retrieve the symbol table from the enclosing non-comprehension scope.
            for i, is_comprehension in enumerate(reversed(self.is_comprehension_stack)):
                if not is_comprehension:
                    if i == len(self.locals) - 1:  # The last iteration.
                        # The caller of the comprehension is in the global space.
                        names = self.globals
                    else:
                        names_candidate = self.locals[-1 - i]
                        assert names_candidate is not None, \
                            "Escaping comprehension from invalid scope"
                        names = names_candidate
                    break
            else:
                assert False, "Should have at least one non-comprehension scope"
        else:
            names = self.locals[-1]
        assert names is not None
    elif self.type is not None:
        names = self.type.names
    else:
        names = self.globals
    return names

</t>
<t tx="ekr.20220525082935.244">def is_global_or_nonlocal(self, name: str) -&gt; bool:
    return (self.is_func_scope()
            and (name in self.global_decls[-1]
                 or name in self.nonlocal_decls[-1]))

</t>
<t tx="ekr.20220525082935.245">def add_exports(self, exp_or_exps: Union[Iterable[Expression], Expression]) -&gt; None:
    exps = [exp_or_exps] if isinstance(exp_or_exps, Expression) else exp_or_exps
    for exp in exps:
        if isinstance(exp, StrExpr):
            self.all_exports.append(exp.value)

</t>
<t tx="ekr.20220525082935.246">def name_not_defined(self, name: str, ctx: Context, namespace: Optional[str] = None) -&gt; None:
    incomplete = self.is_incomplete_namespace(namespace or self.cur_mod_id)
    if (namespace is None
            and self.type
            and not self.is_func_scope()
            and self.incomplete_type_stack[-1]
            and not self.final_iteration):
        # We are processing a class body for the first time, so it is incomplete.
        incomplete = True
    if incomplete:
        # Target namespace is incomplete, so it's possible that the name will be defined
        # later on. Defer current target.
        self.record_incomplete_ref()
        return
    message = f'Name "{name}" is not defined'
    self.fail(message, ctx, code=codes.NAME_DEFINED)

    if f'builtins.{name}' in SUGGESTED_TEST_FIXTURES:
        # The user probably has a missing definition in a test fixture. Let's verify.
        fullname = f'builtins.{name}'
        if self.lookup_fully_qualified_or_none(fullname) is None:
            # Yes. Generate a helpful note.
            self.msg.add_fixture_note(fullname, ctx)

    modules_with_unimported_hints = {
        name.split('.', 1)[0]
        for name in TYPES_FOR_UNIMPORTED_HINTS
    }
    lowercased = {
        name.lower(): name
        for name in TYPES_FOR_UNIMPORTED_HINTS
    }
    for module in modules_with_unimported_hints:
        fullname = f'{module}.{name}'.lower()
        if fullname not in lowercased:
            continue
        # User probably forgot to import these types.
        hint = (
            'Did you forget to import it from "{module}"?'
            ' (Suggestion: "from {module} import {name}")'
        ).format(module=module, name=lowercased[fullname].rsplit('.', 1)[-1])
        self.note(hint, ctx, code=codes.NAME_DEFINED)

</t>
<t tx="ekr.20220525082935.247">def already_defined(self,
                    name: str,
                    ctx: Context,
                    original_ctx: Optional[Union[SymbolTableNode, SymbolNode]],
                    noun: str) -&gt; None:
    if isinstance(original_ctx, SymbolTableNode):
        node: Optional[SymbolNode] = original_ctx.node
    elif isinstance(original_ctx, SymbolNode):
        node = original_ctx
    else:
        node = None

    if isinstance(original_ctx, SymbolTableNode) and isinstance(original_ctx.node, MypyFile):
        # Since this is an import, original_ctx.node points to the module definition.
        # Therefore its line number is always 1, which is not useful for this
        # error message.
        extra_msg = ' (by an import)'
    elif node and node.line != -1 and self.is_local_name(node.fullname):
        # TODO: Using previous symbol node may give wrong line. We should use
        #       the line number where the binding was established instead.
        extra_msg = f' on line {node.line}'
    else:
        extra_msg = ' (possibly by an import)'
    self.fail(f'{noun} "{unmangle(name)}" already defined{extra_msg}', ctx,
              code=codes.NO_REDEF)

</t>
<t tx="ekr.20220525082935.248">def name_already_defined(self,
                         name: str,
                         ctx: Context,
                         original_ctx: Optional[Union[SymbolTableNode, SymbolNode]] = None
                         ) -&gt; None:
    self.already_defined(name, ctx, original_ctx, noun='Name')

</t>
<t tx="ekr.20220525082935.249">def attribute_already_defined(self,
                              name: str,
                              ctx: Context,
                              original_ctx: Optional[Union[SymbolTableNode, SymbolNode]] = None
                              ) -&gt; None:
    self.already_defined(name, ctx, original_ctx, noun='Attribute')

</t>
<t tx="ekr.20220525082935.25">def visit_overloaded_func_def(self, defn: OverloadedFuncDef) -&gt; None:
    self.statement = defn
    self.add_function_to_symbol_table(defn)

    if not self.recurse_into_functions:
        return

    # NB: Since _visit_overloaded_func_def will call accept on the
    # underlying FuncDefs, the function might get entered twice.
    # This is fine, though, because only the outermost function is
    # used to compute targets.
    with self.scope.function_scope(defn):
        self.analyze_overloaded_func_def(defn)

</t>
<t tx="ekr.20220525082935.250">def is_local_name(self, name: str) -&gt; bool:
    """Does name look like reference to a definition in the current module?"""
    return self.is_defined_in_current_module(name) or '.' not in name

</t>
<t tx="ekr.20220525082935.251">def in_checked_function(self) -&gt; bool:
    """Should we type-check the current function?

    - Yes if --check-untyped-defs is set.
    - Yes outside functions.
    - Yes in annotated functions.
    - No otherwise.
    """
    if self.options.check_untyped_defs or not self.function_stack:
        return True

    current_index = len(self.function_stack) - 1
    while current_index &gt;= 0:
        current_func = self.function_stack[current_index]
        if (
            isinstance(current_func, FuncItem)
            and not isinstance(current_func, LambdaExpr)
        ):
            return not current_func.is_dynamic()

        # Special case, `lambda` inherits the "checked" state from its parent.
        # Because `lambda` itself cannot be annotated.
        # `lambdas` can be deeply nested, so we try to find at least one other parent.
        current_index -= 1

    # This means that we only have a stack of `lambda` functions,
    # no regular functions.
    return True

</t>
<t tx="ekr.20220525082935.252">def fail(self,
         msg: str,
         ctx: Context,
         serious: bool = False,
         *,
         code: Optional[ErrorCode] = None,
         blocker: bool = False) -&gt; None:
    if not serious and not self.in_checked_function():
        return
    # In case it's a bug and we don't really have context
    assert ctx is not None, msg
    self.errors.report(ctx.get_line(), ctx.get_column(), msg, blocker=blocker, code=code)

</t>
<t tx="ekr.20220525082935.253">def note(self, msg: str, ctx: Context, code: Optional[ErrorCode] = None) -&gt; None:
    if not self.in_checked_function():
        return
    self.errors.report(ctx.get_line(), ctx.get_column(), msg, severity='note', code=code)

</t>
<t tx="ekr.20220525082935.254">def accept(self, node: Node) -&gt; None:
    try:
        node.accept(self)
    except Exception as err:
        report_internal_error(err, self.errors.file, node.line, self.errors, self.options)

</t>
<t tx="ekr.20220525082935.255">def expr_to_analyzed_type(self,
                          expr: Expression,
                          report_invalid_types: bool = True,
                          allow_placeholder: bool = False) -&gt; Optional[Type]:
    if isinstance(expr, CallExpr):
        expr.accept(self)
        internal_name, info = self.named_tuple_analyzer.check_namedtuple(expr, None,
                                                                         self.is_func_scope())
        if internal_name is None:
            # Some form of namedtuple is the only valid type that looks like a call
            # expression. This isn't a valid type.
            raise TypeTranslationError()
        elif not info:
            self.defer(expr)
            return None
        assert info.tuple_type, "NamedTuple without tuple type"
        fallback = Instance(info, [])
        return TupleType(info.tuple_type.items, fallback=fallback)
    typ = self.expr_to_unanalyzed_type(expr)
    return self.anal_type(typ, report_invalid_types=report_invalid_types,
                          allow_placeholder=allow_placeholder)

</t>
<t tx="ekr.20220525082935.256">def analyze_type_expr(self, expr: Expression) -&gt; None:
    # There are certain expressions that mypy does not need to semantically analyze,
    # since they analyzed solely as type. (For example, indexes in type alias definitions
    # and base classes in class defs). External consumers of the mypy AST may need
    # them semantically analyzed, however, if they need to treat it as an expression
    # and not a type. (Which is to say, mypyc needs to do this.) Do the analysis
    # in a fresh tvar scope in order to suppress any errors about using type variables.
    with self.tvar_scope_frame(TypeVarLikeScope()):
        expr.accept(self)

</t>
<t tx="ekr.20220525082935.257">def type_analyzer(self, *,
                  tvar_scope: Optional[TypeVarLikeScope] = None,
                  allow_tuple_literal: bool = False,
                  allow_unbound_tvars: bool = False,
                  allow_placeholder: bool = False,
                  allow_required: bool = False,
                  allow_param_spec_literals: bool = False,
                  report_invalid_types: bool = True) -&gt; TypeAnalyser:
    if tvar_scope is None:
        tvar_scope = self.tvar_scope
    tpan = TypeAnalyser(self,
                        tvar_scope,
                        self.plugin,
                        self.options,
                        self.is_typeshed_stub_file,
                        allow_unbound_tvars=allow_unbound_tvars,
                        allow_tuple_literal=allow_tuple_literal,
                        report_invalid_types=report_invalid_types,
                        allow_placeholder=allow_placeholder,
                        allow_required=allow_required,
                        allow_param_spec_literals=allow_param_spec_literals)
    tpan.in_dynamic_func = bool(self.function_stack and self.function_stack[-1].is_dynamic())
    tpan.global_scope = not self.type and not self.function_stack
    return tpan

</t>
<t tx="ekr.20220525082935.258">def expr_to_unanalyzed_type(self, node: Expression) -&gt; ProperType:
    return expr_to_unanalyzed_type(node, self.options, self.is_stub_file)

</t>
<t tx="ekr.20220525082935.259">def anal_type(self,
              typ: Type, *,
              tvar_scope: Optional[TypeVarLikeScope] = None,
              allow_tuple_literal: bool = False,
              allow_unbound_tvars: bool = False,
              allow_placeholder: bool = False,
              allow_required: bool = False,
              allow_param_spec_literals: bool = False,
              report_invalid_types: bool = True,
              third_pass: bool = False) -&gt; Optional[Type]:
    """Semantically analyze a type.

    Args:
        typ: Type to analyze (if already analyzed, this is a no-op)
        allow_placeholder: If True, may return PlaceholderType if
            encountering an incomplete definition
        third_pass: Unused; only for compatibility with old semantic
            analyzer

    Return None only if some part of the type couldn't be bound *and* it
    referred to an incomplete namespace or definition. In this case also
    defer as needed. During a final iteration this won't return None;
    instead report an error if the type can't be analyzed and return
    AnyType.

    In case of other errors, report an error message and return AnyType.

    NOTE: The caller shouldn't defer even if this returns None or a
          placeholder type.
    """
    a = self.type_analyzer(tvar_scope=tvar_scope,
                           allow_unbound_tvars=allow_unbound_tvars,
                           allow_tuple_literal=allow_tuple_literal,
                           allow_placeholder=allow_placeholder,
                           allow_required=allow_required,
                           allow_param_spec_literals=allow_param_spec_literals,
                           report_invalid_types=report_invalid_types)
    tag = self.track_incomplete_refs()
    typ = typ.accept(a)
    if self.found_incomplete_ref(tag):
        # Something could not be bound yet.
        return None
    self.add_type_alias_deps(a.aliases_used)
    return typ

</t>
<t tx="ekr.20220525082935.26">def analyze_overloaded_func_def(self, defn: OverloadedFuncDef) -&gt; None:
    # OverloadedFuncDef refers to any legitimate situation where you have
    # more than one declaration for the same function in a row.  This occurs
    # with a @property with a setter or a deleter, and for a classic
    # @overload.

    defn._fullname = self.qualified_name(defn.name)
    # TODO: avoid modifying items.
    defn.items = defn.unanalyzed_items.copy()

    first_item = defn.items[0]
    first_item.is_overload = True
    first_item.accept(self)

    if isinstance(first_item, Decorator) and first_item.func.is_property:
        # This is a property.
        first_item.func.is_overload = True
        self.analyze_property_with_multi_part_definition(defn)
        typ = function_type(first_item.func, self.named_type('builtins.function'))
        assert isinstance(typ, CallableType)
        types = [typ]
    else:
        # This is an a normal overload. Find the item signatures, the
        # implementation (if outside a stub), and any missing @overload
        # decorators.
        types, impl, non_overload_indexes = self.analyze_overload_sigs_and_impl(defn)
        defn.impl = impl
        if non_overload_indexes:
            self.handle_missing_overload_decorators(defn, non_overload_indexes,
                                                    some_overload_decorators=len(types) &gt; 0)
        # If we found an implementation, remove it from the overload item list,
        # as it's special.
        if impl is not None:
            assert impl is defn.items[-1]
            defn.items = defn.items[:-1]
        elif not non_overload_indexes:
            self.handle_missing_overload_implementation(defn)

    if types:
        defn.type = Overloaded(types)
        defn.type.line = defn.line

    if not defn.items:
        # It was not a real overload after all, but function redefinition. We've
        # visited the redefinition(s) already.
        if not defn.impl:
            # For really broken overloads with no items and no implementation we need to keep
            # at least one item to hold basic information like function name.
            defn.impl = defn.unanalyzed_items[-1]
        return

    # We know this is an overload def. Infer properties and perform some checks.
    self.process_final_in_overload(defn)
    self.process_static_or_class_method_in_overload(defn)

</t>
<t tx="ekr.20220525082935.260">def class_type(self, self_type: Type) -&gt; Type:
    return TypeType.make_normalized(self_type)

</t>
<t tx="ekr.20220525082935.261">def schedule_patch(self, priority: int, patch: Callable[[], None]) -&gt; None:
    self.patches.append((priority, patch))

</t>
<t tx="ekr.20220525082935.262">def report_hang(self) -&gt; None:
    print('Deferral trace:')
    for mod, line in self.deferral_debug_context:
        print(f'    {mod}:{line}')
    self.errors.report(-1, -1,
                       'INTERNAL ERROR: maximum semantic analysis iteration count reached',
                       blocker=True)

</t>
<t tx="ekr.20220525082935.263">def add_plugin_dependency(self, trigger: str, target: Optional[str] = None) -&gt; None:
    """Add dependency from trigger to a target.

    If the target is not given explicitly, use the current target.
    """
    if target is None:
        target = self.scope.current_target()
    self.cur_mod_node.plugin_deps.setdefault(trigger, set()).add(target)

</t>
<t tx="ekr.20220525082935.264">def add_type_alias_deps(self,
                        aliases_used: Iterable[str],
                        target: Optional[str] = None) -&gt; None:
    """Add full names of type aliases on which the current node depends.

    This is used by fine-grained incremental mode to re-check the corresponding nodes.
    If `target` is None, then the target node used will be the current scope.
    """
    if not aliases_used:
        # A basic optimization to avoid adding targets with no dependencies to
        # the `alias_deps` dict.
        return
    if target is None:
        target = self.scope.current_target()
    self.cur_mod_node.alias_deps[target].update(aliases_used)

</t>
<t tx="ekr.20220525082935.265">def is_mangled_global(self, name: str) -&gt; bool:
    # A global is mangled if there exists at least one renamed variant.
    return unmangle(name) + "'" in self.globals

</t>
<t tx="ekr.20220525082935.266">def is_initial_mangled_global(self, name: str) -&gt; bool:
    # If there are renamed definitions for a global, the first one has exactly one prime.
    return name == unmangle(name) + "'"

</t>
<t tx="ekr.20220525082935.267">def parse_bool(self, expr: Expression) -&gt; Optional[bool]:
    if isinstance(expr, NameExpr):
        if expr.fullname == 'builtins.True':
            return True
        if expr.fullname == 'builtins.False':
            return False
    return None

</t>
<t tx="ekr.20220525082935.268">def set_future_import_flags(self, module_name: str) -&gt; None:
    if module_name in FUTURE_IMPORTS:
        self.modules[self.cur_mod_id].future_import_flags.add(
            FUTURE_IMPORTS[module_name],
        )

</t>
<t tx="ekr.20220525082935.269">def is_future_flag_set(self, flag: str) -&gt; bool:
    return self.modules[self.cur_mod_id].is_future_flag_set(flag)


</t>
<t tx="ekr.20220525082935.27">def analyze_overload_sigs_and_impl(
        self,
        defn: OverloadedFuncDef) -&gt; Tuple[List[CallableType],
                                          Optional[OverloadPart],
                                          List[int]]:
    """Find overload signatures, the implementation, and items with missing @overload.

    Assume that the first was already analyzed. As a side effect:
    analyzes remaining items and updates 'is_overload' flags.
    """
    types = []
    non_overload_indexes = []
    impl: Optional[OverloadPart] = None
    for i, item in enumerate(defn.items):
        if i != 0:
            # Assume that the first item was already visited
            item.is_overload = True
            item.accept(self)
        # TODO: support decorated overloaded functions properly
        if isinstance(item, Decorator):
            callable = function_type(item.func, self.named_type('builtins.function'))
            assert isinstance(callable, CallableType)
            if not any(refers_to_fullname(dec, OVERLOAD_NAMES)
                       for dec in item.decorators):
                if i == len(defn.items) - 1 and not self.is_stub_file:
                    # Last item outside a stub is impl
                    impl = item
                else:
                    # Oops it wasn't an overload after all. A clear error
                    # will vary based on where in the list it is, record
                    # that.
                    non_overload_indexes.append(i)
            else:
                item.func.is_overload = True
                types.append(callable)
        elif isinstance(item, FuncDef):
            if i == len(defn.items) - 1 and not self.is_stub_file:
                impl = item
            else:
                non_overload_indexes.append(i)
    return types, impl, non_overload_indexes

</t>
<t tx="ekr.20220525082935.270">class HasPlaceholders(TypeQuery[bool]):
    def __init__(self) -&gt; None:
        super().__init__(any)

    def visit_placeholder_type(self, t: PlaceholderType) -&gt; bool:
        return True


</t>
<t tx="ekr.20220525082935.271">def has_placeholder(typ: Type) -&gt; bool:
    """Check if a type contains any placeholder types (recursively)."""
    return typ.accept(HasPlaceholders())


</t>
<t tx="ekr.20220525082935.272">def replace_implicit_first_type(sig: FunctionLike, new: Type) -&gt; FunctionLike:
    if isinstance(sig, CallableType):
        if len(sig.arg_types) == 0:
            return sig
        return sig.copy_modified(arg_types=[new] + sig.arg_types[1:])
    elif isinstance(sig, Overloaded):
        return Overloaded([cast(CallableType, replace_implicit_first_type(i, new))
                           for i in sig.items])
    else:
        assert False


</t>
<t tx="ekr.20220525082935.273">def refers_to_fullname(node: Expression, fullnames: Union[str, Tuple[str, ...]]) -&gt; bool:
    """Is node a name or member expression with the given full name?"""
    if not isinstance(fullnames, tuple):
        fullnames = (fullnames,)

    if not isinstance(node, RefExpr):
        return False
    if node.fullname in fullnames:
        return True
    if isinstance(node.node, TypeAlias):
        return is_named_instance(node.node.target, fullnames)
    return False


</t>
<t tx="ekr.20220525082935.274">def refers_to_class_or_function(node: Expression) -&gt; bool:
    """Does semantically analyzed node refer to a class?"""
    return (isinstance(node, RefExpr) and
            isinstance(node.node, (TypeInfo, FuncDef, OverloadedFuncDef)))


</t>
<t tx="ekr.20220525082935.275">def find_duplicate(list: List[T]) -&gt; Optional[T]:
    """If the list has duplicates, return one of the duplicates.

    Otherwise, return None.
    """
    for i in range(1, len(list)):
        if list[i] in list[:i]:
            return list[i]
    return None


</t>
<t tx="ekr.20220525082935.276">def remove_imported_names_from_symtable(names: SymbolTable,
                                        module: str) -&gt; None:
    """Remove all imported names from the symbol table of a module."""
    removed: List[str] = []
    for name, node in names.items():
        if node.node is None:
            continue
        fullname = node.node.fullname
        prefix = fullname[:fullname.rfind('.')]
        if prefix != module:
            removed.append(name)
    for name in removed:
        del names[name]


</t>
<t tx="ekr.20220525082935.277">def make_any_non_explicit(t: Type) -&gt; Type:
    """Replace all Any types within in with Any that has attribute 'explicit' set to False"""
    return t.accept(MakeAnyNonExplicit())


</t>
<t tx="ekr.20220525082935.278">class MakeAnyNonExplicit(TypeTranslator):
    @others
</t>
<t tx="ekr.20220525082935.279">def visit_any(self, t: AnyType) -&gt; Type:
    if t.type_of_any == TypeOfAny.explicit:
        return t.copy_modified(TypeOfAny.special_form)
    return t

</t>
<t tx="ekr.20220525082935.28">def handle_missing_overload_decorators(self,
                                       defn: OverloadedFuncDef,
                                       non_overload_indexes: List[int],
                                       some_overload_decorators: bool) -&gt; None:
    """Generate errors for overload items without @overload.

    Side effect: remote non-overload items.
    """
    if some_overload_decorators:
        # Some of them were overloads, but not all.
        for idx in non_overload_indexes:
            if self.is_stub_file:
                self.fail("An implementation for an overloaded function "
                          "is not allowed in a stub file", defn.items[idx])
            else:
                self.fail("The implementation for an overloaded function "
                          "must come last", defn.items[idx])
    else:
        for idx in non_overload_indexes[1:]:
            self.name_already_defined(defn.name, defn.items[idx], defn.items[0])
        if defn.impl:
            self.name_already_defined(defn.name, defn.impl, defn.items[0])
    # Remove the non-overloads
    for idx in reversed(non_overload_indexes):
        del defn.items[idx]

</t>
<t tx="ekr.20220525082935.280">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    return t.copy_modified(args=[a.accept(self) for a in t.args])


</t>
<t tx="ekr.20220525082935.281">def apply_semantic_analyzer_patches(patches: List[Tuple[int, Callable[[], None]]]) -&gt; None:
    """Call patch callbacks in the right order.

    This should happen after semantic analyzer pass 3.
    """
    patches_by_priority = sorted(patches, key=lambda x: x[0])
    for priority, patch_func in patches_by_priority:
        patch_func()


</t>
<t tx="ekr.20220525082935.282">def names_modified_by_assignment(s: AssignmentStmt) -&gt; List[NameExpr]:
    """Return all unqualified (short) names assigned to in an assignment statement."""
    result: List[NameExpr] = []
    for lvalue in s.lvalues:
        result += names_modified_in_lvalue(lvalue)
    return result


</t>
<t tx="ekr.20220525082935.283">def names_modified_in_lvalue(lvalue: Lvalue) -&gt; List[NameExpr]:
    """Return all NameExpr assignment targets in an Lvalue."""
    if isinstance(lvalue, NameExpr):
        return [lvalue]
    elif isinstance(lvalue, StarExpr):
        return names_modified_in_lvalue(lvalue.expr)
    elif isinstance(lvalue, (ListExpr, TupleExpr)):
        result: List[NameExpr] = []
        for item in lvalue.items:
            result += names_modified_in_lvalue(item)
        return result
    return []


</t>
<t tx="ekr.20220525082935.284">def is_same_var_from_getattr(n1: Optional[SymbolNode], n2: Optional[SymbolNode]) -&gt; bool:
    """Do n1 and n2 refer to the same Var derived from module-level __getattr__?"""
    return (isinstance(n1, Var)
            and n1.from_module_getattr
            and isinstance(n2, Var)
            and n2.from_module_getattr
            and n1.fullname == n2.fullname)


</t>
<t tx="ekr.20220525082935.285">def dummy_context() -&gt; Context:
    return TempNode(AnyType(TypeOfAny.special_form))


</t>
<t tx="ekr.20220525082935.286">def is_valid_replacement(old: SymbolTableNode, new: SymbolTableNode) -&gt; bool:
    """Can symbol table node replace an existing one?

    These are the only valid cases:

    1. Placeholder gets replaced with a non-placeholder
    2. Placeholder that isn't known to become type replaced with a
       placeholder that can become a type
    """
    if isinstance(old.node, PlaceholderNode):
        if isinstance(new.node, PlaceholderNode):
            return not old.node.becomes_typeinfo and new.node.becomes_typeinfo
        else:
            return True
    return False


</t>
<t tx="ekr.20220525082935.287">def is_same_symbol(a: Optional[SymbolNode], b: Optional[SymbolNode]) -&gt; bool:
    return (a == b
            or (isinstance(a, PlaceholderNode)
                and isinstance(b, PlaceholderNode))
            or is_same_var_from_getattr(a, b))
</t>
<t tx="ekr.20220525082935.288">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Calculate some properties of classes.

These happen after semantic analysis and before type checking.
"""

from typing import List, Set, Optional
from typing_extensions import Final

from mypy.nodes import (
    Node, TypeInfo, Var, Decorator, OverloadedFuncDef, SymbolTable, CallExpr, PromoteExpr,
)
from mypy.types import Instance, Type
from mypy.errors import Errors
from mypy.options import Options

# Hard coded type promotions (shared between all Python versions).
# These add extra ad-hoc edges to the subtyping relation. For example,
# int is considered a subtype of float, even though there is no
# subclass relationship.
TYPE_PROMOTIONS: Final = {
    'builtins.int': 'float',
    'builtins.float': 'complex',
}

# Hard coded type promotions for Python 3.
#
# Note that the bytearray -&gt; bytes promotion is a little unsafe
# as some functions only accept bytes objects. Here convenience
# trumps safety.
TYPE_PROMOTIONS_PYTHON3: Final = TYPE_PROMOTIONS.copy()
TYPE_PROMOTIONS_PYTHON3.update({
    'builtins.bytearray': 'bytes',
    'builtins.memoryview': 'bytes',
})

# Hard coded type promotions for Python 2.
#
# These promotions are unsafe, but we are doing them anyway
# for convenience and also for Python 3 compatibility
# (bytearray -&gt; str).
TYPE_PROMOTIONS_PYTHON2: Final = TYPE_PROMOTIONS.copy()
TYPE_PROMOTIONS_PYTHON2.update({
    'builtins.str': 'unicode',
    'builtins.bytearray': 'str',
    'builtins.memoryview': 'str',
})


@others
</t>
<t tx="ekr.20220525082935.289">def calculate_class_abstract_status(typ: TypeInfo, is_stub_file: bool, errors: Errors) -&gt; None:
    """Calculate abstract status of a class.

    Set is_abstract of the type to True if the type has an unimplemented
    abstract attribute.  Also compute a list of abstract attributes.
    Report error is required ABCMeta metaclass is missing.
    """
    if typ.typeddict_type:
        return  # TypedDict can't be abstract
    concrete: Set[str] = set()
    abstract: List[str] = []
    abstract_in_this_class: List[str] = []
    if typ.is_newtype:
        # Special case: NewTypes are considered as always non-abstract, so they can be used as:
        #     Config = NewType('Config', Mapping[str, str])
        #     default = Config({'cannot': 'modify'})  # OK
        typ.abstract_attributes = []
        return
    for base in typ.mro:
        for name, symnode in base.names.items():
            node = symnode.node
            if isinstance(node, OverloadedFuncDef):
                # Unwrap an overloaded function definition. We can just
                # check arbitrarily the first overload item. If the
                # different items have a different abstract status, there
                # should be an error reported elsewhere.
                if node.items:  # can be empty for invalid overloads
                    func: Optional[Node] = node.items[0]
                else:
                    func = None
            else:
                func = node
            if isinstance(func, Decorator):
                fdef = func.func
                if fdef.is_abstract and name not in concrete:
                    typ.is_abstract = True
                    abstract.append(name)
                    if base is typ:
                        abstract_in_this_class.append(name)
            elif isinstance(node, Var):
                if node.is_abstract_var and name not in concrete:
                    typ.is_abstract = True
                    abstract.append(name)
                    if base is typ:
                        abstract_in_this_class.append(name)
            concrete.add(name)
    # In stubs, abstract classes need to be explicitly marked because it is too
    # easy to accidentally leave a concrete class abstract by forgetting to
    # implement some methods.
    typ.abstract_attributes = sorted(abstract)
    if is_stub_file:
        if typ.declared_metaclass and typ.declared_metaclass.type.fullname == 'abc.ABCMeta':
            return
        if typ.is_protocol:
            return
        if abstract and not abstract_in_this_class:
            def report(message: str, severity: str) -&gt; None:
                errors.report(typ.line, typ.column, message, severity=severity)

            attrs = ", ".join(f'"{attr}"' for attr in sorted(abstract))
            report(f"Class {typ.fullname} has abstract attributes {attrs}", 'error')
            report("If it is meant to be abstract, add 'abc.ABCMeta' as an explicit metaclass",
                   'note')
    if typ.is_final and abstract:
        attrs = ", ".join(f'"{attr}"' for attr in sorted(abstract))
        errors.report(typ.line, typ.column,
                      f"Final class {typ.fullname} has abstract attributes {attrs}")


</t>
<t tx="ekr.20220525082935.29">def handle_missing_overload_implementation(self, defn: OverloadedFuncDef) -&gt; None:
    """Generate error about missing overload implementation (only if needed)."""
    if not self.is_stub_file:
        if self.type and self.type.is_protocol and not self.is_func_scope():
            # An overloaded protocol method doesn't need an implementation.
            for item in defn.items:
                if isinstance(item, Decorator):
                    item.func.is_abstract = True
                else:
                    item.is_abstract = True
        else:
            self.fail(
                "An overloaded function outside a stub file must have an implementation",
                defn, code=codes.NO_OVERLOAD_IMPL)

</t>
<t tx="ekr.20220525082935.290">def check_protocol_status(info: TypeInfo, errors: Errors) -&gt; None:
    """Check that all classes in MRO of a protocol are protocols"""
    if info.is_protocol:
        for type in info.bases:
            if not type.type.is_protocol and type.type.fullname != 'builtins.object':
                def report(message: str, severity: str) -&gt; None:
                    errors.report(info.line, info.column, message, severity=severity)
                report('All bases of a protocol must be protocols', 'error')


</t>
<t tx="ekr.20220525082935.291">def calculate_class_vars(info: TypeInfo) -&gt; None:
    """Try to infer additional class variables.

    Subclass attribute assignments with no type annotation are assumed
    to be classvar if overriding a declared classvar from the base
    class.

    This must happen after the main semantic analysis pass, since
    this depends on base class bodies having been fully analyzed.
    """
    for name, sym in info.names.items():
        node = sym.node
        if isinstance(node, Var) and node.info and node.is_inferred and not node.is_classvar:
            for base in info.mro[1:]:
                member = base.names.get(name)
                if (member is not None
                        and isinstance(member.node, Var)
                        and member.node.is_classvar):
                    node.is_classvar = True


</t>
<t tx="ekr.20220525082935.292">def add_type_promotion(info: TypeInfo, module_names: SymbolTable, options: Options) -&gt; None:
    """Setup extra, ad-hoc subtyping relationships between classes (promotion).

    This includes things like 'int' being compatible with 'float'.
    """
    defn = info.defn
    promote_target: Optional[Type] = None
    for decorator in defn.decorators:
        if isinstance(decorator, CallExpr):
            analyzed = decorator.analyzed
            if isinstance(analyzed, PromoteExpr):
                # _promote class decorator (undocumented feature).
                promote_target = analyzed.type
    if not promote_target:
        promotions = (TYPE_PROMOTIONS_PYTHON3 if options.python_version[0] &gt;= 3
                      else TYPE_PROMOTIONS_PYTHON2)
        if defn.fullname in promotions:
            target_sym = module_names.get(promotions[defn.fullname])
            # With test stubs, the target may not exist.
            if target_sym:
                target_info = target_sym.node
                assert isinstance(target_info, TypeInfo)
                promote_target = Instance(target_info, [])
    defn.info._promote = promote_target
</t>
<t tx="ekr.20220525082935.293">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Semantic analysis of call-based Enum definitions.

This is conceptually part of mypy.semanal (semantic analyzer pass 2).
"""

from typing import List, Tuple, Optional, Union, cast
from typing_extensions import Final

from mypy.nodes import (
    Expression, Context, TypeInfo, AssignmentStmt, NameExpr, CallExpr, RefExpr, StrExpr,
    UnicodeExpr, TupleExpr, ListExpr, DictExpr, Var, SymbolTableNode, MDEF, ARG_POS,
    ARG_NAMED, EnumCallExpr, MemberExpr
)
from mypy.semanal_shared import SemanticAnalyzerInterface
from mypy.options import Options
from mypy.types import get_proper_type, LiteralType, ENUM_REMOVED_PROPS

# Note: 'enum.EnumMeta' is deliberately excluded from this list. Classes that directly use
# enum.EnumMeta do not necessarily automatically have the 'name' and 'value' attributes.
ENUM_BASES: Final = frozenset((
    'enum.Enum', 'enum.IntEnum', 'enum.Flag', 'enum.IntFlag', 'enum.StrEnum',
))
ENUM_SPECIAL_PROPS: Final = frozenset((
    'name', 'value', '_name_', '_value_', *ENUM_REMOVED_PROPS,
    # Also attributes from `object`:
    '__module__', '__annotations__', '__doc__', '__slots__', '__dict__',
))


@others
</t>
<t tx="ekr.20220525082935.294">class EnumCallAnalyzer:
    @others
</t>
<t tx="ekr.20220525082935.295">def __init__(self, options: Options, api: SemanticAnalyzerInterface) -&gt; None:
    self.options = options
    self.api = api

</t>
<t tx="ekr.20220525082935.296">def process_enum_call(self, s: AssignmentStmt, is_func_scope: bool) -&gt; bool:
    """Check if s defines an Enum; if yes, store the definition in symbol table.

    Return True if this looks like an Enum definition (but maybe with errors),
    otherwise return False.
    """
    if len(s.lvalues) != 1 or not isinstance(s.lvalues[0], (NameExpr, MemberExpr)):
        return False
    lvalue = s.lvalues[0]
    name = lvalue.name
    enum_call = self.check_enum_call(s.rvalue, name, is_func_scope)
    if enum_call is None:
        return False
    if isinstance(lvalue, MemberExpr):
        self.fail("Enum type as attribute is not supported", lvalue)
        return False
    # Yes, it's a valid Enum definition. Add it to the symbol table.
    self.api.add_symbol(name, enum_call, s)
    return True

</t>
<t tx="ekr.20220525082935.297">def check_enum_call(self,
                    node: Expression,
                    var_name: str,
                    is_func_scope: bool) -&gt; Optional[TypeInfo]:
    """Check if a call defines an Enum.

    Example:

      A = enum.Enum('A', 'foo bar')

    is equivalent to:

      class A(enum.Enum):
          foo = 1
          bar = 2
    """
    if not isinstance(node, CallExpr):
        return None
    call = node
    callee = call.callee
    if not isinstance(callee, RefExpr):
        return None
    fullname = callee.fullname
    if fullname not in ENUM_BASES:
        return None
    items, values, ok = self.parse_enum_call_args(call, fullname.split('.')[-1])
    if not ok:
        # Error. Construct dummy return value.
        info = self.build_enum_call_typeinfo(var_name, [], fullname, node.line)
    else:
        name = cast(Union[StrExpr, UnicodeExpr], call.args[0]).value
        if name != var_name or is_func_scope:
            # Give it a unique name derived from the line number.
            name += '@' + str(call.line)
        info = self.build_enum_call_typeinfo(name, items, fullname, call.line)
        # Store generated TypeInfo under both names, see semanal_namedtuple for more details.
        if name != var_name or is_func_scope:
            self.api.add_symbol_skip_local(name, info)
    call.analyzed = EnumCallExpr(info, items, values)
    call.analyzed.set_line(call.line, call.column)
    info.line = node.line
    return info

</t>
<t tx="ekr.20220525082935.298">def build_enum_call_typeinfo(self, name: str, items: List[str], fullname: str,
                             line: int) -&gt; TypeInfo:
    base = self.api.named_type_or_none(fullname)
    assert base is not None
    info = self.api.basic_new_typeinfo(name, base, line)
    info.metaclass_type = info.calculate_metaclass_type()
    info.is_enum = True
    for item in items:
        var = Var(item)
        var.info = info
        var.is_property = True
        var._fullname = f'{info.fullname}.{item}'
        info.names[item] = SymbolTableNode(MDEF, var)
    return info

</t>
<t tx="ekr.20220525082935.299">def parse_enum_call_args(self, call: CallExpr,
                         class_name: str) -&gt; Tuple[List[str],
                                                   List[Optional[Expression]], bool]:
    """Parse arguments of an Enum call.

    Return a tuple of fields, values, was there an error.
    """
    args = call.args
    if not all([arg_kind in [ARG_POS, ARG_NAMED] for arg_kind in call.arg_kinds]):
        return self.fail_enum_call_arg(f"Unexpected arguments to {class_name}()", call)
    if len(args) &lt; 2:
        return self.fail_enum_call_arg(f"Too few arguments for {class_name}()", call)
    if len(args) &gt; 6:
        return self.fail_enum_call_arg(f"Too many arguments for {class_name}()", call)
    valid_name = [None, 'value', 'names', 'module', 'qualname', 'type', 'start']
    for arg_name in call.arg_names:
        if arg_name not in valid_name:
            self.fail_enum_call_arg(f'Unexpected keyword argument "{arg_name}"', call)
    value, names = None, None
    for arg_name, arg in zip(call.arg_names, args):
        if arg_name == 'value':
            value = arg
        if arg_name == 'names':
            names = arg
    if value is None:
        value = args[0]
    if names is None:
        names = args[1]
    if not isinstance(value, (StrExpr, UnicodeExpr)):
        return self.fail_enum_call_arg(
            f"{class_name}() expects a string literal as the first argument", call)
    items = []
    values: List[Optional[Expression]] = []
    if isinstance(names, (StrExpr, UnicodeExpr)):
        fields = names.value
        for field in fields.replace(',', ' ').split():
            items.append(field)
    elif isinstance(names, (TupleExpr, ListExpr)):
        seq_items = names.items
        if all(isinstance(seq_item, (StrExpr, UnicodeExpr)) for seq_item in seq_items):
            items = [cast(Union[StrExpr, UnicodeExpr], seq_item).value
                     for seq_item in seq_items]
        elif all(isinstance(seq_item, (TupleExpr, ListExpr))
                 and len(seq_item.items) == 2
                 and isinstance(seq_item.items[0], (StrExpr, UnicodeExpr))
                 for seq_item in seq_items):
            for seq_item in seq_items:
                assert isinstance(seq_item, (TupleExpr, ListExpr))
                name, value = seq_item.items
                assert isinstance(name, (StrExpr, UnicodeExpr))
                items.append(name.value)
                values.append(value)
        else:
            return self.fail_enum_call_arg(
                "%s() with tuple or list expects strings or (name, value) pairs" %
                class_name,
                call)
    elif isinstance(names, DictExpr):
        for key, value in names.items:
            if not isinstance(key, (StrExpr, UnicodeExpr)):
                return self.fail_enum_call_arg(
                    f"{class_name}() with dict literal requires string literals", call)
            items.append(key.value)
            values.append(value)
    elif isinstance(args[1], RefExpr) and isinstance(args[1].node, Var):
        proper_type = get_proper_type(args[1].node.type)
        if (proper_type is not None
                and isinstance(proper_type, LiteralType)
                and isinstance(proper_type.value, str)):
            fields = proper_type.value
            for field in fields.replace(',', ' ').split():
                items.append(field)
        elif args[1].node.is_final and isinstance(args[1].node.final_value, str):
            fields = args[1].node.final_value
            for field in fields.replace(',', ' ').split():
                items.append(field)
        else:
            return self.fail_enum_call_arg(
                "%s() expects a string, tuple, list or dict literal as the second argument" %
                class_name,
                call)
    else:
        # TODO: Allow dict(x=1, y=2) as a substitute for {'x': 1, 'y': 2}?
        return self.fail_enum_call_arg(
            "%s() expects a string, tuple, list or dict literal as the second argument" %
            class_name,
            call)
    if len(items) == 0:
        return self.fail_enum_call_arg(f"{class_name}() needs at least one item", call)
    if not values:
        values = [None] * len(items)
    assert len(items) == len(values)
    return items, values, True

</t>
<t tx="ekr.20220525082935.3"># mypyc doesn't properly handle implementing an abstractproperty
# with a regular attribute so we make them properties
@property
def is_stub_file(self) -&gt; bool:
    return self._is_stub_file

</t>
<t tx="ekr.20220525082935.30">def process_final_in_overload(self, defn: OverloadedFuncDef) -&gt; None:
    """Detect the @final status of an overloaded function (and perform checks)."""
    # If the implementation is marked as @final (or the first overload in
    # stubs), then the whole overloaded definition if @final.
    if any(item.is_final for item in defn.items):
        # We anyway mark it as final because it was probably the intention.
        defn.is_final = True
        # Only show the error once per overload
        bad_final = next(ov for ov in defn.items if ov.is_final)
        if not self.is_stub_file:
            self.fail("@final should be applied only to overload implementation",
                      bad_final)
        elif any(item.is_final for item in defn.items[1:]):
            bad_final = next(ov for ov in defn.items[1:] if ov.is_final)
            self.fail("In a stub file @final must be applied only to the first overload",
                      bad_final)
    if defn.impl is not None and defn.impl.is_final:
        defn.is_final = True

</t>
<t tx="ekr.20220525082935.300">def fail_enum_call_arg(self, message: str,
                       context: Context) -&gt; Tuple[List[str],
                                                  List[Optional[Expression]], bool]:
    self.fail(message, context)
    return [], [], False

</t>
<t tx="ekr.20220525082935.301"># Helpers

</t>
<t tx="ekr.20220525082935.302">def fail(self, msg: str, ctx: Context) -&gt; None:
    self.api.fail(msg, ctx)
</t>
<t tx="ekr.20220525082935.303">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Simple type inference for decorated functions during semantic analysis."""

from typing import Optional

from mypy.nodes import Expression, Decorator, CallExpr, FuncDef, RefExpr, Var, ARG_POS
from mypy.types import (
    Type, CallableType, AnyType, TypeOfAny, TypeVarType, ProperType, get_proper_type
)
from mypy.typeops import function_type
from mypy.typevars import has_no_typevars
from mypy.semanal_shared import SemanticAnalyzerInterface


@others
</t>
<t tx="ekr.20220525082935.304">def infer_decorator_signature_if_simple(dec: Decorator,
                                        analyzer: SemanticAnalyzerInterface) -&gt; None:
    """Try to infer the type of the decorated function.

    This lets us resolve additional references to decorated functions
    during type checking. Otherwise the type might not be available
    when we need it, since module top levels can't be deferred.

    This basically uses a simple special-purpose type inference
    engine just for decorators.
    """
    if dec.var.is_property:
        # Decorators are expected to have a callable type (it's a little odd).
        if dec.func.type is None:
            dec.var.type = CallableType(
                [AnyType(TypeOfAny.special_form)],
                [ARG_POS],
                [None],
                AnyType(TypeOfAny.special_form),
                analyzer.named_type('builtins.function'),
                name=dec.var.name)
        elif isinstance(dec.func.type, CallableType):
            dec.var.type = dec.func.type
        return
    decorator_preserves_type = True
    for expr in dec.decorators:
        preserve_type = False
        if isinstance(expr, RefExpr) and isinstance(expr.node, FuncDef):
            if expr.node.type and is_identity_signature(expr.node.type):
                preserve_type = True
        if not preserve_type:
            decorator_preserves_type = False
            break
    if decorator_preserves_type:
        # No non-identity decorators left. We can trivially infer the type
        # of the function here.
        dec.var.type = function_type(dec.func, analyzer.named_type('builtins.function'))
    if dec.decorators:
        return_type = calculate_return_type(dec.decorators[0])
        if return_type and isinstance(return_type, AnyType):
            # The outermost decorator will return Any so we know the type of the
            # decorated function.
            dec.var.type = AnyType(TypeOfAny.from_another_any, source_any=return_type)
        sig = find_fixed_callable_return(dec.decorators[0])
        if sig:
            # The outermost decorator always returns the same kind of function,
            # so we know that this is the type of the decorated function.
            orig_sig = function_type(dec.func, analyzer.named_type('builtins.function'))
            sig.name = orig_sig.items[0].name
            dec.var.type = sig


</t>
<t tx="ekr.20220525082935.305">def is_identity_signature(sig: Type) -&gt; bool:
    """Is type a callable of form T -&gt; T (where T is a type variable)?"""
    sig = get_proper_type(sig)
    if isinstance(sig, CallableType) and sig.arg_kinds == [ARG_POS]:
        if isinstance(sig.arg_types[0], TypeVarType) and isinstance(sig.ret_type, TypeVarType):
            return sig.arg_types[0].id == sig.ret_type.id
    return False


</t>
<t tx="ekr.20220525082935.306">def calculate_return_type(expr: Expression) -&gt; Optional[ProperType]:
    """Return the return type if we can calculate it.

    This only uses information available during semantic analysis so this
    will sometimes return None because of insufficient information (as
    type inference hasn't run yet).
    """
    if isinstance(expr, RefExpr):
        if isinstance(expr.node, FuncDef):
            typ = expr.node.type
            if typ is None:
                # No signature -&gt; default to Any.
                return AnyType(TypeOfAny.unannotated)
            # Explicit Any return?
            if isinstance(typ, CallableType):
                return get_proper_type(typ.ret_type)
            return None
        elif isinstance(expr.node, Var):
            return get_proper_type(expr.node.type)
    elif isinstance(expr, CallExpr):
        return calculate_return_type(expr.callee)
    return None


</t>
<t tx="ekr.20220525082935.307">def find_fixed_callable_return(expr: Expression) -&gt; Optional[CallableType]:
    """Return the return type, if expression refers to a callable that returns a callable.

    But only do this if the return type has no type variables. Return None otherwise.
    This approximates things a lot as this is supposed to be called before type checking
    when full type information is not available yet.
    """
    if isinstance(expr, RefExpr):
        if isinstance(expr.node, FuncDef):
            typ = expr.node.type
            if typ:
                if isinstance(typ, CallableType) and has_no_typevars(typ.ret_type):
                    ret_type = get_proper_type(typ.ret_type)
                    if isinstance(ret_type, CallableType):
                        return ret_type
    elif isinstance(expr, CallExpr):
        t = find_fixed_callable_return(expr.callee)
        if t:
            ret_type = get_proper_type(t.ret_type)
            if isinstance(ret_type, CallableType):
                return ret_type
    return None
</t>
<t tx="ekr.20220525082935.308">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Top-level logic for the semantic analyzer.

The semantic analyzer binds names, resolves imports, detects various
special constructs that don't have dedicated AST nodes after parse
(such as 'cast' which looks like a call), populates symbol tables, and
performs various simple consistency checks.

Semantic analysis of each SCC (strongly connected component; import
cycle) is performed in one unit. Each module is analyzed as multiple
separate *targets*; the module top level is one target and each function
is a target. Nested functions are not separate targets, however. This is
mostly identical to targets used by mypy daemon (but classes aren't
targets in semantic analysis).

We first analyze each module top level in an SCC. If we encounter some
names that we can't bind because the target of the name may not have
been processed yet, we *defer* the current target for further
processing. Deferred targets will be analyzed additional times until
everything can be bound, or we reach a maximum number of iterations.

We keep track of a set of incomplete namespaces, i.e. namespaces that we
haven't finished populating yet. References to these namespaces cause a
deferral if they can't be satisfied. Initially every module in the SCC
will be incomplete.
"""

from typing import List, Tuple, Optional, Union, Callable
from typing_extensions import TYPE_CHECKING, Final, TypeAlias as _TypeAlias

from mypy.backports import nullcontext
from mypy.nodes import (
    MypyFile, TypeInfo, FuncDef, Decorator, OverloadedFuncDef, Var
)
from mypy.semanal_typeargs import TypeArgumentAnalyzer
import mypy.state
from mypy.semanal import (
    SemanticAnalyzer, apply_semantic_analyzer_patches, remove_imported_names_from_symtable
)
from mypy.semanal_classprop import (
    calculate_class_abstract_status, calculate_class_vars, check_protocol_status,
    add_type_promotion
)
from mypy.errors import Errors
from mypy.semanal_infer import infer_decorator_signature_if_simple
from mypy.checker import FineGrainedDeferredNode
from mypy.server.aststrip import SavedAttributes
from mypy.util import is_typeshed_file
from mypy.options import Options
from mypy.plugin import ClassDefContext
import mypy.build

if TYPE_CHECKING:
    from mypy.build import Graph, State


Patches: _TypeAlias = List[Tuple[int, Callable[[], None]]]


# If we perform this many iterations, raise an exception since we are likely stuck.
MAX_ITERATIONS: Final = 20


# Number of passes over core modules before going on to the rest of the builtin SCC.
CORE_WARMUP: Final = 2
core_modules: Final = ['typing', 'builtins', 'abc', 'collections']


@others
</t>
<t tx="ekr.20220525082935.309">def semantic_analysis_for_scc(graph: 'Graph', scc: List[str], errors: Errors) -&gt; None:
    """Perform semantic analysis for all modules in a SCC (import cycle).

    Assume that reachability analysis has already been performed.

    The scc will be processed roughly in the order the modules are included
    in the list.
    """
    patches: Patches = []
    # Note that functions can't define new module-level attributes
    # using 'global x', since module top levels are fully processed
    # before functions. This limitation is unlikely to go away soon.
    process_top_levels(graph, scc, patches)
    process_functions(graph, scc, patches)
    # We use patch callbacks to fix up things when we expect relatively few
    # callbacks to be required.
    apply_semantic_analyzer_patches(patches)
    # This pass might need fallbacks calculated above.
    check_type_arguments(graph, scc, errors)
    # Run class decorator hooks (they requite complete MROs and no placeholders).
    apply_class_plugin_hooks(graph, scc, errors)
    calculate_class_properties(graph, scc, errors)
    check_blockers(graph, scc)
    # Clean-up builtins, so that TypeVar etc. are not accessible without importing.
    if 'builtins' in scc:
        cleanup_builtin_scc(graph['builtins'])


</t>
<t tx="ekr.20220525082935.31">def process_static_or_class_method_in_overload(self, defn: OverloadedFuncDef) -&gt; None:
    class_status = []
    static_status = []
    for item in defn.items:
        if isinstance(item, Decorator):
            inner = item.func
        elif isinstance(item, FuncDef):
            inner = item
        else:
            assert False, f"The 'item' variable is an unexpected type: {type(item)}"
        class_status.append(inner.is_class)
        static_status.append(inner.is_static)

    if defn.impl is not None:
        if isinstance(defn.impl, Decorator):
            inner = defn.impl.func
        elif isinstance(defn.impl, FuncDef):
            inner = defn.impl
        else:
            assert False, f"Unexpected impl type: {type(defn.impl)}"
        class_status.append(inner.is_class)
        static_status.append(inner.is_static)

    if len(set(class_status)) != 1:
        self.msg.overload_inconsistently_applies_decorator('classmethod', defn)
    elif len(set(static_status)) != 1:
        self.msg.overload_inconsistently_applies_decorator('staticmethod', defn)
    else:
        defn.is_class = class_status[0]
        defn.is_static = static_status[0]

</t>
<t tx="ekr.20220525082935.310">def cleanup_builtin_scc(state: 'State') -&gt; None:
    """Remove imported names from builtins namespace.

    This way names imported from typing in builtins.pyi aren't available
    by default (without importing them). We can only do this after processing
    the whole SCC is finished, when the imported names aren't needed for
    processing builtins.pyi itself.
    """
    assert state.tree is not None
    remove_imported_names_from_symtable(state.tree.names, 'builtins')


</t>
<t tx="ekr.20220525082935.311">def semantic_analysis_for_targets(
        state: 'State',
        nodes: List[FineGrainedDeferredNode],
        graph: 'Graph',
        saved_attrs: SavedAttributes) -&gt; None:
    """Semantically analyze only selected nodes in a given module.

    This essentially mirrors the logic of semantic_analysis_for_scc()
    except that we process only some targets. This is used in fine grained
    incremental mode, when propagating an update.

    The saved_attrs are implicitly declared instance attributes (attributes
    defined on self) removed by AST stripper that may need to be reintroduced
    here.  They must be added before any methods are analyzed.
    """
    patches: Patches = []
    if any(isinstance(n.node, MypyFile) for n in nodes):
        # Process module top level first (if needed).
        process_top_levels(graph, [state.id], patches)
    restore_saved_attrs(saved_attrs)
    analyzer = state.manager.semantic_analyzer
    for n in nodes:
        if isinstance(n.node, MypyFile):
            # Already done above.
            continue
        process_top_level_function(analyzer, state, state.id,
                                   n.node.fullname, n.node, n.active_typeinfo, patches)
    apply_semantic_analyzer_patches(patches)

    check_type_arguments_in_targets(nodes, state, state.manager.errors)
    calculate_class_properties(graph, [state.id], state.manager.errors)
    apply_class_plugin_hooks(graph, [state.id], state.manager.errors)


</t>
<t tx="ekr.20220525082935.312">def restore_saved_attrs(saved_attrs: SavedAttributes) -&gt; None:
    """Restore instance variables removed during AST strip that haven't been added yet."""
    for (cdef, name), sym in saved_attrs.items():
        info = cdef.info
        existing = info.get(name)
        defined_in_this_class = name in info.names
        assert isinstance(sym.node, Var)
        # This needs to mimic the logic in SemanticAnalyzer.analyze_member_lvalue()
        # regarding the existing variable in class body or in a superclass:
        # If the attribute of self is not defined in superclasses, create a new Var.
        if (existing is None or
                # (An abstract Var is considered as not defined.)
                (isinstance(existing.node, Var) and existing.node.is_abstract_var) or
                # Also an explicit declaration on self creates a new Var unless
                # there is already one defined in the class body.
                sym.node.explicit_self_type and not defined_in_this_class):
            info.names[name] = sym


</t>
<t tx="ekr.20220525082935.313">def process_top_levels(graph: 'Graph', scc: List[str], patches: Patches) -&gt; None:
    # Process top levels until everything has been bound.

    # Reverse order of the scc so the first modules in the original list will be
    # be processed first. This helps with performance.
    scc = list(reversed(scc))

    # Initialize ASTs and symbol tables.
    for id in scc:
        state = graph[id]
        assert state.tree is not None
        state.manager.semantic_analyzer.prepare_file(state.tree)

    # Initially all namespaces in the SCC are incomplete (well they are empty).
    state.manager.incomplete_namespaces.update(scc)

    worklist = scc[:]
    # HACK: process core stuff first. This is mostly needed to support defining
    # named tuples in builtin SCC.
    if all(m in worklist for m in core_modules):
        worklist += list(reversed(core_modules)) * CORE_WARMUP
    final_iteration = False
    iteration = 0
    analyzer = state.manager.semantic_analyzer
    analyzer.deferral_debug_context.clear()

    while worklist:
        iteration += 1
        if iteration &gt; MAX_ITERATIONS:
            # Just pick some module inside the current SCC for error context.
            assert state.tree is not None
            with analyzer.file_context(state.tree, state.options):
                analyzer.report_hang()
            break
        if final_iteration:
            # Give up. It's impossible to bind all names.
            state.manager.incomplete_namespaces.clear()
        all_deferred: List[str] = []
        any_progress = False
        while worklist:
            next_id = worklist.pop()
            state = graph[next_id]
            assert state.tree is not None
            deferred, incomplete, progress = semantic_analyze_target(next_id, state,
                                                                     state.tree,
                                                                     None,
                                                                     final_iteration,
                                                                     patches)
            all_deferred += deferred
            any_progress = any_progress or progress
            if not incomplete:
                state.manager.incomplete_namespaces.discard(next_id)
        if final_iteration:
            assert not all_deferred, 'Must not defer during final iteration'
        # Reverse to process the targets in the same order on every iteration. This avoids
        # processing the same target twice in a row, which is inefficient.
        worklist = list(reversed(all_deferred))
        final_iteration = not any_progress


</t>
<t tx="ekr.20220525082935.314">def process_functions(graph: 'Graph', scc: List[str], patches: Patches) -&gt; None:
    # Process functions.
    for module in scc:
        tree = graph[module].tree
        assert tree is not None
        analyzer = graph[module].manager.semantic_analyzer
        # In principle, functions can be processed in arbitrary order,
        # but _methods_ must be processed in the order they are defined,
        # because some features (most notably partial types) depend on
        # order of definitions on self.
        #
        # There can be multiple generated methods per line. Use target
        # name as the second sort key to get a repeatable sort order on
        # Python 3.5, which doesn't preserve dictionary order.
        targets = sorted(get_all_leaf_targets(tree), key=lambda x: (x[1].line, x[0]))
        for target, node, active_type in targets:
            assert isinstance(node, (FuncDef, OverloadedFuncDef, Decorator))
            process_top_level_function(analyzer,
                                       graph[module],
                                       module,
                                       target,
                                       node,
                                       active_type,
                                       patches)


</t>
<t tx="ekr.20220525082935.315">def process_top_level_function(analyzer: 'SemanticAnalyzer',
                               state: 'State',
                               module: str,
                               target: str,
                               node: Union[FuncDef, OverloadedFuncDef, Decorator],
                               active_type: Optional[TypeInfo],
                               patches: Patches) -&gt; None:
    """Analyze single top-level function or method.

    Process the body of the function (including nested functions) again and again,
    until all names have been resolved (or iteration limit reached).
    """
    # We need one more iteration after incomplete is False (e.g. to report errors, if any).
    final_iteration = False
    incomplete = True
    # Start in the incomplete state (no missing names will be reported on first pass).
    # Note that we use module name, since functions don't create qualified names.
    deferred = [module]
    analyzer.deferral_debug_context.clear()
    analyzer.incomplete_namespaces.add(module)
    iteration = 0
    while deferred:
        iteration += 1
        if iteration == MAX_ITERATIONS:
            # Just pick some module inside the current SCC for error context.
            assert state.tree is not None
            with analyzer.file_context(state.tree, state.options):
                analyzer.report_hang()
            break
        if not (deferred or incomplete) or final_iteration:
            # OK, this is one last pass, now missing names will be reported.
            analyzer.incomplete_namespaces.discard(module)
        deferred, incomplete, progress = semantic_analyze_target(target, state, node, active_type,
                                                                 final_iteration, patches)
        if final_iteration:
            assert not deferred, 'Must not defer during final iteration'
        if not progress:
            final_iteration = True

    analyzer.incomplete_namespaces.discard(module)
    # After semantic analysis is done, discard local namespaces
    # to avoid memory hoarding.
    analyzer.saved_locals.clear()


</t>
<t tx="ekr.20220525082935.316">TargetInfo = Tuple[str, Union[MypyFile, FuncDef, OverloadedFuncDef, Decorator], Optional[TypeInfo]]


</t>
<t tx="ekr.20220525082935.317">def get_all_leaf_targets(file: MypyFile) -&gt; List[TargetInfo]:
    """Return all leaf targets in a symbol table (module-level and methods)."""
    result: List[TargetInfo] = []
    for fullname, node, active_type in file.local_definitions():
        if isinstance(node.node, (FuncDef, OverloadedFuncDef, Decorator)):
            result.append((fullname, node.node, active_type))
    return result


</t>
<t tx="ekr.20220525082935.318">def semantic_analyze_target(target: str,
                            state: 'State',
                            node: Union[MypyFile, FuncDef, OverloadedFuncDef, Decorator],
                            active_type: Optional[TypeInfo],
                            final_iteration: bool,
                            patches: Patches) -&gt; Tuple[List[str], bool, bool]:
    """Semantically analyze a single target.

    Return tuple with these items:
    - list of deferred targets
    - was some definition incomplete (need to run another pass)
    - were any new names defined (or placeholders replaced)
    """
    state.manager.processed_targets.append(target)
    tree = state.tree
    assert tree is not None
    analyzer = state.manager.semantic_analyzer
    # TODO: Move initialization to somewhere else
    analyzer.global_decls = [set()]
    analyzer.nonlocal_decls = [set()]
    analyzer.globals = tree.names
    analyzer.progress = False
    with state.wrap_context(check_blockers=False):
        refresh_node = node
        if isinstance(refresh_node, Decorator):
            # Decorator expressions will be processed as part of the module top level.
            refresh_node = refresh_node.func
        analyzer.refresh_partial(refresh_node,
                                 patches,
                                 final_iteration,
                                 file_node=tree,
                                 options=state.options,
                                 active_type=active_type)
        if isinstance(node, Decorator):
            infer_decorator_signature_if_simple(node, analyzer)
    for dep in analyzer.imports:
        state.add_dependency(dep)
        priority = mypy.build.PRI_LOW
        if priority &lt;= state.priorities.get(dep, priority):
            state.priorities[dep] = priority

    # Clear out some stale data to avoid memory leaks and astmerge
    # validity check confusion
    analyzer.statement = None
    del analyzer.cur_mod_node

    if analyzer.deferred:
        return [target], analyzer.incomplete, analyzer.progress
    else:
        return [], analyzer.incomplete, analyzer.progress


</t>
<t tx="ekr.20220525082935.319">def check_type_arguments(graph: 'Graph', scc: List[str], errors: Errors) -&gt; None:
    for module in scc:
        state = graph[module]
        assert state.tree
        analyzer = TypeArgumentAnalyzer(errors,
                                        state.options,
                                        is_typeshed_file(state.path or ''))
        with state.wrap_context():
            with mypy.state.state.strict_optional_set(state.options.strict_optional):
                state.tree.accept(analyzer)


</t>
<t tx="ekr.20220525082935.32">def analyze_property_with_multi_part_definition(self, defn: OverloadedFuncDef) -&gt; None:
    """Analyze a property defined using multiple methods (e.g., using @x.setter).

    Assume that the first method (@property) has already been analyzed.
    """
    defn.is_property = True
    items = defn.items
    first_item = cast(Decorator, defn.items[0])
    deleted_items = []
    for i, item in enumerate(items[1:]):
        if isinstance(item, Decorator):
            if len(item.decorators) == 1:
                node = item.decorators[0]
                if isinstance(node, MemberExpr):
                    if node.name == 'setter':
                        # The first item represents the entire property.
                        first_item.var.is_settable_property = True
                        # Get abstractness from the original definition.
                        item.func.is_abstract = first_item.func.is_abstract
            else:
                self.fail("Decorated property not supported", item)
            item.func.accept(self)
        else:
            self.fail(f'Unexpected definition for property "{first_item.func.name}"',
                      item)
            deleted_items.append(i + 1)
    for i in reversed(deleted_items):
        del items[i]

</t>
<t tx="ekr.20220525082935.320">def check_type_arguments_in_targets(targets: List[FineGrainedDeferredNode], state: 'State',
                                    errors: Errors) -&gt; None:
    """Check type arguments against type variable bounds and restrictions.

    This mirrors the logic in check_type_arguments() except that we process only
    some targets. This is used in fine grained incremental mode.
    """
    analyzer = TypeArgumentAnalyzer(errors,
                                    state.options,
                                    is_typeshed_file(state.path or ''))
    with state.wrap_context():
        with mypy.state.state.strict_optional_set(state.options.strict_optional):
            for target in targets:
                func: Optional[Union[FuncDef, OverloadedFuncDef]] = None
                if isinstance(target.node, (FuncDef, OverloadedFuncDef)):
                    func = target.node
                saved = (state.id, target.active_typeinfo, func)  # module, class, function
                with errors.scope.saved_scope(saved) if errors.scope else nullcontext():
                    analyzer.recurse_into_functions = func is not None
                    target.node.accept(analyzer)


</t>
<t tx="ekr.20220525082935.321">def apply_class_plugin_hooks(graph: 'Graph', scc: List[str], errors: Errors) -&gt; None:
    """Apply class plugin hooks within a SCC.

    We run these after to the main semantic analysis so that the hooks
    don't need to deal with incomplete definitions such as placeholder
    types.

    Note that some hooks incorrectly run during the main semantic
    analysis pass, for historical reasons.
    """
    num_passes = 0
    incomplete = True
    # If we encounter a base class that has not been processed, we'll run another
    # pass. This should eventually reach a fixed point.
    while incomplete:
        assert num_passes &lt; 10, "Internal error: too many class plugin hook passes"
        num_passes += 1
        incomplete = False
        for module in scc:
            state = graph[module]
            tree = state.tree
            assert tree
            for _, node, _ in tree.local_definitions():
                if isinstance(node.node, TypeInfo):
                    if not apply_hooks_to_class(state.manager.semantic_analyzer,
                                                module, node.node, state.options, tree, errors):
                        incomplete = True


</t>
<t tx="ekr.20220525082935.322">def apply_hooks_to_class(self: SemanticAnalyzer,
                         module: str,
                         info: TypeInfo,
                         options: Options,
                         file_node: MypyFile,
                         errors: Errors) -&gt; bool:
    # TODO: Move more class-related hooks here?
    defn = info.defn
    ok = True
    for decorator in defn.decorators:
        with self.file_context(file_node, options, info):
            decorator_name = self.get_fullname_for_hook(decorator)
            if decorator_name:
                hook = self.plugin.get_class_decorator_hook_2(decorator_name)
                if hook:
                    ok = ok and hook(ClassDefContext(defn, decorator, self))
    return ok


</t>
<t tx="ekr.20220525082935.323">def calculate_class_properties(graph: 'Graph', scc: List[str], errors: Errors) -&gt; None:
    for module in scc:
        state = graph[module]
        tree = state.tree
        assert tree
        for _, node, _ in tree.local_definitions():
            if isinstance(node.node, TypeInfo):
                with state.manager.semantic_analyzer.file_context(tree, state.options, node.node):
                    calculate_class_abstract_status(node.node, tree.is_stub, errors)
                    check_protocol_status(node.node, errors)
                    calculate_class_vars(node.node)
                    add_type_promotion(node.node, tree.names, graph[module].options)


</t>
<t tx="ekr.20220525082935.324">def check_blockers(graph: 'Graph', scc: List[str]) -&gt; None:
    for module in scc:
        graph[module].check_blockers()
</t>
<t tx="ekr.20220525082935.325">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Semantic analysis of named tuple definitions.

This is conceptually part of mypy.semanal.
"""

from contextlib import contextmanager
from typing import Tuple, List, Dict, Mapping, Optional, Union, cast, Iterator
from typing_extensions import Final

from mypy.types import (
    Type, TupleType, AnyType, TypeOfAny, CallableType, TypeType, TypeVarType,
    UnboundType, LiteralType,
)
from mypy.semanal_shared import (
    SemanticAnalyzerInterface, set_callable_name, calculate_tuple_fallback, PRIORITY_FALLBACKS
)
from mypy.nodes import (
    Var, EllipsisExpr, Argument, StrExpr, BytesExpr, UnicodeExpr, ExpressionStmt, NameExpr,
    AssignmentStmt, PassStmt, Decorator, FuncBase, ClassDef, Expression, RefExpr, TypeInfo,
    NamedTupleExpr, CallExpr, Context, TupleExpr, ListExpr, SymbolTableNode, FuncDef, Block,
    TempNode, SymbolTable, TypeVarExpr, ARG_POS, ARG_NAMED_OPT, ARG_OPT, MDEF
)
from mypy.options import Options
from mypy.exprtotype import expr_to_unanalyzed_type, TypeTranslationError
from mypy.util import get_unique_redefinition_name

# Matches "_prohibited" in typing.py, but adds __annotations__, which works at runtime but can't
# easily be supported in a static checker.
NAMEDTUPLE_PROHIBITED_NAMES: Final = (
    "__new__",
    "__init__",
    "__slots__",
    "__getnewargs__",
    "_fields",
    "_field_defaults",
    "_field_types",
    "_make",
    "_replace",
    "_asdict",
    "_source",
    "__annotations__",
)

NAMEDTUP_CLASS_ERROR: Final = (
    "Invalid statement in NamedTuple definition; " 'expected "field_name: field_type [= default]"'
)

SELF_TVAR_NAME: Final = "_NT"


@others
</t>
<t tx="ekr.20220525082935.326">class NamedTupleAnalyzer:
    @others
</t>
<t tx="ekr.20220525082935.327">def __init__(self, options: Options, api: SemanticAnalyzerInterface) -&gt; None:
    self.options = options
    self.api = api

</t>
<t tx="ekr.20220525082935.328">def analyze_namedtuple_classdef(self, defn: ClassDef, is_stub_file: bool,
                                is_func_scope: bool
                                ) -&gt; Tuple[bool, Optional[TypeInfo]]:
    """Analyze if given class definition can be a named tuple definition.

    Return a tuple where first item indicates whether this can possibly be a named tuple,
    and the second item is the corresponding TypeInfo (may be None if not ready and should be
    deferred).
    """
    for base_expr in defn.base_type_exprs:
        if isinstance(base_expr, RefExpr):
            self.api.accept(base_expr)
            if base_expr.fullname == 'typing.NamedTuple':
                result = self.check_namedtuple_classdef(defn, is_stub_file)
                if result is None:
                    # This is a valid named tuple, but some types are incomplete.
                    return True, None
                items, types, default_items = result
                if is_func_scope and '@' not in defn.name:
                    defn.name += '@' + str(defn.line)
                info = self.build_namedtuple_typeinfo(
                    defn.name, items, types, default_items, defn.line)
                defn.info = info
                defn.analyzed = NamedTupleExpr(info, is_typed=True)
                defn.analyzed.line = defn.line
                defn.analyzed.column = defn.column
                # All done: this is a valid named tuple with all types known.
                return True, info
    # This can't be a valid named tuple.
    return False, None

</t>
<t tx="ekr.20220525082935.329">def check_namedtuple_classdef(self, defn: ClassDef, is_stub_file: bool
                              ) -&gt; Optional[Tuple[List[str],
                                            List[Type],
                                            Dict[str, Expression]]]:
    """Parse and validate fields in named tuple class definition.

    Return a three tuple:
      * field names
      * field types
      * field default values
    or None, if any of the types are not ready.
    """
    if self.options.python_version &lt; (3, 6) and not is_stub_file:
        self.fail('NamedTuple class syntax is only supported in Python 3.6', defn)
        return [], [], {}
    if len(defn.base_type_exprs) &gt; 1:
        self.fail('NamedTuple should be a single base', defn)
    items: List[str] = []
    types: List[Type] = []
    default_items: Dict[str, Expression] = {}
    for stmt in defn.defs.body:
        if not isinstance(stmt, AssignmentStmt):
            # Still allow pass or ... (for empty namedtuples).
            if (isinstance(stmt, PassStmt) or
                (isinstance(stmt, ExpressionStmt) and
                    isinstance(stmt.expr, EllipsisExpr))):
                continue
            # Also allow methods, including decorated ones.
            if isinstance(stmt, (Decorator, FuncBase)):
                continue
            # And docstrings.
            if (isinstance(stmt, ExpressionStmt) and
                    isinstance(stmt.expr, StrExpr)):
                continue
            self.fail(NAMEDTUP_CLASS_ERROR, stmt)
        elif len(stmt.lvalues) &gt; 1 or not isinstance(stmt.lvalues[0], NameExpr):
            # An assignment, but an invalid one.
            self.fail(NAMEDTUP_CLASS_ERROR, stmt)
        else:
            # Append name and type in this case...
            name = stmt.lvalues[0].name
            items.append(name)
            if stmt.type is None:
                types.append(AnyType(TypeOfAny.unannotated))
            else:
                analyzed = self.api.anal_type(stmt.type)
                if analyzed is None:
                    # Something is incomplete. We need to defer this named tuple.
                    return None
                types.append(analyzed)
            # ...despite possible minor failures that allow further analyzis.
            if name.startswith('_'):
                self.fail('NamedTuple field name cannot start with an underscore: {}'
                          .format(name), stmt)
            if stmt.type is None or hasattr(stmt, 'new_syntax') and not stmt.new_syntax:
                self.fail(NAMEDTUP_CLASS_ERROR, stmt)
            elif isinstance(stmt.rvalue, TempNode):
                # x: int assigns rvalue to TempNode(AnyType())
                if default_items:
                    self.fail('Non-default NamedTuple fields cannot follow default fields',
                              stmt)
            else:
                default_items[name] = stmt.rvalue
    return items, types, default_items

</t>
<t tx="ekr.20220525082935.33">def add_function_to_symbol_table(self, func: Union[FuncDef, OverloadedFuncDef]) -&gt; None:
    if self.is_class_scope():
        assert self.type is not None
        func.info = self.type
    func._fullname = self.qualified_name(func.name)
    self.add_symbol(func.name, func, func)

</t>
<t tx="ekr.20220525082935.330">def check_namedtuple(self,
                     node: Expression,
                     var_name: Optional[str],
                     is_func_scope: bool) -&gt; Tuple[Optional[str], Optional[TypeInfo]]:
    """Check if a call defines a namedtuple.

    The optional var_name argument is the name of the variable to
    which this is assigned, if any.

    Return a tuple of two items:
      * Internal name of the named tuple (e.g. the name passed as an argument to namedtuple)
        or None if it is not a valid named tuple
      * Corresponding TypeInfo, or None if not ready.

    If the definition is invalid but looks like a namedtuple,
    report errors but return (some) TypeInfo.
    """
    if not isinstance(node, CallExpr):
        return None, None
    call = node
    callee = call.callee
    if not isinstance(callee, RefExpr):
        return None, None
    fullname = callee.fullname
    if fullname == 'collections.namedtuple':
        is_typed = False
    elif fullname == 'typing.NamedTuple':
        is_typed = True
    else:
        return None, None
    result = self.parse_namedtuple_args(call, fullname)
    if result:
        items, types, defaults, typename, ok = result
    else:
        # Error. Construct dummy return value.
        if var_name:
            name = var_name
            if is_func_scope:
                name += '@' + str(call.line)
        else:
            name = var_name = 'namedtuple@' + str(call.line)
        info = self.build_namedtuple_typeinfo(name, [], [], {}, node.line)
        self.store_namedtuple_info(info, var_name, call, is_typed)
        if name != var_name or is_func_scope:
            # NOTE: we skip local namespaces since they are not serialized.
            self.api.add_symbol_skip_local(name, info)
        return var_name, info
    if not ok:
        # This is a valid named tuple but some types are not ready.
        return typename, None

    # We use the variable name as the class name if it exists. If
    # it doesn't, we use the name passed as an argument. We prefer
    # the variable name because it should be unique inside a
    # module, and so we don't need to disambiguate it with a line
    # number.
    if var_name:
        name = var_name
    else:
        name = typename

    if var_name is None or is_func_scope:
        # There are two special cases where need to give it a unique name derived
        # from the line number:
        #   * This is a base class expression, since it often matches the class name:
        #         class NT(NamedTuple('NT', [...])):
        #             ...
        #   * This is a local (function or method level) named tuple, since
        #     two methods of a class can define a named tuple with the same name,
        #     and they will be stored in the same namespace (see below).
        name += '@' + str(call.line)
    if len(defaults) &gt; 0:
        default_items = {
            arg_name: default
            for arg_name, default in zip(items[-len(defaults):], defaults)
        }
    else:
        default_items = {}
    info = self.build_namedtuple_typeinfo(name, items, types, default_items, node.line)
    # If var_name is not None (i.e. this is not a base class expression), we always
    # store the generated TypeInfo under var_name in the current scope, so that
    # other definitions can use it.
    if var_name:
        self.store_namedtuple_info(info, var_name, call, is_typed)
    # There are three cases where we need to store the generated TypeInfo
    # second time (for the purpose of serialization):
    #   * If there is a name mismatch like One = NamedTuple('Other', [...])
    #     we also store the info under name 'Other@lineno', this is needed
    #     because classes are (de)serialized using their actual fullname, not
    #     the name of l.h.s.
    #   * If this is a method level named tuple. It can leak from the method
    #     via assignment to self attribute and therefore needs to be serialized
    #     (local namespaces are not serialized).
    #   * If it is a base class expression. It was not stored above, since
    #     there is no var_name (but it still needs to be serialized
    #     since it is in MRO of some class).
    if name != var_name or is_func_scope:
        # NOTE: we skip local namespaces since they are not serialized.
        self.api.add_symbol_skip_local(name, info)
    return typename, info

</t>
<t tx="ekr.20220525082935.331">def store_namedtuple_info(self, info: TypeInfo, name: str,
                          call: CallExpr, is_typed: bool) -&gt; None:
    self.api.add_symbol(name, info, call)
    call.analyzed = NamedTupleExpr(info, is_typed=is_typed)
    call.analyzed.set_line(call.line, call.column)

</t>
<t tx="ekr.20220525082935.332">def parse_namedtuple_args(self, call: CallExpr, fullname: str
                          ) -&gt; Optional[Tuple[List[str], List[Type], List[Expression],
                                        str, bool]]:
    """Parse a namedtuple() call into data needed to construct a type.

    Returns a 5-tuple:
    - List of argument names
    - List of argument types
    - List of default values
    - First argument of namedtuple
    - Whether all types are ready.

    Return None if the definition didn't typecheck.
    """
    type_name = 'NamedTuple' if fullname == 'typing.NamedTuple' else 'namedtuple'
    # TODO: Share code with check_argument_count in checkexpr.py?
    args = call.args
    if len(args) &lt; 2:
        self.fail(f'Too few arguments for "{type_name}()"', call)
        return None
    defaults: List[Expression] = []
    if len(args) &gt; 2:
        # Typed namedtuple doesn't support additional arguments.
        if fullname == 'typing.NamedTuple':
            self.fail('Too many arguments for "NamedTuple()"', call)
            return None
        for i, arg_name in enumerate(call.arg_names[2:], 2):
            if arg_name == 'defaults':
                arg = args[i]
                # We don't care what the values are, as long as the argument is an iterable
                # and we can count how many defaults there are.
                if isinstance(arg, (ListExpr, TupleExpr)):
                    defaults = list(arg.items)
                else:
                    self.fail(
                        "List or tuple literal expected as the defaults argument to "
                        "{}()".format(type_name),
                        arg
                    )
                break
    if call.arg_kinds[:2] != [ARG_POS, ARG_POS]:
        self.fail(f'Unexpected arguments to "{type_name}()"', call)
        return None
    if not isinstance(args[0], (StrExpr, BytesExpr, UnicodeExpr)):
        self.fail(
            f'"{type_name}()" expects a string literal as the first argument', call)
        return None
    typename = cast(Union[StrExpr, BytesExpr, UnicodeExpr], call.args[0]).value
    types: List[Type] = []
    if not isinstance(args[1], (ListExpr, TupleExpr)):
        if (fullname == 'collections.namedtuple'
                and isinstance(args[1], (StrExpr, BytesExpr, UnicodeExpr))):
            str_expr = args[1]
            items = str_expr.value.replace(',', ' ').split()
        else:
            self.fail(
                'List or tuple literal expected as the second argument to "{}()"'.format(
                    type_name,
                ),
                call,
            )
            return None
    else:
        listexpr = args[1]
        if fullname == 'collections.namedtuple':
            # The fields argument contains just names, with implicit Any types.
            if any(not isinstance(item, (StrExpr, BytesExpr, UnicodeExpr))
                   for item in listexpr.items):
                self.fail('String literal expected as "namedtuple()" item', call)
                return None
            items = [cast(Union[StrExpr, BytesExpr, UnicodeExpr], item).value
                     for item in listexpr.items]
        else:
            # The fields argument contains (name, type) tuples.
            result = self.parse_namedtuple_fields_with_types(listexpr.items, call)
            if result is None:
                # One of the types is not ready, defer.
                return None
            items, types, _, ok = result
            if not ok:
                return [], [], [], typename, False
    if not types:
        types = [AnyType(TypeOfAny.unannotated) for _ in items]
    underscore = [item for item in items if item.startswith('_')]
    if underscore:
        self.fail(f'"{type_name}()" field names cannot start with an underscore: '
                  + ', '.join(underscore), call)
    if len(defaults) &gt; len(items):
        self.fail(f'Too many defaults given in call to "{type_name}()"', call)
        defaults = defaults[:len(items)]
    return items, types, defaults, typename, True

</t>
<t tx="ekr.20220525082935.333">def parse_namedtuple_fields_with_types(self, nodes: List[Expression], context: Context
                                       ) -&gt; Optional[Tuple[List[str], List[Type],
                                                           List[Expression], bool]]:
    """Parse typed named tuple fields.

    Return (names, types, defaults, whether types are all ready), or None if error occurred.
    """
    items: List[str] = []
    types: List[Type] = []
    for item in nodes:
        if isinstance(item, TupleExpr):
            if len(item.items) != 2:
                self.fail('Invalid "NamedTuple()" field definition', item)
                return None
            name, type_node = item.items
            if isinstance(name, (StrExpr, BytesExpr, UnicodeExpr)):
                items.append(name.value)
            else:
                self.fail('Invalid "NamedTuple()" field name', item)
                return None
            try:
                type = expr_to_unanalyzed_type(type_node, self.options, self.api.is_stub_file)
            except TypeTranslationError:
                self.fail('Invalid field type', type_node)
                return None
            analyzed = self.api.anal_type(type)
            # Workaround #4987 and avoid introducing a bogus UnboundType
            if isinstance(analyzed, UnboundType):
                analyzed = AnyType(TypeOfAny.from_error)
            # These should be all known, otherwise we would defer in visit_assignment_stmt().
            if analyzed is None:
                return [], [], [], False
            types.append(analyzed)
        else:
            self.fail('Tuple expected as "NamedTuple()" field', item)
            return None
    return items, types, [], True

</t>
<t tx="ekr.20220525082935.334">def build_namedtuple_typeinfo(self,
                              name: str,
                              items: List[str],
                              types: List[Type],
                              default_items: Mapping[str, Expression],
                              line: int) -&gt; TypeInfo:
    strtype = self.api.named_type('builtins.str')
    implicit_any = AnyType(TypeOfAny.special_form)
    basetuple_type = self.api.named_type('builtins.tuple', [implicit_any])
    dictype = (self.api.named_type_or_none('builtins.dict', [strtype, implicit_any])
               or self.api.named_type('builtins.object'))
    # Actual signature should return OrderedDict[str, Union[types]]
    ordereddictype = (self.api.named_type_or_none('builtins.dict', [strtype, implicit_any])
                      or self.api.named_type('builtins.object'))
    fallback = self.api.named_type('builtins.tuple', [implicit_any])
    # Note: actual signature should accept an invariant version of Iterable[UnionType[types]].
    # but it can't be expressed. 'new' and 'len' should be callable types.
    iterable_type = self.api.named_type_or_none('typing.Iterable', [implicit_any])
    function_type = self.api.named_type('builtins.function')

    literals: List[Type] = [LiteralType(item, strtype) for item in items]
    match_args_type = TupleType(literals, basetuple_type)

    info = self.api.basic_new_typeinfo(name, fallback, line)
    info.is_named_tuple = True
    tuple_base = TupleType(types, fallback)
    info.tuple_type = tuple_base
    info.line = line
    # For use by mypyc.
    info.metadata['namedtuple'] = {'fields': items.copy()}

    # We can't calculate the complete fallback type until after semantic
    # analysis, since otherwise base classes might be incomplete. Postpone a
    # callback function that patches the fallback.
    self.api.schedule_patch(PRIORITY_FALLBACKS,
                            lambda: calculate_tuple_fallback(tuple_base))

    def add_field(var: Var, is_initialized_in_class: bool = False,
                  is_property: bool = False) -&gt; None:
        var.info = info
        var.is_initialized_in_class = is_initialized_in_class
        var.is_property = is_property
        var._fullname = f'{info.fullname}.{var.name}'
        info.names[var.name] = SymbolTableNode(MDEF, var)

    fields = [Var(item, typ) for item, typ in zip(items, types)]
    for var in fields:
        add_field(var, is_property=True)
    # We can't share Vars between fields and method arguments, since they
    # have different full names (the latter are normally used as local variables
    # in functions, so their full names are set to short names when generated methods
    # are analyzed).
    vars = [Var(item, typ) for item, typ in zip(items, types)]

    tuple_of_strings = TupleType([strtype for _ in items], basetuple_type)
    add_field(Var('_fields', tuple_of_strings), is_initialized_in_class=True)
    add_field(Var('_field_types', dictype), is_initialized_in_class=True)
    add_field(Var('_field_defaults', dictype), is_initialized_in_class=True)
    add_field(Var('_source', strtype), is_initialized_in_class=True)
    add_field(Var('__annotations__', ordereddictype), is_initialized_in_class=True)
    add_field(Var('__doc__', strtype), is_initialized_in_class=True)
    add_field(Var('__match_args__', match_args_type), is_initialized_in_class=True)

    tvd = TypeVarType(SELF_TVAR_NAME, info.fullname + '.' + SELF_TVAR_NAME,
                     -1, [], info.tuple_type)
    selftype = tvd

    def add_method(funcname: str,
                   ret: Type,
                   args: List[Argument],
                   is_classmethod: bool = False,
                   is_new: bool = False,
                   ) -&gt; None:
        if is_classmethod or is_new:
            first = [Argument(Var('_cls'), TypeType.make_normalized(selftype), None, ARG_POS)]
        else:
            first = [Argument(Var('_self'), selftype, None, ARG_POS)]
        args = first + args

        types = [arg.type_annotation for arg in args]
        items = [arg.variable.name for arg in args]
        arg_kinds = [arg.kind for arg in args]
        assert None not in types
        signature = CallableType(cast(List[Type], types), arg_kinds, items, ret,
                                 function_type)
        signature.variables = [tvd]
        func = FuncDef(funcname, args, Block([]))
        func.info = info
        func.is_class = is_classmethod
        func.type = set_callable_name(signature, func)
        func._fullname = info.fullname + '.' + funcname
        func.line = line
        if is_classmethod:
            v = Var(funcname, func.type)
            v.is_classmethod = True
            v.info = info
            v._fullname = func._fullname
            func.is_decorated = True
            dec = Decorator(func, [NameExpr('classmethod')], v)
            dec.line = line
            sym = SymbolTableNode(MDEF, dec)
        else:
            sym = SymbolTableNode(MDEF, func)
        sym.plugin_generated = True
        info.names[funcname] = sym

    add_method('_replace', ret=selftype,
               args=[Argument(var, var.type, EllipsisExpr(), ARG_NAMED_OPT) for var in vars])

    def make_init_arg(var: Var) -&gt; Argument:
        default = default_items.get(var.name, None)
        kind = ARG_POS if default is None else ARG_OPT
        return Argument(var, var.type, default, kind)

    add_method('__new__', ret=selftype,
               args=[make_init_arg(var) for var in vars],
               is_new=True)
    add_method('_asdict', args=[], ret=ordereddictype)
    special_form_any = AnyType(TypeOfAny.special_form)
    add_method('_make', ret=selftype, is_classmethod=True,
               args=[Argument(Var('iterable', iterable_type), iterable_type, None, ARG_POS),
                     Argument(Var('new'), special_form_any, EllipsisExpr(), ARG_NAMED_OPT),
                     Argument(Var('len'), special_form_any, EllipsisExpr(), ARG_NAMED_OPT)])

    self_tvar_expr = TypeVarExpr(SELF_TVAR_NAME, info.fullname + '.' + SELF_TVAR_NAME,
                                 [], info.tuple_type)
    info.names[SELF_TVAR_NAME] = SymbolTableNode(MDEF, self_tvar_expr)
    return info

</t>
<t tx="ekr.20220525082935.335">@contextmanager
def save_namedtuple_body(self, named_tuple_info: TypeInfo) -&gt; Iterator[None]:
    """Preserve the generated body of class-based named tuple and then restore it.

    Temporarily clear the names dict so we don't get errors about duplicate names
    that were already set in build_namedtuple_typeinfo (we already added the tuple
    field names while generating the TypeInfo, and actual duplicates are
    already reported).
    """
    nt_names = named_tuple_info.names
    named_tuple_info.names = SymbolTable()

    yield

    # Make sure we didn't use illegal names, then reset the names in the typeinfo.
    for prohibited in NAMEDTUPLE_PROHIBITED_NAMES:
        if prohibited in named_tuple_info.names:
            if nt_names.get(prohibited) is named_tuple_info.names[prohibited]:
                continue
            ctx = named_tuple_info.names[prohibited].node
            assert ctx is not None
            self.fail(f'Cannot overwrite NamedTuple attribute "{prohibited}"',
                      ctx)

    # Restore the names in the original symbol table. This ensures that the symbol
    # table contains the field objects created by build_namedtuple_typeinfo. Exclude
    # __doc__, which can legally be overwritten by the class.
    for key, value in nt_names.items():
        if key in named_tuple_info.names:
            if key == '__doc__':
                continue
            sym = named_tuple_info.names[key]
            if isinstance(sym.node, (FuncBase, Decorator)) and not sym.plugin_generated:
                # Keep user-defined methods as is.
                continue
            # Keep existing (user-provided) definitions under mangled names, so they
            # get semantically analyzed.
            r_key = get_unique_redefinition_name(key, named_tuple_info.names)
            named_tuple_info.names[r_key] = sym
        named_tuple_info.names[key] = value

</t>
<t tx="ekr.20220525082935.336"># Helpers

</t>
<t tx="ekr.20220525082935.337">def fail(self, msg: str, ctx: Context) -&gt; None:
    self.api.fail(msg, ctx)
</t>
<t tx="ekr.20220525082935.338">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Semantic analysis of NewType definitions.

This is conceptually part of mypy.semanal (semantic analyzer pass 2).
"""

from typing import Tuple, Optional

from mypy.types import (
    Type, Instance, CallableType, NoneType, TupleType, AnyType, PlaceholderType,
    TypeOfAny, get_proper_type
)
from mypy.nodes import (
    AssignmentStmt, NewTypeExpr, CallExpr, NameExpr, RefExpr, Context, StrExpr, BytesExpr,
    UnicodeExpr, Block, FuncDef, Argument, TypeInfo, Var, SymbolTableNode, MDEF, ARG_POS,
    PlaceholderNode
)
from mypy.semanal_shared import SemanticAnalyzerInterface
from mypy.options import Options
from mypy.exprtotype import expr_to_unanalyzed_type, TypeTranslationError
from mypy.typeanal import check_for_explicit_any, has_any_from_unimported_type
from mypy.messages import MessageBuilder, format_type
from mypy.errorcodes import ErrorCode
from mypy import errorcodes as codes


@others
</t>
<t tx="ekr.20220525082935.339">class NewTypeAnalyzer:
    @others
</t>
<t tx="ekr.20220525082935.34">def analyze_arg_initializers(self, defn: FuncItem) -&gt; None:
    with self.tvar_scope_frame(self.tvar_scope.method_frame()):
        # Analyze default arguments
        for arg in defn.arguments:
            if arg.initializer:
                arg.initializer.accept(self)

</t>
<t tx="ekr.20220525082935.340">def __init__(self,
             options: Options,
             api: SemanticAnalyzerInterface,
             msg: MessageBuilder) -&gt; None:
    self.options = options
    self.api = api
    self.msg = msg

</t>
<t tx="ekr.20220525082935.341">def process_newtype_declaration(self, s: AssignmentStmt) -&gt; bool:
    """Check if s declares a NewType; if yes, store it in symbol table.

    Return True if it's a NewType declaration. The current target may be
    deferred as a side effect if the base type is not ready, even if
    the return value is True.

    The logic in this function mostly copies the logic for visit_class_def()
    with a single (non-Generic) base.
    """
    var_name, call = self.analyze_newtype_declaration(s)
    if var_name is None or call is None:
        return False
    name = var_name
    # OK, now we know this is a NewType. But the base type may be not ready yet,
    # add placeholder as we do for ClassDef.

    if self.api.is_func_scope():
        name += '@' + str(s.line)
    fullname = self.api.qualified_name(name)

    if (not call.analyzed or
            isinstance(call.analyzed, NewTypeExpr) and not call.analyzed.info):
        # Start from labeling this as a future class, as we do for normal ClassDefs.
        placeholder = PlaceholderNode(fullname, s, s.line, becomes_typeinfo=True)
        self.api.add_symbol(var_name, placeholder, s, can_defer=False)

    old_type, should_defer = self.check_newtype_args(var_name, call, s)
    old_type = get_proper_type(old_type)
    if not call.analyzed:
        call.analyzed = NewTypeExpr(var_name, old_type, line=call.line, column=call.column)
    if old_type is None:
        if should_defer:
            # Base type is not ready.
            self.api.defer()
            return True

    # Create the corresponding class definition if the aliased type is subtypeable
    if isinstance(old_type, TupleType):
        newtype_class_info = self.build_newtype_typeinfo(name, old_type,
                                                         old_type.partial_fallback, s.line)
        newtype_class_info.tuple_type = old_type
    elif isinstance(old_type, Instance):
        if old_type.type.is_protocol:
            self.fail("NewType cannot be used with protocol classes", s)
        newtype_class_info = self.build_newtype_typeinfo(name, old_type, old_type, s.line)
    else:
        if old_type is not None:
            message = "Argument 2 to NewType(...) must be subclassable (got {})"
            self.fail(message.format(format_type(old_type)), s, code=codes.VALID_NEWTYPE)
        # Otherwise the error was already reported.
        old_type = AnyType(TypeOfAny.from_error)
        object_type = self.api.named_type('builtins.object')
        newtype_class_info = self.build_newtype_typeinfo(name, old_type, object_type, s.line)
        newtype_class_info.fallback_to_any = True

    check_for_explicit_any(old_type, self.options, self.api.is_typeshed_stub_file, self.msg,
                           context=s)

    if self.options.disallow_any_unimported and has_any_from_unimported_type(old_type):
        self.msg.unimported_type_becomes_any("Argument 2 to NewType(...)", old_type, s)

    # If so, add it to the symbol table.
    assert isinstance(call.analyzed, NewTypeExpr)
    # As we do for normal classes, create the TypeInfo only once, then just
    # update base classes on next iterations (to get rid of placeholders there).
    if not call.analyzed.info:
        call.analyzed.info = newtype_class_info
    else:
        call.analyzed.info.bases = newtype_class_info.bases
    self.api.add_symbol(var_name, call.analyzed.info, s)
    if self.api.is_func_scope():
        self.api.add_symbol_skip_local(name, call.analyzed.info)
    newtype_class_info.line = s.line
    return True

</t>
<t tx="ekr.20220525082935.342">def analyze_newtype_declaration(self,
        s: AssignmentStmt) -&gt; Tuple[Optional[str], Optional[CallExpr]]:
    """Return the NewType call expression if `s` is a newtype declaration or None otherwise."""
    name, call = None, None
    if (len(s.lvalues) == 1
            and isinstance(s.lvalues[0], NameExpr)
            and isinstance(s.rvalue, CallExpr)
            and isinstance(s.rvalue.callee, RefExpr)
            and s.rvalue.callee.fullname == 'typing.NewType'):
        name = s.lvalues[0].name

        if s.type:
            self.fail("Cannot declare the type of a NewType declaration", s)

        names = self.api.current_symbol_table()
        existing = names.get(name)
        # Give a better error message than generic "Name already defined".
        if (existing and
                not isinstance(existing.node, PlaceholderNode) and not s.rvalue.analyzed):
            self.fail(f'Cannot redefine "{name}" as a NewType', s)

        # This dummy NewTypeExpr marks the call as sufficiently analyzed; it will be
        # overwritten later with a fully complete NewTypeExpr if there are no other
        # errors with the NewType() call.
        call = s.rvalue

    return name, call

</t>
<t tx="ekr.20220525082935.343">def check_newtype_args(self, name: str, call: CallExpr,
                       context: Context) -&gt; Tuple[Optional[Type], bool]:
    """Ananlyze base type in NewType call.

    Return a tuple (type, should defer).
    """
    has_failed = False
    args, arg_kinds = call.args, call.arg_kinds
    if len(args) != 2 or arg_kinds[0] != ARG_POS or arg_kinds[1] != ARG_POS:
        self.fail("NewType(...) expects exactly two positional arguments", context)
        return None, False

    # Check first argument
    if not isinstance(args[0], (StrExpr, BytesExpr, UnicodeExpr)):
        self.fail("Argument 1 to NewType(...) must be a string literal", context)
        has_failed = True
    elif args[0].value != name:
        msg = 'String argument 1 "{}" to NewType(...) does not match variable name "{}"'
        self.fail(msg.format(args[0].value, name), context)
        has_failed = True

    # Check second argument
    msg = "Argument 2 to NewType(...) must be a valid type"
    try:
        unanalyzed_type = expr_to_unanalyzed_type(args[1], self.options, self.api.is_stub_file)
    except TypeTranslationError:
        self.fail(msg, context)
        return None, False

    # We want to use our custom error message (see above), so we suppress
    # the default error message for invalid types here.
    old_type = get_proper_type(self.api.anal_type(unanalyzed_type,
                                                  report_invalid_types=False))
    should_defer = False
    if old_type is None or isinstance(old_type, PlaceholderType):
        should_defer = True

    # The caller of this function assumes that if we return a Type, it's always
    # a valid one. So, we translate AnyTypes created from errors into None.
    if isinstance(old_type, AnyType) and old_type.is_from_error:
        self.fail(msg, context)
        return None, False

    return None if has_failed else old_type, should_defer

</t>
<t tx="ekr.20220525082935.344">def build_newtype_typeinfo(self, name: str, old_type: Type, base_type: Instance,
                           line: int) -&gt; TypeInfo:
    info = self.api.basic_new_typeinfo(name, base_type, line)
    info.is_newtype = True

    # Add __init__ method
    args = [Argument(Var('self'), NoneType(), None, ARG_POS),
            self.make_argument('item', old_type)]
    signature = CallableType(
        arg_types=[Instance(info, []), old_type],
        arg_kinds=[arg.kind for arg in args],
        arg_names=['self', 'item'],
        ret_type=NoneType(),
        fallback=self.api.named_type('builtins.function'),
        name=name)
    init_func = FuncDef('__init__', args, Block([]), typ=signature)
    init_func.info = info
    init_func._fullname = info.fullname + '.__init__'
    info.names['__init__'] = SymbolTableNode(MDEF, init_func)

    return info

</t>
<t tx="ekr.20220525082935.345"># Helpers

</t>
<t tx="ekr.20220525082935.346">def make_argument(self, name: str, type: Type) -&gt; Argument:
    return Argument(Var(name), type, None, ARG_POS)

</t>
<t tx="ekr.20220525082935.347">def fail(self, msg: str, ctx: Context, *, code: Optional[ErrorCode] = None) -&gt; None:
    self.api.fail(msg, ctx, code=code)
</t>
<t tx="ekr.20220525082935.348">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Block/import reachability analysis."""

from mypy.nodes import (
    MypyFile, AssertStmt, IfStmt, Block, AssignmentStmt, ExpressionStmt, ReturnStmt, ForStmt,
    MatchStmt, Import, ImportAll, ImportFrom, ClassDef, FuncDef
)
from mypy.traverser import TraverserVisitor
from mypy.options import Options
from mypy.reachability import (
    infer_reachability_of_if_statement, assert_will_always_fail,
    infer_reachability_of_match_statement
)


@others
</t>
<t tx="ekr.20220525082935.349">class SemanticAnalyzerPreAnalysis(TraverserVisitor):
    """Analyze reachability of blocks and imports and other local things.

    This runs before semantic analysis, so names have not been bound. Imports are
    also not resolved yet, so we can only access the current module.

    This determines static reachability of blocks and imports due to version and
    platform checks, among others.

    The main entry point is 'visit_file'.

    Reachability of imports needs to be determined very early in the build since
    this affects which modules will ultimately be processed.

    Consider this example:

      import sys

      def do_stuff():
          # type: () -&gt; None:
          if sys.python_version &lt; (3,):
              import xyz  # Only available in Python 2
              xyz.whatever()
          ...

    The block containing 'import xyz' is unreachable in Python 3 mode. The import
    shouldn't be processed in Python 3 mode, even if the module happens to exist.
    """

    @others
</t>
<t tx="ekr.20220525082935.35">def analyze_function_body(self, defn: FuncItem) -&gt; None:
    is_method = self.is_class_scope()
    with self.tvar_scope_frame(self.tvar_scope.method_frame()):
        # Bind the type variables again to visit the body.
        if defn.type:
            a = self.type_analyzer()
            a.bind_function_type_variables(cast(CallableType, defn.type), defn)
        self.function_stack.append(defn)
        with self.enter(defn):
            for arg in defn.arguments:
                self.add_local(arg.variable, defn)

            # The first argument of a non-static, non-class method is like 'self'
            # (though the name could be different), having the enclosing class's
            # instance type.
            if is_method and not defn.is_static and not defn.is_class and defn.arguments:
                defn.arguments[0].variable.is_self = True

            defn.body.accept(self)
        self.function_stack.pop()

</t>
<t tx="ekr.20220525082935.350">def visit_file(self, file: MypyFile, fnam: str, mod_id: str, options: Options) -&gt; None:
    self.pyversion = options.python_version
    self.platform = options.platform
    self.cur_mod_id = mod_id
    self.cur_mod_node = file
    self.options = options
    self.is_global_scope = True

    for i, defn in enumerate(file.defs):
        defn.accept(self)
        if isinstance(defn, AssertStmt) and assert_will_always_fail(defn, options):
            # We've encountered an assert that's always false,
            # e.g. assert sys.platform == 'lol'.  Truncate the
            # list of statements.  This mutates file.defs too.
            del file.defs[i + 1:]
            break

</t>
<t tx="ekr.20220525082935.351">def visit_func_def(self, node: FuncDef) -&gt; None:
    old_global_scope = self.is_global_scope
    self.is_global_scope = False
    super().visit_func_def(node)
    self.is_global_scope = old_global_scope
    file_node = self.cur_mod_node
    if (self.is_global_scope
            and file_node.is_stub
            and node.name == '__getattr__'
            and file_node.is_package_init_file()):
        # __init__.pyi with __getattr__ means that any submodules are assumed
        # to exist, even if there is no stub. Note that we can't verify that the
        # return type is compatible, since we haven't bound types yet.
        file_node.is_partial_stub_package = True

</t>
<t tx="ekr.20220525082935.352">def visit_class_def(self, node: ClassDef) -&gt; None:
    old_global_scope = self.is_global_scope
    self.is_global_scope = False
    super().visit_class_def(node)
    self.is_global_scope = old_global_scope

</t>
<t tx="ekr.20220525082935.353">def visit_import_from(self, node: ImportFrom) -&gt; None:
    node.is_top_level = self.is_global_scope
    super().visit_import_from(node)

</t>
<t tx="ekr.20220525082935.354">def visit_import_all(self, node: ImportAll) -&gt; None:
    node.is_top_level = self.is_global_scope
    super().visit_import_all(node)

</t>
<t tx="ekr.20220525082935.355">def visit_import(self, node: Import) -&gt; None:
    node.is_top_level = self.is_global_scope
    super().visit_import(node)

</t>
<t tx="ekr.20220525082935.356">def visit_if_stmt(self, s: IfStmt) -&gt; None:
    infer_reachability_of_if_statement(s, self.options)
    for expr in s.expr:
        expr.accept(self)
    for node in s.body:
        node.accept(self)
    if s.else_body:
        s.else_body.accept(self)

</t>
<t tx="ekr.20220525082935.357">def visit_block(self, b: Block) -&gt; None:
    if b.is_unreachable:
        return
    super().visit_block(b)

</t>
<t tx="ekr.20220525082935.358">def visit_match_stmt(self, s: MatchStmt) -&gt; None:
    infer_reachability_of_match_statement(s, self.options)
    for guard in s.guards:
        if guard is not None:
            guard.accept(self)
    for body in s.bodies:
        body.accept(self)

</t>
<t tx="ekr.20220525082935.359"># The remaining methods are an optimization: don't visit nested expressions
# of common statements, since they can have no effect.

</t>
<t tx="ekr.20220525082935.36">def check_classvar_in_signature(self, typ: ProperType) -&gt; None:
    if isinstance(typ, Overloaded):
        for t in typ.items:  # type: ProperType
            self.check_classvar_in_signature(t)
        return
    if not isinstance(typ, CallableType):
        return
    for t in get_proper_types(typ.arg_types) + [get_proper_type(typ.ret_type)]:
        if self.is_classvar(t):
            self.fail_invalid_classvar(t)
            # Show only one error per signature
            break

</t>
<t tx="ekr.20220525082935.360">def visit_assignment_stmt(self, s: AssignmentStmt) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082935.361">def visit_expression_stmt(self, s: ExpressionStmt) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082935.362">def visit_return_stmt(self, s: ReturnStmt) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082935.363">def visit_for_stmt(self, s: ForStmt) -&gt; None:
    s.body.accept(self)
    if s.else_body is not None:
        s.else_body.accept(self)
</t>
<t tx="ekr.20220525082935.364">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Shared definitions used by different parts of semantic analysis."""

from abc import abstractmethod

from typing import Optional, List, Callable, Union
from typing_extensions import Final, Protocol
from mypy_extensions import trait

from mypy.nodes import (
    Context, SymbolTableNode, FuncDef, Node, TypeInfo, Expression,
    SymbolNode, SymbolTable
)
from mypy.types import (
    Type, FunctionLike, Instance, TupleType, TPDICT_FB_NAMES, ProperType, get_proper_type,
    ParamSpecType, ParamSpecFlavor, Parameters, TypeVarId
)
from mypy.tvar_scope import TypeVarLikeScope
from mypy.errorcodes import ErrorCode
from mypy import join

# Priorities for ordering of patches within the "patch" phase of semantic analysis
# (after the main pass):

# Fix fallbacks (does joins)
PRIORITY_FALLBACKS: Final = 1


@others
</t>
<t tx="ekr.20220525082935.365">@trait
class SemanticAnalyzerCoreInterface:
    """A core abstract interface to generic semantic analyzer functionality.

    This is implemented by both semantic analyzer passes 2 and 3.
    """

    @others
</t>
<t tx="ekr.20220525082935.366">@abstractmethod
def lookup_qualified(self, name: str, ctx: Context,
                     suppress_errors: bool = False) -&gt; Optional[SymbolTableNode]:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.367">@abstractmethod
def lookup_fully_qualified(self, name: str) -&gt; SymbolTableNode:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.368">@abstractmethod
def lookup_fully_qualified_or_none(self, name: str) -&gt; Optional[SymbolTableNode]:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.369">@abstractmethod
def fail(self, msg: str, ctx: Context, serious: bool = False, *,
         blocker: bool = False, code: Optional[ErrorCode] = None) -&gt; None:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.37">def check_function_signature(self, fdef: FuncItem) -&gt; None:
    sig = fdef.type
    assert isinstance(sig, CallableType)
    if len(sig.arg_types) &lt; len(fdef.arguments):
        self.fail('Type signature has too few arguments', fdef)
        # Add dummy Any arguments to prevent crashes later.
        num_extra_anys = len(fdef.arguments) - len(sig.arg_types)
        extra_anys = [AnyType(TypeOfAny.from_error)] * num_extra_anys
        sig.arg_types.extend(extra_anys)
    elif len(sig.arg_types) &gt; len(fdef.arguments):
        self.fail('Type signature has too many arguments', fdef, blocker=True)

</t>
<t tx="ekr.20220525082935.370">@abstractmethod
def note(self, msg: str, ctx: Context, *, code: Optional[ErrorCode] = None) -&gt; None:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.371">@abstractmethod
def record_incomplete_ref(self) -&gt; None:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.372">@abstractmethod
def defer(self) -&gt; None:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.373">@abstractmethod
def is_incomplete_namespace(self, fullname: str) -&gt; bool:
    """Is a module or class namespace potentially missing some definitions?"""
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.374">@property
@abstractmethod
def final_iteration(self) -&gt; bool:
    """Is this the final iteration of semantic analysis?"""
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.375">@abstractmethod
def is_future_flag_set(self, flag: str) -&gt; bool:
    """Is the specific __future__ feature imported"""
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.376">@property
@abstractmethod
def is_stub_file(self) -&gt; bool:
    raise NotImplementedError


</t>
<t tx="ekr.20220525082935.377">@trait
class SemanticAnalyzerInterface(SemanticAnalyzerCoreInterface):
    """A limited abstract interface to some generic semantic analyzer pass 2 functionality.

    We use this interface for various reasons:

    * Looser coupling
    * Cleaner import graph
    * Less need to pass around callback functions
    """

    @others
</t>
<t tx="ekr.20220525082935.378">@abstractmethod
def lookup(self, name: str, ctx: Context,
           suppress_errors: bool = False) -&gt; Optional[SymbolTableNode]:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.379">@abstractmethod
def named_type(self, fullname: str,
               args: Optional[List[Type]] = None) -&gt; Instance:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.38">def visit_decorator(self, dec: Decorator) -&gt; None:
    self.statement = dec
    # TODO: better don't modify them at all.
    dec.decorators = dec.original_decorators.copy()
    dec.func.is_conditional = self.block_depth[-1] &gt; 0
    if not dec.is_overload:
        self.add_symbol(dec.name, dec, dec)
    dec.func._fullname = self.qualified_name(dec.name)
    for d in dec.decorators:
        d.accept(self)
    removed: List[int] = []
    no_type_check = False
    for i, d in enumerate(dec.decorators):
        # A bunch of decorators are special cased here.
        if refers_to_fullname(d, 'abc.abstractmethod'):
            removed.append(i)
            dec.func.is_abstract = True
            self.check_decorated_function_is_method('abstractmethod', dec)
        elif refers_to_fullname(d, ('asyncio.coroutines.coroutine', 'types.coroutine')):
            removed.append(i)
            dec.func.is_awaitable_coroutine = True
        elif refers_to_fullname(d, 'builtins.staticmethod'):
            removed.append(i)
            dec.func.is_static = True
            dec.var.is_staticmethod = True
            self.check_decorated_function_is_method('staticmethod', dec)
        elif refers_to_fullname(d, 'builtins.classmethod'):
            removed.append(i)
            dec.func.is_class = True
            dec.var.is_classmethod = True
            self.check_decorated_function_is_method('classmethod', dec)
        elif refers_to_fullname(d, (
                'builtins.property',
                'abc.abstractproperty',
                'functools.cached_property')):
            removed.append(i)
            dec.func.is_property = True
            dec.var.is_property = True
            if refers_to_fullname(d, 'abc.abstractproperty'):
                dec.func.is_abstract = True
            elif refers_to_fullname(d, 'functools.cached_property'):
                dec.var.is_settable_property = True
            self.check_decorated_function_is_method('property', dec)
            if len(dec.func.arguments) &gt; 1:
                self.fail('Too many arguments', dec.func)
        elif refers_to_fullname(d, 'typing.no_type_check'):
            dec.var.type = AnyType(TypeOfAny.special_form)
            no_type_check = True
        elif refers_to_fullname(d, FINAL_DECORATOR_NAMES):
            if self.is_class_scope():
                assert self.type is not None, "No type set at class scope"
                if self.type.is_protocol:
                    self.msg.protocol_members_cant_be_final(d)
                else:
                    dec.func.is_final = True
                    dec.var.is_final = True
                removed.append(i)
            else:
                self.fail("@final cannot be used with non-method functions", d)
    for i in reversed(removed):
        del dec.decorators[i]
    if (not dec.is_overload or dec.var.is_property) and self.type:
        dec.var.info = self.type
        dec.var.is_initialized_in_class = True
    if not no_type_check and self.recurse_into_functions:
        dec.func.accept(self)
    if dec.decorators and dec.var.is_property:
        self.fail('Decorated property not supported', dec)
    if dec.func.is_abstract and dec.func.is_final:
        self.fail(f"Method {dec.func.name} is both abstract and final", dec)

</t>
<t tx="ekr.20220525082935.380">@abstractmethod
def named_type_or_none(self, fullname: str,
                       args: Optional[List[Type]] = None) -&gt; Optional[Instance]:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.381">@abstractmethod
def accept(self, node: Node) -&gt; None:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.382">@abstractmethod
def anal_type(self, t: Type, *,
              tvar_scope: Optional[TypeVarLikeScope] = None,
              allow_tuple_literal: bool = False,
              allow_unbound_tvars: bool = False,
              allow_required: bool = False,
              report_invalid_types: bool = True) -&gt; Optional[Type]:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.383">@abstractmethod
def basic_new_typeinfo(self, name: str, basetype_or_fallback: Instance, line: int) -&gt; TypeInfo:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.384">@abstractmethod
def schedule_patch(self, priority: int, fn: Callable[[], None]) -&gt; None:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.385">@abstractmethod
def add_symbol_table_node(self, name: str, stnode: SymbolTableNode) -&gt; bool:
    """Add node to the current symbol table."""
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.386">@abstractmethod
def current_symbol_table(self) -&gt; SymbolTable:
    """Get currently active symbol table.

    May be module, class, or local namespace.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.387">@abstractmethod
def add_symbol(self, name: str, node: SymbolNode, context: Context,
               module_public: bool = True, module_hidden: bool = False,
               can_defer: bool = True) -&gt; bool:
    """Add symbol to the current symbol table."""
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.388">@abstractmethod
def add_symbol_skip_local(self, name: str, node: SymbolNode) -&gt; None:
    """Add symbol to the current symbol table, skipping locals.

    This is used to store symbol nodes in a symbol table that
    is going to be serialized (local namespaces are not serialized).
    See implementation docstring for more details.
    """
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.389">@abstractmethod
def parse_bool(self, expr: Expression) -&gt; Optional[bool]:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.39">def check_decorated_function_is_method(self, decorator: str,
                                       context: Context) -&gt; None:
    if not self.type or self.is_func_scope():
        self.fail(f'"{decorator}" used with a non-method', context)

</t>
<t tx="ekr.20220525082935.390">@abstractmethod
def qualified_name(self, n: str) -&gt; str:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.391">@property
@abstractmethod
def is_typeshed_stub_file(self) -&gt; bool:
    raise NotImplementedError

</t>
<t tx="ekr.20220525082935.392">@abstractmethod
def is_func_scope(self) -&gt; bool:
    raise NotImplementedError


</t>
<t tx="ekr.20220525082935.393">def set_callable_name(sig: Type, fdef: FuncDef) -&gt; ProperType:
    sig = get_proper_type(sig)
    if isinstance(sig, FunctionLike):
        if fdef.info:
            if fdef.info.fullname in TPDICT_FB_NAMES:
                # Avoid exposing the internal _TypedDict name.
                class_name = 'TypedDict'
            else:
                class_name = fdef.info.name
            return sig.with_name(
                f'{fdef.name} of {class_name}')
        else:
            return sig.with_name(fdef.name)
    else:
        return sig


</t>
<t tx="ekr.20220525082935.394">def calculate_tuple_fallback(typ: TupleType) -&gt; None:
    """Calculate a precise item type for the fallback of a tuple type.

    This must be called only after the main semantic analysis pass, since joins
    aren't available before that.

    Note that there is an apparent chicken and egg problem with respect
    to verifying type arguments against bounds. Verifying bounds might
    require fallbacks, but we might use the bounds to calculate the
    fallbacks. In practice this is not a problem, since the worst that
    can happen is that we have invalid type argument values, and these
    can happen in later stages as well (they will generate errors, but
    we don't prevent their existence).
    """
    fallback = typ.partial_fallback
    assert fallback.type.fullname == 'builtins.tuple'
    fallback.args = (join.join_type_list(list(typ.items)),) + fallback.args[1:]


</t>
<t tx="ekr.20220525082935.395">class _NamedTypeCallback(Protocol):
    def __call__(
        self, fully_qualified_name: str, args: Optional[List[Type]] = None
    ) -&gt; Instance: ...


</t>
<t tx="ekr.20220525082935.396">def paramspec_args(
    name: str, fullname: str, id: Union[TypeVarId, int], *,
    named_type_func: _NamedTypeCallback, line: int = -1, column: int = -1,
    prefix: Optional[Parameters] = None
) -&gt; ParamSpecType:
    return ParamSpecType(
        name,
        fullname,
        id,
        flavor=ParamSpecFlavor.ARGS,
        upper_bound=named_type_func('builtins.tuple', [named_type_func('builtins.object')]),
        line=line,
        column=column,
        prefix=prefix
    )


</t>
<t tx="ekr.20220525082935.397">def paramspec_kwargs(
    name: str, fullname: str, id: Union[TypeVarId, int], *,
    named_type_func: _NamedTypeCallback, line: int = -1, column: int = -1,
    prefix: Optional[Parameters] = None
) -&gt; ParamSpecType:
    return ParamSpecType(
        name,
        fullname,
        id,
        flavor=ParamSpecFlavor.KWARGS,
        upper_bound=named_type_func(
            'builtins.dict',
            [named_type_func('builtins.str'), named_type_func('builtins.object')]
        ),
        line=line,
        column=column,
        prefix=prefix
    )
</t>
<t tx="ekr.20220525082935.398">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Verify properties of type arguments, like 'int' in C[int] being valid.

This must happen after semantic analysis since there can be placeholder
types until the end of semantic analysis, and these break various type
operations, including subtype checks.
"""

from typing import List, Optional, Set

from mypy.nodes import TypeInfo, Context, MypyFile, FuncItem, ClassDef, Block, FakeInfo
from mypy.types import (
    Type, Instance, TypeVarType, AnyType, get_proper_types, TypeAliasType, ParamSpecType,
    UnpackType, TupleType, TypeVarTupleType, TypeOfAny, get_proper_type
)
from mypy.mixedtraverser import MixedTraverserVisitor
from mypy.subtypes import is_subtype
from mypy.sametypes import is_same_type
from mypy.errors import Errors
from mypy.scope import Scope
from mypy.options import Options
from mypy.errorcodes import ErrorCode
from mypy import message_registry, errorcodes as codes
from mypy.messages import format_type


@others
</t>
<t tx="ekr.20220525082935.399">class TypeArgumentAnalyzer(MixedTraverserVisitor):
    @others
</t>
<t tx="ekr.20220525082935.4">@property
def is_typeshed_stub_file(self) -&gt; bool:
    return self._is_typeshed_stub_file

</t>
<t tx="ekr.20220525082935.40">#
# Classes
#

</t>
<t tx="ekr.20220525082935.400">def __init__(self, errors: Errors, options: Options, is_typeshed_file: bool) -&gt; None:
    self.errors = errors
    self.options = options
    self.is_typeshed_file = is_typeshed_file
    self.scope = Scope()
    # Should we also analyze function definitions, or only module top-levels?
    self.recurse_into_functions = True
    # Keep track of the type aliases already visited. This is needed to avoid
    # infinite recursion on types like A = Union[int, List[A]].
    self.seen_aliases: Set[TypeAliasType] = set()

</t>
<t tx="ekr.20220525082935.401">def visit_mypy_file(self, o: MypyFile) -&gt; None:
    self.errors.set_file(o.path, o.fullname, scope=self.scope)
    with self.scope.module_scope(o.fullname):
        super().visit_mypy_file(o)

</t>
<t tx="ekr.20220525082935.402">def visit_func(self, defn: FuncItem) -&gt; None:
    if not self.recurse_into_functions:
        return
    with self.scope.function_scope(defn):
        super().visit_func(defn)

</t>
<t tx="ekr.20220525082935.403">def visit_class_def(self, defn: ClassDef) -&gt; None:
    with self.scope.class_scope(defn.info):
        super().visit_class_def(defn)

</t>
<t tx="ekr.20220525082935.404">def visit_block(self, o: Block) -&gt; None:
    if not o.is_unreachable:
        super().visit_block(o)

</t>
<t tx="ekr.20220525082935.405">def visit_type_alias_type(self, t: TypeAliasType) -&gt; None:
    super().visit_type_alias_type(t)
    if t in self.seen_aliases:
        # Avoid infinite recursion on recursive type aliases.
        # Note: it is fine to skip the aliases we have already seen in non-recursive types,
        # since errors there have already already reported.
        return
    self.seen_aliases.add(t)
    get_proper_type(t).accept(self)

</t>
<t tx="ekr.20220525082935.406">def visit_instance(self, t: Instance) -&gt; None:
    # Type argument counts were checked in the main semantic analyzer pass. We assume
    # that the counts are correct here.
    info = t.type
    if isinstance(info, FakeInfo):
        return  # https://github.com/python/mypy/issues/11079
    for (i, arg), tvar in zip(enumerate(t.args), info.defn.type_vars):
        if isinstance(tvar, TypeVarType):
            if isinstance(arg, ParamSpecType):
                # TODO: Better message
                self.fail(f'Invalid location for ParamSpec "{arg.name}"', t)
                continue
            if tvar.values:
                if isinstance(arg, TypeVarType):
                    arg_values = arg.values
                    if not arg_values:
                        self.fail(
                            message_registry.INVALID_TYPEVAR_AS_TYPEARG.format(
                                arg.name, info.name),
                            t, code=codes.TYPE_VAR)
                        continue
                else:
                    arg_values = [arg]
                self.check_type_var_values(info, arg_values, tvar.name, tvar.values, i + 1, t)
            if not is_subtype(arg, tvar.upper_bound):
                self.fail(
                    message_registry.INVALID_TYPEVAR_ARG_BOUND.format(
                        format_type(arg), info.name, format_type(tvar.upper_bound)),
                    t, code=codes.TYPE_VAR)
    super().visit_instance(t)

</t>
<t tx="ekr.20220525082935.407">def visit_unpack_type(self, typ: UnpackType) -&gt; None:
    proper_type = get_proper_type(typ.type)
    if isinstance(proper_type, TupleType):
        return
    if isinstance(proper_type, TypeVarTupleType):
        return
    if isinstance(proper_type, Instance) and proper_type.type.fullname == "builtins.tuple":
        return
    if isinstance(proper_type, AnyType) and proper_type.type_of_any == TypeOfAny.from_error:
        return

    # TODO: Infer something when it can't be unpacked to allow rest of
    # typechecking to work.
    self.fail(message_registry.INVALID_UNPACK.format(proper_type), typ)

</t>
<t tx="ekr.20220525082935.408">def check_type_var_values(self, type: TypeInfo, actuals: List[Type], arg_name: str,
                          valids: List[Type], arg_number: int, context: Context) -&gt; None:
    for actual in get_proper_types(actuals):
        if (not isinstance(actual, AnyType) and
                not any(is_same_type(actual, value)
                        for value in valids)):
            if len(actuals) &gt; 1 or not isinstance(actual, Instance):
                self.fail(
                    message_registry.INVALID_TYPEVAR_ARG_VALUE.format(type.name),
                    context, code=codes.TYPE_VAR)
            else:
                class_name = f'"{type.name}"'
                actual_type_name = f'"{actual.type.name}"'
                self.fail(
                    message_registry.INCOMPATIBLE_TYPEVAR_VALUE.format(
                        arg_name, class_name, actual_type_name),
                    context,
                    code=codes.TYPE_VAR)

</t>
<t tx="ekr.20220525082935.409">def fail(self, msg: str, context: Context, *, code: Optional[ErrorCode] = None) -&gt; None:
    self.errors.report(context.get_line(), context.get_column(), msg, code=code)
</t>
<t tx="ekr.20220525082935.41">def visit_class_def(self, defn: ClassDef) -&gt; None:
    self.statement = defn
    self.incomplete_type_stack.append(not defn.info)
    namespace = self.qualified_name(defn.name)
    with self.tvar_scope_frame(self.tvar_scope.class_frame(namespace)):
        self.analyze_class(defn)
    self.incomplete_type_stack.pop()

</t>
<t tx="ekr.20220525082935.410">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Semantic analysis of TypedDict definitions."""

from mypy.backports import OrderedDict
from typing import Optional, List, Set, Tuple
from typing_extensions import Final

from mypy.types import (
    Type, AnyType, TypeOfAny, TypedDictType, TPDICT_NAMES, RequiredType,
)
from mypy.nodes import (
    CallExpr, TypedDictExpr, Expression, NameExpr, Context, StrExpr, BytesExpr, UnicodeExpr,
    ClassDef, RefExpr, TypeInfo, AssignmentStmt, PassStmt, ExpressionStmt, EllipsisExpr, TempNode,
    DictExpr, ARG_POS, ARG_NAMED
)
from mypy.semanal_shared import SemanticAnalyzerInterface
from mypy.exprtotype import expr_to_unanalyzed_type, TypeTranslationError
from mypy.options import Options
from mypy.typeanal import check_for_explicit_any, has_any_from_unimported_type
from mypy.messages import MessageBuilder
from mypy.errorcodes import ErrorCode
from mypy import errorcodes as codes

TPDICT_CLASS_ERROR: Final = (
    "Invalid statement in TypedDict definition; " 'expected "field_name: field_type"'
)


@others
</t>
<t tx="ekr.20220525082935.411">class TypedDictAnalyzer:
    @others
</t>
<t tx="ekr.20220525082935.412">def __init__(self,
             options: Options,
             api: SemanticAnalyzerInterface,
             msg: MessageBuilder) -&gt; None:
    self.options = options
    self.api = api
    self.msg = msg

</t>
<t tx="ekr.20220525082935.413">def analyze_typeddict_classdef(self, defn: ClassDef) -&gt; Tuple[bool, Optional[TypeInfo]]:
    """Analyze a class that may define a TypedDict.

    Assume that base classes have been analyzed already.

    Note: Unlike normal classes, we won't create a TypeInfo until
    the whole definition of the TypeDict (including the body and all
    key names and types) is complete.  This is mostly because we
    store the corresponding TypedDictType in the TypeInfo.

    Return (is this a TypedDict, new TypeInfo). Specifics:
     * If we couldn't finish due to incomplete reference anywhere in
       the definition, return (True, None).
     * If this is not a TypedDict, return (False, None).
    """
    possible = False
    for base_expr in defn.base_type_exprs:
        if isinstance(base_expr, RefExpr):
            self.api.accept(base_expr)
            if base_expr.fullname in TPDICT_NAMES or self.is_typeddict(base_expr):
                possible = True
    if possible:
        if (len(defn.base_type_exprs) == 1 and
                isinstance(defn.base_type_exprs[0], RefExpr) and
                defn.base_type_exprs[0].fullname in TPDICT_NAMES):
            # Building a new TypedDict
            fields, types, required_keys = self.analyze_typeddict_classdef_fields(defn)
            if fields is None:
                return True, None  # Defer
            info = self.build_typeddict_typeinfo(defn.name, fields, types, required_keys,
                                                 defn.line)
            defn.analyzed = TypedDictExpr(info)
            defn.analyzed.line = defn.line
            defn.analyzed.column = defn.column
            return True, info

        # Extending/merging existing TypedDicts
        typeddict_bases = []
        typeddict_bases_set = set()
        for expr in defn.base_type_exprs:
            if isinstance(expr, RefExpr) and expr.fullname in TPDICT_NAMES:
                if 'TypedDict' not in typeddict_bases_set:
                    typeddict_bases_set.add('TypedDict')
                else:
                    self.fail('Duplicate base class "TypedDict"', defn)
            elif isinstance(expr, RefExpr) and self.is_typeddict(expr):
                assert expr.fullname
                if expr.fullname not in typeddict_bases_set:
                    typeddict_bases_set.add(expr.fullname)
                    typeddict_bases.append(expr)
                else:
                    assert isinstance(expr.node, TypeInfo)
                    self.fail(f'Duplicate base class "{expr.node.name}"', defn)
            else:
                self.fail("All bases of a new TypedDict must be TypedDict types", defn)

        keys: List[str] = []
        types = []
        required_keys = set()
        # Iterate over bases in reverse order so that leftmost base class' keys take precedence
        for base in reversed(typeddict_bases):
            assert isinstance(base, RefExpr)
            assert isinstance(base.node, TypeInfo)
            assert isinstance(base.node.typeddict_type, TypedDictType)
            base_typed_dict = base.node.typeddict_type
            base_items = base_typed_dict.items
            valid_items = base_items.copy()
            for key in base_items:
                if key in keys:
                    self.fail('Overwriting TypedDict field "{}" while merging'
                              .format(key), defn)
            keys.extend(valid_items.keys())
            types.extend(valid_items.values())
            required_keys.update(base_typed_dict.required_keys)
        new_keys, new_types, new_required_keys = self.analyze_typeddict_classdef_fields(defn,
                                                                                        keys)
        if new_keys is None:
            return True, None  # Defer
        keys.extend(new_keys)
        types.extend(new_types)
        required_keys.update(new_required_keys)
        info = self.build_typeddict_typeinfo(defn.name, keys, types, required_keys, defn.line)
        defn.analyzed = TypedDictExpr(info)
        defn.analyzed.line = defn.line
        defn.analyzed.column = defn.column
        return True, info
    return False, None

</t>
<t tx="ekr.20220525082935.414">def analyze_typeddict_classdef_fields(
        self,
        defn: ClassDef,
        oldfields: Optional[List[str]] = None) -&gt; Tuple[Optional[List[str]],
                                                        List[Type],
                                                        Set[str]]:
    """Analyze fields defined in a TypedDict class definition.

    This doesn't consider inherited fields (if any). Also consider totality,
    if given.

    Return tuple with these items:
     * List of keys (or None if found an incomplete reference --&gt; deferral)
     * List of types for each key
     * Set of required keys
    """
    fields: List[str] = []
    types: List[Type] = []
    for stmt in defn.defs.body:
        if not isinstance(stmt, AssignmentStmt):
            # Still allow pass or ... (for empty TypedDict's).
            if (not isinstance(stmt, PassStmt) and
                not (isinstance(stmt, ExpressionStmt) and
                     isinstance(stmt.expr, (EllipsisExpr, StrExpr)))):
                self.fail(TPDICT_CLASS_ERROR, stmt)
        elif len(stmt.lvalues) &gt; 1 or not isinstance(stmt.lvalues[0], NameExpr):
            # An assignment, but an invalid one.
            self.fail(TPDICT_CLASS_ERROR, stmt)
        else:
            name = stmt.lvalues[0].name
            if name in (oldfields or []):
                self.fail('Overwriting TypedDict field "{}" while extending'
                          .format(name), stmt)
            if name in fields:
                self.fail(f'Duplicate TypedDict key "{name}"', stmt)
                continue
            # Append name and type in this case...
            fields.append(name)
            if stmt.type is None:
                types.append(AnyType(TypeOfAny.unannotated))
            else:
                analyzed = self.api.anal_type(stmt.type, allow_required=True)
                if analyzed is None:
                    return None, [], set()  # Need to defer
                types.append(analyzed)
            # ...despite possible minor failures that allow further analyzis.
            if stmt.type is None or hasattr(stmt, 'new_syntax') and not stmt.new_syntax:
                self.fail(TPDICT_CLASS_ERROR, stmt)
            elif not isinstance(stmt.rvalue, TempNode):
                # x: int assigns rvalue to TempNode(AnyType())
                self.fail('Right hand side values are not supported in TypedDict', stmt)
    total: Optional[bool] = True
    if 'total' in defn.keywords:
        total = self.api.parse_bool(defn.keywords['total'])
        if total is None:
            self.fail('Value of "total" must be True or False', defn)
            total = True
    required_keys = {
        field
        for (field, t) in zip(fields, types)
        if (total or (
            isinstance(t, RequiredType) and  # type: ignore[misc]
            t.required
        )) and not (
            isinstance(t, RequiredType) and  # type: ignore[misc]
            not t.required
        )
    }
    types = [  # unwrap Required[T] to just T
        t.item if isinstance(t, RequiredType) else t  # type: ignore[misc]
        for t in types
    ]

    return fields, types, required_keys

</t>
<t tx="ekr.20220525082935.415">def check_typeddict(self,
                    node: Expression,
                    var_name: Optional[str],
                    is_func_scope: bool) -&gt; Tuple[bool, Optional[TypeInfo]]:
    """Check if a call defines a TypedDict.

    The optional var_name argument is the name of the variable to
    which this is assigned, if any.

    Return a pair (is it a typed dict, corresponding TypeInfo).

    If the definition is invalid but looks like a TypedDict,
    report errors but return (some) TypeInfo. If some type is not ready,
    return (True, None).
    """
    if not isinstance(node, CallExpr):
        return False, None
    call = node
    callee = call.callee
    if not isinstance(callee, RefExpr):
        return False, None
    fullname = callee.fullname
    if fullname not in TPDICT_NAMES:
        return False, None
    res = self.parse_typeddict_args(call)
    if res is None:
        # This is a valid typed dict, but some type is not ready.
        # The caller should defer this until next iteration.
        return True, None
    name, items, types, total, ok = res
    if not ok:
        # Error. Construct dummy return value.
        info = self.build_typeddict_typeinfo('TypedDict', [], [], set(), call.line)
    else:
        if var_name is not None and name != var_name:
            self.fail(
                'First argument "{}" to TypedDict() does not match variable name "{}"'.format(
                    name, var_name), node, code=codes.NAME_MATCH)
        if name != var_name or is_func_scope:
            # Give it a unique name derived from the line number.
            name += '@' + str(call.line)
        required_keys = {
            field
            for (field, t) in zip(items, types)
            if (total or (
                isinstance(t, RequiredType) and  # type: ignore[misc]
                t.required
            )) and not (
                isinstance(t, RequiredType) and  # type: ignore[misc]
                not t.required
            )
        }
        types = [  # unwrap Required[T] to just T
            t.item if isinstance(t, RequiredType) else t  # type: ignore[misc]
            for t in types
        ]
        info = self.build_typeddict_typeinfo(name, items, types, required_keys, call.line)
        info.line = node.line
        # Store generated TypeInfo under both names, see semanal_namedtuple for more details.
        if name != var_name or is_func_scope:
            self.api.add_symbol_skip_local(name, info)
    if var_name:
        self.api.add_symbol(var_name, info, node)
    call.analyzed = TypedDictExpr(info)
    call.analyzed.set_line(call.line, call.column)
    return True, info

</t>
<t tx="ekr.20220525082935.416">def parse_typeddict_args(
        self, call: CallExpr) -&gt; Optional[Tuple[str, List[str], List[Type], bool, bool]]:
    """Parse typed dict call expression.

    Return names, types, totality, was there an error during parsing.
    If some type is not ready, return None.
    """
    # TODO: Share code with check_argument_count in checkexpr.py?
    args = call.args
    if len(args) &lt; 2:
        return self.fail_typeddict_arg("Too few arguments for TypedDict()", call)
    if len(args) &gt; 3:
        return self.fail_typeddict_arg("Too many arguments for TypedDict()", call)
    # TODO: Support keyword arguments
    if call.arg_kinds not in ([ARG_POS, ARG_POS], [ARG_POS, ARG_POS, ARG_NAMED]):
        return self.fail_typeddict_arg("Unexpected arguments to TypedDict()", call)
    if len(args) == 3 and call.arg_names[2] != 'total':
        return self.fail_typeddict_arg(
            f'Unexpected keyword argument "{call.arg_names[2]}" for "TypedDict"', call)
    if not isinstance(args[0], (StrExpr, BytesExpr, UnicodeExpr)):
        return self.fail_typeddict_arg(
            "TypedDict() expects a string literal as the first argument", call)
    if not isinstance(args[1], DictExpr):
        return self.fail_typeddict_arg(
            "TypedDict() expects a dictionary literal as the second argument", call)
    total: Optional[bool] = True
    if len(args) == 3:
        total = self.api.parse_bool(call.args[2])
        if total is None:
            return self.fail_typeddict_arg(
                'TypedDict() "total" argument must be True or False', call)
    dictexpr = args[1]
    res = self.parse_typeddict_fields_with_types(dictexpr.items, call)
    if res is None:
        # One of the types is not ready, defer.
        return None
    items, types, ok = res
    for t in types:
        check_for_explicit_any(t, self.options, self.api.is_typeshed_stub_file, self.msg,
                               context=call)

    if self.options.disallow_any_unimported:
        for t in types:
            if has_any_from_unimported_type(t):
                self.msg.unimported_type_becomes_any("Type of a TypedDict key", t, dictexpr)
    assert total is not None
    return args[0].value, items, types, total, ok

</t>
<t tx="ekr.20220525082935.417">def parse_typeddict_fields_with_types(
        self,
        dict_items: List[Tuple[Optional[Expression], Expression]],
        context: Context) -&gt; Optional[Tuple[List[str], List[Type], bool]]:
    """Parse typed dict items passed as pairs (name expression, type expression).

    Return names, types, was there an error. If some type is not ready, return None.
    """
    seen_keys = set()
    items: List[str] = []
    types: List[Type] = []
    for (field_name_expr, field_type_expr) in dict_items:
        if isinstance(field_name_expr, (StrExpr, BytesExpr, UnicodeExpr)):
            key = field_name_expr.value
            items.append(key)
            if key in seen_keys:
                self.fail(f'Duplicate TypedDict key "{key}"', field_name_expr)
            seen_keys.add(key)
        else:
            name_context = field_name_expr or field_type_expr
            self.fail_typeddict_arg("Invalid TypedDict() field name", name_context)
            return [], [], False
        try:
            type = expr_to_unanalyzed_type(field_type_expr, self.options,
                                           self.api.is_stub_file)
        except TypeTranslationError:
            if (isinstance(field_type_expr, CallExpr) and
                    isinstance(field_type_expr.callee, RefExpr) and
                    field_type_expr.callee.fullname in TPDICT_NAMES):
                self.fail_typeddict_arg(
                    'Inline TypedDict types not supported; use assignment to define TypedDict',
                    field_type_expr)
            else:
                self.fail_typeddict_arg('Invalid field type', field_type_expr)
            return [], [], False
        analyzed = self.api.anal_type(type, allow_required=True)
        if analyzed is None:
            return None
        types.append(analyzed)
    return items, types, True

</t>
<t tx="ekr.20220525082935.418">def fail_typeddict_arg(self, message: str,
                       context: Context) -&gt; Tuple[str, List[str], List[Type], bool, bool]:
    self.fail(message, context)
    return '', [], [], True, False

</t>
<t tx="ekr.20220525082935.419">def build_typeddict_typeinfo(self, name: str, items: List[str],
                             types: List[Type],
                             required_keys: Set[str],
                             line: int) -&gt; TypeInfo:
    # Prefer typing then typing_extensions if available.
    fallback = (self.api.named_type_or_none('typing._TypedDict', []) or
                self.api.named_type_or_none('typing_extensions._TypedDict', []) or
                self.api.named_type_or_none('mypy_extensions._TypedDict', []))
    assert fallback is not None
    info = self.api.basic_new_typeinfo(name, fallback, line)
    info.typeddict_type = TypedDictType(OrderedDict(zip(items, types)), required_keys,
                                        fallback)
    return info

</t>
<t tx="ekr.20220525082935.42">def analyze_class(self, defn: ClassDef) -&gt; None:
    fullname = self.qualified_name(defn.name)
    if not defn.info and not self.is_core_builtin_class(defn):
        # Add placeholder so that self-references in base classes can be
        # resolved.  We don't want this to cause a deferral, since if there
        # are no incomplete references, we'll replace this with a TypeInfo
        # before returning.
        placeholder = PlaceholderNode(fullname, defn, defn.line, becomes_typeinfo=True)
        self.add_symbol(defn.name, placeholder, defn, can_defer=False)

    tag = self.track_incomplete_refs()

    # Restore base classes after previous iteration (things like Generic[T] might be removed).
    defn.base_type_exprs.extend(defn.removed_base_type_exprs)
    defn.removed_base_type_exprs.clear()

    self.update_metaclass(defn)

    bases = defn.base_type_exprs
    bases, tvar_defs, is_protocol = self.clean_up_bases_and_infer_type_variables(
        defn, bases, context=defn)

    for tvd in tvar_defs:
        if (isinstance(tvd, TypeVarType)
                and any(has_placeholder(t) for t in [tvd.upper_bound] + tvd.values)):
            # Some type variable bounds or values are not ready, we need
            # to re-analyze this class.
            self.defer()

    self.analyze_class_keywords(defn)
    result = self.analyze_base_classes(bases)

    if result is None or self.found_incomplete_ref(tag):
        # Something was incomplete. Defer current target.
        self.mark_incomplete(defn.name, defn)
        return

    base_types, base_error = result
    if any(isinstance(base, PlaceholderType) for base, _ in base_types):
        # We need to know the TypeInfo of each base to construct the MRO. Placeholder types
        # are okay in nested positions, since they can't affect the MRO.
        self.mark_incomplete(defn.name, defn)
        return

    is_typeddict, info = self.typed_dict_analyzer.analyze_typeddict_classdef(defn)
    if is_typeddict:
        for decorator in defn.decorators:
            decorator.accept(self)
            if isinstance(decorator, RefExpr):
                if decorator.fullname in FINAL_DECORATOR_NAMES:
                    self.fail("@final cannot be used with TypedDict", decorator)
        if info is None:
            self.mark_incomplete(defn.name, defn)
        else:
            self.prepare_class_def(defn, info)
        return

    if self.analyze_namedtuple_classdef(defn):
        return

    # Create TypeInfo for class now that base classes and the MRO can be calculated.
    self.prepare_class_def(defn)

    defn.type_vars = tvar_defs
    defn.info.type_vars = []
    # we want to make sure any additional logic in add_type_vars gets run
    defn.info.add_type_vars()
    if base_error:
        defn.info.fallback_to_any = True

    with self.scope.class_scope(defn.info):
        self.configure_base_classes(defn, base_types)
        defn.info.is_protocol = is_protocol
        self.analyze_metaclass(defn)
        defn.info.runtime_protocol = False
        for decorator in defn.decorators:
            self.analyze_class_decorator(defn, decorator)
        self.analyze_class_body_common(defn)

</t>
<t tx="ekr.20220525082935.420"># Helpers

</t>
<t tx="ekr.20220525082935.421">def is_typeddict(self, expr: Expression) -&gt; bool:
    return (isinstance(expr, RefExpr) and isinstance(expr.node, TypeInfo) and
            expr.node.typeddict_type is not None)

</t>
<t tx="ekr.20220525082935.422">def fail(self, msg: str, ctx: Context, *, code: Optional[ErrorCode] = None) -&gt; None:
    self.api.fail(msg, ctx, code=code)

</t>
<t tx="ekr.20220525082935.423">def note(self, msg: str, ctx: Context) -&gt; None:
    self.api.note(msg, ctx)
</t>
<t tx="ekr.20220525082935.424">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from typing import Optional
from typing_extensions import Final

"""Shared logic between our three mypy parser files."""


_NON_BINARY_MAGIC_METHODS: Final = {
    "__abs__",
    "__call__",
    "__complex__",
    "__contains__",
    "__del__",
    "__delattr__",
    "__delitem__",
    "__enter__",
    "__exit__",
    "__float__",
    "__getattr__",
    "__getattribute__",
    "__getitem__",
    "__hex__",
    "__init__",
    "__init_subclass__",
    "__int__",
    "__invert__",
    "__iter__",
    "__len__",
    "__long__",
    "__neg__",
    "__new__",
    "__nonzero__",
    "__oct__",
    "__pos__",
    "__repr__",
    "__reversed__",
    "__setattr__",
    "__setitem__",
    "__str__",
    "__unicode__",
}

MAGIC_METHODS_ALLOWING_KWARGS: Final = {
    "__init__",
    "__init_subclass__",
    "__new__",
    "__call__",
    "__setattr__",
}

BINARY_MAGIC_METHODS: Final = {
    "__add__",
    "__and__",
    "__cmp__",
    "__divmod__",
    "__div__",
    "__eq__",
    "__floordiv__",
    "__ge__",
    "__gt__",
    "__iadd__",
    "__iand__",
    "__idiv__",
    "__ifloordiv__",
    "__ilshift__",
    "__imatmul__",
    "__imod__",
    "__imul__",
    "__ior__",
    "__ipow__",
    "__irshift__",
    "__isub__",
    "__itruediv__",
    "__ixor__",
    "__le__",
    "__lshift__",
    "__lt__",
    "__matmul__",
    "__mod__",
    "__mul__",
    "__ne__",
    "__or__",
    "__pow__",
    "__radd__",
    "__rand__",
    "__rdiv__",
    "__rfloordiv__",
    "__rlshift__",
    "__rmatmul__",
    "__rmod__",
    "__rmul__",
    "__ror__",
    "__rpow__",
    "__rrshift__",
    "__rshift__",
    "__rsub__",
    "__rtruediv__",
    "__rxor__",
    "__sub__",
    "__truediv__",
    "__xor__",
}

assert not (_NON_BINARY_MAGIC_METHODS &amp; BINARY_MAGIC_METHODS)

MAGIC_METHODS: Final = _NON_BINARY_MAGIC_METHODS | BINARY_MAGIC_METHODS

MAGIC_METHODS_POS_ARGS_ONLY: Final = MAGIC_METHODS - MAGIC_METHODS_ALLOWING_KWARGS


@others
</t>
<t tx="ekr.20220525082935.425">def special_function_elide_names(name: str) -&gt; bool:
    return name in MAGIC_METHODS_POS_ARGS_ONLY


</t>
<t tx="ekr.20220525082935.426">def argument_elide_name(name: Optional[str]) -&gt; bool:
    return name is not None and name.startswith("__") and not name.endswith("__")
</t>
<t tx="ekr.20220525082935.427">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Type inference constraint solving"""

from typing import List, Dict, Optional
from collections import defaultdict

from mypy.types import Type, AnyType, UninhabitedType, TypeVarId, TypeOfAny, get_proper_type
from mypy.constraints import Constraint, SUPERTYPE_OF
from mypy.join import join_types
from mypy.meet import meet_types
from mypy.subtypes import is_subtype


@others
</t>
<t tx="ekr.20220525082935.428">def solve_constraints(vars: List[TypeVarId], constraints: List[Constraint],
                      strict: bool = True) -&gt; List[Optional[Type]]:
    """Solve type constraints.

    Return the best type(s) for type variables; each type can be None if the value of the variable
    could not be solved.

    If a variable has no constraints, if strict=True then arbitrarily
    pick NoneType as the value of the type variable.  If strict=False,
    pick AnyType.
    """
    # Collect a list of constraints for each type variable.
    cmap: Dict[TypeVarId, List[Constraint]] = defaultdict(list)
    for con in constraints:
        cmap[con.type_var].append(con)

    res: List[Optional[Type]] = []

    # Solve each type variable separately.
    for tvar in vars:
        bottom: Optional[Type] = None
        top: Optional[Type] = None
        candidate: Optional[Type] = None

        # Process each constraint separately, and calculate the lower and upper
        # bounds based on constraints. Note that we assume that the constraint
        # targets do not have constraint references.
        for c in cmap.get(tvar, []):
            if c.op == SUPERTYPE_OF:
                if bottom is None:
                    bottom = c.target
                else:
                    bottom = join_types(bottom, c.target)
            else:
                if top is None:
                    top = c.target
                else:
                    top = meet_types(top, c.target)

        top = get_proper_type(top)
        bottom = get_proper_type(bottom)
        if isinstance(top, AnyType) or isinstance(bottom, AnyType):
            source_any = top if isinstance(top, AnyType) else bottom
            assert isinstance(source_any, AnyType)
            res.append(AnyType(TypeOfAny.from_another_any, source_any=source_any))
            continue
        elif bottom is None:
            if top:
                candidate = top
            else:
                # No constraints for type variable -- 'UninhabitedType' is the most specific type.
                if strict:
                    candidate = UninhabitedType()
                    candidate.ambiguous = True
                else:
                    candidate = AnyType(TypeOfAny.special_form)
        elif top is None:
            candidate = bottom
        elif is_subtype(bottom, top):
            candidate = bottom
        else:
            candidate = None
        res.append(candidate)

    return res
</t>
<t tx="ekr.20220525082935.429">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Split namespace for argparse to allow separating options by prefix.

We use this to direct some options to an Options object and some to a
regular namespace.
"""

# In its own file largely because mypyc doesn't support its use of
# __getattr__/__setattr__ and has some issues with __dict__

import argparse

from typing import Tuple, Any


@others
</t>
<t tx="ekr.20220525082935.43">def is_core_builtin_class(self, defn: ClassDef) -&gt; bool:
    return self.cur_mod_id == 'builtins' and defn.name in CORE_BUILTIN_CLASSES

</t>
<t tx="ekr.20220525082935.430">class SplitNamespace(argparse.Namespace):
    @others
</t>
<t tx="ekr.20220525082935.431">def __init__(self, standard_namespace: object, alt_namespace: object, alt_prefix: str) -&gt; None:
    self.__dict__['_standard_namespace'] = standard_namespace
    self.__dict__['_alt_namespace'] = alt_namespace
    self.__dict__['_alt_prefix'] = alt_prefix

</t>
<t tx="ekr.20220525082935.432">def _get(self) -&gt; Tuple[Any, Any]:
    return (self._standard_namespace, self._alt_namespace)

</t>
<t tx="ekr.20220525082935.433">def __setattr__(self, name: str, value: Any) -&gt; None:
    if name.startswith(self._alt_prefix):
        setattr(self._alt_namespace, name[len(self._alt_prefix):], value)
    else:
        setattr(self._standard_namespace, name, value)

</t>
<t tx="ekr.20220525082935.434">def __getattr__(self, name: str) -&gt; Any:
    if name.startswith(self._alt_prefix):
        return getattr(self._alt_namespace, name[len(self._alt_prefix):])
    else:
        return getattr(self._standard_namespace, name)
</t>
<t tx="ekr.20220525082935.435">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from contextlib import contextmanager
from typing import Optional, Tuple, Iterator

from typing_extensions import Final

# These are global mutable state. Don't add anything here unless there's a very
# good reason.


@others
state: Final = StrictOptionalState(strict_optional=False)
find_occurrences: Optional[Tuple[str, str]] = None
</t>
<t tx="ekr.20220525082935.436">class StrictOptionalState:
    # Wrap this in a class since it's faster that using a module-level attribute.

    @others
</t>
<t tx="ekr.20220525082935.437">def __init__(self, strict_optional: bool) -&gt; None:
    # Value varies by file being processed
    self.strict_optional = strict_optional

</t>
<t tx="ekr.20220525082935.438">@contextmanager
def strict_optional_set(self, value: bool) -&gt; Iterator[None]:
    saved = self.strict_optional
    self.strict_optional = value
    try:
        yield
    finally:
        self.strict_optional = saved


</t>
<t tx="ekr.20220525082935.439">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Utilities for calculating and reporting statistics about types."""

import os
from collections import Counter
from contextlib import contextmanager

import typing
from typing import Dict, List, cast, Optional, Union, Iterator
from typing_extensions import Final

from mypy.traverser import TraverserVisitor
from mypy.typeanal import collect_all_inner_types
from mypy.types import (
    Type, AnyType, Instance, FunctionLike, TupleType, TypeVarType, TypeQuery, CallableType,
    TypeOfAny, get_proper_type, get_proper_types
)
from mypy import nodes
from mypy.nodes import (
    Expression, FuncDef, TypeApplication, AssignmentStmt, NameExpr, CallExpr, MypyFile,
    MemberExpr, OpExpr, ComparisonExpr, IndexExpr, UnaryExpr, YieldFromExpr, RefExpr, ClassDef,
    AssignmentExpr, ImportFrom, Import, ImportAll, PassStmt, BreakStmt, ContinueStmt, StrExpr,
    BytesExpr, UnicodeExpr, IntExpr, FloatExpr, ComplexExpr, EllipsisExpr, ExpressionStmt, Node
)
from mypy.util import correct_relative_import
from mypy.argmap import map_formals_to_actuals

TYPE_EMPTY: Final = 0
TYPE_UNANALYZED: Final = 1  # type of non-typechecked code
TYPE_PRECISE: Final = 2
TYPE_IMPRECISE: Final = 3
TYPE_ANY: Final = 4

precision_names: Final = [
    'empty',
    'unanalyzed',
    'precise',
    'imprecise',
    'any',
]


@others
</t>
<t tx="ekr.20220525082935.44">def analyze_class_body_common(self, defn: ClassDef) -&gt; None:
    """Parts of class body analysis that are common to all kinds of class defs."""
    self.enter_class(defn.info)
    defn.defs.accept(self)
    self.apply_class_plugin_hooks(defn)
    self.leave_class()

</t>
<t tx="ekr.20220525082935.440">class StatisticsVisitor(TraverserVisitor):
    @others
</t>
<t tx="ekr.20220525082935.441">def __init__(self,
             inferred: bool,
             filename: str,
             modules: Dict[str, MypyFile],
             typemap: Optional[Dict[Expression, Type]] = None,
             all_nodes: bool = False,
             visit_untyped_defs: bool = True) -&gt; None:
    self.inferred = inferred
    self.filename = filename
    self.modules = modules
    self.typemap = typemap
    self.all_nodes = all_nodes
    self.visit_untyped_defs = visit_untyped_defs

    self.num_precise_exprs = 0
    self.num_imprecise_exprs = 0
    self.num_any_exprs = 0

    self.num_simple_types = 0
    self.num_generic_types = 0
    self.num_tuple_types = 0
    self.num_function_types = 0
    self.num_typevar_types = 0
    self.num_complex_types = 0
    self.num_any_types = 0

    self.line = -1

    self.line_map: Dict[int, int] = {}

    self.type_of_any_counter: typing.Counter[int] = Counter()
    self.any_line_map: Dict[int, List[AnyType]] = {}

    # For each scope (top level/function), whether the scope was type checked
    # (annotated function).
    #
    # TODO: Handle --check-untyped-defs
    self.checked_scopes = [True]

    self.output: List[str] = []

    TraverserVisitor.__init__(self)

</t>
<t tx="ekr.20220525082935.442">def visit_mypy_file(self, o: MypyFile) -&gt; None:
    self.cur_mod_node = o
    self.cur_mod_id = o.fullname
    super().visit_mypy_file(o)

</t>
<t tx="ekr.20220525082935.443">def visit_import_from(self, imp: ImportFrom) -&gt; None:
    self.process_import(imp)

</t>
<t tx="ekr.20220525082935.444">def visit_import_all(self, imp: ImportAll) -&gt; None:
    self.process_import(imp)

</t>
<t tx="ekr.20220525082935.445">def process_import(self, imp: Union[ImportFrom, ImportAll]) -&gt; None:
    import_id, ok = correct_relative_import(self.cur_mod_id,
                                            imp.relative,
                                            imp.id,
                                            self.cur_mod_node.is_package_init_file())
    if ok and import_id in self.modules:
        kind = TYPE_PRECISE
    else:
        kind = TYPE_ANY
    self.record_line(imp.line, kind)

</t>
<t tx="ekr.20220525082935.446">def visit_import(self, imp: Import) -&gt; None:
    if all(id in self.modules for id, _ in imp.ids):
        kind = TYPE_PRECISE
    else:
        kind = TYPE_ANY
    self.record_line(imp.line, kind)

</t>
<t tx="ekr.20220525082935.447">def visit_func_def(self, o: FuncDef) -&gt; None:
    with self.enter_scope(o):
        self.line = o.line
        if len(o.expanded) &gt; 1 and o.expanded != [o] * len(o.expanded):
            if o in o.expanded:
                print('{}:{}: ERROR: cycle in function expansion; skipping'.format(
                    self.filename,
                    o.get_line()))
                return
            for defn in o.expanded:
                self.visit_func_def(cast(FuncDef, defn))
        else:
            if o.type:
                sig = cast(CallableType, o.type)
                arg_types = sig.arg_types
                if (sig.arg_names and sig.arg_names[0] == 'self' and
                        not self.inferred):
                    arg_types = arg_types[1:]
                for arg in arg_types:
                    self.type(arg)
                self.type(sig.ret_type)
            elif self.all_nodes:
                self.record_line(self.line, TYPE_ANY)
            if not o.is_dynamic() or self.visit_untyped_defs:
                super().visit_func_def(o)

</t>
<t tx="ekr.20220525082935.448">@contextmanager
def enter_scope(self, o: FuncDef) -&gt; Iterator[None]:
    self.checked_scopes.append(o.type is not None and self.checked_scopes[-1])
    yield None
    self.checked_scopes.pop()

</t>
<t tx="ekr.20220525082935.449">def is_checked_scope(self) -&gt; bool:
    return self.checked_scopes[-1]

</t>
<t tx="ekr.20220525082935.45">def analyze_namedtuple_classdef(self, defn: ClassDef) -&gt; bool:
    """Check if this class can define a named tuple."""
    if defn.info and defn.info.is_named_tuple:
        # Don't reprocess everything. We just need to process methods defined
        # in the named tuple class body.
        is_named_tuple, info = True, defn.info  # type: bool, Optional[TypeInfo]
    else:
        is_named_tuple, info = self.named_tuple_analyzer.analyze_namedtuple_classdef(
            defn, self.is_stub_file, self.is_func_scope())
    if is_named_tuple:
        if info is None:
            self.mark_incomplete(defn.name, defn)
        else:
            self.prepare_class_def(defn, info)
            with self.scope.class_scope(defn.info):
                with self.named_tuple_analyzer.save_namedtuple_body(info):
                    self.analyze_class_body_common(defn)
        return True
    return False

</t>
<t tx="ekr.20220525082935.450">def visit_class_def(self, o: ClassDef) -&gt; None:
    self.record_line(o.line, TYPE_PRECISE)  # TODO: Look at base classes
    # Override this method because we don't want to analyze base_type_exprs (base_type_exprs
    # are base classes in a class declaration).
    # While base_type_exprs are technically expressions, type analyzer does not visit them and
    # they are not in the typemap.
    for d in o.decorators:
        d.accept(self)
    o.defs.accept(self)

</t>
<t tx="ekr.20220525082935.451">def visit_type_application(self, o: TypeApplication) -&gt; None:
    self.line = o.line
    for t in o.types:
        self.type(t)
    super().visit_type_application(o)

</t>
<t tx="ekr.20220525082935.452">def visit_assignment_stmt(self, o: AssignmentStmt) -&gt; None:
    self.line = o.line
    if (isinstance(o.rvalue, nodes.CallExpr) and
            isinstance(o.rvalue.analyzed, nodes.TypeVarExpr)):
        # Type variable definition -- not a real assignment.
        return
    if o.type:
        self.type(o.type)
    elif self.inferred and not self.all_nodes:
        # if self.all_nodes is set, lvalues will be visited later
        for lvalue in o.lvalues:
            if isinstance(lvalue, nodes.TupleExpr):
                items = lvalue.items
            else:
                items = [lvalue]
            for item in items:
                if isinstance(item, RefExpr) and item.is_inferred_def:
                    if self.typemap is not None:
                        self.type(self.typemap.get(item))
    super().visit_assignment_stmt(o)

</t>
<t tx="ekr.20220525082935.453">def visit_expression_stmt(self, o: ExpressionStmt) -&gt; None:
    if isinstance(o.expr, (StrExpr, UnicodeExpr, BytesExpr)):
        # Docstring
        self.record_line(o.line, TYPE_EMPTY)
    else:
        super().visit_expression_stmt(o)

</t>
<t tx="ekr.20220525082935.454">def visit_pass_stmt(self, o: PassStmt) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20220525082935.455">def visit_break_stmt(self, o: BreakStmt) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20220525082935.456">def visit_continue_stmt(self, o: ContinueStmt) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20220525082935.457">def visit_name_expr(self, o: NameExpr) -&gt; None:
    if o.fullname in ('builtins.None',
                      'builtins.True',
                      'builtins.False',
                      'builtins.Ellipsis'):
        self.record_precise_if_checked_scope(o)
    else:
        self.process_node(o)
        super().visit_name_expr(o)

</t>
<t tx="ekr.20220525082935.458">def visit_yield_from_expr(self, o: YieldFromExpr) -&gt; None:
    if o.expr:
        o.expr.accept(self)

</t>
<t tx="ekr.20220525082935.459">def visit_call_expr(self, o: CallExpr) -&gt; None:
    self.process_node(o)
    if o.analyzed:
        o.analyzed.accept(self)
    else:
        o.callee.accept(self)
        for a in o.args:
            a.accept(self)
        self.record_call_target_precision(o)

</t>
<t tx="ekr.20220525082935.46">def apply_class_plugin_hooks(self, defn: ClassDef) -&gt; None:
    """Apply a plugin hook that may infer a more precise definition for a class."""

    for decorator in defn.decorators:
        decorator_name = self.get_fullname_for_hook(decorator)
        if decorator_name:
            hook = self.plugin.get_class_decorator_hook(decorator_name)
            if hook:
                hook(ClassDefContext(defn, decorator, self))

    if defn.metaclass:
        metaclass_name = self.get_fullname_for_hook(defn.metaclass)
        if metaclass_name:
            hook = self.plugin.get_metaclass_hook(metaclass_name)
            if hook:
                hook(ClassDefContext(defn, defn.metaclass, self))

    for base_expr in defn.base_type_exprs:
        base_name = self.get_fullname_for_hook(base_expr)
        if base_name:
            hook = self.plugin.get_base_class_hook(base_name)
            if hook:
                hook(ClassDefContext(defn, base_expr, self))

</t>
<t tx="ekr.20220525082935.460">def record_call_target_precision(self, o: CallExpr) -&gt; None:
    """Record precision of formal argument types used in a call."""
    if not self.typemap or o.callee not in self.typemap:
        # Type not available.
        return
    callee_type = get_proper_type(self.typemap[o.callee])
    if isinstance(callee_type, CallableType):
        self.record_callable_target_precision(o, callee_type)
    else:
        pass  # TODO: Handle overloaded functions, etc.

</t>
<t tx="ekr.20220525082935.461">def record_callable_target_precision(self, o: CallExpr, callee: CallableType) -&gt; None:
    """Record imprecision caused by callee argument types.

    This only considers arguments passed in a call expression. Arguments
    with default values that aren't provided in a call arguably don't
    contribute to typing imprecision at the *call site* (but they
    contribute at the function definition).
    """
    assert self.typemap
    typemap = self.typemap
    actual_to_formal = map_formals_to_actuals(
        o.arg_kinds,
        o.arg_names,
        callee.arg_kinds,
        callee.arg_names,
        lambda n: typemap[o.args[n]])
    for formals in actual_to_formal:
        for n in formals:
            formal = get_proper_type(callee.arg_types[n])
            if isinstance(formal, AnyType):
                self.record_line(o.line, TYPE_ANY)
            elif is_imprecise(formal):
                self.record_line(o.line, TYPE_IMPRECISE)

</t>
<t tx="ekr.20220525082935.462">def visit_member_expr(self, o: MemberExpr) -&gt; None:
    self.process_node(o)
    super().visit_member_expr(o)

</t>
<t tx="ekr.20220525082935.463">def visit_op_expr(self, o: OpExpr) -&gt; None:
    self.process_node(o)
    super().visit_op_expr(o)

</t>
<t tx="ekr.20220525082935.464">def visit_comparison_expr(self, o: ComparisonExpr) -&gt; None:
    self.process_node(o)
    super().visit_comparison_expr(o)

</t>
<t tx="ekr.20220525082935.465">def visit_index_expr(self, o: IndexExpr) -&gt; None:
    self.process_node(o)
    super().visit_index_expr(o)

</t>
<t tx="ekr.20220525082935.466">def visit_assignment_expr(self, o: AssignmentExpr) -&gt; None:
    self.process_node(o)
    super().visit_assignment_expr(o)

</t>
<t tx="ekr.20220525082935.467">def visit_unary_expr(self, o: UnaryExpr) -&gt; None:
    self.process_node(o)
    super().visit_unary_expr(o)

</t>
<t tx="ekr.20220525082935.468">def visit_str_expr(self, o: StrExpr) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20220525082935.469">def visit_unicode_expr(self, o: UnicodeExpr) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20220525082935.47">def get_fullname_for_hook(self, expr: Expression) -&gt; Optional[str]:
    if isinstance(expr, CallExpr):
        return self.get_fullname_for_hook(expr.callee)
    elif isinstance(expr, IndexExpr):
        return self.get_fullname_for_hook(expr.base)
    elif isinstance(expr, RefExpr):
        if expr.fullname:
            return expr.fullname
        # If we don't have a fullname look it up. This happens because base classes are
        # analyzed in a different manner (see exprtotype.py) and therefore those AST
        # nodes will not have full names.
        sym = self.lookup_type_node(expr)
        if sym:
            return sym.fullname
    return None

</t>
<t tx="ekr.20220525082935.470">def visit_bytes_expr(self, o: BytesExpr) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20220525082935.471">def visit_int_expr(self, o: IntExpr) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20220525082935.472">def visit_float_expr(self, o: FloatExpr) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20220525082935.473">def visit_complex_expr(self, o: ComplexExpr) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20220525082935.474">def visit_ellipsis(self, o: EllipsisExpr) -&gt; None:
    self.record_precise_if_checked_scope(o)

</t>
<t tx="ekr.20220525082935.475"># Helpers

</t>
<t tx="ekr.20220525082935.476">def process_node(self, node: Expression) -&gt; None:
    if self.all_nodes:
        if self.typemap is not None:
            self.line = node.line
            self.type(self.typemap.get(node))

</t>
<t tx="ekr.20220525082935.477">def record_precise_if_checked_scope(self, node: Node) -&gt; None:
    if isinstance(node, Expression) and self.typemap and node not in self.typemap:
        kind = TYPE_UNANALYZED
    elif self.is_checked_scope():
        kind = TYPE_PRECISE
    else:
        kind = TYPE_ANY
    self.record_line(node.line, kind)

</t>
<t tx="ekr.20220525082935.478">def type(self, t: Optional[Type]) -&gt; None:
    t = get_proper_type(t)

    if not t:
        # If an expression does not have a type, it is often due to dead code.
        # Don't count these because there can be an unanalyzed value on a line with other
        # analyzed expressions, which overwrite the TYPE_UNANALYZED.
        self.record_line(self.line, TYPE_UNANALYZED)
        return

    if isinstance(t, AnyType) and is_special_form_any(t):
        # TODO: What if there is an error in special form definition?
        self.record_line(self.line, TYPE_PRECISE)
        return

    if isinstance(t, AnyType):
        self.log('  !! Any type around line %d' % self.line)
        self.num_any_exprs += 1
        self.record_line(self.line, TYPE_ANY)
    elif ((not self.all_nodes and is_imprecise(t)) or
          (self.all_nodes and is_imprecise2(t))):
        self.log('  !! Imprecise type around line %d' % self.line)
        self.num_imprecise_exprs += 1
        self.record_line(self.line, TYPE_IMPRECISE)
    else:
        self.num_precise_exprs += 1
        self.record_line(self.line, TYPE_PRECISE)

    for typ in get_proper_types(collect_all_inner_types(t)) + [t]:
        if isinstance(typ, AnyType):
            typ = get_original_any(typ)
            if is_special_form_any(typ):
                continue
            self.type_of_any_counter[typ.type_of_any] += 1
            self.num_any_types += 1
            if self.line in self.any_line_map:
                self.any_line_map[self.line].append(typ)
            else:
                self.any_line_map[self.line] = [typ]
        elif isinstance(typ, Instance):
            if typ.args:
                if any(is_complex(arg) for arg in typ.args):
                    self.num_complex_types += 1
                else:
                    self.num_generic_types += 1
            else:
                self.num_simple_types += 1
        elif isinstance(typ, FunctionLike):
            self.num_function_types += 1
        elif isinstance(typ, TupleType):
            if any(is_complex(item) for item in typ.items):
                self.num_complex_types += 1
            else:
                self.num_tuple_types += 1
        elif isinstance(typ, TypeVarType):
            self.num_typevar_types += 1

</t>
<t tx="ekr.20220525082935.479">def log(self, string: str) -&gt; None:
    self.output.append(string)

</t>
<t tx="ekr.20220525082935.48">def analyze_class_keywords(self, defn: ClassDef) -&gt; None:
    for value in defn.keywords.values():
        value.accept(self)

</t>
<t tx="ekr.20220525082935.480">def record_line(self, line: int, precision: int) -&gt; None:
    self.line_map[line] = max(precision,
                              self.line_map.get(line, TYPE_EMPTY))


</t>
<t tx="ekr.20220525082935.481">def dump_type_stats(tree: MypyFile,
                    path: str,
                    modules: Dict[str, MypyFile],
                    inferred: bool = False,
                    typemap: Optional[Dict[Expression, Type]] = None) -&gt; None:
    if is_special_module(path):
        return
    print(path)
    visitor = StatisticsVisitor(inferred,
                                filename=tree.fullname,
                                modules=modules,
                                typemap=typemap)
    tree.accept(visitor)
    for line in visitor.output:
        print(line)
    print('  ** precision **')
    print('  precise  ', visitor.num_precise_exprs)
    print('  imprecise', visitor.num_imprecise_exprs)
    print('  any      ', visitor.num_any_exprs)
    print('  ** kinds **')
    print('  simple   ', visitor.num_simple_types)
    print('  generic  ', visitor.num_generic_types)
    print('  function ', visitor.num_function_types)
    print('  tuple    ', visitor.num_tuple_types)
    print('  TypeVar  ', visitor.num_typevar_types)
    print('  complex  ', visitor.num_complex_types)
    print('  any      ', visitor.num_any_types)


</t>
<t tx="ekr.20220525082935.482">def is_special_module(path: str) -&gt; bool:
    return os.path.basename(path) in ('abc.pyi', 'typing.pyi', 'builtins.pyi')


</t>
<t tx="ekr.20220525082935.483">def is_imprecise(t: Type) -&gt; bool:
    return t.accept(HasAnyQuery())


</t>
<t tx="ekr.20220525082935.484">class HasAnyQuery(TypeQuery[bool]):
    def __init__(self) -&gt; None:
        super().__init__(any)

    def visit_any(self, t: AnyType) -&gt; bool:
        return not is_special_form_any(t)


</t>
<t tx="ekr.20220525082935.485">def is_imprecise2(t: Type) -&gt; bool:
    return t.accept(HasAnyQuery2())


</t>
<t tx="ekr.20220525082935.486">class HasAnyQuery2(HasAnyQuery):
    def visit_callable_type(self, t: CallableType) -&gt; bool:
        # We don't want to flag references to functions with some Any
        # argument types (etc.) since they generally don't mean trouble.
        return False


</t>
<t tx="ekr.20220525082935.487">def is_generic(t: Type) -&gt; bool:
    t = get_proper_type(t)
    return isinstance(t, Instance) and bool(t.args)


</t>
<t tx="ekr.20220525082935.488">def is_complex(t: Type) -&gt; bool:
    t = get_proper_type(t)
    return is_generic(t) or isinstance(t, (FunctionLike, TupleType,
                                           TypeVarType))


</t>
<t tx="ekr.20220525082935.489">def ensure_dir_exists(dir: str) -&gt; None:
    if not os.path.exists(dir):
        os.makedirs(dir)


</t>
<t tx="ekr.20220525082935.49">def enter_class(self, info: TypeInfo) -&gt; None:
    # Remember previous active class
    self.type_stack.append(self.type)
    self.locals.append(None)  # Add class scope
    self.is_comprehension_stack.append(False)
    self.block_depth.append(-1)  # The class body increments this to 0
    self.type = info
    self.missing_names.append(set())

</t>
<t tx="ekr.20220525082935.490">def is_special_form_any(t: AnyType) -&gt; bool:
    return get_original_any(t).type_of_any == TypeOfAny.special_form


</t>
<t tx="ekr.20220525082935.491">def get_original_any(t: AnyType) -&gt; AnyType:
    if t.type_of_any == TypeOfAny.from_another_any:
        assert t.source_any
        assert t.source_any.type_of_any != TypeOfAny.from_another_any
        t = t.source_any
    return t
</t>
<t tx="ekr.20220525082935.492">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Conversion of parse tree nodes to strings."""

import re
import os

from typing import Any, List, Tuple, Optional, Union, Sequence
from typing_extensions import TYPE_CHECKING

from mypy.util import short_type, IdMapper
import mypy.nodes
from mypy.visitor import NodeVisitor

if TYPE_CHECKING:
    import mypy.patterns


@others
</t>
<t tx="ekr.20220525082935.493">class StrConv(NodeVisitor[str]):
    """Visitor for converting a node to a human-readable string.

    For example, an MypyFile node from program '1' is converted into
    something like this:

      MypyFile:1(
        fnam
        ExpressionStmt:1(
          IntExpr(1)))
    """

    @others
</t>
<t tx="ekr.20220525082935.494">def __init__(self, show_ids: bool = False) -&gt; None:
    self.show_ids = show_ids
    self.id_mapper: Optional[IdMapper] = None
    if show_ids:
        self.id_mapper = IdMapper()

</t>
<t tx="ekr.20220525082935.495">def get_id(self, o: object) -&gt; Optional[int]:
    if self.id_mapper:
        return self.id_mapper.id(o)
    return None

</t>
<t tx="ekr.20220525082935.496">def format_id(self, o: object) -&gt; str:
    if self.id_mapper:
        return f'&lt;{self.get_id(o)}&gt;'
    else:
        return ''

</t>
<t tx="ekr.20220525082935.497">def dump(self, nodes: Sequence[object], obj: 'mypy.nodes.Context') -&gt; str:
    """Convert a list of items to a multiline pretty-printed string.

    The tag is produced from the type name of obj and its line
    number. See mypy.util.dump_tagged for a description of the nodes
    argument.
    """
    tag = short_type(obj) + ':' + str(obj.get_line())
    if self.show_ids:
        assert self.id_mapper is not None
        tag += f'&lt;{self.get_id(obj)}&gt;'
    return dump_tagged(nodes, tag, self)

</t>
<t tx="ekr.20220525082935.498">def func_helper(self, o: 'mypy.nodes.FuncItem') -&gt; List[object]:
    """Return a list in a format suitable for dump() that represents the
    arguments and the body of a function. The caller can then decorate the
    array with information specific to methods, global functions or
    anonymous functions.
    """
    args: List[Union[mypy.nodes.Var, Tuple[str, List[mypy.nodes.Node]]]] = []
    extra: List[Tuple[str, List[mypy.nodes.Var]]] = []
    for arg in o.arguments:
        kind: mypy.nodes.ArgKind = arg.kind
        if kind.is_required():
            args.append(arg.variable)
        elif kind.is_optional():
            assert arg.initializer is not None
            args.append(('default', [arg.variable, arg.initializer]))
        elif kind == mypy.nodes.ARG_STAR:
            extra.append(('VarArg', [arg.variable]))
        elif kind == mypy.nodes.ARG_STAR2:
            extra.append(('DictVarArg', [arg.variable]))
    a: List[Any] = []
    if args:
        a.append(('Args', args))
    if o.type:
        a.append(o.type)
    if o.is_generator:
        a.append('Generator')
    a.extend(extra)
    a.append(o.body)
    return a

</t>
<t tx="ekr.20220525082935.499"># Top-level structures

</t>
<t tx="ekr.20220525082935.5">@property
def final_iteration(self) -&gt; bool:
    return self._final_iteration

</t>
<t tx="ekr.20220525082935.50">def leave_class(self) -&gt; None:
    """ Restore analyzer state. """
    self.block_depth.pop()
    self.locals.pop()
    self.is_comprehension_stack.pop()
    self.type = self.type_stack.pop()
    self.missing_names.pop()

</t>
<t tx="ekr.20220525082935.500">def visit_mypy_file(self, o: 'mypy.nodes.MypyFile') -&gt; str:
    # Skip implicit definitions.
    a: List[Any] = [o.defs]
    if o.is_bom:
        a.insert(0, 'BOM')
    # Omit path to special file with name "main". This is used to simplify
    # test case descriptions; the file "main" is used by default in many
    # test cases.
    if o.path != 'main':
        # Insert path. Normalize directory separators to / to unify test
        # case# output in all platforms.
        a.insert(0, o.path.replace(os.sep, '/'))
    if o.ignored_lines:
        a.append('IgnoredLines(%s)' % ', '.join(str(line)
                                                for line in sorted(o.ignored_lines)))
    return self.dump(a, o)

</t>
<t tx="ekr.20220525082935.501">def visit_import(self, o: 'mypy.nodes.Import') -&gt; str:
    a = []
    for id, as_id in o.ids:
        if as_id is not None:
            a.append(f'{id} : {as_id}')
        else:
            a.append(id)
    return f"Import:{o.line}({', '.join(a)})"

</t>
<t tx="ekr.20220525082935.502">def visit_import_from(self, o: 'mypy.nodes.ImportFrom') -&gt; str:
    a = []
    for name, as_name in o.names:
        if as_name is not None:
            a.append(f'{name} : {as_name}')
        else:
            a.append(name)
    return f"ImportFrom:{o.line}({'.' * o.relative + o.id}, [{', '.join(a)}])"

</t>
<t tx="ekr.20220525082935.503">def visit_import_all(self, o: 'mypy.nodes.ImportAll') -&gt; str:
    return f"ImportAll:{o.line}({'.' * o.relative + o.id})"

</t>
<t tx="ekr.20220525082935.504"># Definitions

</t>
<t tx="ekr.20220525082935.505">def visit_func_def(self, o: 'mypy.nodes.FuncDef') -&gt; str:
    a = self.func_helper(o)
    a.insert(0, o.name)
    arg_kinds = {arg.kind for arg in o.arguments}
    if len(arg_kinds &amp; {mypy.nodes.ARG_NAMED, mypy.nodes.ARG_NAMED_OPT}) &gt; 0:
        a.insert(1, f'MaxPos({o.max_pos})')
    if o.is_abstract:
        a.insert(-1, 'Abstract')
    if o.is_static:
        a.insert(-1, 'Static')
    if o.is_class:
        a.insert(-1, 'Class')
    if o.is_property:
        a.insert(-1, 'Property')
    return self.dump(a, o)

</t>
<t tx="ekr.20220525082935.506">def visit_overloaded_func_def(self, o: 'mypy.nodes.OverloadedFuncDef') -&gt; str:
    a: Any = o.items[:]
    if o.type:
        a.insert(0, o.type)
    if o.impl:
        a.insert(0, o.impl)
    if o.is_static:
        a.insert(-1, 'Static')
    if o.is_class:
        a.insert(-1, 'Class')
    return self.dump(a, o)

</t>
<t tx="ekr.20220525082935.507">def visit_class_def(self, o: 'mypy.nodes.ClassDef') -&gt; str:
    a = [o.name, o.defs.body]
    # Display base types unless they are implicitly just builtins.object
    # (in this case base_type_exprs is empty).
    if o.base_type_exprs:
        if o.info and o.info.bases:
            if (len(o.info.bases) != 1
                    or o.info.bases[0].type.fullname != 'builtins.object'):
                a.insert(1, ('BaseType', o.info.bases))
        else:
            a.insert(1, ('BaseTypeExpr', o.base_type_exprs))
    if o.type_vars:
        a.insert(1, ('TypeVars', o.type_vars))
    if o.metaclass:
        a.insert(1, f'Metaclass({o.metaclass})')
    if o.decorators:
        a.insert(1, ('Decorators', o.decorators))
    if o.info and o.info._promote:
        a.insert(1, f'Promote({o.info._promote})')
    if o.info and o.info.tuple_type:
        a.insert(1, ('TupleType', [o.info.tuple_type]))
    if o.info and o.info.fallback_to_any:
        a.insert(1, 'FallbackToAny')
    return self.dump(a, o)

</t>
<t tx="ekr.20220525082935.508">def visit_var(self, o: 'mypy.nodes.Var') -&gt; str:
    lst = ''
    # Add :nil line number tag if no line number is specified to remain
    # compatible with old test case descriptions that assume this.
    if o.line &lt; 0:
        lst = ':nil'
    return 'Var' + lst + '(' + o.name + ')'

</t>
<t tx="ekr.20220525082935.509">def visit_global_decl(self, o: 'mypy.nodes.GlobalDecl') -&gt; str:
    return self.dump([o.names], o)

</t>
<t tx="ekr.20220525082935.51">def analyze_class_decorator(self, defn: ClassDef, decorator: Expression) -&gt; None:
    decorator.accept(self)
    if isinstance(decorator, RefExpr):
        if decorator.fullname in RUNTIME_PROTOCOL_DECOS:
            if defn.info.is_protocol:
                defn.info.runtime_protocol = True
            else:
                self.fail('@runtime_checkable can only be used with protocol classes',
                          defn)
        elif decorator.fullname in FINAL_DECORATOR_NAMES:
            defn.info.is_final = True

</t>
<t tx="ekr.20220525082935.510">def visit_nonlocal_decl(self, o: 'mypy.nodes.NonlocalDecl') -&gt; str:
    return self.dump([o.names], o)

</t>
<t tx="ekr.20220525082935.511">def visit_decorator(self, o: 'mypy.nodes.Decorator') -&gt; str:
    return self.dump([o.var, o.decorators, o.func], o)

</t>
<t tx="ekr.20220525082935.512"># Statements

</t>
<t tx="ekr.20220525082935.513">def visit_block(self, o: 'mypy.nodes.Block') -&gt; str:
    return self.dump(o.body, o)

</t>
<t tx="ekr.20220525082935.514">def visit_expression_stmt(self, o: 'mypy.nodes.ExpressionStmt') -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20220525082935.515">def visit_assignment_stmt(self, o: 'mypy.nodes.AssignmentStmt') -&gt; str:
    a: List[Any] = []
    if len(o.lvalues) &gt; 1:
        a = [('Lvalues', o.lvalues)]
    else:
        a = [o.lvalues[0]]
    a.append(o.rvalue)
    if o.type:
        a.append(o.type)
    return self.dump(a, o)

</t>
<t tx="ekr.20220525082935.516">def visit_operator_assignment_stmt(self, o: 'mypy.nodes.OperatorAssignmentStmt') -&gt; str:
    return self.dump([o.op, o.lvalue, o.rvalue], o)

</t>
<t tx="ekr.20220525082935.517">def visit_while_stmt(self, o: 'mypy.nodes.WhileStmt') -&gt; str:
    a: List[Any] = [o.expr, o.body]
    if o.else_body:
        a.append(('Else', o.else_body.body))
    return self.dump(a, o)

</t>
<t tx="ekr.20220525082935.518">def visit_for_stmt(self, o: 'mypy.nodes.ForStmt') -&gt; str:
    a: List[Any] = []
    if o.is_async:
        a.append(('Async', ''))
    a.append(o.index)
    if o.index_type:
        a.append(o.index_type)
    a.extend([o.expr, o.body])
    if o.else_body:
        a.append(('Else', o.else_body.body))
    return self.dump(a, o)

</t>
<t tx="ekr.20220525082935.519">def visit_return_stmt(self, o: 'mypy.nodes.ReturnStmt') -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20220525082935.52">def clean_up_bases_and_infer_type_variables(
        self,
        defn: ClassDef,
        base_type_exprs: List[Expression],
        context: Context) -&gt; Tuple[List[Expression],
                                   List[TypeVarLikeType],
                                   bool]:
    """Remove extra base classes such as Generic and infer type vars.

    For example, consider this class:

      class Foo(Bar, Generic[T]): ...

    Now we will remove Generic[T] from bases of Foo and infer that the
    type variable 'T' is a type argument of Foo.

    Note that this is performed *before* semantic analysis.

    Returns (remaining base expressions, inferred type variables, is protocol).
    """
    removed: List[int] = []
    declared_tvars: TypeVarLikeList = []
    is_protocol = False
    for i, base_expr in enumerate(base_type_exprs):
        self.analyze_type_expr(base_expr)

        try:
            base = self.expr_to_unanalyzed_type(base_expr)
        except TypeTranslationError:
            # This error will be caught later.
            continue
        result = self.analyze_class_typevar_declaration(base)
        if result is not None:
            if declared_tvars:
                self.fail('Only single Generic[...] or Protocol[...] can be in bases', context)
            removed.append(i)
            tvars = result[0]
            is_protocol |= result[1]
            declared_tvars.extend(tvars)
        if isinstance(base, UnboundType):
            sym = self.lookup_qualified(base.name, base)
            if sym is not None and sym.node is not None:
                if sym.node.fullname in PROTOCOL_NAMES and i not in removed:
                    # also remove bare 'Protocol' bases
                    removed.append(i)
                    is_protocol = True

    all_tvars = self.get_all_bases_tvars(base_type_exprs, removed)
    if declared_tvars:
        if len(remove_dups(declared_tvars)) &lt; len(declared_tvars):
            self.fail("Duplicate type variables in Generic[...] or Protocol[...]", context)
        declared_tvars = remove_dups(declared_tvars)
        if not set(all_tvars).issubset(set(declared_tvars)):
            self.fail("If Generic[...] or Protocol[...] is present"
                      " it should list all type variables", context)
            # In case of error, Generic tvars will go first
            declared_tvars = remove_dups(declared_tvars + all_tvars)
    else:
        declared_tvars = all_tvars
    for i in reversed(removed):
        # We need to actually remove the base class expressions like Generic[T],
        # mostly because otherwise they will create spurious dependencies in fine
        # grained incremental mode.
        defn.removed_base_type_exprs.append(defn.base_type_exprs[i])
        del base_type_exprs[i]
    tvar_defs: List[TypeVarLikeType] = []
    for name, tvar_expr in declared_tvars:
        tvar_def = self.tvar_scope.bind_new(name, tvar_expr)
        tvar_defs.append(tvar_def)
    return base_type_exprs, tvar_defs, is_protocol

</t>
<t tx="ekr.20220525082935.520">def visit_if_stmt(self, o: 'mypy.nodes.IfStmt') -&gt; str:
    a: List[Any] = []
    for i in range(len(o.expr)):
        a.append(('If', [o.expr[i]]))
        a.append(('Then', o.body[i].body))

    if not o.else_body:
        return self.dump(a, o)
    else:
        return self.dump([a, ('Else', o.else_body.body)], o)

</t>
<t tx="ekr.20220525082935.521">def visit_break_stmt(self, o: 'mypy.nodes.BreakStmt') -&gt; str:
    return self.dump([], o)

</t>
<t tx="ekr.20220525082935.522">def visit_continue_stmt(self, o: 'mypy.nodes.ContinueStmt') -&gt; str:
    return self.dump([], o)

</t>
<t tx="ekr.20220525082935.523">def visit_pass_stmt(self, o: 'mypy.nodes.PassStmt') -&gt; str:
    return self.dump([], o)

</t>
<t tx="ekr.20220525082935.524">def visit_raise_stmt(self, o: 'mypy.nodes.RaiseStmt') -&gt; str:
    return self.dump([o.expr, o.from_expr], o)

</t>
<t tx="ekr.20220525082935.525">def visit_assert_stmt(self, o: 'mypy.nodes.AssertStmt') -&gt; str:
    if o.msg is not None:
        return self.dump([o.expr, o.msg], o)
    else:
        return self.dump([o.expr], o)

</t>
<t tx="ekr.20220525082935.526">def visit_await_expr(self, o: 'mypy.nodes.AwaitExpr') -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20220525082935.527">def visit_del_stmt(self, o: 'mypy.nodes.DelStmt') -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20220525082935.528">def visit_try_stmt(self, o: 'mypy.nodes.TryStmt') -&gt; str:
    a: List[Any] = [o.body]

    for i in range(len(o.vars)):
        a.append(o.types[i])
        if o.vars[i]:
            a.append(o.vars[i])
        a.append(o.handlers[i])

    if o.else_body:
        a.append(('Else', o.else_body.body))
    if o.finally_body:
        a.append(('Finally', o.finally_body.body))

    return self.dump(a, o)

</t>
<t tx="ekr.20220525082935.529">def visit_with_stmt(self, o: 'mypy.nodes.WithStmt') -&gt; str:
    a: List[Any] = []
    if o.is_async:
        a.append(('Async', ''))
    for i in range(len(o.expr)):
        a.append(('Expr', [o.expr[i]]))
        if o.target[i]:
            a.append(('Target', [o.target[i]]))
    if o.unanalyzed_type:
        a.append(o.unanalyzed_type)
    return self.dump(a + [o.body], o)

</t>
<t tx="ekr.20220525082935.53">def analyze_class_typevar_declaration(
    self,
    base: Type
) -&gt; Optional[Tuple[TypeVarLikeList, bool]]:
    """Analyze type variables declared using Generic[...] or Protocol[...].

    Args:
        base: Non-analyzed base class

    Return None if the base class does not declare type variables. Otherwise,
    return the type variables.
    """
    if not isinstance(base, UnboundType):
        return None
    unbound = base
    sym = self.lookup_qualified(unbound.name, unbound)
    if sym is None or sym.node is None:
        return None
    if (sym.node.fullname == 'typing.Generic' or
            sym.node.fullname in PROTOCOL_NAMES and base.args):
        is_proto = sym.node.fullname != 'typing.Generic'
        tvars: TypeVarLikeList = []
        for arg in unbound.args:
            tag = self.track_incomplete_refs()
            tvar = self.analyze_unbound_tvar(arg)
            if tvar:
                tvars.append(tvar)
            elif not self.found_incomplete_ref(tag):
                self.fail('Free type variable expected in %s[...]' %
                          sym.node.name, base)
        return tvars, is_proto
    return None

</t>
<t tx="ekr.20220525082935.530">def visit_print_stmt(self, o: 'mypy.nodes.PrintStmt') -&gt; str:
    a: List[Any] = o.args[:]
    if o.target:
        a.append(('Target', [o.target]))
    if o.newline:
        a.append('Newline')
    return self.dump(a, o)

</t>
<t tx="ekr.20220525082935.531">def visit_exec_stmt(self, o: 'mypy.nodes.ExecStmt') -&gt; str:
    return self.dump([o.expr, o.globals, o.locals], o)

</t>
<t tx="ekr.20220525082935.532">def visit_match_stmt(self, o: 'mypy.nodes.MatchStmt') -&gt; str:
    a: List[Any] = [o.subject]
    for i in range(len(o.patterns)):
        a.append(('Pattern', [o.patterns[i]]))
        if o.guards[i] is not None:
            a.append(('Guard', [o.guards[i]]))
        a.append(('Body', o.bodies[i].body))
    return self.dump(a, o)

</t>
<t tx="ekr.20220525082935.533"># Expressions

# Simple expressions

</t>
<t tx="ekr.20220525082935.534">def visit_int_expr(self, o: 'mypy.nodes.IntExpr') -&gt; str:
    return f'IntExpr({o.value})'

</t>
<t tx="ekr.20220525082935.535">def visit_str_expr(self, o: 'mypy.nodes.StrExpr') -&gt; str:
    return f'StrExpr({self.str_repr(o.value)})'

</t>
<t tx="ekr.20220525082935.536">def visit_bytes_expr(self, o: 'mypy.nodes.BytesExpr') -&gt; str:
    return f'BytesExpr({self.str_repr(o.value)})'

</t>
<t tx="ekr.20220525082935.537">def visit_unicode_expr(self, o: 'mypy.nodes.UnicodeExpr') -&gt; str:
    return f'UnicodeExpr({self.str_repr(o.value)})'

</t>
<t tx="ekr.20220525082935.538">def str_repr(self, s: str) -&gt; str:
    s = re.sub(r'\\u[0-9a-fA-F]{4}', lambda m: '\\' + m.group(0), s)
    return re.sub('[^\\x20-\\x7e]',
                  lambda m: r'\u%.4x' % ord(m.group(0)), s)

</t>
<t tx="ekr.20220525082935.539">def visit_float_expr(self, o: 'mypy.nodes.FloatExpr') -&gt; str:
    return f'FloatExpr({o.value})'

</t>
<t tx="ekr.20220525082935.54">def analyze_unbound_tvar(self, t: Type) -&gt; Optional[Tuple[str, TypeVarLikeExpr]]:
    if not isinstance(t, UnboundType):
        return None
    unbound = t
    sym = self.lookup_qualified(unbound.name, unbound)
    if sym and isinstance(sym.node, PlaceholderNode):
        self.record_incomplete_ref()
    if sym and isinstance(sym.node, ParamSpecExpr):
        if sym.fullname and not self.tvar_scope.allow_binding(sym.fullname):
            # It's bound by our type variable scope
            return None
        return unbound.name, sym.node
    if sym is None or not isinstance(sym.node, TypeVarExpr):
        return None
    elif sym.fullname and not self.tvar_scope.allow_binding(sym.fullname):
        # It's bound by our type variable scope
        return None
    else:
        assert isinstance(sym.node, TypeVarExpr)
        return unbound.name, sym.node

</t>
<t tx="ekr.20220525082935.540">def visit_complex_expr(self, o: 'mypy.nodes.ComplexExpr') -&gt; str:
    return f'ComplexExpr({o.value})'

</t>
<t tx="ekr.20220525082935.541">def visit_ellipsis(self, o: 'mypy.nodes.EllipsisExpr') -&gt; str:
    return 'Ellipsis'

</t>
<t tx="ekr.20220525082935.542">def visit_star_expr(self, o: 'mypy.nodes.StarExpr') -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20220525082935.543">def visit_name_expr(self, o: 'mypy.nodes.NameExpr') -&gt; str:
    pretty = self.pretty_name(o.name, o.kind, o.fullname,
                              o.is_inferred_def or o.is_special_form,
                              o.node)
    if isinstance(o.node, mypy.nodes.Var) and o.node.is_final:
        pretty += f' = {o.node.final_value}'
    return short_type(o) + '(' + pretty + ')'

</t>
<t tx="ekr.20220525082935.544">def pretty_name(self, name: str, kind: Optional[int], fullname: Optional[str],
                is_inferred_def: bool, target_node: 'Optional[mypy.nodes.Node]' = None) -&gt; str:
    n = name
    if is_inferred_def:
        n += '*'
    if target_node:
        id = self.format_id(target_node)
    else:
        id = ''
    if isinstance(target_node, mypy.nodes.MypyFile) and name == fullname:
        n += id
    elif kind == mypy.nodes.GDEF or (fullname != name and
                                     fullname is not None):
        # Append fully qualified name for global references.
        n += f' [{fullname}{id}]'
    elif kind == mypy.nodes.LDEF:
        # Add tag to signify a local reference.
        n += f' [l{id}]'
    elif kind == mypy.nodes.MDEF:
        # Add tag to signify a member reference.
        n += f' [m{id}]'
    else:
        n += id
    return n

</t>
<t tx="ekr.20220525082935.545">def visit_member_expr(self, o: 'mypy.nodes.MemberExpr') -&gt; str:
    pretty = self.pretty_name(o.name, o.kind, o.fullname, o.is_inferred_def, o.node)
    return self.dump([o.expr, pretty], o)

</t>
<t tx="ekr.20220525082935.546">def visit_yield_expr(self, o: 'mypy.nodes.YieldExpr') -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20220525082935.547">def visit_yield_from_expr(self, o: 'mypy.nodes.YieldFromExpr') -&gt; str:
    if o.expr:
        return self.dump([o.expr.accept(self)], o)
    else:
        return self.dump([], o)

</t>
<t tx="ekr.20220525082935.548">def visit_call_expr(self, o: 'mypy.nodes.CallExpr') -&gt; str:
    if o.analyzed:
        return o.analyzed.accept(self)
    args: List[mypy.nodes.Expression] = []
    extra: List[Union[str, Tuple[str, List[Any]]]] = []
    for i, kind in enumerate(o.arg_kinds):
        if kind in [mypy.nodes.ARG_POS, mypy.nodes.ARG_STAR]:
            args.append(o.args[i])
            if kind == mypy.nodes.ARG_STAR:
                extra.append('VarArg')
        elif kind == mypy.nodes.ARG_NAMED:
            extra.append(('KwArgs', [o.arg_names[i], o.args[i]]))
        elif kind == mypy.nodes.ARG_STAR2:
            extra.append(('DictVarArg', [o.args[i]]))
        else:
            raise RuntimeError(f"unknown kind {kind}")
    a: List[Any] = [o.callee, ("Args", args)]
    return self.dump(a + extra, o)

</t>
<t tx="ekr.20220525082935.549">def visit_op_expr(self, o: 'mypy.nodes.OpExpr') -&gt; str:
    return self.dump([o.op, o.left, o.right], o)

</t>
<t tx="ekr.20220525082935.55">def get_all_bases_tvars(self,
                        base_type_exprs: List[Expression],
                        removed: List[int]) -&gt; TypeVarLikeList:
    """Return all type variable references in bases."""
    tvars: TypeVarLikeList = []
    for i, base_expr in enumerate(base_type_exprs):
        if i not in removed:
            try:
                base = self.expr_to_unanalyzed_type(base_expr)
            except TypeTranslationError:
                # This error will be caught later.
                continue
            base_tvars = base.accept(TypeVarLikeQuery(self.lookup_qualified, self.tvar_scope))
            tvars.extend(base_tvars)
    return remove_dups(tvars)

</t>
<t tx="ekr.20220525082935.550">def visit_comparison_expr(self, o: 'mypy.nodes.ComparisonExpr') -&gt; str:
    return self.dump([o.operators, o.operands], o)

</t>
<t tx="ekr.20220525082935.551">def visit_cast_expr(self, o: 'mypy.nodes.CastExpr') -&gt; str:
    return self.dump([o.expr, o.type], o)

</t>
<t tx="ekr.20220525082935.552">def visit_assert_type_expr(self, o: 'mypy.nodes.AssertTypeExpr') -&gt; str:
    return self.dump([o.expr, o.type], o)

</t>
<t tx="ekr.20220525082935.553">def visit_reveal_expr(self, o: 'mypy.nodes.RevealExpr') -&gt; str:
    if o.kind == mypy.nodes.REVEAL_TYPE:
        return self.dump([o.expr], o)
    else:
        # REVEAL_LOCALS
        return self.dump([o.local_nodes], o)

</t>
<t tx="ekr.20220525082935.554">def visit_assignment_expr(self, o: 'mypy.nodes.AssignmentExpr') -&gt; str:
    return self.dump([o.target, o.value], o)

</t>
<t tx="ekr.20220525082935.555">def visit_unary_expr(self, o: 'mypy.nodes.UnaryExpr') -&gt; str:
    return self.dump([o.op, o.expr], o)

</t>
<t tx="ekr.20220525082935.556">def visit_list_expr(self, o: 'mypy.nodes.ListExpr') -&gt; str:
    return self.dump(o.items, o)

</t>
<t tx="ekr.20220525082935.557">def visit_dict_expr(self, o: 'mypy.nodes.DictExpr') -&gt; str:
    return self.dump([[k, v] for k, v in o.items], o)

</t>
<t tx="ekr.20220525082935.558">def visit_set_expr(self, o: 'mypy.nodes.SetExpr') -&gt; str:
    return self.dump(o.items, o)

</t>
<t tx="ekr.20220525082935.559">def visit_tuple_expr(self, o: 'mypy.nodes.TupleExpr') -&gt; str:
    return self.dump(o.items, o)

</t>
<t tx="ekr.20220525082935.56">def prepare_class_def(self, defn: ClassDef, info: Optional[TypeInfo] = None) -&gt; None:
    """Prepare for the analysis of a class definition.

    Create an empty TypeInfo and store it in a symbol table, or if the 'info'
    argument is provided, store it instead (used for magic type definitions).
    """
    if not defn.info:
        defn.fullname = self.qualified_name(defn.name)
        # TODO: Nested classes
        info = info or self.make_empty_type_info(defn)
        defn.info = info
        info.defn = defn
        if not self.is_func_scope():
            info._fullname = self.qualified_name(defn.name)
        else:
            info._fullname = info.name
    local_name = defn.name
    if '@' in local_name:
        local_name = local_name.split('@')[0]
    self.add_symbol(local_name, defn.info, defn)
    if self.is_nested_within_func_scope():
        # We need to preserve local classes, let's store them
        # in globals under mangled unique names
        #
        # TODO: Putting local classes into globals breaks assumptions in fine-grained
        #       incremental mode and we should avoid it. In general, this logic is too
        #       ad-hoc and needs to be removed/refactored.
        if '@' not in defn.info._fullname:
            global_name = defn.info.name + '@' + str(defn.line)
            defn.info._fullname = self.cur_mod_id + '.' + global_name
        else:
            # Preserve name from previous fine-grained incremental run.
            global_name = defn.info.name
        defn.fullname = defn.info._fullname
        if defn.info.is_named_tuple:
            # Named tuple nested within a class is stored in the class symbol table.
            self.add_symbol_skip_local(global_name, defn.info)
        else:
            self.globals[global_name] = SymbolTableNode(GDEF, defn.info)

</t>
<t tx="ekr.20220525082935.560">def visit_index_expr(self, o: 'mypy.nodes.IndexExpr') -&gt; str:
    if o.analyzed:
        return o.analyzed.accept(self)
    return self.dump([o.base, o.index], o)

</t>
<t tx="ekr.20220525082935.561">def visit_super_expr(self, o: 'mypy.nodes.SuperExpr') -&gt; str:
    return self.dump([o.name, o.call], o)

</t>
<t tx="ekr.20220525082935.562">def visit_type_application(self, o: 'mypy.nodes.TypeApplication') -&gt; str:
    return self.dump([o.expr, ('Types', o.types)], o)

</t>
<t tx="ekr.20220525082935.563">def visit_type_var_expr(self, o: 'mypy.nodes.TypeVarExpr') -&gt; str:
    import mypy.types

    a: List[Any] = []
    if o.variance == mypy.nodes.COVARIANT:
        a += ['Variance(COVARIANT)']
    if o.variance == mypy.nodes.CONTRAVARIANT:
        a += ['Variance(CONTRAVARIANT)']
    if o.values:
        a += [('Values', o.values)]
    if not mypy.types.is_named_instance(o.upper_bound, 'builtins.object'):
        a += [f'UpperBound({o.upper_bound})']
    return self.dump(a, o)

</t>
<t tx="ekr.20220525082935.564">def visit_paramspec_expr(self, o: 'mypy.nodes.ParamSpecExpr') -&gt; str:
    import mypy.types

    a: List[Any] = []
    if o.variance == mypy.nodes.COVARIANT:
        a += ['Variance(COVARIANT)']
    if o.variance == mypy.nodes.CONTRAVARIANT:
        a += ['Variance(CONTRAVARIANT)']
    if not mypy.types.is_named_instance(o.upper_bound, 'builtins.object'):
        a += [f'UpperBound({o.upper_bound})']
    return self.dump(a, o)

</t>
<t tx="ekr.20220525082935.565">def visit_type_var_tuple_expr(self, o: 'mypy.nodes.TypeVarTupleExpr') -&gt; str:
    import mypy.types

    a: List[Any] = []
    if o.variance == mypy.nodes.COVARIANT:
        a += ['Variance(COVARIANT)']
    if o.variance == mypy.nodes.CONTRAVARIANT:
        a += ['Variance(CONTRAVARIANT)']
    if not mypy.types.is_named_instance(o.upper_bound, 'builtins.object'):
        a += [f'UpperBound({o.upper_bound})']
    return self.dump(a, o)

</t>
<t tx="ekr.20220525082935.566">def visit_type_alias_expr(self, o: 'mypy.nodes.TypeAliasExpr') -&gt; str:
    return f'TypeAliasExpr({o.type})'

</t>
<t tx="ekr.20220525082935.567">def visit_namedtuple_expr(self, o: 'mypy.nodes.NamedTupleExpr') -&gt; str:
    return f'NamedTupleExpr:{o.line}({o.info.name}, {o.info.tuple_type})'

</t>
<t tx="ekr.20220525082935.568">def visit_enum_call_expr(self, o: 'mypy.nodes.EnumCallExpr') -&gt; str:
    return f'EnumCallExpr:{o.line}({o.info.name}, {o.items})'

</t>
<t tx="ekr.20220525082935.569">def visit_typeddict_expr(self, o: 'mypy.nodes.TypedDictExpr') -&gt; str:
    return f'TypedDictExpr:{o.line}({o.info.name})'

</t>
<t tx="ekr.20220525082935.57">def make_empty_type_info(self, defn: ClassDef) -&gt; TypeInfo:
    if (self.is_module_scope()
            and self.cur_mod_id == 'builtins'
            and defn.name in CORE_BUILTIN_CLASSES):
        # Special case core built-in classes. A TypeInfo was already
        # created for it before semantic analysis, but with a dummy
        # ClassDef. Patch the real ClassDef object.
        info = self.globals[defn.name].node
        assert isinstance(info, TypeInfo)
    else:
        info = TypeInfo(SymbolTable(), defn, self.cur_mod_id)
        info.set_line(defn)
    return info

</t>
<t tx="ekr.20220525082935.570">def visit__promote_expr(self, o: 'mypy.nodes.PromoteExpr') -&gt; str:
    return f'PromoteExpr:{o.line}({o.type})'

</t>
<t tx="ekr.20220525082935.571">def visit_newtype_expr(self, o: 'mypy.nodes.NewTypeExpr') -&gt; str:
    return f'NewTypeExpr:{o.line}({o.name}, {self.dump([o.old_type], o)})'

</t>
<t tx="ekr.20220525082935.572">def visit_lambda_expr(self, o: 'mypy.nodes.LambdaExpr') -&gt; str:
    a = self.func_helper(o)
    return self.dump(a, o)

</t>
<t tx="ekr.20220525082935.573">def visit_generator_expr(self, o: 'mypy.nodes.GeneratorExpr') -&gt; str:
    condlists = o.condlists if any(o.condlists) else None
    return self.dump([o.left_expr, o.indices, o.sequences, condlists], o)

</t>
<t tx="ekr.20220525082935.574">def visit_list_comprehension(self, o: 'mypy.nodes.ListComprehension') -&gt; str:
    return self.dump([o.generator], o)

</t>
<t tx="ekr.20220525082935.575">def visit_set_comprehension(self, o: 'mypy.nodes.SetComprehension') -&gt; str:
    return self.dump([o.generator], o)

</t>
<t tx="ekr.20220525082935.576">def visit_dictionary_comprehension(self, o: 'mypy.nodes.DictionaryComprehension') -&gt; str:
    condlists = o.condlists if any(o.condlists) else None
    return self.dump([o.key, o.value, o.indices, o.sequences, condlists], o)

</t>
<t tx="ekr.20220525082935.577">def visit_conditional_expr(self, o: 'mypy.nodes.ConditionalExpr') -&gt; str:
    return self.dump([('Condition', [o.cond]), o.if_expr, o.else_expr], o)

</t>
<t tx="ekr.20220525082935.578">def visit_slice_expr(self, o: 'mypy.nodes.SliceExpr') -&gt; str:
    a: List[Any] = [o.begin_index, o.end_index, o.stride]
    if not a[0]:
        a[0] = '&lt;empty&gt;'
    if not a[1]:
        a[1] = '&lt;empty&gt;'
    return self.dump(a, o)

</t>
<t tx="ekr.20220525082935.579">def visit_backquote_expr(self, o: 'mypy.nodes.BackquoteExpr') -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20220525082935.58">def get_name_repr_of_expr(self, expr: Expression) -&gt; Optional[str]:
    """Try finding a short simplified textual representation of a base class expression."""
    if isinstance(expr, NameExpr):
        return expr.name
    if isinstance(expr, MemberExpr):
        return get_member_expr_fullname(expr)
    if isinstance(expr, IndexExpr):
        return self.get_name_repr_of_expr(expr.base)
    if isinstance(expr, CallExpr):
        return self.get_name_repr_of_expr(expr.callee)
    return None

</t>
<t tx="ekr.20220525082935.580">def visit_temp_node(self, o: 'mypy.nodes.TempNode') -&gt; str:
    return self.dump([o.type], o)

</t>
<t tx="ekr.20220525082935.581">def visit_as_pattern(self, o: 'mypy.patterns.AsPattern') -&gt; str:
    return self.dump([o.pattern, o.name], o)

</t>
<t tx="ekr.20220525082935.582">def visit_or_pattern(self, o: 'mypy.patterns.OrPattern') -&gt; str:
    return self.dump(o.patterns, o)

</t>
<t tx="ekr.20220525082935.583">def visit_value_pattern(self, o: 'mypy.patterns.ValuePattern') -&gt; str:
    return self.dump([o.expr], o)

</t>
<t tx="ekr.20220525082935.584">def visit_singleton_pattern(self, o: 'mypy.patterns.SingletonPattern') -&gt; str:
    return self.dump([o.value], o)

</t>
<t tx="ekr.20220525082935.585">def visit_sequence_pattern(self, o: 'mypy.patterns.SequencePattern') -&gt; str:
    return self.dump(o.patterns, o)

</t>
<t tx="ekr.20220525082935.586">def visit_starred_pattern(self, o: 'mypy.patterns.StarredPattern') -&gt; str:
    return self.dump([o.capture], o)

</t>
<t tx="ekr.20220525082935.587">def visit_mapping_pattern(self, o: 'mypy.patterns.MappingPattern') -&gt; str:
    a: List[Any] = []
    for i in range(len(o.keys)):
        a.append(('Key', [o.keys[i]]))
        a.append(('Value', [o.values[i]]))
    if o.rest is not None:
        a.append(('Rest', [o.rest]))
    return self.dump(a, o)

</t>
<t tx="ekr.20220525082935.588">def visit_class_pattern(self, o: 'mypy.patterns.ClassPattern') -&gt; str:
    a: List[Any] = [o.class_ref]
    if len(o.positionals) &gt; 0:
        a.append(('Positionals', o.positionals))
    for i in range(len(o.keyword_keys)):
        a.append(('Keyword', [o.keyword_keys[i], o.keyword_values[i]]))

    return self.dump(a, o)


</t>
<t tx="ekr.20220525082935.589">def dump_tagged(nodes: Sequence[object], tag: Optional[str], str_conv: 'StrConv') -&gt; str:
    """Convert an array into a pretty-printed multiline string representation.

    The format is
      tag(
        item1..
        itemN)
    Individual items are formatted like this:
     - arrays are flattened
     - pairs (str, array) are converted recursively, so that str is the tag
     - other items are converted to strings and indented
    """
    from mypy.types import Type, TypeStrVisitor

    a: List[str] = []
    if tag:
        a.append(tag + '(')
    for n in nodes:
        if isinstance(n, list):
            if n:
                a.append(dump_tagged(n, None, str_conv))
        elif isinstance(n, tuple):
            s = dump_tagged(n[1], n[0], str_conv)
            a.append(indent(s, 2))
        elif isinstance(n, mypy.nodes.Node):
            a.append(indent(n.accept(str_conv), 2))
        elif isinstance(n, Type):
            a.append(indent(n.accept(TypeStrVisitor(str_conv.id_mapper)), 2))
        elif n is not None:
            a.append(indent(str(n), 2))
    if tag:
        a[-1] += ')'
    return '\n'.join(a)


</t>
<t tx="ekr.20220525082935.59">def analyze_base_classes(
        self,
        base_type_exprs: List[Expression]) -&gt; Optional[Tuple[List[Tuple[ProperType,
                                                                        Expression]],
                                                             bool]]:
    """Analyze base class types.

    Return None if some definition was incomplete. Otherwise, return a tuple
    with these items:

     * List of (analyzed type, original expression) tuples
     * Boolean indicating whether one of the bases had a semantic analysis error
    """
    is_error = False
    bases = []
    for base_expr in base_type_exprs:
        if (isinstance(base_expr, RefExpr) and
                base_expr.fullname in ('typing.NamedTuple',) + TPDICT_NAMES):
            # Ignore magic bases for now.
            continue

        try:
            base = self.expr_to_analyzed_type(base_expr, allow_placeholder=True)
        except TypeTranslationError:
            name = self.get_name_repr_of_expr(base_expr)
            if isinstance(base_expr, CallExpr):
                msg = 'Unsupported dynamic base class'
            else:
                msg = 'Invalid base class'
            if name:
                msg += f' "{name}"'
            self.fail(msg, base_expr)
            is_error = True
            continue
        if base is None:
            return None
        base = get_proper_type(base)
        bases.append((base, base_expr))
    return bases, is_error

</t>
<t tx="ekr.20220525082935.590">def indent(s: str, n: int) -&gt; str:
    """Indent all the lines in s (separated by newlines) by n spaces."""
    s = ' ' * n + s
    s = s.replace('\n', '\n' + ' ' * n)
    return s
</t>
<t tx="ekr.20220525082935.591">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Parsing/inferring signatures from documentation.

This module provides several functions to generate better stubs using
docstrings and Sphinx docs (.rst files).
"""
import re
import io
import contextlib
import tokenize

from typing import (
    Optional, MutableMapping, MutableSequence, List, Sequence, Tuple, NamedTuple, Any
)
from typing_extensions import Final

# Type alias for signatures strings in format ('func_name', '(arg, opt_arg=False)').
Sig = Tuple[str, str]


_TYPE_RE: Final = re.compile(r"^[a-zA-Z_][\w\[\], ]*(\.[a-zA-Z_][\w\[\], ]*)*$")
_ARG_NAME_RE: Final = re.compile(r"\**[A-Za-z_][A-Za-z0-9_]*$")


@others
</t>
<t tx="ekr.20220525082935.592">def is_valid_type(s: str) -&gt; bool:
    """Try to determine whether a string might be a valid type annotation."""
    if s in ('True', 'False', 'retval'):
        return False
    if ',' in s and '[' not in s:
        return False
    return _TYPE_RE.match(s) is not None


</t>
<t tx="ekr.20220525082935.593">class ArgSig:
    """Signature info for a single argument."""

    @others
</t>
<t tx="ekr.20220525082935.594">def __init__(self, name: str, type: Optional[str] = None, default: bool = False):
    self.name = name
    if type and not is_valid_type(type):
        raise ValueError("Invalid type: " + type)
    self.type = type
    # Does this argument have a default value?
    self.default = default

</t>
<t tx="ekr.20220525082935.595">def __repr__(self) -&gt; str:
    return "ArgSig(name={}, type={}, default={})".format(repr(self.name), repr(self.type),
                                                        repr(self.default))

</t>
<t tx="ekr.20220525082935.596">def __eq__(self, other: Any) -&gt; bool:
    if isinstance(other, ArgSig):
        return (self.name == other.name and self.type == other.type and
                self.default == other.default)
    return False


</t>
<t tx="ekr.20220525082935.597">class FunctionSig(NamedTuple):
    name: str
    args: List[ArgSig]
    ret_type: str


</t>
<t tx="ekr.20220525082935.598"># States of the docstring parser.
STATE_INIT: Final = 1
STATE_FUNCTION_NAME: Final = 2
STATE_ARGUMENT_LIST: Final = 3
STATE_ARGUMENT_TYPE: Final = 4
STATE_ARGUMENT_DEFAULT: Final = 5
STATE_RETURN_VALUE: Final = 6
STATE_OPEN_BRACKET: Final = 7  # For generic types.


</t>
<t tx="ekr.20220525082935.599">class DocStringParser:
    """Parse function signatures in documentation."""

    @others
</t>
<t tx="ekr.20220525082935.6">#
# Preparing module (performed before semantic analysis)
#

</t>
<t tx="ekr.20220525082935.60">def configure_base_classes(self,
                           defn: ClassDef,
                           bases: List[Tuple[ProperType, Expression]]) -&gt; None:
    """Set up base classes.

    This computes several attributes on the corresponding TypeInfo defn.info
    related to the base classes: defn.info.bases, defn.info.mro, and
    miscellaneous others (at least tuple_type, fallback_to_any, and is_enum.)
    """
    base_types: List[Instance] = []
    info = defn.info

    info.tuple_type = None
    for base, base_expr in bases:
        if isinstance(base, TupleType):
            actual_base = self.configure_tuple_base_class(defn, base, base_expr)
            base_types.append(actual_base)
        elif isinstance(base, Instance):
            if base.type.is_newtype:
                self.fail('Cannot subclass "NewType"', defn)
            base_types.append(base)
        elif isinstance(base, AnyType):
            if self.options.disallow_subclassing_any:
                if isinstance(base_expr, (NameExpr, MemberExpr)):
                    msg = f'Class cannot subclass "{base_expr.name}" (has type "Any")'
                else:
                    msg = 'Class cannot subclass value of type "Any"'
                self.fail(msg, base_expr)
            info.fallback_to_any = True
        else:
            msg = 'Invalid base class'
            name = self.get_name_repr_of_expr(base_expr)
            if name:
                msg += f' "{name}"'
            self.fail(msg, base_expr)
            info.fallback_to_any = True
        if self.options.disallow_any_unimported and has_any_from_unimported_type(base):
            if isinstance(base_expr, (NameExpr, MemberExpr)):
                prefix = f"Base type {base_expr.name}"
            else:
                prefix = "Base type"
            self.msg.unimported_type_becomes_any(prefix, base, base_expr)
        check_for_explicit_any(base, self.options, self.is_typeshed_stub_file, self.msg,
                               context=base_expr)

    # Add 'object' as implicit base if there is no other base class.
    if not base_types and defn.fullname != 'builtins.object':
        base_types.append(self.object_type())

    info.bases = base_types

    # Calculate the MRO.
    if not self.verify_base_classes(defn):
        self.set_dummy_mro(defn.info)
        return
    self.calculate_class_mro(defn, self.object_type)

</t>
<t tx="ekr.20220525082935.600">def __init__(self, function_name: str) -&gt; None:
    # Only search for signatures of function with this name.
    self.function_name = function_name
    self.state = [STATE_INIT]
    self.accumulator = ""
    self.arg_type: Optional[str] = None
    self.arg_name = ""
    self.arg_default: Optional[str] = None
    self.ret_type = "Any"
    self.found = False
    self.args: List[ArgSig] = []
    # Valid signatures found so far.
    self.signatures: List[FunctionSig] = []

</t>
<t tx="ekr.20220525082935.601">def add_token(self, token: tokenize.TokenInfo) -&gt; None:
    """Process next token from the token stream."""
    if (token.type == tokenize.NAME and token.string == self.function_name and
            self.state[-1] == STATE_INIT):
        self.state.append(STATE_FUNCTION_NAME)

    elif (token.type == tokenize.OP and token.string == '(' and
          self.state[-1] == STATE_FUNCTION_NAME):
        self.state.pop()
        self.accumulator = ""
        self.found = True
        self.state.append(STATE_ARGUMENT_LIST)

    elif self.state[-1] == STATE_FUNCTION_NAME:
        # Reset state, function name not followed by '('.
        self.state.pop()

    elif (token.type == tokenize.OP and token.string in ('[', '(', '{') and
          self.state[-1] != STATE_INIT):
        self.accumulator += token.string
        self.state.append(STATE_OPEN_BRACKET)

    elif (token.type == tokenize.OP and token.string in (']', ')', '}') and
          self.state[-1] == STATE_OPEN_BRACKET):
        self.accumulator += token.string
        self.state.pop()

    elif (token.type == tokenize.OP and token.string == ':' and
          self.state[-1] == STATE_ARGUMENT_LIST):
        self.arg_name = self.accumulator
        self.accumulator = ""
        self.state.append(STATE_ARGUMENT_TYPE)

    elif (token.type == tokenize.OP and token.string == '=' and
          self.state[-1] in (STATE_ARGUMENT_LIST, STATE_ARGUMENT_TYPE)):
        if self.state[-1] == STATE_ARGUMENT_TYPE:
            self.arg_type = self.accumulator
            self.state.pop()
        else:
            self.arg_name = self.accumulator
        self.accumulator = ""
        self.state.append(STATE_ARGUMENT_DEFAULT)

    elif (token.type == tokenize.OP and token.string in (',', ')') and
          self.state[-1] in (STATE_ARGUMENT_LIST, STATE_ARGUMENT_DEFAULT,
                             STATE_ARGUMENT_TYPE)):
        if self.state[-1] == STATE_ARGUMENT_DEFAULT:
            self.arg_default = self.accumulator
            self.state.pop()
        elif self.state[-1] == STATE_ARGUMENT_TYPE:
            self.arg_type = self.accumulator
            self.state.pop()
        elif self.state[-1] == STATE_ARGUMENT_LIST:
            self.arg_name = self.accumulator
            if not (token.string == ')' and self.accumulator.strip() == '') \
                    and not _ARG_NAME_RE.match(self.arg_name):
                # Invalid argument name.
                self.reset()
                return

        if token.string == ')':
            self.state.pop()

        # arg_name is empty when there are no args. e.g. func()
        if self.arg_name:
            try:
                self.args.append(ArgSig(name=self.arg_name, type=self.arg_type,
                                        default=bool(self.arg_default)))
            except ValueError:
                # wrong type, use Any
                self.args.append(ArgSig(name=self.arg_name, type=None,
                                        default=bool(self.arg_default)))
        self.arg_name = ""
        self.arg_type = None
        self.arg_default = None
        self.accumulator = ""

    elif token.type == tokenize.OP and token.string == '-&gt;' and self.state[-1] == STATE_INIT:
        self.accumulator = ""
        self.state.append(STATE_RETURN_VALUE)

    # ENDMAKER is necessary for python 3.4 and 3.5.
    elif (token.type in (tokenize.NEWLINE, tokenize.ENDMARKER) and
          self.state[-1] in (STATE_INIT, STATE_RETURN_VALUE)):
        if self.state[-1] == STATE_RETURN_VALUE:
            if not is_valid_type(self.accumulator):
                self.reset()
                return
            self.ret_type = self.accumulator
            self.accumulator = ""
            self.state.pop()

        if self.found:
            self.signatures.append(FunctionSig(name=self.function_name, args=self.args,
                                               ret_type=self.ret_type))
            self.found = False
        self.args = []
        self.ret_type = 'Any'
        # Leave state as INIT.
    else:
        self.accumulator += token.string

</t>
<t tx="ekr.20220525082935.602">def reset(self) -&gt; None:
    self.state = [STATE_INIT]
    self.args = []
    self.found = False
    self.accumulator = ""

</t>
<t tx="ekr.20220525082935.603">def get_signatures(self) -&gt; List[FunctionSig]:
    """Return sorted copy of the list of signatures found so far."""
    def has_arg(name: str, signature: FunctionSig) -&gt; bool:
        return any(x.name == name for x in signature.args)

    def args_kwargs(signature: FunctionSig) -&gt; bool:
        return has_arg('*args', signature) and has_arg('**kwargs', signature)

    # Move functions with (*args, **kwargs) in their signature to last place.
    return list(sorted(self.signatures, key=lambda x: 1 if args_kwargs(x) else 0))


</t>
<t tx="ekr.20220525082935.604">def infer_sig_from_docstring(docstr: Optional[str], name: str) -&gt; Optional[List[FunctionSig]]:
    """Convert function signature to list of TypedFunctionSig

    Look for function signatures of function in docstring. Signature is a string of
    the format &lt;function_name&gt;(&lt;signature&gt;) -&gt; &lt;return type&gt; or perhaps without
    the return type.

    Returns empty list, when no signature is found, one signature in typical case,
    multiple signatures, if docstring specifies multiple signatures for overload functions.
    Return None if the docstring is empty.

    Arguments:
        * docstr: docstring
        * name: name of function for which signatures are to be found
    """
    if not docstr:
        return None

    state = DocStringParser(name)
    # Return all found signatures, even if there is a parse error after some are found.
    with contextlib.suppress(tokenize.TokenError):
        try:
            tokens = tokenize.tokenize(io.BytesIO(docstr.encode('utf-8')).readline)
            for token in tokens:
                state.add_token(token)
        except IndentationError:
            return None
    sigs = state.get_signatures()

    @others
    # Return only signatures that have unique argument names. Mypy fails on non-unique arg names.
    return [sig for sig in sigs if is_unique_args(sig)]


</t>
<t tx="ekr.20220525082935.605">def is_unique_args(sig: FunctionSig) -&gt; bool:
    """return true if function argument names are unique"""
    return len(sig.args) == len({arg.name for arg in sig.args})

</t>
<t tx="ekr.20220525082935.606">def infer_arg_sig_from_anon_docstring(docstr: str) -&gt; List[ArgSig]:
    """Convert signature in form of "(self: TestClass, arg0: str='ada')" to List[TypedArgList]."""
    ret = infer_sig_from_docstring("stub" + docstr, "stub")
    if ret:
        return ret[0].args
    return []


</t>
<t tx="ekr.20220525082935.607">def infer_ret_type_sig_from_docstring(docstr: str, name: str) -&gt; Optional[str]:
    """Convert signature in form of "func(self: TestClass, arg0) -&gt; int" to their return type."""
    ret = infer_sig_from_docstring(docstr, name)
    if ret:
        return ret[0].ret_type
    return None


</t>
<t tx="ekr.20220525082935.608">def infer_ret_type_sig_from_anon_docstring(docstr: str) -&gt; Optional[str]:
    """Convert signature in form of "(self: TestClass, arg0) -&gt; int" to their return type."""
    return infer_ret_type_sig_from_docstring("stub" + docstr.strip(), "stub")


</t>
<t tx="ekr.20220525082935.609">def parse_signature(sig: str) -&gt; Optional[Tuple[str,
                                                List[str],
                                                List[str]]]:
    """Split function signature into its name, positional an optional arguments.

    The expected format is "func_name(arg, opt_arg=False)". Return the name of function
    and lists of positional and optional argument names.
    """
    m = re.match(r'([.a-zA-Z0-9_]+)\(([^)]*)\)', sig)
    if not m:
        return None
    name = m.group(1)
    name = name.split('.')[-1]
    arg_string = m.group(2)
    if not arg_string.strip():
        # Simple case -- no arguments.
        return name, [], []

    args = [arg.strip() for arg in arg_string.split(',')]
    positional = []
    optional = []
    i = 0
    while i &lt; len(args):
        # Accept optional arguments as in both formats: x=None and [x].
        if args[i].startswith('[') or '=' in args[i]:
            break
        positional.append(args[i].rstrip('['))
        i += 1
        if args[i - 1].endswith('['):
            break
    while i &lt; len(args):
        arg = args[i]
        arg = arg.strip('[]')
        arg = arg.split('=')[0]
        optional.append(arg)
        i += 1
    return name, positional, optional


</t>
<t tx="ekr.20220525082935.61">def configure_tuple_base_class(self,
                               defn: ClassDef,
                               base: TupleType,
                               base_expr: Expression) -&gt; Instance:
    info = defn.info

    # There may be an existing valid tuple type from previous semanal iterations.
    # Use equality to check if it is the case.
    if info.tuple_type and info.tuple_type != base:
        self.fail("Class has two incompatible bases derived from tuple", defn)
        defn.has_incompatible_baseclass = True
    info.tuple_type = base
    if isinstance(base_expr, CallExpr):
        defn.analyzed = NamedTupleExpr(base.partial_fallback.type)
        defn.analyzed.line = defn.line
        defn.analyzed.column = defn.column

    if base.partial_fallback.type.fullname == 'builtins.tuple':
        # Fallback can only be safely calculated after semantic analysis, since base
        # classes may be incomplete. Postpone the calculation.
        self.schedule_patch(PRIORITY_FALLBACKS, lambda: calculate_tuple_fallback(base))

    return base.partial_fallback

</t>
<t tx="ekr.20220525082935.610">def build_signature(positional: Sequence[str],
                    optional: Sequence[str]) -&gt; str:
    """Build function signature from lists of positional and optional argument names."""
    args: MutableSequence[str] = []
    args.extend(positional)
    for arg in optional:
        if arg.startswith('*'):
            args.append(arg)
        else:
            args.append(f'{arg}=...')
    sig = f"({', '.join(args)})"
    # Ad-hoc fixes.
    sig = sig.replace('(self)', '')
    return sig


</t>
<t tx="ekr.20220525082935.611">def parse_all_signatures(lines: Sequence[str]) -&gt; Tuple[List[Sig],
                                                        List[Sig]]:
    """Parse all signatures in a given reST document.

    Return lists of found signatures for functions and classes.
    """
    sigs = []
    class_sigs = []
    for line in lines:
        line = line.strip()
        m = re.match(r'\.\. *(function|method|class) *:: *[a-zA-Z_]', line)
        if m:
            sig = line.split('::')[1].strip()
            parsed = parse_signature(sig)
            if parsed:
                name, fixed, optional = parsed
                if m.group(1) != 'class':
                    sigs.append((name, build_signature(fixed, optional)))
                else:
                    class_sigs.append((name, build_signature(fixed, optional)))

    return sorted(sigs), sorted(class_sigs)


</t>
<t tx="ekr.20220525082935.612">def find_unique_signatures(sigs: Sequence[Sig]) -&gt; List[Sig]:
    """Remove names with duplicate found signatures."""
    sig_map: MutableMapping[str, List[str]] = {}
    for name, sig in sigs:
        sig_map.setdefault(name, []).append(sig)

    result = []
    for name, name_sigs in sig_map.items():
        if len(set(name_sigs)) == 1:
            result.append((name, name_sigs[0]))
    return sorted(result)


</t>
<t tx="ekr.20220525082935.613">def infer_prop_type_from_docstring(docstr: Optional[str]) -&gt; Optional[str]:
    """Check for Google/Numpy style docstring type annotation for a property.

    The docstring has the format "&lt;type&gt;: &lt;descriptions&gt;".
    In the type string, we allow the following characters:
    * dot: because sometimes classes are annotated using full path
    * brackets: to allow type hints like List[int]
    * comma/space: things like Tuple[int, int]
    """
    if not docstr:
        return None
    test_str = r'^([a-zA-Z0-9_, \.\[\]]*): '
    m = re.match(test_str, docstr)
    return m.group(1) if m else None
</t>
<t tx="ekr.20220525082935.614">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
#!/usr/bin/env python3
"""Generator of dynamically typed draft stubs for arbitrary modules.

The logic of this script can be split in three steps:
* parsing options and finding sources:
  - use runtime imports be default (to find also C modules)
  - or use mypy's mechanisms, if importing is prohibited
* (optionally) semantically analysing the sources using mypy (as a single set)
* emitting the stubs text:
  - for Python modules: from ASTs using StubGenerator
  - for C modules using runtime introspection and (optionally) Sphinx docs

During first and third steps some problematic files can be skipped, but any
blocking error during second step will cause the whole program to stop.

Basic usage:

  $ stubgen foo.py bar.py some_directory
  =&gt; Generate out/foo.pyi, out/bar.pyi, and stubs for some_directory (recursively).

  $ stubgen -m urllib.parse
  =&gt; Generate out/urllib/parse.pyi.

  $ stubgen -p urllib
  =&gt; Generate stubs for whole urlib package (recursively).

For Python 2 mode, use --py2:

  $ stubgen --py2 -m textwrap

For C modules, you can get more precise function signatures by parsing .rst (Sphinx)
documentation for extra information. For this, use the --doc-dir option:

  $ stubgen --doc-dir &lt;DIR&gt;/Python-3.4.2/Doc/library -m curses

Note: The generated stubs should be verified manually.

TODO:
 - support stubs for C modules in Python 2 mode
 - detect 'if PY2 / is_py2' etc. and either preserve those or only include Python 2 or 3 case
 - maybe use .rst docs also for Python modules
 - maybe export more imported names if there is no __all__ (this affects ssl.SSLError, for example)
   - a quick and dirty heuristic would be to turn this on if a module has something like
     'from x import y as _y'
 - we don't seem to always detect properties ('closed' in 'io', for example)
"""

import glob
import os
import os.path
import sys
import traceback
import argparse
from collections import defaultdict

from typing import (
    List, Dict, Tuple, Iterable, Mapping, Optional, Set, Union, cast,
)
from typing_extensions import Final

import mypy.build
import mypy.parse
import mypy.traverser
import mypy.mixedtraverser
import mypy.util
from mypy import defaults
from mypy.modulefinder import (
    ModuleNotFoundReason, FindModuleCache, SearchPaths, BuildSource, default_lib_path
)
from mypy.nodes import (
    Expression, IntExpr, UnaryExpr, StrExpr, BytesExpr, NameExpr, FloatExpr, MemberExpr,
    TupleExpr, ListExpr, ComparisonExpr, CallExpr, IndexExpr, EllipsisExpr,
    ClassDef, MypyFile, Decorator, AssignmentStmt, TypeInfo,
    IfStmt, ImportAll, ImportFrom, Import, FuncDef, FuncBase, Block,
    Statement, OverloadedFuncDef, ARG_POS, ARG_STAR, ARG_STAR2, ARG_NAMED,
)
from mypy.stubgenc import generate_stub_for_c_module
from mypy.stubutil import (
    default_py2_interpreter, CantImport, generate_guarded,
    walk_packages, find_module_path_and_all_py2, find_module_path_and_all_py3,
    report_missing, fail_missing, remove_misplaced_type_comments, common_dir_prefix
)
from mypy.stubdoc import parse_all_signatures, find_unique_signatures, Sig
from mypy.options import Options as MypyOptions
from mypy.types import (
    Type, TypeStrVisitor, CallableType, UnboundType, NoneType, TupleType, TypeList, Instance,
    AnyType, get_proper_type, OVERLOAD_NAMES
)
from mypy.visitor import NodeVisitor
from mypy.find_sources import create_source_list, InvalidSourceList
from mypy.build import build
from mypy.errors import CompileError, Errors
from mypy.traverser import all_yield_expressions, has_return_statement, has_yield_expression
from mypy.moduleinspect import ModuleInspect

TYPING_MODULE_NAMES: Final = (
    'typing',
    'typing_extensions',
)

# Common ways of naming package containing vendored modules.
VENDOR_PACKAGES: Final = [
    'packages',
    'vendor',
    'vendored',
    '_vendor',
    '_vendored_packages',
]

# Avoid some file names that are unnecessary or likely to cause trouble (\n for end of path).
BLACKLIST: Final = [
    '/six.py\n',  # Likely vendored six; too dynamic for us to handle
    '/vendored/',  # Vendored packages
    '/vendor/',  # Vendored packages
    '/_vendor/',
    '/_vendored_packages/',
]

# Special-cased names that are implicitly exported from the stub (from m import y as y).
EXTRA_EXPORTED: Final = {
    'pyasn1_modules.rfc2437.univ',
    'pyasn1_modules.rfc2459.char',
    'pyasn1_modules.rfc2459.univ',
}

# These names should be omitted from generated stubs.
IGNORED_DUNDERS: Final = {
    '__all__',
    '__author__',
    '__version__',
    '__about__',
    '__copyright__',
    '__email__',
    '__license__',
    '__summary__',
    '__title__',
    '__uri__',
    '__str__',
    '__repr__',
    '__getstate__',
    '__setstate__',
    '__slots__',
}

# These methods are expected to always return a non-trivial value.
METHODS_WITH_RETURN_VALUE: Final = {
    '__ne__',
    '__eq__',
    '__lt__',
    '__le__',
    '__gt__',
    '__ge__',
    '__hash__',
    '__iter__',
}


@others
if __name__ == '__main__':
    main()
</t>
<t tx="ekr.20220525082935.615">class Options:
    """Represents stubgen options.

    This class is mutable to simplify testing.
    """
    @others
</t>
<t tx="ekr.20220525082935.616">def __init__(self,
             pyversion: Tuple[int, int],
             no_import: bool,
             doc_dir: str,
             search_path: List[str],
             interpreter: str,
             parse_only: bool,
             ignore_errors: bool,
             include_private: bool,
             output_dir: str,
             modules: List[str],
             packages: List[str],
             files: List[str],
             verbose: bool,
             quiet: bool,
             export_less: bool) -&gt; None:
    # See parse_options for descriptions of the flags.
    self.pyversion = pyversion
    self.no_import = no_import
    self.doc_dir = doc_dir
    self.search_path = search_path
    self.interpreter = interpreter
    self.decointerpreter = interpreter
    self.parse_only = parse_only
    self.ignore_errors = ignore_errors
    self.include_private = include_private
    self.output_dir = output_dir
    self.modules = modules
    self.packages = packages
    self.files = files
    self.verbose = verbose
    self.quiet = quiet
    self.export_less = export_less


</t>
<t tx="ekr.20220525082935.617">class StubSource:
    """A single source for stub: can be a Python or C module.

    A simple extension of BuildSource that also carries the AST and
    the value of __all__ detected at runtime.
    """
    @others
</t>
<t tx="ekr.20220525082935.618">def __init__(self, module: str, path: Optional[str] = None,
             runtime_all: Optional[List[str]] = None) -&gt; None:
    self.source = BuildSource(path, module, None)
    self.runtime_all = runtime_all
    self.ast: Optional[MypyFile] = None

</t>
<t tx="ekr.20220525082935.619">@property
def module(self) -&gt; str:
    return self.source.module

</t>
<t tx="ekr.20220525082935.62">def set_dummy_mro(self, info: TypeInfo) -&gt; None:
    # Give it an MRO consisting of just the class itself and object.
    info.mro = [info, self.object_type().type]
    info.bad_mro = True

</t>
<t tx="ekr.20220525082935.620">@property
def path(self) -&gt; Optional[str]:
    return self.source.path


</t>
<t tx="ekr.20220525082935.621"># What was generated previously in the stub file. We keep track of these to generate
# nicely formatted output (add empty line between non-empty classes, for example).
EMPTY: Final = "EMPTY"
FUNC: Final = "FUNC"
CLASS: Final = "CLASS"
EMPTY_CLASS: Final = "EMPTY_CLASS"
VAR: Final = "VAR"
NOT_IN_ALL: Final = "NOT_IN_ALL"

# Indicates that we failed to generate a reasonable output
# for a given node. These should be manually replaced by a user.

ERROR_MARKER: Final = "&lt;ERROR&gt;"


</t>
<t tx="ekr.20220525082935.622">class AnnotationPrinter(TypeStrVisitor):
    """Visitor used to print existing annotations in a file.

    The main difference from TypeStrVisitor is a better treatment of
    unbound types.

    Notes:
    * This visitor doesn't add imports necessary for annotations, this is done separately
      by ImportTracker.
    * It can print all kinds of types, but the generated strings may not be valid (notably
      callable types) since it prints the same string that reveal_type() does.
    * For Instance types it prints the fully qualified names.
    """
    @others
</t>
<t tx="ekr.20220525082935.623"># TODO: Generate valid string representation for callable types.
# TODO: Use short names for Instances.
def __init__(self, stubgen: 'StubGenerator') -&gt; None:
    super().__init__()
    self.stubgen = stubgen

</t>
<t tx="ekr.20220525082935.624">def visit_any(self, t: AnyType) -&gt; str:
    s = super().visit_any(t)
    self.stubgen.import_tracker.require_name(s)
    return s

</t>
<t tx="ekr.20220525082935.625">def visit_unbound_type(self, t: UnboundType) -&gt; str:
    s = t.name
    self.stubgen.import_tracker.require_name(s)
    if t.args:
        s += f'[{self.args_str(t.args)}]'
    return s

</t>
<t tx="ekr.20220525082935.626">def visit_none_type(self, t: NoneType) -&gt; str:
    return "None"

</t>
<t tx="ekr.20220525082935.627">def visit_type_list(self, t: TypeList) -&gt; str:
    return f'[{self.list_str(t.items)}]'

</t>
<t tx="ekr.20220525082935.628">def args_str(self, args: Iterable[Type]) -&gt; str:
    """Convert an array of arguments to strings and join the results with commas.

    The main difference from list_str is the preservation of quotes for string
    arguments
    """
    types = ['builtins.bytes', 'builtins.unicode']
    res = []
    for arg in args:
        arg_str = arg.accept(self)
        if isinstance(arg, UnboundType) and arg.original_str_fallback in types:
            res.append(f"'{arg_str}'")
        else:
            res.append(arg_str)
    return ', '.join(res)


</t>
<t tx="ekr.20220525082935.629">class AliasPrinter(NodeVisitor[str]):
    """Visitor used to collect type aliases _and_ type variable definitions.

    Visit r.h.s of the definition to get the string representation of type alias.
    """
    @others
</t>
<t tx="ekr.20220525082935.63">def calculate_class_mro(self, defn: ClassDef,
                        obj_type: Optional[Callable[[], Instance]] = None) -&gt; None:
    """Calculate method resolution order for a class.

    `obj_type` may be omitted in the third pass when all classes are already analyzed.
    It exists just to fill in empty base class list during second pass in case of
    an import cycle.
    """
    try:
        calculate_mro(defn.info, obj_type)
    except MroError:
        self.fail('Cannot determine consistent method resolution '
                  'order (MRO) for "%s"' % defn.name, defn)
        self.set_dummy_mro(defn.info)
    # Allow plugins to alter the MRO to handle the fact that `def mro()`
    # on metaclasses permits MRO rewriting.
    if defn.fullname:
        hook = self.plugin.get_customize_class_mro_hook(defn.fullname)
        if hook:
            hook(ClassDefContext(defn, FakeExpression(), self))

</t>
<t tx="ekr.20220525082935.630">def __init__(self, stubgen: 'StubGenerator') -&gt; None:
    self.stubgen = stubgen
    super().__init__()

</t>
<t tx="ekr.20220525082935.631">def visit_call_expr(self, node: CallExpr) -&gt; str:
    # Call expressions are not usually types, but we also treat `X = TypeVar(...)` as a
    # type alias that has to be preserved (even if TypeVar is not the same as an alias)
    callee = node.callee.accept(self)
    args = []
    for name, arg, kind in zip(node.arg_names, node.args, node.arg_kinds):
        if kind == ARG_POS:
            args.append(arg.accept(self))
        elif kind == ARG_STAR:
            args.append('*' + arg.accept(self))
        elif kind == ARG_STAR2:
            args.append('**' + arg.accept(self))
        elif kind == ARG_NAMED:
            args.append(f'{name}={arg.accept(self)}')
        else:
            raise ValueError(f"Unknown argument kind {kind} in call")
    return f"{callee}({', '.join(args)})"

</t>
<t tx="ekr.20220525082935.632">def visit_name_expr(self, node: NameExpr) -&gt; str:
    self.stubgen.import_tracker.require_name(node.name)
    return node.name

</t>
<t tx="ekr.20220525082935.633">def visit_member_expr(self, o: MemberExpr) -&gt; str:
    node: Expression = o
    trailer = ''
    while isinstance(node, MemberExpr):
        trailer = '.' + node.name + trailer
        node = node.expr
    if not isinstance(node, NameExpr):
        return ERROR_MARKER
    self.stubgen.import_tracker.require_name(node.name)
    return node.name + trailer

</t>
<t tx="ekr.20220525082935.634">def visit_str_expr(self, node: StrExpr) -&gt; str:
    return repr(node.value)

</t>
<t tx="ekr.20220525082935.635">def visit_index_expr(self, node: IndexExpr) -&gt; str:
    base = node.base.accept(self)
    index = node.index.accept(self)
    return f"{base}[{index}]"

</t>
<t tx="ekr.20220525082935.636">def visit_tuple_expr(self, node: TupleExpr) -&gt; str:
    return ", ".join(n.accept(self) for n in node.items)

</t>
<t tx="ekr.20220525082935.637">def visit_list_expr(self, node: ListExpr) -&gt; str:
    return f"[{', '.join(n.accept(self) for n in node.items)}]"

</t>
<t tx="ekr.20220525082935.638">def visit_ellipsis(self, node: EllipsisExpr) -&gt; str:
    return "..."


</t>
<t tx="ekr.20220525082935.639">class ImportTracker:
    """Record necessary imports during stub generation."""

    @others
</t>
<t tx="ekr.20220525082935.64">def update_metaclass(self, defn: ClassDef) -&gt; None:
    """Lookup for special metaclass declarations, and update defn fields accordingly.

    * __metaclass__ attribute in Python 2
    * six.with_metaclass(M, B1, B2, ...)
    * @six.add_metaclass(M)
    * future.utils.with_metaclass(M, B1, B2, ...)
    * past.utils.with_metaclass(M, B1, B2, ...)
    """

    # Look for "__metaclass__ = &lt;metaclass&gt;" in Python 2
    python2_meta_expr: Optional[Expression] = None
    if self.options.python_version[0] == 2:
        for body_node in defn.defs.body:
            if isinstance(body_node, ClassDef) and body_node.name == "__metaclass__":
                self.fail("Metaclasses defined as inner classes are not supported", body_node)
                break
            elif isinstance(body_node, AssignmentStmt) and len(body_node.lvalues) == 1:
                lvalue = body_node.lvalues[0]
                if isinstance(lvalue, NameExpr) and lvalue.name == "__metaclass__":
                    python2_meta_expr = body_node.rvalue

    # Look for six.with_metaclass(M, B1, B2, ...)
    with_meta_expr: Optional[Expression] = None
    if len(defn.base_type_exprs) == 1:
        base_expr = defn.base_type_exprs[0]
        if isinstance(base_expr, CallExpr) and isinstance(base_expr.callee, RefExpr):
            base_expr.accept(self)
            if (base_expr.callee.fullname in {'six.with_metaclass',
                                              'future.utils.with_metaclass',
                                              'past.utils.with_metaclass'}
                    and len(base_expr.args) &gt;= 1
                    and all(kind == ARG_POS for kind in base_expr.arg_kinds)):
                with_meta_expr = base_expr.args[0]
                defn.base_type_exprs = base_expr.args[1:]

    # Look for @six.add_metaclass(M)
    add_meta_expr: Optional[Expression] = None
    for dec_expr in defn.decorators:
        if isinstance(dec_expr, CallExpr) and isinstance(dec_expr.callee, RefExpr):
            dec_expr.callee.accept(self)
            if (dec_expr.callee.fullname == 'six.add_metaclass'
                and len(dec_expr.args) == 1
                    and dec_expr.arg_kinds[0] == ARG_POS):
                add_meta_expr = dec_expr.args[0]
                break

    metas = {defn.metaclass, python2_meta_expr, with_meta_expr, add_meta_expr} - {None}
    if len(metas) == 0:
        return
    if len(metas) &gt; 1:
        self.fail("Multiple metaclass definitions", defn)
        return
    defn.metaclass = metas.pop()

</t>
<t tx="ekr.20220525082935.640">def __init__(self) -&gt; None:
    # module_for['foo'] has the module name where 'foo' was imported from, or None if
    # 'foo' is a module imported directly; examples
    #     'from pkg.m import f as foo' ==&gt; module_for['foo'] == 'pkg.m'
    #     'from m import f' ==&gt; module_for['f'] == 'm'
    #     'import m' ==&gt; module_for['m'] == None
    #     'import pkg.m' ==&gt; module_for['pkg.m'] == None
    #                    ==&gt; module_for['pkg'] == None
    self.module_for: Dict[str, Optional[str]] = {}

    # direct_imports['foo'] is the module path used when the name 'foo' was added to the
    # namespace.
    #   import foo.bar.baz  ==&gt; direct_imports['foo'] == 'foo.bar.baz'
    #                       ==&gt; direct_imports['foo.bar'] == 'foo.bar.baz'
    #                       ==&gt; direct_imports['foo.bar.baz'] == 'foo.bar.baz'
    self.direct_imports: Dict[str, str] = {}

    # reverse_alias['foo'] is the name that 'foo' had originally when imported with an
    # alias; examples
    #     'import numpy as np' ==&gt; reverse_alias['np'] == 'numpy'
    #     'import foo.bar as bar' ==&gt; reverse_alias['bar'] == 'foo.bar'
    #     'from decimal import Decimal as D' ==&gt; reverse_alias['D'] == 'Decimal'
    self.reverse_alias: Dict[str, str] = {}

    # required_names is the set of names that are actually used in a type annotation
    self.required_names: Set[str] = set()

    # Names that should be reexported if they come from another module
    self.reexports: Set[str] = set()

</t>
<t tx="ekr.20220525082935.641">def add_import_from(self, module: str, names: List[Tuple[str, Optional[str]]]) -&gt; None:
    for name, alias in names:
        if alias:
            # 'from {module} import {name} as {alias}'
            self.module_for[alias] = module
            self.reverse_alias[alias] = name
        else:
            # 'from {module} import {name}'
            self.module_for[name] = module
            self.reverse_alias.pop(name, None)
        self.direct_imports.pop(alias or name, None)

</t>
<t tx="ekr.20220525082935.642">def add_import(self, module: str, alias: Optional[str] = None) -&gt; None:
    if alias:
        # 'import {module} as {alias}'
        self.module_for[alias] = None
        self.reverse_alias[alias] = module
    else:
        # 'import {module}'
        name = module
        # add module and its parent packages
        while name:
            self.module_for[name] = None
            self.direct_imports[name] = module
            self.reverse_alias.pop(name, None)
            name = name.rpartition('.')[0]

</t>
<t tx="ekr.20220525082935.643">def require_name(self, name: str) -&gt; None:
    self.required_names.add(name.split('.')[0])

</t>
<t tx="ekr.20220525082935.644">def reexport(self, name: str) -&gt; None:
    """Mark a given non qualified name as needed in __all__.

    This means that in case it comes from a module, it should be
    imported with an alias even is the alias is the same as the name.
    """
    self.require_name(name)
    self.reexports.add(name)

</t>
<t tx="ekr.20220525082935.645">def import_lines(self) -&gt; List[str]:
    """The list of required import lines (as strings with python code)."""
    result = []

    # To summarize multiple names imported from a same module, we collect those
    # in the `module_map` dictionary, mapping a module path to the list of names that should
    # be imported from it. the names can also be alias in the form 'original as alias'
    module_map: Mapping[str, List[str]] = defaultdict(list)

    for name in sorted(self.required_names):
        # If we haven't seen this name in an import statement, ignore it
        if name not in self.module_for:
            continue

        m = self.module_for[name]
        if m is not None:
            # This name was found in a from ... import ...
            # Collect the name in the module_map
            if name in self.reverse_alias:
                name = f'{self.reverse_alias[name]} as {name}'
            elif name in self.reexports:
                name = f'{name} as {name}'
            module_map[m].append(name)
        else:
            # This name was found in an import ...
            # We can already generate the import line
            if name in self.reverse_alias:
                source = self.reverse_alias[name]
                result.append(f"import {source} as {name}\n")
            elif name in self.reexports:
                assert '.' not in name  # Because reexports only has nonqualified names
                result.append(f"import {name} as {name}\n")
            else:
                result.append(f"import {self.direct_imports[name]}\n")

    # Now generate all the from ... import ... lines collected in module_map
    for module, names in sorted(module_map.items()):
        result.append(f"from {module} import {', '.join(sorted(names))}\n")
    return result


</t>
<t tx="ekr.20220525082935.646">def find_defined_names(file: MypyFile) -&gt; Set[str]:
    finder = DefinitionFinder()
    file.accept(finder)
    return finder.names


</t>
<t tx="ekr.20220525082935.647">class DefinitionFinder(mypy.traverser.TraverserVisitor):
    """Find names of things defined at the top level of a module."""

    # TODO: Assignment statements etc.

    @others
</t>
<t tx="ekr.20220525082935.648">def __init__(self) -&gt; None:
    # Short names of things defined at the top level.
    self.names: Set[str] = set()

</t>
<t tx="ekr.20220525082935.649">def visit_class_def(self, o: ClassDef) -&gt; None:
    # Don't recurse into classes, as we only keep track of top-level definitions.
    self.names.add(o.name)

</t>
<t tx="ekr.20220525082935.65">def verify_base_classes(self, defn: ClassDef) -&gt; bool:
    info = defn.info
    cycle = False
    for base in info.bases:
        baseinfo = base.type
        if self.is_base_class(info, baseinfo):
            self.fail('Cycle in inheritance hierarchy', defn)
            cycle = True
        if baseinfo.fullname == 'builtins.bool':
            self.fail('"%s" is not a valid base class' %
                      baseinfo.name, defn, blocker=True)
            return False
    dup = find_duplicate(info.direct_base_classes())
    if dup:
        self.fail(f'Duplicate base class "{dup.name}"', defn, blocker=True)
        return False
    return not cycle

</t>
<t tx="ekr.20220525082935.650">def visit_func_def(self, o: FuncDef) -&gt; None:
    # Don't recurse, as we only keep track of top-level definitions.
    self.names.add(o.name)


</t>
<t tx="ekr.20220525082935.651">def find_referenced_names(file: MypyFile) -&gt; Set[str]:
    finder = ReferenceFinder()
    file.accept(finder)
    return finder.refs


</t>
<t tx="ekr.20220525082935.652">class ReferenceFinder(mypy.mixedtraverser.MixedTraverserVisitor):
    """Find all name references (both local and global)."""

    # TODO: Filter out local variable and class attribute references

    @others
</t>
<t tx="ekr.20220525082935.653">def __init__(self) -&gt; None:
    # Short names of things defined at the top level.
    self.refs: Set[str] = set()

</t>
<t tx="ekr.20220525082935.654">def visit_block(self, block: Block) -&gt; None:
    if not block.is_unreachable:
        super().visit_block(block)

</t>
<t tx="ekr.20220525082935.655">def visit_name_expr(self, e: NameExpr) -&gt; None:
    self.refs.add(e.name)

</t>
<t tx="ekr.20220525082935.656">def visit_instance(self, t: Instance) -&gt; None:
    self.add_ref(t.type.fullname)
    super().visit_instance(t)

</t>
<t tx="ekr.20220525082935.657">def visit_unbound_type(self, t: UnboundType) -&gt; None:
    if t.name:
        self.add_ref(t.name)

</t>
<t tx="ekr.20220525082935.658">def visit_tuple_type(self, t: TupleType) -&gt; None:
    # Ignore fallback
    for item in t.items:
        item.accept(self)

</t>
<t tx="ekr.20220525082935.659">def visit_callable_type(self, t: CallableType) -&gt; None:
    # Ignore fallback
    for arg in t.arg_types:
        arg.accept(self)
    t.ret_type.accept(self)

</t>
<t tx="ekr.20220525082935.66">def is_base_class(self, t: TypeInfo, s: TypeInfo) -&gt; bool:
    """Determine if t is a base class of s (but do not use mro)."""
    # Search the base class graph for t, starting from s.
    worklist = [s]
    visited = {s}
    while worklist:
        nxt = worklist.pop()
        if nxt == t:
            return True
        for base in nxt.bases:
            if base.type not in visited:
                worklist.append(base.type)
                visited.add(base.type)
    return False

</t>
<t tx="ekr.20220525082935.660">def add_ref(self, fullname: str) -&gt; None:
    self.refs.add(fullname.split('.')[-1])


</t>
<t tx="ekr.20220525082935.661">class StubGenerator(mypy.traverser.TraverserVisitor):
    """Generate stub text from a mypy AST."""

    @others
</t>
<t tx="ekr.20220525082935.662">def __init__(self,
             _all_: Optional[List[str]], pyversion: Tuple[int, int],
             include_private: bool = False,
             analyzed: bool = False,
             export_less: bool = False) -&gt; None:
    # Best known value of __all__.
    self._all_ = _all_
    self._output: List[str] = []
    self._decorators: List[str] = []
    self._import_lines: List[str] = []
    # Current indent level (indent is hardcoded to 4 spaces).
    self._indent = ''
    # Stack of defined variables (per scope).
    self._vars: List[List[str]] = [[]]
    # What was generated previously in the stub file.
    self._state = EMPTY
    self._toplevel_names: List[str] = []
    self._pyversion = pyversion
    self._include_private = include_private
    self.import_tracker = ImportTracker()
    # Was the tree semantically analysed before?
    self.analyzed = analyzed
    # Disable implicit exports of package-internal imports?
    self.export_less = export_less
    # Add imports that could be implicitly generated
    self.import_tracker.add_import_from("typing", [("NamedTuple", None)])
    # Names in __all__ are required
    for name in _all_ or ():
        if name not in IGNORED_DUNDERS:
            self.import_tracker.reexport(name)
    self.defined_names: Set[str] = set()
    # Short names of methods defined in the body of the current class
    self.method_names: Set[str] = set()

</t>
<t tx="ekr.20220525082935.663">def visit_mypy_file(self, o: MypyFile) -&gt; None:
    self.module = o.fullname  # Current module being processed
    self.path = o.path
    self.defined_names = find_defined_names(o)
    self.referenced_names = find_referenced_names(o)
    known_imports = {
        "_typeshed": ["Incomplete"],
        "typing": ["Any", "TypeVar"],
        "collections.abc": ["Generator"],
    }
    for pkg, imports in known_imports.items():
        for t in imports:
            if t not in self.defined_names:
                alias = None
            else:
                alias = '_' + t
            self.import_tracker.add_import_from(pkg, [(t, alias)])
    super().visit_mypy_file(o)
    undefined_names = [name for name in self._all_ or []
                       if name not in self._toplevel_names]
    if undefined_names:
        if self._state != EMPTY:
            self.add('\n')
        self.add('# Names in __all__ with no definition:\n')
        for name in sorted(undefined_names):
            self.add(f'#   {name}\n')

</t>
<t tx="ekr.20220525082935.664">def visit_overloaded_func_def(self, o: OverloadedFuncDef) -&gt; None:
    """@property with setters and getters, or @overload chain"""
    overload_chain = False
    for item in o.items:
        if not isinstance(item, Decorator):
            continue

        if self.is_private_name(item.func.name, item.func.fullname):
            continue

        is_abstract, is_overload = self.process_decorator(item)

        if not overload_chain:
            self.visit_func_def(item.func, is_abstract=is_abstract, is_overload=is_overload)
            if is_overload:
                overload_chain = True
        elif overload_chain and is_overload:
            self.visit_func_def(item.func, is_abstract=is_abstract, is_overload=is_overload)
        else:
            # skip the overload implementation and clear the decorator we just processed
            self.clear_decorators()

</t>
<t tx="ekr.20220525082935.665">def visit_func_def(self, o: FuncDef, is_abstract: bool = False,
                   is_overload: bool = False) -&gt; None:
    if (self.is_private_name(o.name, o.fullname)
            or self.is_not_in_all(o.name)
            or (self.is_recorded_name(o.name) and not is_overload)):
        self.clear_decorators()
        return
    if not self._indent and self._state not in (EMPTY, FUNC) and not o.is_awaitable_coroutine:
        self.add('\n')
    if not self.is_top_level():
        self_inits = find_self_initializers(o)
        for init, value in self_inits:
            if init in self.method_names:
                # Can't have both an attribute and a method/property with the same name.
                continue
            init_code = self.get_init(init, value)
            if init_code:
                self.add(init_code)
    # dump decorators, just before "def ..."
    for s in self._decorators:
        self.add(s)
    self.clear_decorators()
    self.add(f"{self._indent}{'async ' if o.is_coroutine else ''}def {o.name}(")
    self.record_name(o.name)
    args: List[str] = []
    for i, arg_ in enumerate(o.arguments):
        var = arg_.variable
        kind = arg_.kind
        name = var.name
        annotated_type = (o.unanalyzed_type.arg_types[i]
                          if isinstance(o.unanalyzed_type, CallableType) else None)
        # I think the name check is incorrect: there are libraries which
        # name their 0th argument other than self/cls
        is_self_arg = i == 0 and name == 'self'
        is_cls_arg = i == 0 and name == 'cls'
        annotation = ""
        if annotated_type and not is_self_arg and not is_cls_arg:
            # Luckily, an argument explicitly annotated with "Any" has
            # type "UnboundType" and will not match.
            if not isinstance(get_proper_type(annotated_type), AnyType):
                annotation = f": {self.print_annotation(annotated_type)}"

        if kind.is_named() and not any(arg.startswith('*') for arg in args):
            args.append('*')

        if arg_.initializer:
            if not annotation:
                typename = self.get_str_type_of_node(arg_.initializer, True, False)
                if typename == '':
                    annotation = '=...'
                else:
                    annotation = f': {typename} = ...'
            else:
                annotation += ' = ...'
            arg = name + annotation
        elif kind == ARG_STAR:
            arg = f'*{name}{annotation}'
        elif kind == ARG_STAR2:
            arg = f'**{name}{annotation}'
        else:
            arg = name + annotation
        args.append(arg)
    retname = None
    if o.name != '__init__' and isinstance(o.unanalyzed_type, CallableType):
        if isinstance(get_proper_type(o.unanalyzed_type.ret_type), AnyType):
            # Luckily, a return type explicitly annotated with "Any" has
            # type "UnboundType" and will enter the else branch.
            retname = None  # implicit Any
        else:
            retname = self.print_annotation(o.unanalyzed_type.ret_type)
    elif isinstance(o, FuncDef) and (o.is_abstract or o.name in METHODS_WITH_RETURN_VALUE):
        # Always assume abstract methods return Any unless explicitly annotated. Also
        # some dunder methods should not have a None return type.
        retname = None  # implicit Any
    elif has_yield_expression(o):
        self.add_abc_import('Generator')
        yield_name = 'None'
        send_name = 'None'
        return_name = 'None'
        for expr, in_assignment in all_yield_expressions(o):
            if expr.expr is not None and not self.is_none_expr(expr.expr):
                self.add_typing_import('Incomplete')
                yield_name = 'Incomplete'
            if in_assignment:
                self.add_typing_import('Incomplete')
                send_name = 'Incomplete'
        if has_return_statement(o):
            self.add_typing_import('Incomplete')
            return_name = 'Incomplete'
        generator_name = self.typing_name('Generator')
        retname = f'{generator_name}[{yield_name}, {send_name}, {return_name}]'
    elif not has_return_statement(o) and not is_abstract:
        retname = 'None'
    retfield = ''
    if retname is not None:
        retfield = ' -&gt; ' + retname

    self.add(', '.join(args))
    self.add(f"){retfield}: ...\n")
    self._state = FUNC

</t>
<t tx="ekr.20220525082935.666">def is_none_expr(self, expr: Expression) -&gt; bool:
    return isinstance(expr, NameExpr) and expr.name == "None"

</t>
<t tx="ekr.20220525082935.667">def visit_decorator(self, o: Decorator) -&gt; None:
    if self.is_private_name(o.func.name, o.func.fullname):
        return

    is_abstract, _ = self.process_decorator(o)
    self.visit_func_def(o.func, is_abstract=is_abstract)

</t>
<t tx="ekr.20220525082935.668">def process_decorator(self, o: Decorator) -&gt; Tuple[bool, bool]:
    """Process a series of decorators.

    Only preserve certain special decorators such as @abstractmethod.

    Return a pair of booleans:
    - True if any of the decorators makes a method abstract.
    - True if any of the decorators is typing.overload.
    """
    is_abstract = False
    is_overload = False
    for decorator in o.original_decorators:
        if isinstance(decorator, NameExpr):
            i_is_abstract, i_is_overload = self.process_name_expr_decorator(decorator, o)
            is_abstract = is_abstract or i_is_abstract
            is_overload = is_overload or i_is_overload
        elif isinstance(decorator, MemberExpr):
            i_is_abstract, i_is_overload = self.process_member_expr_decorator(decorator, o)
            is_abstract = is_abstract or i_is_abstract
            is_overload = is_overload or i_is_overload
    return is_abstract, is_overload

</t>
<t tx="ekr.20220525082935.669">def process_name_expr_decorator(self, expr: NameExpr, context: Decorator) -&gt; Tuple[bool, bool]:
    """Process a function decorator of form @foo.

    Only preserve certain special decorators such as @abstractmethod.

    Return a pair of booleans:
    - True if the decorator makes a method abstract.
    - True if the decorator is typing.overload.
    """
    is_abstract = False
    is_overload = False
    name = expr.name
    if name in ('property', 'staticmethod', 'classmethod'):
        self.add_decorator(name)
    elif self.import_tracker.module_for.get(name) in ('asyncio',
                                                      'asyncio.coroutines',
                                                      'types'):
        self.add_coroutine_decorator(context.func, name, name)
    elif self.refers_to_fullname(name, 'abc.abstractmethod'):
        self.add_decorator(name)
        self.import_tracker.require_name(name)
        is_abstract = True
    elif self.refers_to_fullname(name, 'abc.abstractproperty'):
        self.add_decorator('property')
        self.add_decorator('abc.abstractmethod')
        is_abstract = True
    elif self.refers_to_fullname(name, OVERLOAD_NAMES):
        self.add_decorator(name)
        self.add_typing_import('overload')
        is_overload = True
    return is_abstract, is_overload

</t>
<t tx="ekr.20220525082935.67">def analyze_metaclass(self, defn: ClassDef) -&gt; None:
    if defn.metaclass:
        metaclass_name = None
        if isinstance(defn.metaclass, NameExpr):
            metaclass_name = defn.metaclass.name
        elif isinstance(defn.metaclass, MemberExpr):
            metaclass_name = get_member_expr_fullname(defn.metaclass)
        if metaclass_name is None:
            self.fail(f'Dynamic metaclass not supported for "{defn.name}"', defn.metaclass)
            return
        sym = self.lookup_qualified(metaclass_name, defn.metaclass)
        if sym is None:
            # Probably a name error - it is already handled elsewhere
            return
        if isinstance(sym.node, Var) and isinstance(get_proper_type(sym.node.type), AnyType):
            # 'Any' metaclass -- just ignore it.
            #
            # TODO: A better approach would be to record this information
            #       and assume that the type object supports arbitrary
            #       attributes, similar to an 'Any' base class.
            return
        if isinstance(sym.node, PlaceholderNode):
            self.defer(defn)
            return
        if not isinstance(sym.node, TypeInfo) or sym.node.tuple_type is not None:
            self.fail(f'Invalid metaclass "{metaclass_name}"', defn.metaclass)
            return
        if not sym.node.is_metaclass():
            self.fail('Metaclasses not inheriting from "type" are not supported',
                      defn.metaclass)
            return
        inst = fill_typevars(sym.node)
        assert isinstance(inst, Instance)
        defn.info.declared_metaclass = inst
    defn.info.metaclass_type = defn.info.calculate_metaclass_type()
    if any(info.is_protocol for info in defn.info.mro):
        if (not defn.info.metaclass_type or
                defn.info.metaclass_type.type.fullname == 'builtins.type'):
            # All protocols and their subclasses have ABCMeta metaclass by default.
            # TODO: add a metaclass conflict check if there is another metaclass.
            abc_meta = self.named_type_or_none('abc.ABCMeta', [])
            if abc_meta is not None:  # May be None in tests with incomplete lib-stub.
                defn.info.metaclass_type = abc_meta
    if defn.info.metaclass_type is None:
        # Inconsistency may happen due to multiple baseclasses even in classes that
        # do not declare explicit metaclass, but it's harder to catch at this stage
        if defn.metaclass is not None:
            self.fail(f'Inconsistent metaclass structure for "{defn.name}"', defn)
    else:
        if defn.info.metaclass_type.type.has_base('enum.EnumMeta'):
            defn.info.is_enum = True
            if defn.type_vars:
                self.fail("Enum class cannot be generic", defn)

</t>
<t tx="ekr.20220525082935.670">def refers_to_fullname(self, name: str, fullname: Union[str, Tuple[str, ...]]) -&gt; bool:
    if isinstance(fullname, tuple):
        return any(self.refers_to_fullname(name, fname) for fname in fullname)
    module, short = fullname.rsplit('.', 1)
    return (self.import_tracker.module_for.get(name) == module and
            (name == short or
             self.import_tracker.reverse_alias.get(name) == short))

</t>
<t tx="ekr.20220525082935.671">def process_member_expr_decorator(self, expr: MemberExpr, context: Decorator) -&gt; Tuple[bool,
                                                                                       bool]:
    """Process a function decorator of form @foo.bar.

    Only preserve certain special decorators such as @abstractmethod.

    Return a pair of booleans:
    - True if the decorator makes a method abstract.
    - True if the decorator is typing.overload.
    """
    is_abstract = False
    is_overload = False
    if expr.name == 'setter' and isinstance(expr.expr, NameExpr):
        self.add_decorator(f'{expr.expr.name}.setter')
    elif (isinstance(expr.expr, NameExpr) and
          (expr.expr.name == 'abc' or
           self.import_tracker.reverse_alias.get(expr.expr.name) == 'abc') and
          expr.name in ('abstractmethod', 'abstractproperty')):
        if expr.name == 'abstractproperty':
            self.import_tracker.require_name(expr.expr.name)
            self.add_decorator('%s' % ('property'))
            self.add_decorator('{}.{}'.format(expr.expr.name, 'abstractmethod'))
        else:
            self.import_tracker.require_name(expr.expr.name)
            self.add_decorator(f'{expr.expr.name}.{expr.name}')
        is_abstract = True
    elif expr.name == 'coroutine':
        if (isinstance(expr.expr, MemberExpr) and
            expr.expr.name == 'coroutines' and
            isinstance(expr.expr.expr, NameExpr) and
                (expr.expr.expr.name == 'asyncio' or
                 self.import_tracker.reverse_alias.get(expr.expr.expr.name) ==
                    'asyncio')):
            self.add_coroutine_decorator(context.func,
                                         '%s.coroutines.coroutine' %
                                         (expr.expr.expr.name,),
                                         expr.expr.expr.name)
        elif (isinstance(expr.expr, NameExpr) and
              (expr.expr.name in ('asyncio', 'types') or
               self.import_tracker.reverse_alias.get(expr.expr.name) in
                ('asyncio', 'asyncio.coroutines', 'types'))):
            self.add_coroutine_decorator(context.func,
                                         expr.expr.name + '.coroutine',
                                         expr.expr.name)
    elif (isinstance(expr.expr, NameExpr) and
          (expr.expr.name in TYPING_MODULE_NAMES or
           self.import_tracker.reverse_alias.get(expr.expr.name) in TYPING_MODULE_NAMES) and
          expr.name == 'overload'):
        self.import_tracker.require_name(expr.expr.name)
        self.add_decorator(f"{expr.expr.name}.overload")
        is_overload = True
    return is_abstract, is_overload

</t>
<t tx="ekr.20220525082935.672">def visit_class_def(self, o: ClassDef) -&gt; None:
    self.method_names = find_method_names(o.defs.body)
    sep: Optional[int] = None
    if not self._indent and self._state != EMPTY:
        sep = len(self._output)
        self.add('\n')
    self.add(f'{self._indent}class {o.name}')
    self.record_name(o.name)
    base_types = self.get_base_types(o)
    if base_types:
        for base in base_types:
            self.import_tracker.require_name(base)
    if isinstance(o.metaclass, (NameExpr, MemberExpr)):
        meta = o.metaclass.accept(AliasPrinter(self))
        base_types.append('metaclass=' + meta)
    elif self.analyzed and o.info.is_abstract:
        base_types.append('metaclass=abc.ABCMeta')
        self.import_tracker.add_import('abc')
        self.import_tracker.require_name('abc')
    elif self.analyzed and o.info.is_protocol:
        type_str = 'Protocol'
        if o.info.type_vars:
            type_str += f'[{", ".join(o.info.type_vars)}]'
        base_types.append(type_str)
        self.add_typing_import('Protocol')
    if base_types:
        self.add(f"({', '.join(base_types)})")
    self.add(':\n')
    n = len(self._output)
    self._indent += '    '
    self._vars.append([])
    super().visit_class_def(o)
    self._indent = self._indent[:-4]
    self._vars.pop()
    self._vars[-1].append(o.name)
    if len(self._output) == n:
        if self._state == EMPTY_CLASS and sep is not None:
            self._output[sep] = ''
        self._output[-1] = self._output[-1][:-1] + ' ...\n'
        self._state = EMPTY_CLASS
    else:
        self._state = CLASS
    self.method_names = set()

</t>
<t tx="ekr.20220525082935.673">def get_base_types(self, cdef: ClassDef) -&gt; List[str]:
    """Get list of base classes for a class."""
    base_types: List[str] = []
    for base in cdef.base_type_exprs:
        if isinstance(base, NameExpr):
            if base.name != 'object':
                base_types.append(base.name)
        elif isinstance(base, MemberExpr):
            modname = get_qualified_name(base.expr)
            base_types.append(f'{modname}.{base.name}')
        elif isinstance(base, IndexExpr):
            p = AliasPrinter(self)
            base_types.append(base.accept(p))
    return base_types

</t>
<t tx="ekr.20220525082935.674">def visit_block(self, o: Block) -&gt; None:
    # Unreachable statements may be partially uninitialized and that may
    # cause trouble.
    if not o.is_unreachable:
        super().visit_block(o)

</t>
<t tx="ekr.20220525082935.675">def visit_assignment_stmt(self, o: AssignmentStmt) -&gt; None:
    foundl = []

    for lvalue in o.lvalues:
        if isinstance(lvalue, NameExpr) and self.is_namedtuple(o.rvalue):
            assert isinstance(o.rvalue, CallExpr)
            self.process_namedtuple(lvalue, o.rvalue)
            continue
        if (self.is_top_level() and
                isinstance(lvalue, NameExpr) and not self.is_private_name(lvalue.name) and
                # it is never an alias with explicit annotation
                not o.unanalyzed_type and self.is_alias_expression(o.rvalue)):
            self.process_typealias(lvalue, o.rvalue)
            continue
        if isinstance(lvalue, TupleExpr) or isinstance(lvalue, ListExpr):
            items = lvalue.items
            if isinstance(o.unanalyzed_type, TupleType):  # type: ignore
                annotations: Iterable[Optional[Type]] = o.unanalyzed_type.items
            else:
                annotations = [None] * len(items)
        else:
            items = [lvalue]
            annotations = [o.unanalyzed_type]
        sep = False
        found = False
        for item, annotation in zip(items, annotations):
            if isinstance(item, NameExpr):
                init = self.get_init(item.name, o.rvalue, annotation)
                if init:
                    found = True
                    if not sep and not self._indent and \
                            self._state not in (EMPTY, VAR):
                        init = '\n' + init
                        sep = True
                    self.add(init)
                    self.record_name(item.name)
        foundl.append(found)

    if all(foundl):
        self._state = VAR

</t>
<t tx="ekr.20220525082935.676">def is_namedtuple(self, expr: Expression) -&gt; bool:
    if not isinstance(expr, CallExpr):
        return False
    callee = expr.callee
    return ((isinstance(callee, NameExpr) and callee.name.endswith('namedtuple')) or
            (isinstance(callee, MemberExpr) and callee.name == 'namedtuple'))

</t>
<t tx="ekr.20220525082935.677">def process_namedtuple(self, lvalue: NameExpr, rvalue: CallExpr) -&gt; None:
    if self._state != EMPTY:
        self.add('\n')
    if isinstance(rvalue.args[1], StrExpr):
        items = rvalue.args[1].value.replace(',', ' ').split()
    elif isinstance(rvalue.args[1], (ListExpr, TupleExpr)):
        list_items = cast(List[StrExpr], rvalue.args[1].items)
        items = [item.value for item in list_items]
    else:
        self.add(f'{self._indent}{lvalue.name}: Incomplete')
        self.import_tracker.require_name('Incomplete')
        return
    self.import_tracker.require_name('NamedTuple')
    self.add(f'{self._indent}class {lvalue.name}(NamedTuple):')
    if len(items) == 0:
        self.add(' ...\n')
    else:
        self.import_tracker.require_name('Incomplete')
        self.add('\n')
        for item in items:
            self.add(f'{self._indent}    {item}: Incomplete\n')
    self._state = CLASS

</t>
<t tx="ekr.20220525082935.678">def is_alias_expression(self, expr: Expression, top_level: bool = True) -&gt; bool:
    """Return True for things that look like target for an alias.

    Used to know if assignments look like type aliases, function alias,
    or module alias.
    """
    # Assignment of TypeVar(...) are passed through
    if (isinstance(expr, CallExpr) and
            isinstance(expr.callee, NameExpr) and
            expr.callee.name == 'TypeVar'):
        return True
    elif isinstance(expr, EllipsisExpr):
        return not top_level
    elif isinstance(expr, NameExpr):
        if expr.name in ('True', 'False'):
            return False
        elif expr.name == 'None':
            return not top_level
        else:
            return not self.is_private_name(expr.name)
    elif isinstance(expr, MemberExpr) and self.analyzed:
        # Also add function and module aliases.
        return ((top_level and isinstance(expr.node, (FuncDef, Decorator, MypyFile))
                 or isinstance(expr.node, TypeInfo)) and
                not self.is_private_member(expr.node.fullname))
    elif (isinstance(expr, IndexExpr) and isinstance(expr.base, NameExpr) and
          not self.is_private_name(expr.base.name)):
        if isinstance(expr.index, TupleExpr):
            indices = expr.index.items
        else:
            indices = [expr.index]
        if expr.base.name == 'Callable' and len(indices) == 2:
            args, ret = indices
            if isinstance(args, EllipsisExpr):
                indices = [ret]
            elif isinstance(args, ListExpr):
                indices = args.items + [ret]
            else:
                return False
        return all(self.is_alias_expression(i, top_level=False) for i in indices)
    else:
        return False

</t>
<t tx="ekr.20220525082935.679">def process_typealias(self, lvalue: NameExpr, rvalue: Expression) -&gt; None:
    p = AliasPrinter(self)
    self.add(f"{lvalue.name} = {rvalue.accept(p)}\n")
    self.record_name(lvalue.name)
    self._vars[-1].append(lvalue.name)

</t>
<t tx="ekr.20220525082935.68">#
# Imports
#

</t>
<t tx="ekr.20220525082935.680">def visit_if_stmt(self, o: IfStmt) -&gt; None:
    # Ignore if __name__ == '__main__'.
    expr = o.expr[0]
    if (isinstance(expr, ComparisonExpr) and
            isinstance(expr.operands[0], NameExpr) and
            isinstance(expr.operands[1], StrExpr) and
            expr.operands[0].name == '__name__' and
            '__main__' in expr.operands[1].value):
        return
    super().visit_if_stmt(o)

</t>
<t tx="ekr.20220525082935.681">def visit_import_all(self, o: ImportAll) -&gt; None:
    self.add_import_line(f"from {'.' * o.relative}{o.id} import *\n")

</t>
<t tx="ekr.20220525082935.682">def visit_import_from(self, o: ImportFrom) -&gt; None:
    exported_names: Set[str] = set()
    import_names = []
    module, relative = translate_module_name(o.id, o.relative)
    if self.module:
        full_module, ok = mypy.util.correct_relative_import(
            self.module, relative, module, self.path.endswith('.__init__.py')
        )
        if not ok:
            full_module = module
    else:
        full_module = module
    if module == '__future__':
        return  # Not preserved
    for name, as_name in o.names:
        if name == 'six':
            # Vendored six -- translate into plain 'import six'.
            self.visit_import(Import([('six', None)]))
            continue
        exported = False
        if as_name is None and self.module and (self.module + '.' + name) in EXTRA_EXPORTED:
            # Special case certain names that should be exported, against our general rules.
            exported = True
        is_private = self.is_private_name(name, full_module + '.' + name)
        if (as_name is None
                and name not in self.referenced_names
                and (not self._all_ or name in IGNORED_DUNDERS)
                and not is_private
                and module not in ('abc', 'asyncio') + TYPING_MODULE_NAMES):
            # An imported name that is never referenced in the module is assumed to be
            # exported, unless there is an explicit __all__. Note that we need to special
            # case 'abc' since some references are deleted during semantic analysis.
            exported = True
        top_level = full_module.split('.')[0]
        if (as_name is None
                and not self.export_less
                and (not self._all_ or name in IGNORED_DUNDERS)
                and self.module
                and not is_private
                and top_level in (self.module.split('.')[0],
                                  '_' + self.module.split('.')[0])):
            # Export imports from the same package, since we can't reliably tell whether they
            # are part of the public API.
            exported = True
        if exported:
            self.import_tracker.reexport(name)
            as_name = name
        import_names.append((name, as_name))
    self.import_tracker.add_import_from('.' * relative + module, import_names)
    self._vars[-1].extend(alias or name for name, alias in import_names)
    for name, alias in import_names:
        self.record_name(alias or name)

    if self._all_:
        # Include "import from"s that import names defined in __all__.
        names = [name for name, alias in o.names
                 if name in self._all_ and alias is None and name not in IGNORED_DUNDERS]
        exported_names.update(names)

</t>
<t tx="ekr.20220525082935.683">def visit_import(self, o: Import) -&gt; None:
    for id, as_id in o.ids:
        self.import_tracker.add_import(id, as_id)
        if as_id is None:
            target_name = id.split('.')[0]
        else:
            target_name = as_id
        self._vars[-1].append(target_name)
        self.record_name(target_name)

</t>
<t tx="ekr.20220525082935.684">def get_init(self, lvalue: str, rvalue: Expression,
             annotation: Optional[Type] = None) -&gt; Optional[str]:
    """Return initializer for a variable.

    Return None if we've generated one already or if the variable is internal.
    """
    if lvalue in self._vars[-1]:
        # We've generated an initializer already for this variable.
        return None
    # TODO: Only do this at module top level.
    if self.is_private_name(lvalue) or self.is_not_in_all(lvalue):
        return None
    self._vars[-1].append(lvalue)
    if annotation is not None:
        typename = self.print_annotation(annotation)
        if (isinstance(annotation, UnboundType) and not annotation.args and
                annotation.name == 'Final' and
                self.import_tracker.module_for.get('Final') in TYPING_MODULE_NAMES):
            # Final without type argument is invalid in stubs.
            final_arg = self.get_str_type_of_node(rvalue)
            typename += f'[{final_arg}]'
    else:
        typename = self.get_str_type_of_node(rvalue)
    return f'{self._indent}{lvalue}: {typename}\n'

</t>
<t tx="ekr.20220525082935.685">def add(self, string: str) -&gt; None:
    """Add text to generated stub."""
    self._output.append(string)

</t>
<t tx="ekr.20220525082935.686">def add_decorator(self, name: str) -&gt; None:
    if not self._indent and self._state not in (EMPTY, FUNC):
        self._decorators.append('\n')
    self._decorators.append(f'{self._indent}@{name}\n')

</t>
<t tx="ekr.20220525082935.687">def clear_decorators(self) -&gt; None:
    self._decorators.clear()

</t>
<t tx="ekr.20220525082935.688">def typing_name(self, name: str) -&gt; str:
    if name in self.defined_names:
        # Avoid name clash between name from typing and a name defined in stub.
        return '_' + name
    else:
        return name

</t>
<t tx="ekr.20220525082935.689">def add_typing_import(self, name: str) -&gt; None:
    """Add a name to be imported from typing, unless it's imported already.

    The import will be internal to the stub.
    """
    name = self.typing_name(name)
    self.import_tracker.require_name(name)

</t>
<t tx="ekr.20220525082935.69">def visit_import(self, i: Import) -&gt; None:
    self.statement = i
    for id, as_id in i.ids:
        # Modules imported in a stub file without using 'import X as X' won't get exported
        # When implicit re-exporting is disabled, we have the same behavior as stubs.
        use_implicit_reexport = not self.is_stub_file and self.options.implicit_reexport
        if as_id is not None:
            base_id = id
            imported_id = as_id
            module_public = use_implicit_reexport or id.split(".")[-1] == as_id
        else:
            base_id = id.split('.')[0]
            imported_id = base_id
            module_public = use_implicit_reexport
        self.add_module_symbol(base_id, imported_id, context=i, module_public=module_public,
                               module_hidden=not module_public)

</t>
<t tx="ekr.20220525082935.690">def add_abc_import(self, name: str) -&gt; None:
    """Add a name to be imported from collections.abc, unless it's imported already.

    The import will be internal to the stub.
    """
    name = self.typing_name(name)
    self.import_tracker.require_name(name)

</t>
<t tx="ekr.20220525082935.691">def add_import_line(self, line: str) -&gt; None:
    """Add a line of text to the import section, unless it's already there."""
    if line not in self._import_lines:
        self._import_lines.append(line)

</t>
<t tx="ekr.20220525082935.692">def add_coroutine_decorator(self, func: FuncDef, name: str, require_name: str) -&gt; None:
    func.is_awaitable_coroutine = True
    self.add_decorator(name)
    self.import_tracker.require_name(require_name)

</t>
<t tx="ekr.20220525082935.693">def output(self) -&gt; str:
    """Return the text for the stub."""
    imports = ''
    if self._import_lines:
        imports += ''.join(self._import_lines)
    imports += ''.join(self.import_tracker.import_lines())
    if imports and self._output:
        imports += '\n'
    return imports + ''.join(self._output)

</t>
<t tx="ekr.20220525082935.694">def is_not_in_all(self, name: str) -&gt; bool:
    if self.is_private_name(name):
        return False
    if self._all_:
        return self.is_top_level() and name not in self._all_
    return False

</t>
<t tx="ekr.20220525082935.695">def is_private_name(self, name: str, fullname: Optional[str] = None) -&gt; bool:
    if self._include_private:
        return False
    if fullname in EXTRA_EXPORTED:
        return False
    return name.startswith('_') and (not name.endswith('__')
                                     or name in IGNORED_DUNDERS)

</t>
<t tx="ekr.20220525082935.696">def is_private_member(self, fullname: str) -&gt; bool:
    parts = fullname.split('.')
    for part in parts:
        if self.is_private_name(part):
            return True
    return False

</t>
<t tx="ekr.20220525082935.697">def get_str_type_of_node(self, rvalue: Expression,
                         can_infer_optional: bool = False,
                         can_be_any: bool = True) -&gt; str:
    if isinstance(rvalue, IntExpr):
        return 'int'
    if isinstance(rvalue, StrExpr):
        return 'str'
    if isinstance(rvalue, BytesExpr):
        return 'bytes'
    if isinstance(rvalue, FloatExpr):
        return 'float'
    if isinstance(rvalue, UnaryExpr) and isinstance(rvalue.expr, IntExpr):
        return 'int'
    if isinstance(rvalue, NameExpr) and rvalue.name in ('True', 'False'):
        return 'bool'
    if can_infer_optional and \
            isinstance(rvalue, NameExpr) and rvalue.name == 'None':
        self.add_typing_import('Incomplete')
        return f"{self.typing_name('Incomplete')} | None"
    if can_be_any:
        self.add_typing_import('Incomplete')
        return self.typing_name('Incomplete')
    else:
        return ''

</t>
<t tx="ekr.20220525082935.698">def print_annotation(self, t: Type) -&gt; str:
    printer = AnnotationPrinter(self)
    return t.accept(printer)

</t>
<t tx="ekr.20220525082935.699">def is_top_level(self) -&gt; bool:
    """Are we processing the top level of a file?"""
    return self._indent == ''

</t>
<t tx="ekr.20220525082935.7">def prepare_file(self, file_node: MypyFile) -&gt; None:
    """Prepare a freshly parsed file for semantic analysis."""
    if 'builtins' in self.modules:
        file_node.names['__builtins__'] = SymbolTableNode(GDEF,
                                                          self.modules['builtins'])
    if file_node.fullname == 'builtins':
        self.prepare_builtins_namespace(file_node)
    if file_node.fullname == 'typing':
        self.prepare_typing_namespace(file_node, type_aliases)
    if file_node.fullname == 'typing_extensions':
        self.prepare_typing_namespace(file_node, typing_extensions_aliases)

</t>
<t tx="ekr.20220525082935.70">def visit_import_from(self, imp: ImportFrom) -&gt; None:
    self.statement = imp
    module_id = self.correct_relative_import(imp)
    module = self.modules.get(module_id)
    for id, as_id in imp.names:
        fullname = module_id + '.' + id
        self.set_future_import_flags(fullname)
        if module is None:
            node = None
        elif module_id == self.cur_mod_id and fullname in self.modules:
            # Submodule takes precedence over definition in surround package, for
            # compatibility with runtime semantics in typical use cases. This
            # could more precisely model runtime semantics by taking into account
            # the line number beyond which the local definition should take
            # precedence, but doesn't seem to be important in most use cases.
            node = SymbolTableNode(GDEF, self.modules[fullname])
        else:
            if id == as_id == '__all__' and module_id in self.export_map:
                self.all_exports[:] = self.export_map[module_id]
            node = module.names.get(id)

        missing_submodule = False
        imported_id = as_id or id

        # Modules imported in a stub file without using 'from Y import X as X' will
        # not get exported.
        # When implicit re-exporting is disabled, we have the same behavior as stubs.
        use_implicit_reexport = not self.is_stub_file and self.options.implicit_reexport
        module_public = use_implicit_reexport or (as_id is not None and id == as_id)

        # If the module does not contain a symbol with the name 'id',
        # try checking if it's a module instead.
        if not node:
            mod = self.modules.get(fullname)
            if mod is not None:
                kind = self.current_symbol_kind()
                node = SymbolTableNode(kind, mod)
            elif fullname in self.missing_modules:
                missing_submodule = True
        # If it is still not resolved, check for a module level __getattr__
        if (module and not node and (module.is_stub or self.options.python_version &gt;= (3, 7))
                and '__getattr__' in module.names):
            # We store the fullname of the original definition so that we can
            # detect whether two imported names refer to the same thing.
            fullname = module_id + '.' + id
            gvar = self.create_getattr_var(module.names['__getattr__'], imported_id, fullname)
            if gvar:
                self.add_symbol(
                    imported_id, gvar, imp, module_public=module_public,
                    module_hidden=not module_public
                )
                continue

        if node and not node.module_hidden:
            self.process_imported_symbol(
                node, module_id, id, imported_id, fullname, module_public, context=imp
            )
        elif module and not missing_submodule:
            # Target module exists but the imported name is missing or hidden.
            self.report_missing_module_attribute(
                module_id, id, imported_id, module_public=module_public,
                module_hidden=not module_public, context=imp
            )
        else:
            # Import of a missing (sub)module.
            self.add_unknown_imported_symbol(
                imported_id, imp, target_name=fullname, module_public=module_public,
                module_hidden=not module_public
            )

</t>
<t tx="ekr.20220525082935.700">def record_name(self, name: str) -&gt; None:
    """Mark a name as defined.

    This only does anything if at the top level of a module.
    """
    if self.is_top_level():
        self._toplevel_names.append(name)

</t>
<t tx="ekr.20220525082935.701">def is_recorded_name(self, name: str) -&gt; bool:
    """Has this name been recorded previously?"""
    return self.is_top_level() and name in self._toplevel_names


</t>
<t tx="ekr.20220525082935.702">def find_method_names(defs: List[Statement]) -&gt; Set[str]:
    # TODO: Traverse into nested definitions
    result = set()
    for defn in defs:
        if isinstance(defn, FuncDef):
            result.add(defn.name)
        elif isinstance(defn, Decorator):
            result.add(defn.func.name)
        elif isinstance(defn, OverloadedFuncDef):
            for item in defn.items:
                result.update(find_method_names([item]))
    return result


</t>
<t tx="ekr.20220525082935.703">class SelfTraverser(mypy.traverser.TraverserVisitor):
    @others
</t>
<t tx="ekr.20220525082935.704">def __init__(self) -&gt; None:
    self.results: List[Tuple[str, Expression]] = []

</t>
<t tx="ekr.20220525082935.705">def visit_assignment_stmt(self, o: AssignmentStmt) -&gt; None:
    lvalue = o.lvalues[0]
    if (isinstance(lvalue, MemberExpr) and
            isinstance(lvalue.expr, NameExpr) and
            lvalue.expr.name == 'self'):
        self.results.append((lvalue.name, o.rvalue))


</t>
<t tx="ekr.20220525082935.706">def find_self_initializers(fdef: FuncBase) -&gt; List[Tuple[str, Expression]]:
    """Find attribute initializers in a method.

    Return a list of pairs (attribute name, r.h.s. expression).
    """
    traverser = SelfTraverser()
    fdef.accept(traverser)
    return traverser.results


</t>
<t tx="ekr.20220525082935.707">def get_qualified_name(o: Expression) -&gt; str:
    if isinstance(o, NameExpr):
        return o.name
    elif isinstance(o, MemberExpr):
        return f'{get_qualified_name(o.expr)}.{o.name}'
    else:
        return ERROR_MARKER


</t>
<t tx="ekr.20220525082935.708">def remove_blacklisted_modules(modules: List[StubSource]) -&gt; List[StubSource]:
    return [module for module in modules
            if module.path is None or not is_blacklisted_path(module.path)]


</t>
<t tx="ekr.20220525082935.709">def is_blacklisted_path(path: str) -&gt; bool:
    return any(substr in (normalize_path_separators(path) + '\n')
               for substr in BLACKLIST)


</t>
<t tx="ekr.20220525082935.71">def process_imported_symbol(self,
                            node: SymbolTableNode,
                            module_id: str,
                            id: str,
                            imported_id: str,
                            fullname: str,
                            module_public: bool,
                            context: ImportBase) -&gt; None:
    module_hidden = not module_public and not (
        # `from package import module` should work regardless of whether package
        # re-exports module
        isinstance(node.node, MypyFile) and fullname in self.modules
    )

    if isinstance(node.node, PlaceholderNode):
        if self.final_iteration:
            self.report_missing_module_attribute(
                module_id, id, imported_id, module_public=module_public,
                module_hidden=module_hidden, context=context
            )
            return
        else:
            # This might become a type.
            self.mark_incomplete(imported_id, node.node,
                                 module_public=module_public,
                                 module_hidden=module_hidden,
                                 becomes_typeinfo=True)
    existing_symbol = self.globals.get(imported_id)
    if (existing_symbol and not isinstance(existing_symbol.node, PlaceholderNode) and
            not isinstance(node.node, PlaceholderNode)):
        # Import can redefine a variable. They get special treatment.
        if self.process_import_over_existing_name(
                imported_id, existing_symbol, node, context):
            return
    if existing_symbol and isinstance(node.node, PlaceholderNode):
        # Imports are special, some redefinitions are allowed, so wait until
        # we know what is the new symbol node.
        return
    # NOTE: we take the original node even for final `Var`s. This is to support
    # a common pattern when constants are re-exported (same applies to import *).
    self.add_imported_symbol(imported_id, node, context,
                             module_public=module_public,
                             module_hidden=module_hidden)

</t>
<t tx="ekr.20220525082935.710">def normalize_path_separators(path: str) -&gt; str:
    if sys.platform == 'win32':
        return path.replace('\\', '/')
    return path


</t>
<t tx="ekr.20220525082935.711">def collect_build_targets(options: Options, mypy_opts: MypyOptions) -&gt; Tuple[List[StubSource],
                                                                             List[StubSource]]:
    """Collect files for which we need to generate stubs.

    Return list of Python modules and C modules.
    """
    if options.packages or options.modules:
        if options.no_import:
            py_modules = find_module_paths_using_search(options.modules,
                                                        options.packages,
                                                        options.search_path,
                                                        options.pyversion)
            c_modules: List[StubSource] = []
        else:
            # Using imports is the default, since we can also find C modules.
            py_modules, c_modules = find_module_paths_using_imports(options.modules,
                                                                    options.packages,
                                                                    options.interpreter,
                                                                    options.pyversion,
                                                                    options.verbose,
                                                                    options.quiet)
    else:
        # Use mypy native source collection for files and directories.
        try:
            source_list = create_source_list(options.files, mypy_opts)
        except InvalidSourceList as e:
            raise SystemExit(str(e)) from e
        py_modules = [StubSource(m.module, m.path) for m in source_list]
        c_modules = []

    py_modules = remove_blacklisted_modules(py_modules)

    return py_modules, c_modules


</t>
<t tx="ekr.20220525082935.712">def find_module_paths_using_imports(modules: List[str],
                                    packages: List[str],
                                    interpreter: str,
                                    pyversion: Tuple[int, int],
                                    verbose: bool,
                                    quiet: bool) -&gt; Tuple[List[StubSource],
                                                          List[StubSource]]:
    """Find path and runtime value of __all__ (if possible) for modules and packages.

    This function uses runtime Python imports to get the information.
    """
    with ModuleInspect() as inspect:
        py_modules: List[StubSource] = []
        c_modules: List[StubSource] = []
        found = list(walk_packages(inspect, packages, verbose))
        modules = modules + found
        modules = [mod
                   for mod in modules
                   if not is_non_library_module(mod)]  # We don't want to run any tests or scripts
        for mod in modules:
            try:
                if pyversion[0] == 2:
                    result = find_module_path_and_all_py2(mod, interpreter)
                else:
                    result = find_module_path_and_all_py3(inspect, mod, verbose)
            except CantImport as e:
                tb = traceback.format_exc()
                if verbose:
                    sys.stdout.write(tb)
                if not quiet:
                    report_missing(mod, e.message, tb)
                continue
            if not result:
                c_modules.append(StubSource(mod))
            else:
                path, runtime_all = result
                py_modules.append(StubSource(mod, path, runtime_all))
        return py_modules, c_modules


</t>
<t tx="ekr.20220525082935.713">def is_non_library_module(module: str) -&gt; bool:
    """Does module look like a test module or a script?"""
    if module.endswith((
            '.tests',
            '.test',
            '.testing',
            '_tests',
            '_test_suite',
            'test_util',
            'test_utils',
            'test_base',
            '.__main__',
            '.conftest',  # Used by pytest
            '.setup',  # Typically an install script
    )):
        return True
    if module.split('.')[-1].startswith('test_'):
        return True
    if ('.tests.' in module
            or '.test.' in module
            or '.testing.' in module
            or '.SelfTest.' in module):
        return True
    return False


</t>
<t tx="ekr.20220525082935.714">def translate_module_name(module: str, relative: int) -&gt; Tuple[str, int]:
    for pkg in VENDOR_PACKAGES:
        for alt in 'six.moves', 'six':
            substr = f'{pkg}.{alt}'
            if (module.endswith('.' + substr)
                    or (module == substr and relative)):
                return alt, 0
            if '.' + substr + '.' in module:
                return alt + '.' + module.partition('.' + substr + '.')[2], 0
    return module, relative


</t>
<t tx="ekr.20220525082935.715">def find_module_paths_using_search(modules: List[str], packages: List[str],
                                   search_path: List[str],
                                   pyversion: Tuple[int, int]) -&gt; List[StubSource]:
    """Find sources for modules and packages requested.

    This function just looks for source files at the file system level.
    This is used if user passes --no-import, and will not find C modules.
    Exit if some of the modules or packages can't be found.
    """
    result: List[StubSource] = []
    typeshed_path = default_lib_path(mypy.build.default_data_dir(), pyversion, None)
    search_paths = SearchPaths(('.',) + tuple(search_path), (), (), tuple(typeshed_path))
    cache = FindModuleCache(search_paths, fscache=None, options=None)
    for module in modules:
        m_result = cache.find_module(module)
        if isinstance(m_result, ModuleNotFoundReason):
            fail_missing(module, m_result)
            module_path = None
        else:
            module_path = m_result
        result.append(StubSource(module, module_path))
    for package in packages:
        p_result = cache.find_modules_recursive(package)
        if p_result:
            fail_missing(package, ModuleNotFoundReason.NOT_FOUND)
        sources = [StubSource(m.module, m.path) for m in p_result]
        result.extend(sources)

    result = [m for m in result if not is_non_library_module(m.module)]

    return result


</t>
<t tx="ekr.20220525082935.716">def mypy_options(stubgen_options: Options) -&gt; MypyOptions:
    """Generate mypy options using the flag passed by user."""
    options = MypyOptions()
    options.follow_imports = 'skip'
    options.incremental = False
    options.ignore_errors = True
    options.semantic_analysis_only = True
    options.python_version = stubgen_options.pyversion
    options.show_traceback = True
    options.transform_source = remove_misplaced_type_comments
    return options


</t>
<t tx="ekr.20220525082935.717">def parse_source_file(mod: StubSource, mypy_options: MypyOptions) -&gt; None:
    """Parse a source file.

    On success, store AST in the corresponding attribute of the stub source.
    If there are syntax errors, print them and exit.
    """
    assert mod.path is not None, "Not found module was not skipped"
    with open(mod.path, 'rb') as f:
        data = f.read()
    source = mypy.util.decode_python_encoding(data, mypy_options.python_version)
    errors = Errors()
    mod.ast = mypy.parse.parse(source, fnam=mod.path, module=mod.module,
                               errors=errors, options=mypy_options)
    mod.ast._fullname = mod.module
    if errors.is_blockers():
        # Syntax error!
        for m in errors.new_messages():
            sys.stderr.write(f'{m}\n')
        sys.exit(1)


</t>
<t tx="ekr.20220525082935.718">def generate_asts_for_modules(py_modules: List[StubSource],
                              parse_only: bool,
                              mypy_options: MypyOptions,
                              verbose: bool) -&gt; None:
    """Use mypy to parse (and optionally analyze) source files."""
    if not py_modules:
        return  # Nothing to do here, but there may be C modules
    if verbose:
        print(f'Processing {len(py_modules)} files...')
    if parse_only:
        for mod in py_modules:
            parse_source_file(mod, mypy_options)
        return
    # Perform full semantic analysis of the source set.
    try:
        res = build([module.source for module in py_modules], mypy_options)
    except CompileError as e:
        raise SystemExit(f"Critical error during semantic analysis: {e}") from e

    for mod in py_modules:
        mod.ast = res.graph[mod.module].tree
        # Use statically inferred __all__ if there is no runtime one.
        if mod.runtime_all is None:
            mod.runtime_all = res.manager.semantic_analyzer.export_map[mod.module]


</t>
<t tx="ekr.20220525082935.719">def generate_stub_from_ast(mod: StubSource,
                           target: str,
                           parse_only: bool = False,
                           pyversion: Tuple[int, int] = defaults.PYTHON3_VERSION,
                           include_private: bool = False,
                           export_less: bool = False) -&gt; None:
    """Use analysed (or just parsed) AST to generate type stub for single file.

    If directory for target doesn't exist it will created. Existing stub
    will be overwritten.
    """
    gen = StubGenerator(mod.runtime_all,
                        pyversion=pyversion,
                        include_private=include_private,
                        analyzed=not parse_only,
                        export_less=export_less)
    assert mod.ast is not None, "This function must be used only with analyzed modules"
    mod.ast.accept(gen)

    # Write output to file.
    subdir = os.path.dirname(target)
    if subdir and not os.path.isdir(subdir):
        os.makedirs(subdir)
    with open(target, 'w') as file:
        file.write(''.join(gen.output()))


</t>
<t tx="ekr.20220525082935.72">def report_missing_module_attribute(
    self, import_id: str, source_id: str, imported_id: str, module_public: bool,
    module_hidden: bool, context: Node
) -&gt; None:
    # Missing attribute.
    if self.is_incomplete_namespace(import_id):
        # We don't know whether the name will be there, since the namespace
        # is incomplete. Defer the current target.
        self.mark_incomplete(
            imported_id, context, module_public=module_public, module_hidden=module_hidden
        )
        return
    message = f'Module "{import_id}" has no attribute "{source_id}"'
    # Suggest alternatives, if any match is found.
    module = self.modules.get(import_id)
    if module:
        if not self.options.implicit_reexport and source_id in module.names.keys():
            message = ('Module "{}" does not explicitly export attribute "{}"'
                       '; implicit reexport disabled'.format(import_id, source_id))
        else:
            alternatives = set(module.names.keys()).difference({source_id})
            matches = best_matches(source_id, alternatives)[:3]
            if matches:
                suggestion = f"; maybe {pretty_seq(matches, 'or')}?"
                message += f"{suggestion}"
    self.fail(message, context, code=codes.ATTR_DEFINED)
    self.add_unknown_imported_symbol(
        imported_id, context, target_name=None, module_public=module_public,
        module_hidden=not module_public
    )

    if import_id == 'typing':
        # The user probably has a missing definition in a test fixture. Let's verify.
        fullname = f'builtins.{source_id.lower()}'
        if (self.lookup_fully_qualified_or_none(fullname) is None and
                fullname in SUGGESTED_TEST_FIXTURES):
            # Yes. Generate a helpful note.
            self.msg.add_fixture_note(fullname, context)

</t>
<t tx="ekr.20220525082935.720">def collect_docs_signatures(doc_dir: str) -&gt; Tuple[Dict[str, str], Dict[str, str]]:
    """Gather all function and class signatures in the docs.

    Return a tuple (function signatures, class signatures).
    Currently only used for C modules.
    """
    all_sigs: List[Sig] = []
    all_class_sigs: List[Sig] = []
    for path in glob.glob(f'{doc_dir}/*.rst'):
        with open(path) as f:
            loc_sigs, loc_class_sigs = parse_all_signatures(f.readlines())
        all_sigs += loc_sigs
        all_class_sigs += loc_class_sigs
    sigs = dict(find_unique_signatures(all_sigs))
    class_sigs = dict(find_unique_signatures(all_class_sigs))
    return sigs, class_sigs


</t>
<t tx="ekr.20220525082935.721">def generate_stubs(options: Options) -&gt; None:
    """Main entry point for the program."""
    mypy_opts = mypy_options(options)
    py_modules, c_modules = collect_build_targets(options, mypy_opts)

    # Collect info from docs (if given):
    sigs = class_sigs = None  # type: Optional[Dict[str, str]]
    if options.doc_dir:
        sigs, class_sigs = collect_docs_signatures(options.doc_dir)

    # Use parsed sources to generate stubs for Python modules.
    generate_asts_for_modules(py_modules, options.parse_only, mypy_opts, options.verbose)
    files = []
    for mod in py_modules:
        assert mod.path is not None, "Not found module was not skipped"
        target = mod.module.replace('.', '/')
        if os.path.basename(mod.path) == '__init__.py':
            target += '/__init__.pyi'
        else:
            target += '.pyi'
        target = os.path.join(options.output_dir, target)
        files.append(target)
        with generate_guarded(mod.module, target, options.ignore_errors, options.verbose):
            generate_stub_from_ast(mod, target,
                                   options.parse_only, options.pyversion,
                                   options.include_private,
                                   options.export_less)

    # Separately analyse C modules using different logic.
    for mod in c_modules:
        if any(py_mod.module.startswith(mod.module + '.')
               for py_mod in py_modules + c_modules):
            target = mod.module.replace('.', '/') + '/__init__.pyi'
        else:
            target = mod.module.replace('.', '/') + '.pyi'
        target = os.path.join(options.output_dir, target)
        files.append(target)
        with generate_guarded(mod.module, target, options.ignore_errors, options.verbose):
            generate_stub_for_c_module(mod.module, target, sigs=sigs, class_sigs=class_sigs)
    num_modules = len(py_modules) + len(c_modules)
    if not options.quiet and num_modules &gt; 0:
        print('Processed %d modules' % num_modules)
        if len(files) == 1:
            print(f'Generated {files[0]}')
        else:
            print(f'Generated files under {common_dir_prefix(files)}' + os.sep)


</t>
<t tx="ekr.20220525082935.722">HEADER = """%(prog)s [-h] [--py2] [more options, see -h]
                     [-m MODULE] [-p PACKAGE] [files ...]"""

DESCRIPTION = """
Generate draft stubs for modules.

Stubs are generated in directory ./out, to avoid overriding files with
manual changes.  This directory is assumed to exist.
"""


</t>
<t tx="ekr.20220525082935.723">def parse_options(args: List[str]) -&gt; Options:
    parser = argparse.ArgumentParser(prog='stubgen',
                                     usage=HEADER,
                                     description=DESCRIPTION)

    parser.add_argument('--py2', action='store_true',
                        help="run in Python 2 mode (default: Python 3 mode)")
    parser.add_argument('--ignore-errors', action='store_true',
                        help="ignore errors when trying to generate stubs for modules")
    parser.add_argument('--no-import', action='store_true',
                        help="don't import the modules, just parse and analyze them "
                             "(doesn't work with C extension modules and might not "
                             "respect __all__)")
    parser.add_argument('--parse-only', action='store_true',
                        help="don't perform semantic analysis of sources, just parse them "
                             "(only applies to Python modules, might affect quality of stubs)")
    parser.add_argument('--include-private', action='store_true',
                        help="generate stubs for objects and members considered private "
                             "(single leading underscore and no trailing underscores)")
    parser.add_argument('--export-less', action='store_true',
                        help=("don't implicitly export all names imported from other modules "
                              "in the same package"))
    parser.add_argument('-v', '--verbose', action='store_true',
                        help="show more verbose messages")
    parser.add_argument('-q', '--quiet', action='store_true',
                        help="show fewer messages")
    parser.add_argument('--doc-dir', metavar='PATH', default='',
                        help="use .rst documentation in PATH (this may result in "
                             "better stubs in some cases; consider setting this to "
                             "DIR/Python-X.Y.Z/Doc/library)")
    parser.add_argument('--search-path', metavar='PATH', default='',
                        help="specify module search directories, separated by ':' "
                             "(currently only used if --no-import is given)")
    parser.add_argument('--python-executable', metavar='PATH', dest='interpreter', default='',
                        help="use Python interpreter at PATH (only works for "
                             "Python 2 right now)")
    parser.add_argument('-o', '--output', metavar='PATH', dest='output_dir', default='out',
                        help="change the output directory [default: %(default)s]")
    parser.add_argument('-m', '--module', action='append', metavar='MODULE',
                        dest='modules', default=[],
                        help="generate stub for module; can repeat for more modules")
    parser.add_argument('-p', '--package', action='append', metavar='PACKAGE',
                        dest='packages', default=[],
                        help="generate stubs for package recursively; can be repeated")
    parser.add_argument(metavar='files', nargs='*', dest='files',
                        help="generate stubs for given files or directories")

    ns = parser.parse_args(args)

    pyversion = defaults.PYTHON2_VERSION if ns.py2 else sys.version_info[:2]
    if not ns.interpreter:
        ns.interpreter = sys.executable if pyversion[0] == 3 else default_py2_interpreter()
    if ns.modules + ns.packages and ns.files:
        parser.error("May only specify one of: modules/packages or files.")
    if ns.quiet and ns.verbose:
        parser.error('Cannot specify both quiet and verbose messages')

    # Create the output folder if it doesn't already exist.
    if not os.path.exists(ns.output_dir):
        os.makedirs(ns.output_dir)

    return Options(pyversion=pyversion,
                   no_import=ns.no_import,
                   doc_dir=ns.doc_dir,
                   search_path=ns.search_path.split(':'),
                   interpreter=ns.interpreter,
                   ignore_errors=ns.ignore_errors,
                   parse_only=ns.parse_only,
                   include_private=ns.include_private,
                   output_dir=ns.output_dir,
                   modules=ns.modules,
                   packages=ns.packages,
                   files=ns.files,
                   verbose=ns.verbose,
                   quiet=ns.quiet,
                   export_less=ns.export_less)


</t>
<t tx="ekr.20220525082935.724">def main() -&gt; None:
    mypy.util.check_python_version('stubgen')
    # Make sure that the current directory is in sys.path so that
    # stubgen can be run on packages in the current directory.
    if not ('' in sys.path or '.' in sys.path):
        sys.path.insert(0, '')

    options = parse_options(sys.argv[1:])
    generate_stubs(options)


</t>
<t tx="ekr.20220525082935.725">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
#!/usr/bin/env python3
"""Stub generator for C modules.

The public interface is via the mypy.stubgen module.
"""

import importlib
import inspect
import os.path
import re
from typing import List, Dict, Tuple, Optional, Mapping, Any, Set
from types import ModuleType
from typing_extensions import Final

from mypy.moduleinspect import is_c_module
from mypy.stubdoc import (
    infer_sig_from_docstring, infer_prop_type_from_docstring, ArgSig,
    infer_arg_sig_from_anon_docstring, infer_ret_type_sig_from_anon_docstring,
    infer_ret_type_sig_from_docstring, FunctionSig
)

# Members of the typing module to consider for importing by default.
_DEFAULT_TYPING_IMPORTS: Final = (
    'Any',
    'Callable',
    'ClassVar',
    'Dict',
    'Iterable',
    'Iterator',
    'List',
    'Optional',
    'Tuple',
    'Union',
)


@others
</t>
<t tx="ekr.20220525082935.726">def generate_stub_for_c_module(module_name: str,
                               target: str,
                               sigs: Optional[Dict[str, str]] = None,
                               class_sigs: Optional[Dict[str, str]] = None) -&gt; None:
    """Generate stub for C module.

    This combines simple runtime introspection (looking for docstrings and attributes
    with simple builtin types) and signatures inferred from .rst documentation (if given).

    If directory for target doesn't exist it will be created. Existing stub
    will be overwritten.
    """
    module = importlib.import_module(module_name)
    assert is_c_module(module), f'{module_name} is not a C module'
    subdir = os.path.dirname(target)
    if subdir and not os.path.isdir(subdir):
        os.makedirs(subdir)
    imports: List[str] = []
    functions: List[str] = []
    done = set()
    items = sorted(module.__dict__.items(), key=lambda x: x[0])
    for name, obj in items:
        if is_c_function(obj):
            generate_c_function_stub(module, name, obj, functions, imports=imports, sigs=sigs)
            done.add(name)
    types: List[str] = []
    for name, obj in items:
        if name.startswith('__') and name.endswith('__'):
            continue
        if is_c_type(obj):
            generate_c_type_stub(module, name, obj, types, imports=imports, sigs=sigs,
                                 class_sigs=class_sigs)
            done.add(name)
    variables = []
    for name, obj in items:
        if name.startswith('__') and name.endswith('__'):
            continue
        if name not in done and not inspect.ismodule(obj):
            type_str = strip_or_import(get_type_fullname(type(obj)), module, imports)
            variables.append(f'{name}: {type_str}')
    output = []
    for line in sorted(set(imports)):
        output.append(line)
    for line in variables:
        output.append(line)
    for line in types:
        if line.startswith('class') and output and output[-1]:
            output.append('')
        output.append(line)
    if output and functions:
        output.append('')
    for line in functions:
        output.append(line)
    output = add_typing_import(output)
    with open(target, 'w') as file:
        for line in output:
            file.write(f'{line}\n')


</t>
<t tx="ekr.20220525082935.727">def add_typing_import(output: List[str]) -&gt; List[str]:
    """Add typing imports for collections/types that occur in the generated stub."""
    names = []
    for name in _DEFAULT_TYPING_IMPORTS:
        if any(re.search(r'\b%s\b' % name, line) for line in output):
            names.append(name)
    if names:
        return [f"from typing import {', '.join(names)}", ''] + output
    else:
        return output[:]


</t>
<t tx="ekr.20220525082935.728">def is_c_function(obj: object) -&gt; bool:
    return inspect.isbuiltin(obj) or type(obj) is type(ord)


</t>
<t tx="ekr.20220525082935.729">def is_c_method(obj: object) -&gt; bool:
    return inspect.ismethoddescriptor(obj) or type(obj) in (type(str.index),
                                                            type(str.__add__),
                                                            type(str.__new__))


</t>
<t tx="ekr.20220525082935.73">def process_import_over_existing_name(self,
                                      imported_id: str, existing_symbol: SymbolTableNode,
                                      module_symbol: SymbolTableNode,
                                      import_node: ImportBase) -&gt; bool:
    if existing_symbol.node is module_symbol.node:
        # We added this symbol on previous iteration.
        return False
    if (existing_symbol.kind in (LDEF, GDEF, MDEF) and
            isinstance(existing_symbol.node, (Var, FuncDef, TypeInfo, Decorator, TypeAlias))):
        # This is a valid import over an existing definition in the file. Construct a dummy
        # assignment that we'll use to type check the import.
        lvalue = NameExpr(imported_id)
        lvalue.kind = existing_symbol.kind
        lvalue.node = existing_symbol.node
        rvalue = NameExpr(imported_id)
        rvalue.kind = module_symbol.kind
        rvalue.node = module_symbol.node
        if isinstance(rvalue.node, TypeAlias):
            # Suppress bogus errors from the dummy assignment if rvalue is an alias.
            # Otherwise mypy may complain that alias is invalid in runtime context.
            rvalue.is_alias_rvalue = True
        assignment = AssignmentStmt([lvalue], rvalue)
        for node in assignment, lvalue, rvalue:
            node.set_line(import_node)
        import_node.assignments.append(assignment)
        return True
    return False

</t>
<t tx="ekr.20220525082935.730">def is_c_classmethod(obj: object) -&gt; bool:
    return inspect.isbuiltin(obj) or type(obj).__name__ in ('classmethod',
                                                            'classmethod_descriptor')


</t>
<t tx="ekr.20220525082935.731">def is_c_property(obj: object) -&gt; bool:
    return inspect.isdatadescriptor(obj) or hasattr(obj, 'fget')


</t>
<t tx="ekr.20220525082935.732">def is_c_property_readonly(prop: Any) -&gt; bool:
    return hasattr(prop, 'fset') and prop.fset is None


</t>
<t tx="ekr.20220525082935.733">def is_c_type(obj: object) -&gt; bool:
    return inspect.isclass(obj) or type(obj) is type(int)


</t>
<t tx="ekr.20220525082935.734">def is_pybind11_overloaded_function_docstring(docstr: str, name: str) -&gt; bool:
    return docstr.startswith(f"{name}(*args, **kwargs)\n" +
                             "Overloaded function.\n\n")


</t>
<t tx="ekr.20220525082935.735">def generate_c_function_stub(module: ModuleType,
                             name: str,
                             obj: object,
                             output: List[str],
                             imports: List[str],
                             self_var: Optional[str] = None,
                             sigs: Optional[Dict[str, str]] = None,
                             class_name: Optional[str] = None,
                             class_sigs: Optional[Dict[str, str]] = None) -&gt; None:
    """Generate stub for a single function or method.

    The result (always a single line) will be appended to 'output'.
    If necessary, any required names will be added to 'imports'.
    The 'class_name' is used to find signature of __init__ or __new__ in
    'class_sigs'.
    """
    if sigs is None:
        sigs = {}
    if class_sigs is None:
        class_sigs = {}

    ret_type = 'None' if name == '__init__' and class_name else 'Any'

    if (
        name in ("__new__", "__init__")
        and name not in sigs
        and class_name
        and class_name in class_sigs
    ):
        inferred: Optional[List[FunctionSig]] = [
            FunctionSig(
                name=name,
                args=infer_arg_sig_from_anon_docstring(class_sigs[class_name]),
                ret_type=ret_type,
            )
        ]
    else:
        docstr = getattr(obj, '__doc__', None)
        inferred = infer_sig_from_docstring(docstr, name)
        if inferred:
            assert docstr is not None
            if is_pybind11_overloaded_function_docstring(docstr, name):
                # Remove pybind11 umbrella (*args, **kwargs) for overloaded functions
                del inferred[-1]
        if not inferred:
            if class_name and name not in sigs:
                inferred = [FunctionSig(name, args=infer_method_sig(name, self_var),
                                        ret_type=ret_type)]
            else:
                inferred = [FunctionSig(name=name,
                                        args=infer_arg_sig_from_anon_docstring(
                                            sigs.get(name, '(*args, **kwargs)')),
                                        ret_type=ret_type)]
        elif class_name and self_var:
            args = inferred[0].args
            if not args or args[0].name != self_var:
                args.insert(0, ArgSig(name=self_var))

    is_overloaded = len(inferred) &gt; 1 if inferred else False
    if is_overloaded:
        imports.append('from typing import overload')
    if inferred:
        for signature in inferred:
            sig = []
            for arg in signature.args:
                if arg.name == self_var:
                    arg_def = self_var
                else:
                    arg_def = arg.name
                    if arg_def == 'None':
                        arg_def = '_none'  # None is not a valid argument name

                    if arg.type:
                        arg_def += ": " + strip_or_import(arg.type, module, imports)

                    if arg.default:
                        arg_def += " = ..."

                sig.append(arg_def)

            if is_overloaded:
                output.append('@overload')
            output.append('def {function}({args}) -&gt; {ret}: ...'.format(
                function=name,
                args=", ".join(sig),
                ret=strip_or_import(signature.ret_type, module, imports)
            ))


</t>
<t tx="ekr.20220525082935.736">def strip_or_import(typ: str, module: ModuleType, imports: List[str]) -&gt; str:
    """Strips unnecessary module names from typ.

    If typ represents a type that is inside module or is a type coming from builtins, remove
    module declaration from it. Return stripped name of the type.

    Arguments:
        typ: name of the type
        module: in which this type is used
        imports: list of import statements (may be modified during the call)
    """
    stripped_type = typ
    if any(c in typ for c in '[,'):
        for subtyp in re.split(r'[\[,\]]', typ):
            strip_or_import(subtyp.strip(), module, imports)
        if module:
            stripped_type = re.sub(
                r'(^|[\[, ]+)' + re.escape(module.__name__ + '.'),
                r'\1',
                typ,
            )
    elif module and typ.startswith(module.__name__ + '.'):
        stripped_type = typ[len(module.__name__) + 1:]
    elif '.' in typ:
        arg_module = typ[:typ.rindex('.')]
        if arg_module == 'builtins':
            stripped_type = typ[len('builtins') + 1:]
        else:
            imports.append(f'import {arg_module}')
    if stripped_type == 'NoneType':
        stripped_type = 'None'
    return stripped_type


</t>
<t tx="ekr.20220525082935.737">def is_static_property(obj: object) -&gt; bool:
    return type(obj).__name__ == 'pybind11_static_property'


</t>
<t tx="ekr.20220525082935.738">def generate_c_property_stub(name: str, obj: object,
                             static_properties: List[str],
                             rw_properties: List[str],
                             ro_properties: List[str], readonly: bool,
                             module: Optional[ModuleType] = None,
                             imports: Optional[List[str]] = None) -&gt; None:
    """Generate property stub using introspection of 'obj'.

    Try to infer type from docstring, append resulting lines to 'output'.
    """

    @others
    # Ignore special properties/attributes.
    if is_skipped_attribute(name):
        return

    inferred = infer_prop_type(getattr(obj, '__doc__', None))
    if not inferred:
        fget = getattr(obj, 'fget', None)
        inferred = infer_prop_type(getattr(fget, '__doc__', None))
    if not inferred:
        inferred = 'Any'

    if module is not None and imports is not None:
        inferred = strip_or_import(inferred, module, imports)

    if is_static_property(obj):
        trailing_comment = "  # read-only" if readonly else ""
        static_properties.append(
            f'{name}: ClassVar[{inferred}] = ...{trailing_comment}'
        )
    else:  # regular property
        if readonly:
            ro_properties.append('@property')
            ro_properties.append(f'def {name}(self) -&gt; {inferred}: ...')
        else:
            rw_properties.append(f'{name}: {inferred}')


</t>
<t tx="ekr.20220525082935.739">def infer_prop_type(docstr: Optional[str]) -&gt; Optional[str]:
    """Infer property type from docstring or docstring signature."""
    if docstr is not None:
        inferred = infer_ret_type_sig_from_anon_docstring(docstr)
        if not inferred:
            inferred = infer_ret_type_sig_from_docstring(docstr, name)
        if not inferred:
            inferred = infer_prop_type_from_docstring(docstr)
        return inferred
    else:
        return None

</t>
<t tx="ekr.20220525082935.74">def correct_relative_import(self, node: Union[ImportFrom, ImportAll]) -&gt; str:
    import_id, ok = correct_relative_import(self.cur_mod_id, node.relative, node.id,
                                            self.cur_mod_node.is_package_init_file())
    if not ok:
        self.fail("Relative import climbs too many namespaces", node)
    return import_id

</t>
<t tx="ekr.20220525082935.740">def generate_c_type_stub(module: ModuleType,
                         class_name: str,
                         obj: type,
                         output: List[str],
                         imports: List[str],
                         sigs: Optional[Dict[str, str]] = None,
                         class_sigs: Optional[Dict[str, str]] = None) -&gt; None:
    """Generate stub for a single class using runtime introspection.

    The result lines will be appended to 'output'. If necessary, any
    required names will be added to 'imports'.
    """
    # typeshed gives obj.__dict__ the not quite correct type Dict[str, Any]
    # (it could be a mappingproxy!), which makes mypyc mad, so obfuscate it.
    obj_dict: Mapping[str, Any] = getattr(obj, "__dict__")  # noqa
    items = sorted(obj_dict.items(), key=lambda x: method_name_sort_key(x[0]))
    methods: List[str] = []
    types: List[str] = []
    static_properties: List[str] = []
    rw_properties: List[str] = []
    ro_properties: List[str] = []
    done: Set[str] = set()
    for attr, value in items:
        if is_c_method(value) or is_c_classmethod(value):
            done.add(attr)
            if not is_skipped_attribute(attr):
                if attr == '__new__':
                    # TODO: We should support __new__.
                    if '__init__' in obj_dict:
                        # Avoid duplicate functions if both are present.
                        # But is there any case where .__new__() has a
                        # better signature than __init__() ?
                        continue
                    attr = '__init__'
                if is_c_classmethod(value):
                    methods.append('@classmethod')
                    self_var = 'cls'
                else:
                    self_var = 'self'
                generate_c_function_stub(module, attr, value, methods, imports=imports,
                                         self_var=self_var, sigs=sigs, class_name=class_name,
                                         class_sigs=class_sigs)
        elif is_c_property(value):
            done.add(attr)
            generate_c_property_stub(attr, value, static_properties, rw_properties, ro_properties,
                                     is_c_property_readonly(value),
                                     module=module, imports=imports)
        elif is_c_type(value):
            generate_c_type_stub(module, attr, value, types, imports=imports, sigs=sigs,
                                 class_sigs=class_sigs)
            done.add(attr)

    for attr, value in items:
        if is_skipped_attribute(attr):
            continue
        if attr not in done:
            static_properties.append('{}: ClassVar[{}] = ...'.format(
                attr, strip_or_import(get_type_fullname(type(value)), module, imports)))
    all_bases = type.mro(obj)
    if all_bases[-1] is object:
        # TODO: Is this always object?
        del all_bases[-1]
    # remove pybind11_object. All classes generated by pybind11 have pybind11_object in their MRO,
    # which only overrides a few functions in object type
    if all_bases and all_bases[-1].__name__ == 'pybind11_object':
        del all_bases[-1]
    # remove the class itself
    all_bases = all_bases[1:]
    # Remove base classes of other bases as redundant.
    bases: List[type] = []
    for base in all_bases:
        if not any(issubclass(b, base) for b in bases):
            bases.append(base)
    if bases:
        bases_str = '(%s)' % ', '.join(
            strip_or_import(
                get_type_fullname(base),
                module,
                imports
            ) for base in bases
        )
    else:
        bases_str = ''
    if types or static_properties or rw_properties or methods or ro_properties:
        output.append(f'class {class_name}{bases_str}:')
        for line in types:
            if output and output[-1] and \
                    not output[-1].startswith('class') and line.startswith('class'):
                output.append('')
            output.append('    ' + line)
        for line in static_properties:
            output.append(f'    {line}')
        for line in rw_properties:
            output.append(f'    {line}')
        for line in methods:
            output.append(f'    {line}')
        for line in ro_properties:
            output.append(f'    {line}')
    else:
        output.append(f'class {class_name}{bases_str}: ...')


</t>
<t tx="ekr.20220525082935.741">def get_type_fullname(typ: type) -&gt; str:
    return f"{typ.__module__}.{getattr(typ, '__qualname__', typ.__name__)}"


</t>
<t tx="ekr.20220525082935.742">def method_name_sort_key(name: str) -&gt; Tuple[int, str]:
    """Sort methods in classes in a typical order.

    I.e.: constructor, normal methods, special methods.
    """
    if name in ('__new__', '__init__'):
        return 0, name
    if name.startswith('__') and name.endswith('__'):
        return 2, name
    return 1, name


</t>
<t tx="ekr.20220525082935.743">def is_pybind_skipped_attribute(attr: str) -&gt; bool:
    return attr.startswith("__pybind11_module_local_")


</t>
<t tx="ekr.20220525082935.744">def is_skipped_attribute(attr: str) -&gt; bool:
    return (attr in ('__getattribute__',
                     '__str__',
                     '__repr__',
                     '__doc__',
                     '__dict__',
                     '__module__',
                     '__weakref__')  # For pickling
            or is_pybind_skipped_attribute(attr)
            )


</t>
<t tx="ekr.20220525082935.745">def infer_method_sig(name: str, self_var: Optional[str] = None) -&gt; List[ArgSig]:
    args: Optional[List[ArgSig]] = None
    if name.startswith('__') and name.endswith('__'):
        name = name[2:-2]
        if name in ('hash', 'iter', 'next', 'sizeof', 'copy', 'deepcopy', 'reduce', 'getinitargs',
                    'int', 'float', 'trunc', 'complex', 'bool', 'abs', 'bytes', 'dir', 'len',
                    'reversed', 'round', 'index', 'enter'):
            args = []
        elif name == 'getitem':
            args = [ArgSig(name='index')]
        elif name == 'setitem':
            args = [ArgSig(name='index'),
                    ArgSig(name='object')]
        elif name in ('delattr', 'getattr'):
            args = [ArgSig(name='name')]
        elif name == 'setattr':
            args = [ArgSig(name='name'),
                    ArgSig(name='value')]
        elif name == 'getstate':
            args = []
        elif name == 'setstate':
            args = [ArgSig(name='state')]
        elif name in ('eq', 'ne', 'lt', 'le', 'gt', 'ge',
                      'add', 'radd', 'sub', 'rsub', 'mul', 'rmul',
                      'mod', 'rmod', 'floordiv', 'rfloordiv', 'truediv', 'rtruediv',
                      'divmod', 'rdivmod', 'pow', 'rpow',
                      'xor', 'rxor', 'or', 'ror', 'and', 'rand', 'lshift', 'rlshift',
                      'rshift', 'rrshift',
                      'contains', 'delitem',
                      'iadd', 'iand', 'ifloordiv', 'ilshift', 'imod', 'imul', 'ior',
                      'ipow', 'irshift', 'isub', 'itruediv', 'ixor'):
            args = [ArgSig(name='other')]
        elif name in ('neg', 'pos', 'invert'):
            args = []
        elif name == 'get':
            args = [ArgSig(name='instance'),
                    ArgSig(name='owner')]
        elif name == 'set':
            args = [ArgSig(name='instance'),
                    ArgSig(name='value')]
        elif name == 'reduce_ex':
            args = [ArgSig(name='protocol')]
        elif name == 'exit':
            args = [ArgSig(name='type'),
                    ArgSig(name='value'),
                    ArgSig(name='traceback')]
    if args is None:
        args = [ArgSig(name='*args'),
                ArgSig(name='**kwargs')]
    return [ArgSig(name=self_var or 'self')] + args
</t>
<t tx="ekr.20220525082935.746">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from typing import Optional


@others
# Stubs for these third-party packages used to be shipped with mypy.
#
# Map package name to PyPI stub distribution name.
#
# Package name can have one or two components ('a' or 'a.b').
legacy_bundled_packages = {
    'aiofiles': StubInfo('types-aiofiles', py_version=3),
    'atomicwrites': StubInfo('types-atomicwrites'),
    'attr': StubInfo('types-attrs'),
    'backports': StubInfo('types-backports'),
    'backports_abc': StubInfo('types-backports_abc'),
    'bleach': StubInfo('types-bleach'),
    'boto': StubInfo('types-boto'),
    'cachetools': StubInfo('types-cachetools'),
    'certifi': StubInfo('types-certifi'),
    'characteristic': StubInfo('types-characteristic'),
    'chardet': StubInfo('types-chardet'),
    'click_spinner': StubInfo('types-click-spinner'),
    'concurrent': StubInfo('types-futures', py_version=2),
    'contextvars': StubInfo('types-contextvars', py_version=3),
    'croniter': StubInfo('types-croniter'),
    'dataclasses': StubInfo('types-dataclasses', py_version=3),
    'dateparser': StubInfo('types-dateparser'),
    'datetimerange': StubInfo('types-DateTimeRange'),
    'dateutil': StubInfo('types-python-dateutil'),
    'decorator': StubInfo('types-decorator'),
    'deprecated': StubInfo('types-Deprecated'),
    'docutils': StubInfo('types-docutils', py_version=3),
    'emoji': StubInfo('types-emoji'),
    'enum': StubInfo('types-enum34', py_version=2),
    'fb303': StubInfo('types-fb303', py_version=2),
    'filelock': StubInfo('types-filelock', py_version=3),
    'first': StubInfo('types-first'),
    'freezegun': StubInfo('types-freezegun', py_version=3),
    'frozendict': StubInfo('types-frozendict', py_version=3),
    'geoip2': StubInfo('types-geoip2'),
    'gflags': StubInfo('types-python-gflags'),
    'google.protobuf': StubInfo('types-protobuf'),
    'ipaddress': StubInfo('types-ipaddress', py_version=2),
    'kazoo': StubInfo('types-kazoo', py_version=2),
    'markdown': StubInfo('types-Markdown'),
    'maxminddb': StubInfo('types-maxminddb'),
    'mock': StubInfo('types-mock'),
    'OpenSSL': StubInfo('types-pyOpenSSL'),
    'paramiko': StubInfo('types-paramiko'),
    'pathlib2': StubInfo('types-pathlib2', py_version=2),
    'pkg_resources': StubInfo('types-setuptools', py_version=3),
    'polib': StubInfo('types-polib'),
    'pycurl': StubInfo('types-pycurl'),
    'pymssql': StubInfo('types-pymssql', py_version=2),
    'pymysql': StubInfo('types-PyMySQL'),
    'pyrfc3339': StubInfo('types-pyRFC3339', py_version=3),
    'python2': StubInfo('types-six'),
    'pytz': StubInfo('types-pytz'),
    'pyVmomi': StubInfo('types-pyvmomi'),
    'redis': StubInfo('types-redis'),
    'requests': StubInfo('types-requests'),
    'retry': StubInfo('types-retry'),
    'routes': StubInfo('types-Routes', py_version=2),
    'scribe': StubInfo('types-scribe', py_version=2),
    'simplejson': StubInfo('types-simplejson'),
    'singledispatch': StubInfo('types-singledispatch'),
    'six': StubInfo('types-six'),
    'slugify': StubInfo('types-python-slugify'),
    'tabulate': StubInfo('types-tabulate'),
    'termcolor': StubInfo('types-termcolor'),
    'toml': StubInfo('types-toml'),
    'tornado': StubInfo('types-tornado', py_version=2),
    'typed_ast': StubInfo('types-typed-ast', py_version=3),
    'tzlocal': StubInfo('types-tzlocal'),
    'ujson': StubInfo('types-ujson'),
    'waitress': StubInfo('types-waitress', py_version=3),
    'yaml': StubInfo('types-PyYAML'),
}
</t>
<t tx="ekr.20220525082935.747">class StubInfo:
    def __init__(self, name: str, py_version: Optional[int] = None) -&gt; None:
        self.name = name
        # If None, compatible with py2+py3, if 2/3, only compatible with py2/py3
        self.py_version = py_version


</t>
<t tx="ekr.20220525082935.748">def is_legacy_bundled_package(prefix: str, py_version: int) -&gt; bool:
    if prefix not in legacy_bundled_packages:
        return False
    package_ver = legacy_bundled_packages[prefix].py_version
    return package_ver is None or package_ver == py_version


</t>
<t tx="ekr.20220525082935.749">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Tests for stubs.

Verify that various things in stubs are consistent with how things behave at runtime.

"""

import argparse
import copy
import enum
import importlib
import inspect
import os
import re
import sys
import types
import typing
import typing_extensions
import warnings
from contextlib import redirect_stdout, redirect_stderr
from functools import singledispatch
from pathlib import Path
from typing import Any, Dict, Generic, Iterator, List, Optional, Tuple, TypeVar, Union, cast

from typing_extensions import Type

import mypy.build
import mypy.modulefinder
import mypy.state
import mypy.types
import mypy.version
from mypy import nodes
from mypy.config_parser import parse_config_file
from mypy.options import Options
from mypy.util import FancyFormatter, bytes_to_human_readable_repr, plural_s, is_dunder


@others
if __name__ == "__main__":
    sys.exit(main())
</t>
<t tx="ekr.20220525082935.75">def visit_import_all(self, i: ImportAll) -&gt; None:
    i_id = self.correct_relative_import(i)
    if i_id in self.modules:
        m = self.modules[i_id]
        if self.is_incomplete_namespace(i_id):
            # Any names could be missing from the current namespace if the target module
            # namespace is incomplete.
            self.mark_incomplete('*', i)
        for name, node in m.names.items():
            fullname = i_id + '.' + name
            self.set_future_import_flags(fullname)
            if node is None:
                continue
            # if '__all__' exists, all nodes not included have had module_public set to
            # False, and we can skip checking '_' because it's been explicitly included.
            if node.module_public and (not name.startswith('_') or '__all__' in m.names):
                if isinstance(node.node, MypyFile):
                    # Star import of submodule from a package, add it as a dependency.
                    self.imports.add(node.node.fullname)
                existing_symbol = self.lookup_current_scope(name)
                if existing_symbol and not isinstance(node.node, PlaceholderNode):
                    # Import can redefine a variable. They get special treatment.
                    if self.process_import_over_existing_name(
                            name, existing_symbol, node, i):
                        continue
                # `from x import *` always reexports symbols
                self.add_imported_symbol(name, node, i,
                                         module_public=True,
                                         module_hidden=False)

    else:
        # Don't add any dummy symbols for 'from x import *' if 'x' is unknown.
        pass

</t>
<t tx="ekr.20220525082935.750">class Missing:
    """Marker object for things that are missing (from a stub or the runtime)."""

    def __repr__(self) -&gt; str:
        return "MISSING"


</t>
<t tx="ekr.20220525082935.751">MISSING = Missing()

T = TypeVar("T")
MaybeMissing = Union[T, Missing]

_formatter = FancyFormatter(sys.stdout, sys.stderr, False)


</t>
<t tx="ekr.20220525082935.752">def _style(message: str, **kwargs: Any) -&gt; str:
    """Wrapper around mypy.util for fancy formatting."""
    kwargs.setdefault("color", "none")
    return _formatter.style(message, **kwargs)


class StubtestFailure(Exception):
    pass


</t>
<t tx="ekr.20220525082935.753">class Error:
    @others
</t>
<t tx="ekr.20220525082935.754">def __init__(
    self,
    object_path: List[str],
    message: str,
    stub_object: MaybeMissing[nodes.Node],
    runtime_object: MaybeMissing[Any],
    *,
    stub_desc: Optional[str] = None,
    runtime_desc: Optional[str] = None
) -&gt; None:
    """Represents an error found by stubtest.

    :param object_path: Location of the object with the error,
        e.g. ``["module", "Class", "method"]``
    :param message: Error message
    :param stub_object: The mypy node representing the stub
    :param runtime_object: Actual object obtained from the runtime
    :param stub_desc: Specialised description for the stub object, should you wish
    :param runtime_desc: Specialised description for the runtime object, should you wish

    """
    self.object_desc = ".".join(object_path)
    self.message = message
    self.stub_object = stub_object
    self.runtime_object = runtime_object
    self.stub_desc = stub_desc or str(getattr(stub_object, "type", stub_object))
    self.runtime_desc = runtime_desc or str(runtime_object)

</t>
<t tx="ekr.20220525082935.755">def is_missing_stub(self) -&gt; bool:
    """Whether or not the error is for something missing from the stub."""
    return isinstance(self.stub_object, Missing)

</t>
<t tx="ekr.20220525082935.756">def is_positional_only_related(self) -&gt; bool:
    """Whether or not the error is for something being (or not being) positional-only."""
    # TODO: This is hacky, use error codes or something more resilient
    return "leading double underscore" in self.message

</t>
<t tx="ekr.20220525082935.757">def get_description(self, concise: bool = False) -&gt; str:
    """Returns a description of the error.

    :param concise: Whether to return a concise, one-line description

    """
    if concise:
        return _style(self.object_desc, bold=True) + " " + self.message

    stub_line = None
    stub_file: None = None
    if not isinstance(self.stub_object, Missing):
        stub_line = self.stub_object.line
    # TODO: Find a way of getting the stub file

    stub_loc_str = ""
    if stub_line:
        stub_loc_str += f" at line {stub_line}"
    if stub_file:
        stub_loc_str += f" in file {Path(stub_file)}"

    runtime_line = None
    runtime_file = None
    if not isinstance(self.runtime_object, Missing):
        try:
            runtime_line = inspect.getsourcelines(self.runtime_object)[1]
        except (OSError, TypeError):
            pass
        try:
            runtime_file = inspect.getsourcefile(self.runtime_object)
        except TypeError:
            pass

    runtime_loc_str = ""
    if runtime_line:
        runtime_loc_str += f" at line {runtime_line}"
    if runtime_file:
        runtime_loc_str += f" in file {Path(runtime_file)}"

    output = [
        _style("error: ", color="red", bold=True),
        _style(self.object_desc, bold=True),
        " ",
        self.message,
        "\n",
        "Stub:",
        _style(stub_loc_str, dim=True),
        "\n",
        _style(self.stub_desc + "\n", color="blue", dim=True),
        "Runtime:",
        _style(runtime_loc_str, dim=True),
        "\n",
        _style(self.runtime_desc + "\n", color="blue", dim=True),
    ]
    return "".join(output)


</t>
<t tx="ekr.20220525082935.758"># ====================
# Core logic
# ====================


</t>
<t tx="ekr.20220525082935.759">def test_module(module_name: str) -&gt; Iterator[Error]:
    """Tests a given module's stub against introspecting it at runtime.

    Requires the stub to have been built already, accomplished by a call to ``build_stubs``.

    :param module_name: The module to test

    """
    stub = get_stub(module_name)
    if stub is None:
        yield Error([module_name], "failed to find stubs", MISSING, None, runtime_desc="N/A")
        return

    try:
        with open(os.devnull, "w") as devnull:
            with warnings.catch_warnings(), redirect_stdout(devnull), redirect_stderr(devnull):
                warnings.simplefilter("ignore")
                runtime = importlib.import_module(module_name)
                # Also run the equivalent of `from module import *`
                # This could have the additional effect of loading not-yet-loaded submodules
                # mentioned in __all__
                __import__(module_name, fromlist=["*"])
    except Exception as e:
        yield Error([module_name], f"failed to import, {type(e).__name__}: {e}", stub, MISSING)
        return

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        yield from verify(stub, runtime, [module_name])


</t>
<t tx="ekr.20220525082935.76">#
# Assignment
#

</t>
<t tx="ekr.20220525082935.760">@singledispatch
def verify(
    stub: MaybeMissing[nodes.Node], runtime: MaybeMissing[Any], object_path: List[str]
) -&gt; Iterator[Error]:
    """Entry point for comparing a stub to a runtime object.

    We use single dispatch based on the type of ``stub``.

    :param stub: The mypy node representing a part of the stub
    :param runtime: The runtime object corresponding to ``stub``

    """
    yield Error(object_path, "is an unknown mypy node", stub, runtime)


</t>
<t tx="ekr.20220525082935.761">@verify.register(nodes.MypyFile)
def verify_mypyfile(
    stub: nodes.MypyFile, runtime: MaybeMissing[types.ModuleType], object_path: List[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        yield Error(object_path, "is not present at runtime", stub, runtime)
        return
    if not isinstance(runtime, types.ModuleType):
        yield Error(object_path, "is not a module", stub, runtime)
        return

    # Check things in the stub
    to_check = {
        m
        for m, o in stub.names.items()
        if not o.module_hidden and (not is_probably_private(m) or hasattr(runtime, m))
    }

    @others
    runtime_public_contents = (
        runtime.__all__
        if hasattr(runtime, "__all__")
        else [
            m
            for m in dir(runtime)
            if not is_probably_private(m)
            # Ensure that the object's module is `runtime`, since in the absence of __all__ we
            # don't have a good way to detect re-exports at runtime.
            and _belongs_to_runtime(runtime, m)
        ]
    )
    # Check all things declared in module's __all__, falling back to our best guess
    to_check.update(runtime_public_contents)
    to_check.difference_update(IGNORED_MODULE_DUNDERS)

    for entry in sorted(to_check):
        stub_entry = stub.names[entry].node if entry in stub.names else MISSING
        if isinstance(stub_entry, nodes.MypyFile):
            # Don't recursively check exported modules, since that leads to infinite recursion
            continue
        assert stub_entry is not None
        try:
            runtime_entry = getattr(runtime, entry, MISSING)
        except Exception:
            # Catch all exceptions in case the runtime raises an unexpected exception
            # from __getattr__ or similar.
            continue
        yield from verify(stub_entry, runtime_entry, object_path + [entry])


</t>
<t tx="ekr.20220525082935.762">def _belongs_to_runtime(r: types.ModuleType, attr: str) -&gt; bool:
    obj = getattr(r, attr)
    try:
        obj_mod = getattr(obj, "__module__", None)
    except Exception:
        return False
    if obj_mod is not None:
        return obj_mod == r.__name__
    return not isinstance(obj, types.ModuleType)

</t>
<t tx="ekr.20220525082935.763">if sys.version_info &gt;= (3, 7):
    _WrapperDescriptorType = types.WrapperDescriptorType
else:
    _WrapperDescriptorType = type(object.__init__)


</t>
<t tx="ekr.20220525082935.764">@verify.register(nodes.TypeInfo)
def verify_typeinfo(
    stub: nodes.TypeInfo, runtime: MaybeMissing[Type[Any]], object_path: List[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        yield Error(object_path, "is not present at runtime", stub, runtime, stub_desc=repr(stub))
        return
    if not isinstance(runtime, type):
        yield Error(object_path, "is not a type", stub, runtime, stub_desc=repr(stub))
        return

    try:
        class SubClass(runtime):  # type: ignore
            pass
    except TypeError:
        # Enum classes are implicitly @final
        if not stub.is_final and not issubclass(runtime, enum.Enum):
            yield Error(
                object_path,
                "cannot be subclassed at runtime, but isn't marked with @final in the stub",
                stub,
                runtime,
                stub_desc=repr(stub),
            )
    except Exception:
        # The class probably wants its subclasses to do something special.
        # Examples: ctypes.Array, ctypes._SimpleCData
        pass

    # Check everything already defined on the stub class itself (i.e. not inherited)
    to_check = set(stub.names)
    # Check all public things on the runtime class
    to_check.update(
        # cast to workaround mypyc complaints
        m
        for m in cast(Any, vars)(runtime)
        if not is_probably_private(m) and m not in IGNORABLE_CLASS_DUNDERS
    )

    for entry in sorted(to_check):
        mangled_entry = entry
        if entry.startswith("__") and not entry.endswith("__"):
            mangled_entry = f"_{stub.name}{entry}"
        stub_to_verify = next((t.names[entry].node for t in stub.mro if entry in t.names), MISSING)
        assert stub_to_verify is not None
        try:
            runtime_attr = getattr(runtime, mangled_entry, MISSING)
        except Exception:
            # Catch all exceptions in case the runtime raises an unexpected exception
            # from __getattr__ or similar.
            continue
        # Do not error for an object missing from the stub
        # If the runtime object is a types.WrapperDescriptorType object
        # and has a non-special dunder name.
        # The vast majority of these are false positives.
        if not (
            isinstance(stub_to_verify, Missing)
            and isinstance(runtime_attr, _WrapperDescriptorType)
            and is_dunder(mangled_entry, exclude_special=True)
        ):
            yield from verify(stub_to_verify, runtime_attr, object_path + [entry])


</t>
<t tx="ekr.20220525082935.765">def _verify_static_class_methods(
    stub: nodes.FuncBase, runtime: Any, object_path: List[str]
) -&gt; Iterator[str]:
    if stub.name in ("__new__", "__init_subclass__", "__class_getitem__"):
        # Special cased by Python, so don't bother checking
        return
    if inspect.isbuiltin(runtime):
        # The isinstance checks don't work reliably for builtins, e.g. datetime.datetime.now, so do
        # something a little hacky that seems to work well
        probably_class_method = isinstance(getattr(runtime, "__self__", None), type)
        if probably_class_method and not stub.is_class:
            yield "runtime is a classmethod but stub is not"
        if not probably_class_method and stub.is_class:
            yield "stub is a classmethod but runtime is not"
        return

    # Look the object up statically, to avoid binding by the descriptor protocol
    static_runtime = importlib.import_module(object_path[0])
    for entry in object_path[1:]:
        try:
            static_runtime = inspect.getattr_static(static_runtime, entry)
        except AttributeError:
            # This can happen with mangled names, ignore for now.
            # TODO: pass more information about ancestors of nodes/objects to verify, so we don't
            # have to do this hacky lookup. Would be useful in a couple other places too.
            return

    if isinstance(static_runtime, classmethod) and not stub.is_class:
        yield "runtime is a classmethod but stub is not"
    if not isinstance(static_runtime, classmethod) and stub.is_class:
        yield "stub is a classmethod but runtime is not"
    if isinstance(static_runtime, staticmethod) and not stub.is_static:
        yield "runtime is a staticmethod but stub is not"
    if not isinstance(static_runtime, staticmethod) and stub.is_static:
        yield "stub is a staticmethod but runtime is not"


</t>
<t tx="ekr.20220525082935.766">def _verify_arg_name(
    stub_arg: nodes.Argument, runtime_arg: inspect.Parameter, function_name: str
) -&gt; Iterator[str]:
    """Checks whether argument names match."""
    # Ignore exact names for most dunder methods
    if is_dunder(function_name, exclude_special=True):
        return

    @others
    # Be more permissive about names matching for positional-only arguments
    if runtime_arg.kind == inspect.Parameter.POSITIONAL_ONLY and names_approx_match(
        stub_arg.variable.name, runtime_arg.name
    ):
        return
    # This comes up with namedtuples, so ignore
    if stub_arg.variable.name == "_self":
        return
    yield (
        'stub argument "{}" differs from runtime argument "{}"'.format(
            stub_arg.variable.name, runtime_arg.name
        )
    )


</t>
<t tx="ekr.20220525082935.767">def strip_prefix(s: str, prefix: str) -&gt; str:
    return s[len(prefix):] if s.startswith(prefix) else s

</t>
<t tx="ekr.20220525082935.768">if strip_prefix(stub_arg.variable.name, "__") == runtime_arg.name:
    return

</t>
<t tx="ekr.20220525082935.769">def names_approx_match(a: str, b: str) -&gt; bool:
    a = a.strip("_")
    b = b.strip("_")
    return a.startswith(b) or b.startswith(a) or len(a) == 1 or len(b) == 1

</t>
<t tx="ekr.20220525082935.77">def visit_assignment_expr(self, s: AssignmentExpr) -&gt; None:
    s.value.accept(self)
    self.analyze_lvalue(s.target, escape_comprehensions=True, has_explicit_value=True)

</t>
<t tx="ekr.20220525082935.770">def _verify_arg_default_value(
    stub_arg: nodes.Argument, runtime_arg: inspect.Parameter
) -&gt; Iterator[str]:
    """Checks whether argument default values are compatible."""
    if runtime_arg.default != inspect.Parameter.empty:
        if stub_arg.kind.is_required():
            yield (
                'runtime argument "{}" has a default value but stub argument does not'.format(
                    runtime_arg.name
                )
            )
        else:
            runtime_type = get_mypy_type_of_runtime_value(runtime_arg.default)
            # Fallback to the type annotation type if var type is missing. The type annotation
            # is an UnboundType, but I don't know enough to know what the pros and cons here are.
            # UnboundTypes have ugly question marks following them, so default to var type.
            # Note we do this same fallback when constructing signatures in from_overloadedfuncdef
            stub_type = stub_arg.variable.type or stub_arg.type_annotation
            if isinstance(stub_type, mypy.types.TypeVarType):
                stub_type = stub_type.upper_bound
            if (
                runtime_type is not None
                and stub_type is not None
                # Avoid false positives for marker objects
                and type(runtime_arg.default) != object
                # And ellipsis
                and runtime_arg.default is not ...
                and not is_subtype_helper(runtime_type, stub_type)
            ):
                yield (
                    'runtime argument "{}" has a default value of type {}, '
                    "which is incompatible with stub argument type {}".format(
                        runtime_arg.name, runtime_type, stub_type
                    )
                )
    else:
        if stub_arg.kind.is_optional():
            yield (
                'stub argument "{}" has a default value but runtime argument does not'.format(
                    stub_arg.variable.name
                )
            )


</t>
<t tx="ekr.20220525082935.771">def maybe_strip_cls(name: str, args: List[nodes.Argument]) -&gt; List[nodes.Argument]:
    if name in ("__init_subclass__", "__class_getitem__"):
        # These are implicitly classmethods. If the stub chooses not to have @classmethod, we
        # should remove the cls argument
        if args[0].variable.name == "cls":
            return args[1:]
    return args


</t>
<t tx="ekr.20220525082935.772">class Signature(Generic[T]):
    @others
</t>
<t tx="ekr.20220525082935.773">def __init__(self) -&gt; None:
    self.pos: List[T] = []
    self.kwonly: Dict[str, T] = {}
    self.varpos: Optional[T] = None
    self.varkw: Optional[T] = None

</t>
<t tx="ekr.20220525082935.774">def __str__(self) -&gt; str:
    def get_name(arg: Any) -&gt; str:
        if isinstance(arg, inspect.Parameter):
            return arg.name
        if isinstance(arg, nodes.Argument):
            return arg.variable.name
        raise AssertionError

    def get_type(arg: Any) -&gt; Optional[str]:
        if isinstance(arg, inspect.Parameter):
            return None
        if isinstance(arg, nodes.Argument):
            return str(arg.variable.type or arg.type_annotation)
        raise AssertionError

    def has_default(arg: Any) -&gt; bool:
        if isinstance(arg, inspect.Parameter):
            return arg.default != inspect.Parameter.empty
        if isinstance(arg, nodes.Argument):
            return arg.kind.is_optional()
        raise AssertionError

    def get_desc(arg: Any) -&gt; str:
        arg_type = get_type(arg)
        return (
            get_name(arg)
            + (f": {arg_type}" if arg_type else "")
            + (" = ..." if has_default(arg) else "")
        )

    kw_only = sorted(self.kwonly.values(), key=lambda a: (has_default(a), get_name(a)))
    ret = "def ("
    ret += ", ".join(
        [get_desc(arg) for arg in self.pos]
        + (["*" + get_name(self.varpos)] if self.varpos else (["*"] if self.kwonly else []))
        + [get_desc(arg) for arg in kw_only]
        + (["**" + get_name(self.varkw)] if self.varkw else [])
    )
    ret += ")"
    return ret

</t>
<t tx="ekr.20220525082935.775">@staticmethod
def from_funcitem(stub: nodes.FuncItem) -&gt; "Signature[nodes.Argument]":
    stub_sig: Signature[nodes.Argument] = Signature()
    stub_args = maybe_strip_cls(stub.name, stub.arguments)
    for stub_arg in stub_args:
        if stub_arg.kind.is_positional():
            stub_sig.pos.append(stub_arg)
        elif stub_arg.kind.is_named():
            stub_sig.kwonly[stub_arg.variable.name] = stub_arg
        elif stub_arg.kind == nodes.ARG_STAR:
            stub_sig.varpos = stub_arg
        elif stub_arg.kind == nodes.ARG_STAR2:
            stub_sig.varkw = stub_arg
        else:
            raise AssertionError
    return stub_sig

</t>
<t tx="ekr.20220525082935.776">@staticmethod
def from_inspect_signature(signature: inspect.Signature) -&gt; "Signature[inspect.Parameter]":
    runtime_sig: Signature[inspect.Parameter] = Signature()
    for runtime_arg in signature.parameters.values():
        if runtime_arg.kind in (
            inspect.Parameter.POSITIONAL_ONLY,
            inspect.Parameter.POSITIONAL_OR_KEYWORD,
        ):
            runtime_sig.pos.append(runtime_arg)
        elif runtime_arg.kind == inspect.Parameter.KEYWORD_ONLY:
            runtime_sig.kwonly[runtime_arg.name] = runtime_arg
        elif runtime_arg.kind == inspect.Parameter.VAR_POSITIONAL:
            runtime_sig.varpos = runtime_arg
        elif runtime_arg.kind == inspect.Parameter.VAR_KEYWORD:
            runtime_sig.varkw = runtime_arg
        else:
            raise AssertionError
    return runtime_sig

</t>
<t tx="ekr.20220525082935.777">@staticmethod
def from_overloadedfuncdef(stub: nodes.OverloadedFuncDef) -&gt; "Signature[nodes.Argument]":
    """Returns a Signature from an OverloadedFuncDef.

    If life were simple, to verify_overloadedfuncdef, we'd just verify_funcitem for each of its
    items. Unfortunately, life isn't simple and overloads are pretty deceitful. So instead, we
    try and combine the overload's items into a single signature that is compatible with any
    lies it might try to tell.

    """
    # For most dunder methods, just assume all args are positional-only
    assume_positional_only = is_dunder(stub.name, exclude_special=True)

    all_args: Dict[str, List[Tuple[nodes.Argument, int]]] = {}
    for func in map(_resolve_funcitem_from_decorator, stub.items):
        assert func is not None
        args = maybe_strip_cls(stub.name, func.arguments)
        for index, arg in enumerate(args):
            # For positional-only args, we allow overloads to have different names for the same
            # argument. To accomplish this, we just make up a fake index-based name.
            name = (
                f"__{index}"
                if arg.variable.name.startswith("__") or assume_positional_only
                else arg.variable.name
            )
            all_args.setdefault(name, []).append((arg, index))

    def get_position(arg_name: str) -&gt; int:
        # We just need this to return the positional args in the correct order.
        return max(index for _, index in all_args[arg_name])

    def get_type(arg_name: str) -&gt; mypy.types.ProperType:
        with mypy.state.state.strict_optional_set(True):
            all_types = [
                arg.variable.type or arg.type_annotation for arg, _ in all_args[arg_name]
            ]
            return mypy.typeops.make_simplified_union([t for t in all_types if t])

    def get_kind(arg_name: str) -&gt; nodes.ArgKind:
        kinds = {arg.kind for arg, _ in all_args[arg_name]}
        if nodes.ARG_STAR in kinds:
            return nodes.ARG_STAR
        if nodes.ARG_STAR2 in kinds:
            return nodes.ARG_STAR2
        # The logic here is based on two tenets:
        # 1) If an arg is ever optional (or unspecified), it is optional
        # 2) If an arg is ever positional, it is positional
        is_opt = (
            len(all_args[arg_name]) &lt; len(stub.items)
            or nodes.ARG_OPT in kinds
            or nodes.ARG_NAMED_OPT in kinds
        )
        is_pos = nodes.ARG_OPT in kinds or nodes.ARG_POS in kinds
        if is_opt:
            return nodes.ARG_OPT if is_pos else nodes.ARG_NAMED_OPT
        return nodes.ARG_POS if is_pos else nodes.ARG_NAMED

    sig: Signature[nodes.Argument] = Signature()
    for arg_name in sorted(all_args, key=get_position):
        # example_arg_name gives us a real name (in case we had a fake index-based name)
        example_arg_name = all_args[arg_name][0][0].variable.name
        arg = nodes.Argument(
            nodes.Var(example_arg_name, get_type(arg_name)),
            type_annotation=None,
            initializer=None,
            kind=get_kind(arg_name),
        )
        if arg.kind.is_positional():
            sig.pos.append(arg)
        elif arg.kind.is_named():
            sig.kwonly[arg.variable.name] = arg
        elif arg.kind == nodes.ARG_STAR:
            sig.varpos = arg
        elif arg.kind == nodes.ARG_STAR2:
            sig.varkw = arg
        else:
            raise AssertionError
    return sig


</t>
<t tx="ekr.20220525082935.778">def _verify_signature(
    stub: Signature[nodes.Argument], runtime: Signature[inspect.Parameter], function_name: str
) -&gt; Iterator[str]:
    # Check positional arguments match up
    for stub_arg, runtime_arg in zip(stub.pos, runtime.pos):
        yield from _verify_arg_name(stub_arg, runtime_arg, function_name)
        yield from _verify_arg_default_value(stub_arg, runtime_arg)
        if (
            runtime_arg.kind == inspect.Parameter.POSITIONAL_ONLY
            and not stub_arg.variable.name.startswith("__")
            and not stub_arg.variable.name.strip("_") == "self"
            and not is_dunder(function_name, exclude_special=True)  # noisy for dunder methods
        ):
            yield (
                'stub argument "{}" should be positional-only '
                '(rename with a leading double underscore, i.e. "__{}")'.format(
                    stub_arg.variable.name, runtime_arg.name
                )
            )
        if (
            runtime_arg.kind != inspect.Parameter.POSITIONAL_ONLY
            and stub_arg.variable.name.startswith("__")
            and not is_dunder(function_name, exclude_special=True)  # noisy for dunder methods
        ):
            yield (
                'stub argument "{}" should be positional or keyword '
                "(remove leading double underscore)".format(stub_arg.variable.name)
            )

    # Check unmatched positional args
    if len(stub.pos) &gt; len(runtime.pos):
        # There are cases where the stub exhaustively lists out the extra parameters the function
        # would take through *args. Hence, a) we can't check that the runtime actually takes those
        # parameters and b) below, we don't enforce that the stub takes *args, since runtime logic
        # may prevent those arguments from actually being accepted.
        if runtime.varpos is None:
            for stub_arg in stub.pos[len(runtime.pos):]:
                # If the variable is in runtime.kwonly, it's just mislabelled as not a
                # keyword-only argument
                if stub_arg.variable.name not in runtime.kwonly:
                    yield f'runtime does not have argument "{stub_arg.variable.name}"'
                else:
                    yield f'stub argument "{stub_arg.variable.name}" is not keyword-only'
            if stub.varpos is not None:
                yield f'runtime does not have *args argument "{stub.varpos.variable.name}"'
    elif len(stub.pos) &lt; len(runtime.pos):
        for runtime_arg in runtime.pos[len(stub.pos):]:
            if runtime_arg.name not in stub.kwonly:
                yield f'stub does not have argument "{runtime_arg.name}"'
            else:
                yield f'runtime argument "{runtime_arg.name}" is not keyword-only'

    # Checks involving *args
    if len(stub.pos) &lt;= len(runtime.pos) or runtime.varpos is None:
        if stub.varpos is None and runtime.varpos is not None:
            yield f'stub does not have *args argument "{runtime.varpos.name}"'
        if stub.varpos is not None and runtime.varpos is None:
            yield f'runtime does not have *args argument "{stub.varpos.variable.name}"'

    # Check keyword-only args
    for arg in sorted(set(stub.kwonly) &amp; set(runtime.kwonly)):
        stub_arg, runtime_arg = stub.kwonly[arg], runtime.kwonly[arg]
        yield from _verify_arg_name(stub_arg, runtime_arg, function_name)
        yield from _verify_arg_default_value(stub_arg, runtime_arg)

    # Check unmatched keyword-only args
    if runtime.varkw is None or not set(runtime.kwonly).issubset(set(stub.kwonly)):
        # There are cases where the stub exhaustively lists out the extra parameters the function
        # would take through *kwargs. Hence, a) we only check if the runtime actually takes those
        # parameters when the above condition holds and b) below, we don't enforce that the stub
        # takes *kwargs, since runtime logic may prevent additional arguments from actually being
        # accepted.
        for arg in sorted(set(stub.kwonly) - set(runtime.kwonly)):
            yield f'runtime does not have argument "{arg}"'
    for arg in sorted(set(runtime.kwonly) - set(stub.kwonly)):
        if arg in {stub_arg.variable.name for stub_arg in stub.pos}:
            # Don't report this if we've reported it before
            if len(stub.pos) &gt; len(runtime.pos) and runtime.varpos is not None:
                yield f'stub argument "{arg}" is not keyword-only'
        else:
            yield f'stub does not have argument "{arg}"'

    # Checks involving **kwargs
    if stub.varkw is None and runtime.varkw is not None:
        # As mentioned above, don't enforce that the stub takes **kwargs.
        # Also check against positional parameters, to avoid a nitpicky message when an argument
        # isn't marked as keyword-only
        stub_pos_names = {stub_arg.variable.name for stub_arg in stub.pos}
        # Ideally we'd do a strict subset check, but in practice the errors from that aren't useful
        if not set(runtime.kwonly).issubset(set(stub.kwonly) | stub_pos_names):
            yield f'stub does not have **kwargs argument "{runtime.varkw.name}"'
    if stub.varkw is not None and runtime.varkw is None:
        yield f'runtime does not have **kwargs argument "{stub.varkw.variable.name}"'


</t>
<t tx="ekr.20220525082935.779">@verify.register(nodes.FuncItem)
def verify_funcitem(
    stub: nodes.FuncItem, runtime: MaybeMissing[Any], object_path: List[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        yield Error(object_path, "is not present at runtime", stub, runtime)
        return

    if not is_probably_a_function(runtime):
        yield Error(object_path, "is not a function", stub, runtime)
        if not callable(runtime):
            return

    for message in _verify_static_class_methods(stub, runtime, object_path):
        yield Error(object_path, "is inconsistent, " + message, stub, runtime)

    signature = safe_inspect_signature(runtime)
    runtime_is_coroutine = inspect.iscoroutinefunction(runtime)

    if signature:
        stub_sig = Signature.from_funcitem(stub)
        runtime_sig = Signature.from_inspect_signature(signature)
        runtime_sig_desc = f'{"async " if runtime_is_coroutine else ""}def {signature}'
        stub_desc = f'def {stub_sig!r}'
    else:
        runtime_sig_desc, stub_desc = None, None

    # Don't raise an error if the stub is a coroutine, but the runtime isn't.
    # That results in false positives.
    # See https://github.com/python/typeshed/issues/7344
    if runtime_is_coroutine and not stub.is_coroutine:
        yield Error(
            object_path,
            'is an "async def" function at runtime, but not in the stub',
            stub,
            runtime,
            stub_desc=stub_desc,
            runtime_desc=runtime_sig_desc
        )

    if not signature:
        return

    for message in _verify_signature(stub_sig, runtime_sig, function_name=stub.name):
        yield Error(
            object_path,
            "is inconsistent, " + message,
            stub,
            runtime,
            runtime_desc=runtime_sig_desc,
        )


</t>
<t tx="ekr.20220525082935.78">def visit_assignment_stmt(self, s: AssignmentStmt) -&gt; None:
    self.statement = s

    # Special case assignment like X = X.
    if self.analyze_identity_global_assignment(s):
        return

    tag = self.track_incomplete_refs()
    s.rvalue.accept(self)
    if self.found_incomplete_ref(tag) or self.should_wait_rhs(s.rvalue):
        # Initializer couldn't be fully analyzed. Defer the current node and give up.
        # Make sure that if we skip the definition of some local names, they can't be
        # added later in this scope, since an earlier definition should take precedence.
        for expr in names_modified_by_assignment(s):
            self.mark_incomplete(expr.name, expr)
        return

    # The r.h.s. is now ready to be classified, first check if it is a special form:
    special_form = False
    # * type alias
    if self.check_and_set_up_type_alias(s):
        s.is_alias_def = True
        special_form = True
    # * type variable definition
    elif self.process_typevar_declaration(s):
        special_form = True
    elif self.process_paramspec_declaration(s):
        special_form = True
    elif self.process_typevartuple_declaration(s):
        special_form = True
    # * type constructors
    elif self.analyze_namedtuple_assign(s):
        special_form = True
    elif self.analyze_typeddict_assign(s):
        special_form = True
    elif self.newtype_analyzer.process_newtype_declaration(s):
        special_form = True
    elif self.analyze_enum_assign(s):
        special_form = True

    if special_form:
        self.record_special_form_lvalue(s)
        return
    # Clear the alias flag if assignment turns out not a special form after all. It
    # may be set to True while there were still placeholders due to forward refs.
    s.is_alias_def = False

    # OK, this is a regular assignment, perform the necessary analysis steps.
    s.is_final_def = self.unwrap_final(s)
    self.analyze_lvalues(s)
    self.check_final_implicit_def(s)
    self.store_final_status(s)
    self.check_classvar(s)
    self.process_type_annotation(s)
    self.apply_dynamic_class_hook(s)
    if not s.type:
        self.process_module_assignment(s.lvalues, s.rvalue, s)
    self.process__all__(s)
    self.process__deletable__(s)
    self.process__slots__(s)

</t>
<t tx="ekr.20220525082935.780">@verify.register(Missing)
def verify_none(
    stub: Missing, runtime: MaybeMissing[Any], object_path: List[str]
) -&gt; Iterator[Error]:
    yield Error(object_path, "is not present in stub", stub, runtime)


</t>
<t tx="ekr.20220525082935.781">@verify.register(nodes.Var)
def verify_var(
    stub: nodes.Var, runtime: MaybeMissing[Any], object_path: List[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        # Don't always yield an error here, because we often can't find instance variables
        if len(object_path) &lt;= 2:
            yield Error(object_path, "is not present at runtime", stub, runtime)
        return

    if (
        stub.is_initialized_in_class
        and is_read_only_property(runtime)
        and (stub.is_settable_property or not stub.is_property)
    ):
        yield Error(
            object_path,
            "is read-only at runtime but not in the stub",
            stub,
            runtime
        )

    runtime_type = get_mypy_type_of_runtime_value(runtime)
    if (
        runtime_type is not None
        and stub.type is not None
        and not is_subtype_helper(runtime_type, stub.type)
    ):
        should_error = True
        # Avoid errors when defining enums, since runtime_type is the enum itself, but we'd
        # annotate it with the type of runtime.value
        if isinstance(runtime, enum.Enum):
            runtime_type = get_mypy_type_of_runtime_value(runtime.value)
            if runtime_type is not None and is_subtype_helper(runtime_type, stub.type):
                should_error = False

        if should_error:
            yield Error(
                object_path,
                f"variable differs from runtime type {runtime_type}",
                stub,
                runtime,
            )


</t>
<t tx="ekr.20220525082935.782">@verify.register(nodes.OverloadedFuncDef)
def verify_overloadedfuncdef(
    stub: nodes.OverloadedFuncDef, runtime: MaybeMissing[Any], object_path: List[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        yield Error(object_path, "is not present at runtime", stub, runtime)
        return

    if stub.is_property:
        # Any property with a setter is represented as an OverloadedFuncDef
        if is_read_only_property(runtime):
            yield Error(
                object_path,
                "is read-only at runtime but not in the stub",
                stub,
                runtime
            )
        return

    if not is_probably_a_function(runtime):
        yield Error(object_path, "is not a function", stub, runtime)
        if not callable(runtime):
            return

    for message in _verify_static_class_methods(stub, runtime, object_path):
        yield Error(object_path, "is inconsistent, " + message, stub, runtime)

    signature = safe_inspect_signature(runtime)
    if not signature:
        return

    stub_sig = Signature.from_overloadedfuncdef(stub)
    runtime_sig = Signature.from_inspect_signature(signature)

    for message in _verify_signature(stub_sig, runtime_sig, function_name=stub.name):
        # TODO: This is a little hacky, but the addition here is super useful
        if "has a default value of type" in message:
            message += (
                ". This is often caused by overloads failing to account for explicitly passing "
                "in the default value."
            )
        yield Error(
            object_path,
            "is inconsistent, " + message,
            stub,
            runtime,
            stub_desc=str(stub.type) + f"\nInferred signature: {stub_sig}",
            runtime_desc="def " + str(signature),
        )


</t>
<t tx="ekr.20220525082935.783">@verify.register(nodes.TypeVarExpr)
def verify_typevarexpr(
    stub: nodes.TypeVarExpr, runtime: MaybeMissing[Any], object_path: List[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        # We seem to insert these typevars into NamedTuple stubs, but they
        # don't exist at runtime. Just ignore!
        if stub.name == "_NT":
            return
        yield Error(object_path, "is not present at runtime", stub, runtime)
        return
    if not isinstance(runtime, TypeVar):
        yield Error(object_path, "is not a TypeVar", stub, runtime)
        return


@verify.register(nodes.ParamSpecExpr)
def verify_paramspecexpr(
    stub: nodes.ParamSpecExpr, runtime: MaybeMissing[Any], object_path: List[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        yield Error(object_path, "is not present at runtime", stub, runtime)
        return
    maybe_paramspec_types = (
        getattr(typing, "ParamSpec", None), getattr(typing_extensions, "ParamSpec", None)
    )
    paramspec_types = tuple([t for t in maybe_paramspec_types if t is not None])
    if not paramspec_types or not isinstance(runtime, paramspec_types):
        yield Error(object_path, "is not a ParamSpec", stub, runtime)
        return


</t>
<t tx="ekr.20220525082935.784">def _verify_readonly_property(stub: nodes.Decorator, runtime: Any) -&gt; Iterator[str]:
    assert stub.func.is_property
    if isinstance(runtime, property):
        return
    if inspect.isdatadescriptor(runtime):
        # It's enough like a property...
        return
    # Sometimes attributes pretend to be properties, for instance, to express that they
    # are read only. So allowlist if runtime_type matches the return type of stub.
    runtime_type = get_mypy_type_of_runtime_value(runtime)
    func_type = (
        stub.func.type.ret_type if isinstance(stub.func.type, mypy.types.CallableType) else None
    )
    if (
        runtime_type is not None
        and func_type is not None
        and is_subtype_helper(runtime_type, func_type)
    ):
        return
    yield "is inconsistent, cannot reconcile @property on stub with runtime object"


</t>
<t tx="ekr.20220525082935.785">def _resolve_funcitem_from_decorator(dec: nodes.OverloadPart) -&gt; Optional[nodes.FuncItem]:
    """Returns a FuncItem that corresponds to the output of the decorator.

    Returns None if we can't figure out what that would be. For convenience, this function also
    accepts FuncItems.
    """
    if isinstance(dec, nodes.FuncItem):
        return dec
    if dec.func.is_property:
        return None

    @others
    func: nodes.FuncItem = dec.func
    for decorator in dec.original_decorators:
        resulting_func = apply_decorator_to_funcitem(decorator, func)
        if resulting_func is None:
            return None
        func = resulting_func
    return func


</t>
<t tx="ekr.20220525082935.786">def apply_decorator_to_funcitem(
    decorator: nodes.Expression, func: nodes.FuncItem
) -&gt; Optional[nodes.FuncItem]:
    if not isinstance(decorator, nodes.RefExpr):
        return None
    if decorator.fullname is None:
        # Happens with namedtuple
        return None
    if decorator.fullname in (
        "builtins.staticmethod",
        "abc.abstractmethod",
    ) or decorator.fullname in mypy.types.OVERLOAD_NAMES:
        return func
    if decorator.fullname == "builtins.classmethod":
        if func.arguments[0].variable.name not in ("cls", "mcs", "metacls"):
            raise StubtestFailure(
                f"unexpected class argument name {func.arguments[0].variable.name!r} "
                f"in {dec.fullname}"
            )
        # FuncItem is written so that copy.copy() actually works, even when compiled
        ret = copy.copy(func)
        # Remove the cls argument, since it's not present in inspect.signature of classmethods
        ret.arguments = ret.arguments[1:]
        return ret
    # Just give up on any other decorators. After excluding properties, we don't run into
    # anything else when running on typeshed's stdlib.
    return None

</t>
<t tx="ekr.20220525082935.787">@verify.register(nodes.Decorator)
def verify_decorator(
    stub: nodes.Decorator, runtime: MaybeMissing[Any], object_path: List[str]
) -&gt; Iterator[Error]:
    if isinstance(runtime, Missing):
        yield Error(object_path, "is not present at runtime", stub, runtime)
        return
    if stub.func.is_property:
        for message in _verify_readonly_property(stub, runtime):
            yield Error(object_path, message, stub, runtime)
        return

    func = _resolve_funcitem_from_decorator(stub)
    if func is not None:
        yield from verify(func, runtime, object_path)


</t>
<t tx="ekr.20220525082935.788">@verify.register(nodes.TypeAlias)
def verify_typealias(
    stub: nodes.TypeAlias, runtime: MaybeMissing[Any], object_path: List[str]
) -&gt; Iterator[Error]:
    stub_target = mypy.types.get_proper_type(stub.target)
    if isinstance(runtime, Missing):
        yield Error(
            object_path, "is not present at runtime", stub, runtime,
            stub_desc=f"Type alias for: {stub_target}"
        )
        return
    if isinstance(stub_target, mypy.types.Instance):
        yield from verify(stub_target.type, runtime, object_path)
        return
    if isinstance(stub_target, mypy.types.UnionType):
        if not getattr(runtime, "__origin__", None) is Union:
            yield Error(object_path, "is not a Union", stub, runtime, stub_desc=str(stub_target))
        # could check Union contents here...
        return
    if isinstance(stub_target, mypy.types.TupleType):
        if tuple not in getattr(runtime, "__mro__", ()):
            yield Error(
                object_path, "is not a subclass of tuple", stub, runtime,
                stub_desc=str(stub_target)
            )
        # could check Tuple contents here...
        return
    if isinstance(stub_target, mypy.types.AnyType):
        return
    yield Error(
        object_path, "is not a recognised type alias", stub, runtime, stub_desc=str(stub_target)
    )


</t>
<t tx="ekr.20220525082935.789"># ====================
# Helpers
# ====================


IGNORED_MODULE_DUNDERS = frozenset(
    {
        "__file__",
        "__doc__",
        "__name__",
        "__builtins__",
        "__package__",
        "__cached__",
        "__loader__",
        "__spec__",
        "__annotations__",
        "__path__",  # mypy adds __path__ to packages, but C packages don't have it
        "__getattr__",  # resulting behaviour might be typed explicitly
        # TODO: remove the following from this list
        "__author__",
        "__version__",
        "__copyright__",
    }
)

IGNORABLE_CLASS_DUNDERS = frozenset(
    {
        # Special attributes
        "__dict__",
        "__text_signature__",
        "__weakref__",
        "__del__",  # Only ever called when an object is being deleted, who cares?
        "__hash__",
        "__getattr__",  # resulting behaviour might be typed explicitly
        "__setattr__",  # defining this on a class can cause worse type checking
        # isinstance/issubclass hooks that type-checkers don't usually care about
        "__instancecheck__",
        "__subclasshook__",
        "__subclasscheck__",
        # Pickle methods
        "__setstate__",
        "__getstate__",
        "__getnewargs__",
        "__getinitargs__",
        "__reduce_ex__",
        "__reduce__",
        # ctypes weirdness
        "__ctype_be__",
        "__ctype_le__",
        "__ctypes_from_outparam__",
        # mypy limitations
        "__abstractmethods__",  # Classes with metaclass=ABCMeta inherit this attribute
        "__new_member__",  # If an enum defines __new__, the method is renamed as __new_member__
        "__dataclass_fields__",  # Generated by dataclasses
        "__dataclass_params__",  # Generated by dataclasses
        "__doc__",  # mypy's semanal for namedtuples assumes this is str, not Optional[str]
        # typing implementation details, consider removing some of these:
        "__parameters__",
        "__origin__",
        "__args__",
        "__orig_bases__",
        "__final__",
        # Consider removing __slots__?
        "__slots__",
    }
)


</t>
<t tx="ekr.20220525082935.79">def analyze_identity_global_assignment(self, s: AssignmentStmt) -&gt; bool:
    """Special case 'X = X' in global scope.

    This allows supporting some important use cases.

    Return true if special casing was applied.
    """
    if not isinstance(s.rvalue, NameExpr) or len(s.lvalues) != 1:
        # Not of form 'X = X'
        return False
    lvalue = s.lvalues[0]
    if not isinstance(lvalue, NameExpr) or s.rvalue.name != lvalue.name:
        # Not of form 'X = X'
        return False
    if self.type is not None or self.is_func_scope():
        # Not in global scope
        return False
    # It's an assignment like 'X = X' in the global scope.
    name = lvalue.name
    sym = self.lookup(name, s)
    if sym is None:
        if self.final_iteration:
            # Fall back to normal assignment analysis.
            return False
        else:
            self.defer()
            return True
    else:
        if sym.node is None:
            # Something special -- fall back to normal assignment analysis.
            return False
        if name not in self.globals:
            # The name is from builtins. Add an alias to the current module.
            self.add_symbol(name, sym.node, s)
        if not isinstance(sym.node, PlaceholderNode):
            for node in s.rvalue, lvalue:
                node.node = sym.node
                node.kind = GDEF
                node.fullname = sym.node.fullname
        return True

</t>
<t tx="ekr.20220525082935.790">def is_probably_private(name: str) -&gt; bool:
    return name.startswith("_") and not is_dunder(name)


</t>
<t tx="ekr.20220525082935.791">def is_probably_a_function(runtime: Any) -&gt; bool:
    return (
        isinstance(runtime, (types.FunctionType, types.BuiltinFunctionType))
        or isinstance(runtime, (types.MethodType, types.BuiltinMethodType))
        or (inspect.ismethoddescriptor(runtime) and callable(runtime))
    )


</t>
<t tx="ekr.20220525082935.792">def is_read_only_property(runtime: object) -&gt; bool:
    return isinstance(runtime, property) and runtime.fset is None


</t>
<t tx="ekr.20220525082935.793">def safe_inspect_signature(runtime: Any) -&gt; Optional[inspect.Signature]:
    try:
        return inspect.signature(runtime)
    except Exception:
        # inspect.signature throws ValueError all the time
        # catch RuntimeError because of https://bugs.python.org/issue39504
        # catch TypeError because of https://github.com/python/typeshed/pull/5762
        # catch AttributeError because of inspect.signature(_curses.window.border)
        return None


</t>
<t tx="ekr.20220525082935.794">def is_subtype_helper(left: mypy.types.Type, right: mypy.types.Type) -&gt; bool:
    """Checks whether ``left`` is a subtype of ``right``."""
    left = mypy.types.get_proper_type(left)
    right = mypy.types.get_proper_type(right)
    if (
        isinstance(left, mypy.types.LiteralType)
        and isinstance(left.value, int)
        and left.value in (0, 1)
        and isinstance(right, mypy.types.Instance)
        and right.type.fullname == "builtins.bool"
    ):
        # Pretend Literal[0, 1] is a subtype of bool to avoid unhelpful errors.
        return True

    if (
        isinstance(right, mypy.types.TypedDictType)
        and isinstance(left, mypy.types.Instance)
        and left.type.fullname == "builtins.dict"
    ):
        # Special case checks against TypedDicts
        return True

    with mypy.state.state.strict_optional_set(True):
        return mypy.subtypes.is_subtype(left, right)


</t>
<t tx="ekr.20220525082935.795">def get_mypy_type_of_runtime_value(runtime: Any) -&gt; Optional[mypy.types.Type]:
    """Returns a mypy type object representing the type of ``runtime``.

    Returns None if we can't find something that works.

    """
    if runtime is None:
        return mypy.types.NoneType()
    if isinstance(runtime, property):
        # Give up on properties to avoid issues with things that are typed as attributes.
        return None

    @others
    if isinstance(
        runtime,
        (types.FunctionType, types.BuiltinFunctionType,
        types.MethodType, types.BuiltinMethodType)
    ):
        builtins = get_stub("builtins")
        assert builtins is not None
        type_info = builtins.names["function"].node
        assert isinstance(type_info, nodes.TypeInfo)
        fallback = mypy.types.Instance(type_info, [anytype()])
        signature = safe_inspect_signature(runtime)
        if signature:
            arg_types = []
            arg_kinds = []
            arg_names = []
            for arg in signature.parameters.values():
                arg_types.append(anytype())
                arg_names.append(
                    None if arg.kind == inspect.Parameter.POSITIONAL_ONLY else arg.name
                )
                has_default = arg.default == inspect.Parameter.empty
                if arg.kind == inspect.Parameter.POSITIONAL_ONLY:
                    arg_kinds.append(nodes.ARG_POS if has_default else nodes.ARG_OPT)
                elif arg.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:
                    arg_kinds.append(nodes.ARG_POS if has_default else nodes.ARG_OPT)
                elif arg.kind == inspect.Parameter.KEYWORD_ONLY:
                    arg_kinds.append(nodes.ARG_NAMED if has_default else nodes.ARG_NAMED_OPT)
                elif arg.kind == inspect.Parameter.VAR_POSITIONAL:
                    arg_kinds.append(nodes.ARG_STAR)
                elif arg.kind == inspect.Parameter.VAR_KEYWORD:
                    arg_kinds.append(nodes.ARG_STAR2)
                else:
                    raise AssertionError
        else:
            arg_types = [anytype(), anytype()]
            arg_kinds = [nodes.ARG_STAR, nodes.ARG_STAR2]
            arg_names = [None, None]

        return mypy.types.CallableType(
            arg_types,
            arg_kinds,
            arg_names,
            ret_type=anytype(),
            fallback=fallback,
            is_ellipsis_args=True,
        )

    # Try and look up a stub for the runtime object
    stub = get_stub(type(runtime).__module__)
    if stub is None:
        return None
    type_name = type(runtime).__name__
    if type_name not in stub.names:
        return None
    type_info = stub.names[type_name].node
    if isinstance(type_info, nodes.Var):
        return type_info.type
    if not isinstance(type_info, nodes.TypeInfo):
        return None

    if isinstance(runtime, tuple):
        # Special case tuples so we construct a valid mypy.types.TupleType
        optional_items = [get_mypy_type_of_runtime_value(v) for v in runtime]
        items = [(i if i is not None else anytype()) for i in optional_items]
        fallback = mypy.types.Instance(type_info, [anytype()])
        return mypy.types.TupleType(items, fallback)

    fallback = mypy.types.Instance(type_info, [anytype() for _ in type_info.type_vars])

    value: Union[bool, int, str]
    if isinstance(runtime, bytes):
        value = bytes_to_human_readable_repr(runtime)
    elif isinstance(runtime, enum.Enum):
        value = runtime.name
    elif isinstance(runtime, (bool, int, str)):
        value = runtime
    else:
        return fallback

    return mypy.types.LiteralType(value=value, fallback=fallback)


</t>
<t tx="ekr.20220525082935.796">def anytype() -&gt; mypy.types.AnyType:
    return mypy.types.AnyType(mypy.types.TypeOfAny.unannotated)

</t>
<t tx="ekr.20220525082935.797"># ====================
# Build and entrypoint
# ====================


_all_stubs: Dict[str, nodes.MypyFile] = {}


</t>
<t tx="ekr.20220525082935.798">def build_stubs(modules: List[str], options: Options, find_submodules: bool = False) -&gt; List[str]:
    """Uses mypy to construct stub objects for the given modules.

    This sets global state that ``get_stub`` can access.

    Returns all modules we might want to check. If ``find_submodules`` is False, this is equal
    to ``modules``.

    :param modules: List of modules to build stubs for.
    :param options: Mypy options for finding and building stubs.
    :param find_submodules: Whether to attempt to find submodules of the given modules as well.

    """
    data_dir = mypy.build.default_data_dir()
    search_path = mypy.modulefinder.compute_search_paths([], options, data_dir)
    find_module_cache = mypy.modulefinder.FindModuleCache(
        search_path, fscache=None, options=options
    )

    all_modules = []
    sources = []
    for module in modules:
        all_modules.append(module)
        if not find_submodules:
            module_path = find_module_cache.find_module(module)
            if not isinstance(module_path, str):
                # test_module will yield an error later when it can't find stubs
                continue
            sources.append(mypy.modulefinder.BuildSource(module_path, module, None))
        else:
            found_sources = find_module_cache.find_modules_recursive(module)
            sources.extend(found_sources)
            all_modules.extend(s.module for s in found_sources if s.module not in all_modules)

    if sources:
        try:
            res = mypy.build.build(sources=sources, options=options)
        except mypy.errors.CompileError as e:
            raise StubtestFailure(f"failed mypy compile:\n{e}") from e
        if res.errors:
            raise StubtestFailure("mypy build errors:\n" + "\n".join(res.errors))

        global _all_stubs
        _all_stubs = res.files

    return all_modules


</t>
<t tx="ekr.20220525082935.799">def get_stub(module: str) -&gt; Optional[nodes.MypyFile]:
    """Returns a stub object for the given module, if we've built one."""
    return _all_stubs.get(module)


</t>
<t tx="ekr.20220525082935.8">def prepare_typing_namespace(self, file_node: MypyFile,
                             aliases: Dict[str, str]) -&gt; None:
    """Remove dummy alias definitions such as List = TypeAlias(object) from typing.

    They will be replaced with real aliases when corresponding targets are ready.
    """
    # This is all pretty unfortunate. typeshed now has a
    # sys.version_info check for OrderedDict, and we shouldn't
    # take it out, because it is correct and a typechecker should
    # use that as a source of truth. But instead we rummage
    # through IfStmts to remove the info first.  (I tried to
    # remove this whole machinery and ran into issues with the
    # builtins/typing import cycle.)
    def helper(defs: List[Statement]) -&gt; None:
        for stmt in defs.copy():
            if isinstance(stmt, IfStmt):
                for body in stmt.body:
                    helper(body.body)
                if stmt.else_body:
                    helper(stmt.else_body.body)
            if (isinstance(stmt, AssignmentStmt) and len(stmt.lvalues) == 1 and
                    isinstance(stmt.lvalues[0], NameExpr)):
                # Assignment to a simple name, remove it if it is a dummy alias.
                if f'{file_node.fullname}.{stmt.lvalues[0].name}' in aliases:
                    defs.remove(stmt)

    helper(file_node.defs)

</t>
<t tx="ekr.20220525082935.80">def should_wait_rhs(self, rv: Expression) -&gt; bool:
    """Can we already classify this r.h.s. of an assignment or should we wait?

    This returns True if we don't have enough information to decide whether
    an assignment is just a normal variable definition or a special form.
    Always return False if this is a final iteration. This will typically cause
    the lvalue to be classified as a variable plus emit an error.
    """
    if self.final_iteration:
        # No chance, nothing has changed.
        return False
    if isinstance(rv, NameExpr):
        n = self.lookup(rv.name, rv)
        if n and isinstance(n.node, PlaceholderNode) and not n.node.becomes_typeinfo:
            return True
    elif isinstance(rv, MemberExpr):
        fname = get_member_expr_fullname(rv)
        if fname:
            n = self.lookup_qualified(fname, rv, suppress_errors=True)
            if n and isinstance(n.node, PlaceholderNode) and not n.node.becomes_typeinfo:
                return True
    elif isinstance(rv, IndexExpr) and isinstance(rv.base, RefExpr):
        return self.should_wait_rhs(rv.base)
    elif isinstance(rv, CallExpr) and isinstance(rv.callee, RefExpr):
        # This is only relevant for builtin SCC where things like 'TypeVar'
        # may be not ready.
        return self.should_wait_rhs(rv.callee)
    return False

</t>
<t tx="ekr.20220525082935.800">def get_typeshed_stdlib_modules(
    custom_typeshed_dir: Optional[str],
    version_info: Optional[Tuple[int, int]] = None
) -&gt; List[str]:
    """Returns a list of stdlib modules in typeshed (for current Python version)."""
    stdlib_py_versions = mypy.modulefinder.load_stdlib_py_versions(custom_typeshed_dir)
    if version_info is None:
        version_info = sys.version_info[0:2]
    # Typeshed's minimum supported Python 3 is Python 3.6
    if sys.version_info &lt; (3, 6):
        version_info = (3, 6)

    @others
    if custom_typeshed_dir:
        typeshed_dir = Path(custom_typeshed_dir)
    else:
        typeshed_dir = Path(mypy.build.default_data_dir()) / "typeshed"
    stdlib_dir = typeshed_dir / "stdlib"

    modules = []
    for path in stdlib_dir.rglob("*.pyi"):
        if path.stem == "__init__":
            path = path.parent
        module = ".".join(path.relative_to(stdlib_dir).parts[:-1] + (path.stem,))
        if exists_in_version(module):
            modules.append(module)
    return sorted(modules)


</t>
<t tx="ekr.20220525082935.801">def exists_in_version(module: str) -&gt; bool:
    assert version_info is not None
    parts = module.split(".")
    for i in range(len(parts), 0, -1):
        current_module = ".".join(parts[:i])
        if current_module in stdlib_py_versions:
            minver, maxver = stdlib_py_versions[current_module]
            return version_info &gt;= minver and (maxver is None or version_info &lt;= maxver)
    return False

</t>
<t tx="ekr.20220525082935.802">def get_allowlist_entries(allowlist_file: str) -&gt; Iterator[str]:
    @others
    with open(allowlist_file) as f:
        for line in f.readlines():
            entry = strip_comments(line)
            if entry:
                yield entry


</t>
<t tx="ekr.20220525082935.803">def strip_comments(s: str) -&gt; str:
    try:
        return s[: s.index("#")].strip()
    except ValueError:
        return s.strip()

</t>
<t tx="ekr.20220525082935.804">class _Arguments:
    modules: List[str]
    concise: bool
    ignore_missing_stub: bool
    ignore_positional_only: bool
    allowlist: List[str]
    generate_allowlist: bool
    ignore_unused_allowlist: bool
    mypy_config_file: str
    custom_typeshed_dir: str
    check_typeshed: bool
    version: str


def test_stubs(args: _Arguments, use_builtins_fixtures: bool = False) -&gt; int:
    """This is stubtest! It's time to test the stubs!"""
    # Load the allowlist. This is a series of strings corresponding to Error.object_desc
    # Values in the dict will store whether we used the allowlist entry or not.
    allowlist = {
        entry: False
        for allowlist_file in args.allowlist
        for entry in get_allowlist_entries(allowlist_file)
    }
    allowlist_regexes = {entry: re.compile(entry) for entry in allowlist}

    # If we need to generate an allowlist, we store Error.object_desc for each error here.
    generated_allowlist = set()

    modules = args.modules
    if args.check_typeshed:
        if args.modules:
            print(
                _style("error:", color="red", bold=True),
                "cannot pass both --check-typeshed and a list of modules",
            )
            return 1
        modules = get_typeshed_stdlib_modules(args.custom_typeshed_dir)
        # typeshed added a stub for __main__, but that causes stubtest to check itself
        annoying_modules = {"antigravity", "this", "__main__"}
        modules = [m for m in modules if m not in annoying_modules]

    if not modules:
        print(
            _style("error:", color="red", bold=True),
            "no modules to check",
        )
        return 1

    options = Options()
    options.incremental = False
    options.custom_typeshed_dir = args.custom_typeshed_dir
    options.config_file = args.mypy_config_file
    options.use_builtins_fixtures = use_builtins_fixtures

    if options.config_file:
        def set_strict_flags() -&gt; None:  # not needed yet
            return
        parse_config_file(options, set_strict_flags, options.config_file, sys.stdout, sys.stderr)

    try:
        modules = build_stubs(modules, options, find_submodules=not args.check_typeshed)
    except StubtestFailure as stubtest_failure:
        print(
            _style("error:", color="red", bold=True),
            f"not checking stubs due to {stubtest_failure}",
        )
        return 1

    exit_code = 0
    error_count = 0
    for module in modules:
        for error in test_module(module):
            # Filter errors
            if args.ignore_missing_stub and error.is_missing_stub():
                continue
            if args.ignore_positional_only and error.is_positional_only_related():
                continue
            if error.object_desc in allowlist:
                allowlist[error.object_desc] = True
                continue
            is_allowlisted = False
            for w in allowlist:
                if allowlist_regexes[w].fullmatch(error.object_desc):
                    allowlist[w] = True
                    is_allowlisted = True
                    break
            if is_allowlisted:
                continue

            # We have errors, so change exit code, and output whatever necessary
            exit_code = 1
            if args.generate_allowlist:
                generated_allowlist.add(error.object_desc)
                continue
            print(error.get_description(concise=args.concise))
            error_count += 1

    # Print unused allowlist entries
    if not args.ignore_unused_allowlist:
        for w in allowlist:
            # Don't consider an entry unused if it regex-matches the empty string
            # This lets us allowlist errors that don't manifest at all on some systems
            if not allowlist[w] and not allowlist_regexes[w].fullmatch(""):
                exit_code = 1
                error_count += 1
                print(f"note: unused allowlist entry {w}")

    # Print the generated allowlist
    if args.generate_allowlist:
        for e in sorted(generated_allowlist):
            print(e)
        exit_code = 0
    elif not args.concise:
        if error_count:
            print(
                _style(
                    f"Found {error_count} error{plural_s(error_count)}"
                    f" (checked {len(modules)} module{plural_s(modules)})",
                    color="red", bold=True
                )
            )
        else:
            print(
                _style(
                    f"Success: no issues found in {len(modules)} module{plural_s(modules)}",
                    color="green", bold=True
                )
            )

    return exit_code


</t>
<t tx="ekr.20220525082935.805">def parse_options(args: List[str]) -&gt; _Arguments:
    parser = argparse.ArgumentParser(
        description="Compares stubs to objects introspected from the runtime."
    )
    parser.add_argument("modules", nargs="*", help="Modules to test")
    parser.add_argument(
        "--concise",
        action="store_true",
        help="Makes stubtest's output more concise, one line per error",
    )
    parser.add_argument(
        "--ignore-missing-stub",
        action="store_true",
        help="Ignore errors for stub missing things that are present at runtime",
    )
    parser.add_argument(
        "--ignore-positional-only",
        action="store_true",
        help="Ignore errors for whether an argument should or shouldn't be positional-only",
    )
    parser.add_argument(
        "--allowlist",
        "--whitelist",
        action="append",
        metavar="FILE",
        default=[],
        help=(
            "Use file as an allowlist. Can be passed multiple times to combine multiple "
            "allowlists. Allowlists can be created with --generate-allowlist. Allowlists "
            "support regular expressions."
        ),
    )
    parser.add_argument(
        "--generate-allowlist",
        "--generate-whitelist",
        action="store_true",
        help="Print an allowlist (to stdout) to be used with --allowlist",
    )
    parser.add_argument(
        "--ignore-unused-allowlist",
        "--ignore-unused-whitelist",
        action="store_true",
        help="Ignore unused allowlist entries",
    )
    parser.add_argument(
        "--mypy-config-file",
        metavar="FILE",
        help=(
            "Use specified mypy config file to determine mypy plugins "
            "and mypy path"
        ),
    )
    parser.add_argument(
        "--custom-typeshed-dir", metavar="DIR", help="Use the custom typeshed in DIR"
    )
    parser.add_argument(
        "--check-typeshed", action="store_true", help="Check all stdlib modules in typeshed"
    )
    parser.add_argument(
        "--version", action="version", version="%(prog)s " + mypy.version.__version__
    )

    return parser.parse_args(args, namespace=_Arguments())


</t>
<t tx="ekr.20220525082935.806">def main() -&gt; int:
    mypy.util.check_python_version("stubtest")
    return test_stubs(parse_options(sys.argv[1:]))


</t>
<t tx="ekr.20220525082935.807">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Utilities for mypy.stubgen, mypy.stubgenc, and mypy.stubdoc modules."""

import sys
import os.path
import json
import subprocess
import re
from contextlib import contextmanager

from typing import Optional, Tuple, List, Iterator, Union
from typing_extensions import overload

from mypy.moduleinspect import ModuleInspect, InspectError
from mypy.modulefinder import ModuleNotFoundReason


# Modules that may fail when imported, or that may have side effects (fully qualified).
NOT_IMPORTABLE_MODULES = ()


@others
</t>
<t tx="ekr.20220525082935.808">class CantImport(Exception):
    def __init__(self, module: str, message: str):
        self.module = module
        self.message = message


</t>
<t tx="ekr.20220525082935.809">def default_py2_interpreter() -&gt; str:
    """Find a system Python 2 interpreter.

    Return full path or exit if failed.
    """
    # TODO: Make this do something reasonable in Windows.
    for candidate in ('/usr/bin/python2', '/usr/bin/python'):
        if not os.path.exists(candidate):
            continue
        output = subprocess.check_output([candidate, '--version'],
                                         stderr=subprocess.STDOUT).strip()
        if b'Python 2' in output:
            return candidate
    raise SystemExit("Can't find a Python 2 interpreter -- "
                     "please use the --python-executable option")


</t>
<t tx="ekr.20220525082935.81">def can_be_type_alias(self, rv: Expression, allow_none: bool = False) -&gt; bool:
    """Is this a valid r.h.s. for an alias definition?

    Note: this function should be only called for expressions where self.should_wait_rhs()
    returns False.
    """
    if isinstance(rv, RefExpr) and self.is_type_ref(rv, bare=True):
        return True
    if isinstance(rv, IndexExpr) and self.is_type_ref(rv.base, bare=False):
        return True
    if self.is_none_alias(rv):
        return True
    if allow_none and isinstance(rv, NameExpr) and rv.fullname == 'builtins.None':
        return True
    if isinstance(rv, OpExpr) and rv.op == '|':
        if self.is_stub_file:
            return True
        if (
            self.can_be_type_alias(rv.left, allow_none=True)
            and self.can_be_type_alias(rv.right, allow_none=True)
        ):
            return True
    return False

</t>
<t tx="ekr.20220525082935.810">def walk_packages(inspect: ModuleInspect,
                  packages: List[str],
                  verbose: bool = False) -&gt; Iterator[str]:
    """Iterates through all packages and sub-packages in the given list.

    This uses runtime imports (in another process) to find both Python and C modules.
    For Python packages we simply pass the __path__ attribute to pkgutil.walk_packages() to
    get the content of the package (all subpackages and modules).  However, packages in C
    extensions do not have this attribute, so we have to roll out our own logic: recursively
    find all modules imported in the package that have matching names.
    """
    for package_name in packages:
        if package_name in NOT_IMPORTABLE_MODULES:
            print(f'{package_name}: Skipped (blacklisted)')
            continue
        if verbose:
            print(f'Trying to import {package_name!r} for runtime introspection')
        try:
            prop = inspect.get_package_properties(package_name)
        except InspectError:
            report_missing(package_name)
            continue
        yield prop.name
        if prop.is_c_module:
            # Recursively iterate through the subpackages
            yield from walk_packages(inspect, prop.subpackages, verbose)
        else:
            yield from prop.subpackages


</t>
<t tx="ekr.20220525082935.811">def find_module_path_and_all_py2(module: str,
                                 interpreter: str) -&gt; Optional[Tuple[Optional[str],
                                                                     Optional[List[str]]]]:
    """Return tuple (module path, module __all__) for a Python 2 module.

    The path refers to the .py/.py[co] file. The second tuple item is
    None if the module doesn't define __all__.

    Raise CantImport if the module can't be imported, or exit if it's a C extension module.
    """
    cmd_template = f'{interpreter} -c "%s"'
    code = ("import importlib, json; mod = importlib.import_module('%s'); "
            "print(mod.__file__); print(json.dumps(getattr(mod, '__all__', None)))") % module
    try:
        output_bytes = subprocess.check_output(cmd_template % code, shell=True)
    except subprocess.CalledProcessError as e:
        path = find_module_path_using_py2_sys_path(module, interpreter)
        if path is None:
            raise CantImport(module, str(e)) from e
        return path, None
    output = output_bytes.decode('ascii').strip().splitlines()
    module_path = output[0]
    if not module_path.endswith(('.py', '.pyc', '.pyo')):
        raise SystemExit('%s looks like a C module; they are not supported for Python 2' %
                         module)
    if module_path.endswith(('.pyc', '.pyo')):
        module_path = module_path[:-1]
    module_all = json.loads(output[1])
    return module_path, module_all


</t>
<t tx="ekr.20220525082935.812">def find_module_path_using_py2_sys_path(module: str,
                                        interpreter: str) -&gt; Optional[str]:
    """Try to find the path of a .py file for a module using Python 2 sys.path.

    Return None if no match was found.
    """
    out = subprocess.run(
        [interpreter, '-c', 'import sys; import json; print(json.dumps(sys.path))'],
        check=True,
        stdout=subprocess.PIPE
    ).stdout
    sys_path = json.loads(out.decode('utf-8'))
    return find_module_path_using_sys_path(module, sys_path)


</t>
<t tx="ekr.20220525082935.813">def find_module_path_using_sys_path(module: str, sys_path: List[str]) -&gt; Optional[str]:
    relative_candidates = (
        module.replace('.', '/') + '.py',
        os.path.join(module.replace('.', '/'), '__init__.py')
    )
    for base in sys_path:
        for relative_path in relative_candidates:
            path = os.path.join(base, relative_path)
            if os.path.isfile(path):
                return path
    return None


</t>
<t tx="ekr.20220525082935.814">def find_module_path_and_all_py3(inspect: ModuleInspect,
                                 module: str,
                                 verbose: bool) -&gt; Optional[Tuple[Optional[str],
                                                                  Optional[List[str]]]]:
    """Find module and determine __all__ for a Python 3 module.

    Return None if the module is a C module. Return (module_path, __all__) if
    it is a Python module. Raise CantImport if import failed.
    """
    if module in NOT_IMPORTABLE_MODULES:
        raise CantImport(module, '')

    # TODO: Support custom interpreters.
    if verbose:
        print(f'Trying to import {module!r} for runtime introspection')
    try:
        mod = inspect.get_package_properties(module)
    except InspectError as e:
        # Fall back to finding the module using sys.path.
        path = find_module_path_using_sys_path(module, sys.path)
        if path is None:
            raise CantImport(module, str(e)) from e
        return path, None
    if mod.is_c_module:
        return None
    return mod.file, mod.all


</t>
<t tx="ekr.20220525082935.815">@contextmanager
def generate_guarded(mod: str, target: str,
                     ignore_errors: bool = True, verbose: bool = False) -&gt; Iterator[None]:
    """Ignore or report errors during stub generation.

    Optionally report success.
    """
    if verbose:
        print(f'Processing {mod}')
    try:
        yield
    except Exception as e:
        if not ignore_errors:
            raise e
        else:
            # --ignore-errors was passed
            print("Stub generation failed for", mod, file=sys.stderr)
    else:
        if verbose:
            print(f'Created {target}')


</t>
<t tx="ekr.20220525082935.816">PY2_MODULES = {'cStringIO', 'urlparse', 'collections.UserDict'}


</t>
<t tx="ekr.20220525082935.817">def report_missing(mod: str, message: Optional[str] = '', traceback: str = '') -&gt; None:
    if message:
        message = ' with error: ' + message
    print(f'{mod}: Failed to import, skipping{message}')
    m = re.search(r"ModuleNotFoundError: No module named '([^']*)'", traceback)
    if m:
        missing_module = m.group(1)
        if missing_module in PY2_MODULES:
            print('note: Try --py2 for Python 2 mode')


</t>
<t tx="ekr.20220525082935.818">def fail_missing(mod: str, reason: ModuleNotFoundReason) -&gt; None:
    if reason is ModuleNotFoundReason.NOT_FOUND:
        clarification = "(consider using --search-path)"
    elif reason is ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS:
        clarification = "(module likely exists, but is not PEP 561 compatible)"
    else:
        clarification = f"(unknown reason '{reason}')"
    raise SystemExit(f"Can't find module '{mod}' {clarification}")


</t>
<t tx="ekr.20220525082935.819">@overload
def remove_misplaced_type_comments(source: bytes) -&gt; bytes: ...


</t>
<t tx="ekr.20220525082935.82">def is_type_ref(self, rv: Expression, bare: bool = False) -&gt; bool:
    """Does this expression refer to a type?

    This includes:
      * Special forms, like Any or Union
      * Classes (except subscripted enums)
      * Other type aliases
      * PlaceholderNodes with becomes_typeinfo=True (these can be not ready class
        definitions, and not ready aliases).

    If bare is True, this is not a base of an index expression, so some special
    forms are not valid (like a bare Union).

    Note: This method should be only used in context of a type alias definition.
    This method can only return True for RefExprs, to check if C[int] is a valid
    target for type alias call this method on expr.base (i.e. on C in C[int]).
    See also can_be_type_alias().
    """
    if not isinstance(rv, RefExpr):
        return False
    if isinstance(rv.node, TypeVarExpr):
        self.fail('Type variable "{}" is invalid as target for type alias'.format(
            rv.fullname), rv)
        return False

    if bare:
        # These three are valid even if bare, for example
        # A = Tuple is just equivalent to A = Tuple[Any, ...].
        valid_refs = {'typing.Any', 'typing.Tuple', 'typing.Callable'}
    else:
        valid_refs = type_constructors

    if isinstance(rv.node, TypeAlias) or rv.fullname in valid_refs:
        return True
    if isinstance(rv.node, TypeInfo):
        if bare:
            return True
        # Assignment color = Color['RED'] defines a variable, not an alias.
        return not rv.node.is_enum
    if isinstance(rv.node, Var):
        return rv.node.fullname in NEVER_NAMES

    if isinstance(rv, NameExpr):
        n = self.lookup(rv.name, rv)
        if n and isinstance(n.node, PlaceholderNode) and n.node.becomes_typeinfo:
            return True
    elif isinstance(rv, MemberExpr):
        fname = get_member_expr_fullname(rv)
        if fname:
            # The r.h.s. for variable definitions may not be a type reference but just
            # an instance attribute, so suppress the errors.
            n = self.lookup_qualified(fname, rv, suppress_errors=True)
            if n and isinstance(n.node, PlaceholderNode) and n.node.becomes_typeinfo:
                return True
    return False

</t>
<t tx="ekr.20220525082935.820">@overload
def remove_misplaced_type_comments(source: str) -&gt; str: ...


</t>
<t tx="ekr.20220525082935.821">def remove_misplaced_type_comments(source: Union[str, bytes]) -&gt; Union[str, bytes]:
    """Remove comments from source that could be understood as misplaced type comments.

    Normal comments may look like misplaced type comments, and since they cause blocking
    parse errors, we want to avoid them.
    """
    if isinstance(source, bytes):
        # This gives us a 1-1 character code mapping, so it's roundtrippable.
        text = source.decode('latin1')
    else:
        text = source

    # Remove something that looks like a variable type comment but that's by itself
    # on a line, as it will often generate a parse error (unless it's # type: ignore).
    text = re.sub(r'^[ \t]*# +type: +["\'a-zA-Z_].*$', '', text, flags=re.MULTILINE)

    # Remove something that looks like a function type comment after docstring,
    # which will result in a parse error.
    text = re.sub(r'""" *\n[ \t\n]*# +type: +\(.*$', '"""\n', text, flags=re.MULTILINE)
    text = re.sub(r"''' *\n[ \t\n]*# +type: +\(.*$", "'''\n", text, flags=re.MULTILINE)

    # Remove something that looks like a badly formed function type comment.
    text = re.sub(r'^[ \t]*# +type: +\([^()]+(\)[ \t]*)?$', '', text, flags=re.MULTILINE)

    if isinstance(source, bytes):
        return text.encode('latin1')
    else:
        return text


</t>
<t tx="ekr.20220525082935.822">def common_dir_prefix(paths: List[str]) -&gt; str:
    if not paths:
        return '.'
    cur = os.path.dirname(os.path.normpath(paths[0]))
    for path in paths[1:]:
        while True:
            path = os.path.dirname(os.path.normpath(path))
            if (cur + os.sep).startswith(path + os.sep):
                cur = path
                break
    return cur or '.'
</t>
<t tx="ekr.20220525082935.823">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
from contextlib import contextmanager

from typing import Any, List, Optional, Callable, Tuple, Iterator, Set, Union, cast, TypeVar
from typing_extensions import Final, TypeAlias as _TypeAlias

from mypy.types import (
    Type, AnyType, UnboundType, TypeVisitor, FormalArgument, NoneType,
    Instance, TypeVarType, CallableType, TupleType, TypedDictType, UnionType, Overloaded,
    ErasedType, PartialType, DeletedType, UninhabitedType, TypeType, is_named_instance,
    FunctionLike, TypeOfAny, LiteralType, get_proper_type, TypeAliasType, ParamSpecType,
    Parameters, UnpackType, TUPLE_LIKE_INSTANCE_NAMES, TypeVarTupleType,
)
import mypy.applytype
import mypy.constraints
import mypy.typeops
import mypy.sametypes
from mypy.erasetype import erase_type
# Circular import; done in the function instead.
# import mypy.solve
from mypy.nodes import (
    FuncBase, Var, Decorator, OverloadedFuncDef, TypeInfo, CONTRAVARIANT, COVARIANT,

)
from mypy.maptype import map_instance_to_supertype
from mypy.expandtype import expand_type_by_instance
from mypy.typestate import TypeState, SubtypeKind
from mypy.options import Options
from mypy.state import state

# Flags for detected protocol members
IS_SETTABLE: Final = 1
IS_CLASSVAR: Final = 2
IS_CLASS_OR_STATIC: Final = 3

TypeParameterChecker: _TypeAlias = Callable[[Type, Type, int], bool]


@others
</t>
<t tx="ekr.20220525082935.824">def check_type_parameter(lefta: Type, righta: Type, variance: int) -&gt; bool:
    if variance == COVARIANT:
        return is_subtype(lefta, righta)
    elif variance == CONTRAVARIANT:
        return is_subtype(righta, lefta)
    else:
        return is_equivalent(lefta, righta)


</t>
<t tx="ekr.20220525082935.825">def ignore_type_parameter(s: Type, t: Type, v: int) -&gt; bool:
    return True


</t>
<t tx="ekr.20220525082935.826">def is_subtype(left: Type, right: Type,
               *,
               ignore_type_params: bool = False,
               ignore_pos_arg_names: bool = False,
               ignore_declared_variance: bool = False,
               ignore_promotions: bool = False,
               options: Optional[Options] = None) -&gt; bool:
    """Is 'left' subtype of 'right'?

    Also consider Any to be a subtype of any type, and vice versa. This
    recursively applies to components of composite types (List[int] is subtype
    of List[Any], for example).

    type_parameter_checker is used to check the type parameters (for example,
    A with B in is_subtype(C[A], C[B]). The default checks for subtype relation
    between the type arguments (e.g., A and B), taking the variance of the
    type var into account.
    """
    if TypeState.is_assumed_subtype(left, right):
        return True
    if (isinstance(left, TypeAliasType) and isinstance(right, TypeAliasType) and
            left.is_recursive and right.is_recursive):
        # This case requires special care because it may cause infinite recursion.
        # Our view on recursive types is known under a fancy name of equirecursive mu-types.
        # Roughly this means that a recursive type is defined as an alias where right hand side
        # can refer to the type as a whole, for example:
        #     A = Union[int, Tuple[A, ...]]
        # and an alias unrolled once represents the *same type*, in our case all these represent
        # the same type:
        #    A
        #    Union[int, Tuple[A, ...]]
        #    Union[int, Tuple[Union[int, Tuple[A, ...]], ...]]
        # The algorithm for subtyping is then essentially under the assumption that left &lt;: right,
        # check that get_proper_type(left) &lt;: get_proper_type(right). On the example above,
        # If we start with:
        #     A = Union[int, Tuple[A, ...]]
        #     B = Union[int, Tuple[B, ...]]
        # When checking if A &lt;: B we push pair (A, B) onto 'assuming' stack, then when after few
        # steps we come back to initial call is_subtype(A, B) and immediately return True.
        with pop_on_exit(TypeState._assuming, left, right):
            return _is_subtype(left, right,
                               ignore_type_params=ignore_type_params,
                               ignore_pos_arg_names=ignore_pos_arg_names,
                               ignore_declared_variance=ignore_declared_variance,
                               ignore_promotions=ignore_promotions,
                               options=options)
    return _is_subtype(left, right,
                       ignore_type_params=ignore_type_params,
                       ignore_pos_arg_names=ignore_pos_arg_names,
                       ignore_declared_variance=ignore_declared_variance,
                       ignore_promotions=ignore_promotions,
                       options=options)


</t>
<t tx="ekr.20220525082935.827">def _is_subtype(left: Type, right: Type,
                *,
                ignore_type_params: bool = False,
                ignore_pos_arg_names: bool = False,
                ignore_declared_variance: bool = False,
                ignore_promotions: bool = False,
                options: Optional[Options] = None) -&gt; bool:
    orig_right = right
    orig_left = left
    left = get_proper_type(left)
    right = get_proper_type(right)

    if (isinstance(right, AnyType) or isinstance(right, UnboundType)
            or isinstance(right, ErasedType)):
        return True
    elif isinstance(right, UnionType) and not isinstance(left, UnionType):
        # Normally, when 'left' is not itself a union, the only way
        # 'left' can be a subtype of the union 'right' is if it is a
        # subtype of one of the items making up the union.
        is_subtype_of_item = any(is_subtype(orig_left, item,
                                            ignore_type_params=ignore_type_params,
                                            ignore_pos_arg_names=ignore_pos_arg_names,
                                            ignore_declared_variance=ignore_declared_variance,
                                            ignore_promotions=ignore_promotions,
                                            options=options)
                                 for item in right.items)
        # Recombine rhs literal types, to make an enum type a subtype
        # of a union of all enum items as literal types. Only do it if
        # the previous check didn't succeed, since recombining can be
        # expensive.
        # `bool` is a special case, because `bool` is `Literal[True, False]`.
        if (not is_subtype_of_item
                and isinstance(left, Instance)
                and (left.type.is_enum or left.type.fullname == 'builtins.bool')):
            right = UnionType(mypy.typeops.try_contracting_literals_in_union(right.items))
            is_subtype_of_item = any(is_subtype(orig_left, item,
                                                ignore_type_params=ignore_type_params,
                                                ignore_pos_arg_names=ignore_pos_arg_names,
                                                ignore_declared_variance=ignore_declared_variance,
                                                ignore_promotions=ignore_promotions,
                                                options=options)
                                     for item in right.items)
        # However, if 'left' is a type variable T, T might also have
        # an upper bound which is itself a union. This case will be
        # handled below by the SubtypeVisitor. We have to check both
        # possibilities, to handle both cases like T &lt;: Union[T, U]
        # and cases like T &lt;: B where B is the upper bound of T and is
        # a union. (See #2314.)
        if not isinstance(left, TypeVarType):
            return is_subtype_of_item
        elif is_subtype_of_item:
            return True
        # otherwise, fall through
    return left.accept(SubtypeVisitor(orig_right,
                                      ignore_type_params=ignore_type_params,
                                      ignore_pos_arg_names=ignore_pos_arg_names,
                                      ignore_declared_variance=ignore_declared_variance,
                                      ignore_promotions=ignore_promotions,
                                      options=options))


</t>
<t tx="ekr.20220525082935.828">def is_equivalent(a: Type, b: Type,
                  *,
                  ignore_type_params: bool = False,
                  ignore_pos_arg_names: bool = False,
                  options: Optional[Options] = None
                  ) -&gt; bool:
    return (
        is_subtype(a, b, ignore_type_params=ignore_type_params,
                   ignore_pos_arg_names=ignore_pos_arg_names, options=options)
        and is_subtype(b, a, ignore_type_params=ignore_type_params,
                       ignore_pos_arg_names=ignore_pos_arg_names, options=options))


</t>
<t tx="ekr.20220525082935.829">class SubtypeVisitor(TypeVisitor[bool]):

    @others
</t>
<t tx="ekr.20220525082935.83">def is_none_alias(self, node: Expression) -&gt; bool:
    """Is this a r.h.s. for a None alias?

    We special case the assignments like Void = type(None), to allow using
    Void in type annotations.
    """
    if isinstance(node, CallExpr):
        if (isinstance(node.callee, NameExpr) and len(node.args) == 1 and
                isinstance(node.args[0], NameExpr)):
            call = self.lookup_qualified(node.callee.name, node.callee)
            arg = self.lookup_qualified(node.args[0].name, node.args[0])
            if (call is not None and call.node and call.node.fullname == 'builtins.type' and
                    arg is not None and arg.node and arg.node.fullname == 'builtins.None'):
                return True
    return False

</t>
<t tx="ekr.20220525082935.830">def __init__(self, right: Type,
             *,
             ignore_type_params: bool,
             ignore_pos_arg_names: bool = False,
             ignore_declared_variance: bool = False,
             ignore_promotions: bool = False,
             options: Optional[Options] = None) -&gt; None:
    self.right = get_proper_type(right)
    self.orig_right = right
    self.ignore_type_params = ignore_type_params
    self.ignore_pos_arg_names = ignore_pos_arg_names
    self.ignore_declared_variance = ignore_declared_variance
    self.ignore_promotions = ignore_promotions
    self.check_type_parameter = (ignore_type_parameter if ignore_type_params else
                                 check_type_parameter)
    self.options = options
    self._subtype_kind = SubtypeVisitor.build_subtype_kind(
        ignore_type_params=ignore_type_params,
        ignore_pos_arg_names=ignore_pos_arg_names,
        ignore_declared_variance=ignore_declared_variance,
        ignore_promotions=ignore_promotions)

</t>
<t tx="ekr.20220525082935.831">@staticmethod
def build_subtype_kind(*,
                       ignore_type_params: bool = False,
                       ignore_pos_arg_names: bool = False,
                       ignore_declared_variance: bool = False,
                       ignore_promotions: bool = False) -&gt; SubtypeKind:
    return (state.strict_optional,
            False,  # is proper subtype?
            ignore_type_params,
            ignore_pos_arg_names,
            ignore_declared_variance,
            ignore_promotions)

</t>
<t tx="ekr.20220525082935.832">def _is_subtype(self, left: Type, right: Type) -&gt; bool:
    return is_subtype(left, right,
                      ignore_type_params=self.ignore_type_params,
                      ignore_pos_arg_names=self.ignore_pos_arg_names,
                      ignore_declared_variance=self.ignore_declared_variance,
                      ignore_promotions=self.ignore_promotions,
                      options=self.options)

</t>
<t tx="ekr.20220525082935.833"># visit_x(left) means: is left (which is an instance of X) a subtype of
# right?

</t>
<t tx="ekr.20220525082935.834">def visit_unbound_type(self, left: UnboundType) -&gt; bool:
    return True

</t>
<t tx="ekr.20220525082935.835">def visit_any(self, left: AnyType) -&gt; bool:
    return True

</t>
<t tx="ekr.20220525082935.836">def visit_none_type(self, left: NoneType) -&gt; bool:
    if state.strict_optional:
        if isinstance(self.right, NoneType) or is_named_instance(self.right,
                                                                 'builtins.object'):
            return True
        if isinstance(self.right, Instance) and self.right.type.is_protocol:
            members = self.right.type.protocol_members
            # None is compatible with Hashable (and other similar protocols). This is
            # slightly sloppy since we don't check the signature of "__hash__".
            return not members or members == ["__hash__"]
        return False
    else:
        return True

</t>
<t tx="ekr.20220525082935.837">def visit_uninhabited_type(self, left: UninhabitedType) -&gt; bool:
    return True

</t>
<t tx="ekr.20220525082935.838">def visit_erased_type(self, left: ErasedType) -&gt; bool:
    return True

</t>
<t tx="ekr.20220525082935.839">def visit_deleted_type(self, left: DeletedType) -&gt; bool:
    return True

</t>
<t tx="ekr.20220525082935.84">def record_special_form_lvalue(self, s: AssignmentStmt) -&gt; None:
    """Record minimal necessary information about l.h.s. of a special form.

    This exists mostly for compatibility with the old semantic analyzer.
    """
    lvalue = s.lvalues[0]
    assert isinstance(lvalue, NameExpr)
    lvalue.is_special_form = True
    if self.current_symbol_kind() == GDEF:
        lvalue.fullname = self.qualified_name(lvalue.name)
    lvalue.kind = self.current_symbol_kind()

</t>
<t tx="ekr.20220525082935.840">def visit_instance(self, left: Instance) -&gt; bool:
    if left.type.fallback_to_any:
        if isinstance(self.right, NoneType):
            # NOTE: `None` is a *non-subclassable* singleton, therefore no class
            # can by a subtype of it, even with an `Any` fallback.
            # This special case is needed to treat descriptors in classes with
            # dynamic base classes correctly, see #5456.
            return False
        return True
    right = self.right
    if isinstance(right, TupleType) and mypy.typeops.tuple_fallback(right).type.is_enum:
        return self._is_subtype(left, mypy.typeops.tuple_fallback(right))
    if isinstance(right, Instance):
        if TypeState.is_cached_subtype_check(self._subtype_kind, left, right):
            return True
        if not self.ignore_promotions:
            for base in left.type.mro:
                if base._promote and self._is_subtype(base._promote, self.right):
                    TypeState.record_subtype_cache_entry(self._subtype_kind, left, right)
                    return True
        rname = right.type.fullname
        # Always try a nominal check if possible,
        # there might be errors that a user wants to silence *once*.
        # NamedTuples are a special case, because `NamedTuple` is not listed
        # in `TypeInfo.mro`, so when `(a: NamedTuple) -&gt; None` is used,
        # we need to check for `is_named_tuple` property
        if ((left.type.has_base(rname) or rname == 'builtins.object'
                or (rname == 'typing.NamedTuple'
                    and any(l.is_named_tuple for l in left.type.mro)))
                and not self.ignore_declared_variance):
            # Map left type to corresponding right instances.
            t = map_instance_to_supertype(left, right.type)
            nominal = True
            for lefta, righta, tvar in zip(t.args, right.args, right.type.defn.type_vars):
                if isinstance(tvar, TypeVarType):
                    if not self.check_type_parameter(lefta, righta, tvar.variance):
                        nominal = False
                else:
                    if not self.check_type_parameter(lefta, righta, COVARIANT):
                        nominal = False
            if nominal:
                TypeState.record_subtype_cache_entry(self._subtype_kind, left, right)
            return nominal
        if right.type.is_protocol and is_protocol_implementation(left, right):
            return True
        return False
    if isinstance(right, TypeType):
        item = right.item
        if isinstance(item, TupleType):
            item = mypy.typeops.tuple_fallback(item)
        if is_named_instance(left, 'builtins.type'):
            return self._is_subtype(TypeType(AnyType(TypeOfAny.special_form)), right)
        if left.type.is_metaclass():
            if isinstance(item, AnyType):
                return True
            if isinstance(item, Instance):
                return is_named_instance(item, 'builtins.object')
    if isinstance(right, LiteralType) and left.last_known_value is not None:
        return self._is_subtype(left.last_known_value, right)
    if isinstance(right, CallableType):
        # Special case: Instance can be a subtype of Callable.
        call = find_member('__call__', left, left, is_operator=True)
        if call:
            return self._is_subtype(call, right)
        return False
    else:
        return False

</t>
<t tx="ekr.20220525082935.841">def visit_type_var(self, left: TypeVarType) -&gt; bool:
    right = self.right
    if isinstance(right, TypeVarType) and left.id == right.id:
        return True
    if left.values and self._is_subtype(
            mypy.typeops.make_simplified_union(left.values), right):
        return True
    return self._is_subtype(left.upper_bound, self.right)

</t>
<t tx="ekr.20220525082935.842">def visit_param_spec(self, left: ParamSpecType) -&gt; bool:
    right = self.right
    if (
        isinstance(right, ParamSpecType)
        and right.id == left.id
        and right.flavor == left.flavor
    ):
        return True
    return self._is_subtype(left.upper_bound, self.right)

</t>
<t tx="ekr.20220525082935.843">def visit_type_var_tuple(self, left: TypeVarTupleType) -&gt; bool:
    right = self.right
    if (
        isinstance(right, TypeVarTupleType)
        and right.id == left.id
    ):
        return True
    return self._is_subtype(left.upper_bound, self.right)

</t>
<t tx="ekr.20220525082935.844">def visit_unpack_type(self, left: UnpackType) -&gt; bool:
    if isinstance(self.right, UnpackType):
        return self._is_subtype(left.type, self.right.type)
    return False

</t>
<t tx="ekr.20220525082935.845">def visit_parameters(self, left: Parameters) -&gt; bool:
    right = self.right
    if isinstance(right, Parameters) or isinstance(right, CallableType):
        return are_parameters_compatible(
            left, right,
            is_compat=self._is_subtype,
            ignore_pos_arg_names=self.ignore_pos_arg_names)
    else:
        return False

</t>
<t tx="ekr.20220525082935.846">def visit_callable_type(self, left: CallableType) -&gt; bool:
    right = self.right
    if isinstance(right, CallableType):
        if left.type_guard is not None and right.type_guard is not None:
            if not self._is_subtype(left.type_guard, right.type_guard):
                return False
        elif right.type_guard is not None and left.type_guard is None:
            # This means that one function has `TypeGuard` and other does not.
            # They are not compatible. See https://github.com/python/mypy/issues/11307
            return False
        return is_callable_compatible(
            left, right,
            is_compat=self._is_subtype,
            ignore_pos_arg_names=self.ignore_pos_arg_names,
            strict_concatenate=self.options.strict_concatenate if self.options else True)
    elif isinstance(right, Overloaded):
        return all(self._is_subtype(left, item) for item in right.items)
    elif isinstance(right, Instance):
        if right.type.is_protocol and right.type.protocol_members == ['__call__']:
            # OK, a callable can implement a protocol with a single `__call__` member.
            # TODO: we should probably explicitly exclude self-types in this case.
            call = find_member('__call__', right, left, is_operator=True)
            assert call is not None
            if self._is_subtype(left, call):
                return True
        return self._is_subtype(left.fallback, right)
    elif isinstance(right, TypeType):
        # This is unsound, we don't check the __init__ signature.
        return left.is_type_obj() and self._is_subtype(left.ret_type, right.item)
    elif isinstance(right, Parameters):
        # this doesn't check return types.... but is needed for is_equivalent
        return are_parameters_compatible(
            left, right,
            is_compat=self._is_subtype,
            ignore_pos_arg_names=self.ignore_pos_arg_names)
    else:
        return False

</t>
<t tx="ekr.20220525082935.847">def visit_tuple_type(self, left: TupleType) -&gt; bool:
    right = self.right
    if isinstance(right, Instance):
        if is_named_instance(right, 'typing.Sized'):
            return True
        elif is_named_instance(right, TUPLE_LIKE_INSTANCE_NAMES):
            if right.args:
                iter_type = right.args[0]
            else:
                iter_type = AnyType(TypeOfAny.special_form)
            return all(self._is_subtype(li, iter_type) for li in left.items)
        elif self._is_subtype(mypy.typeops.tuple_fallback(left), right):
            return True
        return False
    elif isinstance(right, TupleType):
        if len(left.items) != len(right.items):
            return False
        for l, r in zip(left.items, right.items):
            if not self._is_subtype(l, r):
                return False
        rfallback = mypy.typeops.tuple_fallback(right)
        if is_named_instance(rfallback, 'builtins.tuple'):
            # No need to verify fallback. This is useful since the calculated fallback
            # may be inconsistent due to how we calculate joins between unions vs.
            # non-unions. For example, join(int, str) == object, whereas
            # join(Union[int, C], Union[str, C]) == Union[int, str, C].
            return True
        lfallback = mypy.typeops.tuple_fallback(left)
        if not self._is_subtype(lfallback, rfallback):
            return False
        return True
    else:
        return False

</t>
<t tx="ekr.20220525082935.848">def visit_typeddict_type(self, left: TypedDictType) -&gt; bool:
    right = self.right
    if isinstance(right, Instance):
        return self._is_subtype(left.fallback, right)
    elif isinstance(right, TypedDictType):
        if not left.names_are_wider_than(right):
            return False
        for name, l, r in left.zip(right):
            if not is_equivalent(l, r,
                                 ignore_type_params=self.ignore_type_params,
                                 options=self.options):
                return False
            # Non-required key is not compatible with a required key since
            # indexing may fail unexpectedly if a required key is missing.
            # Required key is not compatible with a non-required key since
            # the prior doesn't support 'del' but the latter should support
            # it.
            #
            # NOTE: 'del' support is currently not implemented (#3550). We
            #       don't want to have to change subtyping after 'del' support
            #       lands so here we are anticipating that change.
            if (name in left.required_keys) != (name in right.required_keys):
                return False
        # (NOTE: Fallbacks don't matter.)
        return True
    else:
        return False

</t>
<t tx="ekr.20220525082935.849">def visit_literal_type(self, left: LiteralType) -&gt; bool:
    if isinstance(self.right, LiteralType):
        return left == self.right
    else:
        return self._is_subtype(left.fallback, self.right)

</t>
<t tx="ekr.20220525082935.85">def analyze_enum_assign(self, s: AssignmentStmt) -&gt; bool:
    """Check if s defines an Enum."""
    if isinstance(s.rvalue, CallExpr) and isinstance(s.rvalue.analyzed, EnumCallExpr):
        # Already analyzed enum -- nothing to do here.
        return True
    return self.enum_call_analyzer.process_enum_call(s, self.is_func_scope())

</t>
<t tx="ekr.20220525082935.850">def visit_overloaded(self, left: Overloaded) -&gt; bool:
    right = self.right
    if isinstance(right, Instance):
        if right.type.is_protocol and right.type.protocol_members == ['__call__']:
            # same as for CallableType
            call = find_member('__call__', right, left, is_operator=True)
            assert call is not None
            if self._is_subtype(left, call):
                return True
        return self._is_subtype(left.fallback, right)
    elif isinstance(right, CallableType):
        for item in left.items:
            if self._is_subtype(item, right):
                return True
        return False
    elif isinstance(right, Overloaded):
        if left == self.right:
            # When it is the same overload, then the types are equal.
            return True

        # Ensure each overload in the right side (the supertype) is accounted for.
        previous_match_left_index = -1
        matched_overloads = set()
        possible_invalid_overloads = set()

        for right_index, right_item in enumerate(right.items):
            found_match = False

            for left_index, left_item in enumerate(left.items):
                subtype_match = self._is_subtype(left_item, right_item)

                # Order matters: we need to make sure that the index of
                # this item is at least the index of the previous one.
                if subtype_match and previous_match_left_index &lt;= left_index:
                    if not found_match:
                        # Update the index of the previous match.
                        previous_match_left_index = left_index
                        found_match = True
                        matched_overloads.add(left_item)
                        possible_invalid_overloads.discard(left_item)
                else:
                    # If this one overlaps with the supertype in any way, but it wasn't
                    # an exact match, then it's a potential error.
                    strict_concat = self.options.strict_concatenate if self.options else True
                    if (is_callable_compatible(left_item, right_item,
                                is_compat=self._is_subtype, ignore_return=True,
                                ignore_pos_arg_names=self.ignore_pos_arg_names,
                                strict_concatenate=strict_concat) or
                            is_callable_compatible(right_item, left_item,
                                    is_compat=self._is_subtype, ignore_return=True,
                                    ignore_pos_arg_names=self.ignore_pos_arg_names,
                                    strict_concatenate=strict_concat)):
                        # If this is an overload that's already been matched, there's no
                        # problem.
                        if left_item not in matched_overloads:
                            possible_invalid_overloads.add(left_item)

            if not found_match:
                return False

        if possible_invalid_overloads:
            # There were potentially invalid overloads that were never matched to the
            # supertype.
            return False
        return True
    elif isinstance(right, UnboundType):
        return True
    elif isinstance(right, TypeType):
        # All the items must have the same type object status, so
        # it's sufficient to query only (any) one of them.
        # This is unsound, we don't check all the __init__ signatures.
        return left.is_type_obj() and self._is_subtype(left.items[0], right)
    else:
        return False

</t>
<t tx="ekr.20220525082935.851">def visit_union_type(self, left: UnionType) -&gt; bool:
    if isinstance(self.right, Instance):
        literal_types: Set[Instance] = set()
        # avoid redundant check for union of literals
        for item in left.relevant_items():
            item = get_proper_type(item)
            lit_type = mypy.typeops.simple_literal_type(item)
            if lit_type is not None:
                if lit_type in literal_types:
                    continue
                literal_types.add(lit_type)
                item = lit_type
            if not self._is_subtype(item, self.orig_right):
                return False
        return True
    return all(self._is_subtype(item, self.orig_right) for item in left.items)

</t>
<t tx="ekr.20220525082935.852">def visit_partial_type(self, left: PartialType) -&gt; bool:
    # This is indeterminate as we don't really know the complete type yet.
    if left.type is None:
        # Special case, partial `None`. This might happen when defining
        # class-level attributes with explicit `None`.
        # We can still recover from this.
        # https://github.com/python/mypy/issues/11105
        return self.visit_none_type(NoneType())
    raise RuntimeError(f'Partial type "{left}" cannot be checked with "issubtype()"')

</t>
<t tx="ekr.20220525082935.853">def visit_type_type(self, left: TypeType) -&gt; bool:
    right = self.right
    if isinstance(right, TypeType):
        return self._is_subtype(left.item, right.item)
    if isinstance(right, CallableType):
        # This is unsound, we don't check the __init__ signature.
        return self._is_subtype(left.item, right.ret_type)
    if isinstance(right, Instance):
        if right.type.fullname in ['builtins.object', 'builtins.type']:
            return True
        item = left.item
        if isinstance(item, TypeVarType):
            item = get_proper_type(item.upper_bound)
        if isinstance(item, Instance):
            metaclass = item.type.metaclass_type
            return metaclass is not None and self._is_subtype(metaclass, right)
    return False

</t>
<t tx="ekr.20220525082935.854">def visit_type_alias_type(self, left: TypeAliasType) -&gt; bool:
    assert False, f"This should be never called, got {left}"


</t>
<t tx="ekr.20220525082935.855">T = TypeVar('T', Instance, TypeAliasType)


</t>
<t tx="ekr.20220525082935.856">@contextmanager
def pop_on_exit(stack: List[Tuple[T, T]],
                left: T, right: T) -&gt; Iterator[None]:
    stack.append((left, right))
    yield
    stack.pop()


</t>
<t tx="ekr.20220525082935.857">def is_protocol_implementation(left: Instance, right: Instance,
                               proper_subtype: bool = False) -&gt; bool:
    """Check whether 'left' implements the protocol 'right'.

    If 'proper_subtype' is True, then check for a proper subtype.
    Treat recursive protocols by using the 'assuming' structural subtype matrix
    (in sparse representation, i.e. as a list of pairs (subtype, supertype)),
    see also comment in nodes.TypeInfo. When we enter a check for classes
    (A, P), defined as following::

      class P(Protocol):
          def f(self) -&gt; P: ...
      class A:
          def f(self) -&gt; A: ...

    this results in A being a subtype of P without infinite recursion.
    On every false result, we pop the assumption, thus avoiding an infinite recursion
    as well.
    """
    assert right.type.is_protocol
    # We need to record this check to generate protocol fine-grained dependencies.
    TypeState.record_protocol_subtype_check(left.type, right.type)
    # nominal subtyping currently ignores '__init__' and '__new__' signatures
    members_not_to_check = {'__init__', '__new__'}
    # Trivial check that circumvents the bug described in issue 9771:
    if left.type.is_protocol:
        members_right = set(right.type.protocol_members) - members_not_to_check
        members_left = set(left.type.protocol_members) - members_not_to_check
        if not members_right.issubset(members_left):
            return False
    assuming = right.type.assuming_proper if proper_subtype else right.type.assuming
    for (l, r) in reversed(assuming):
        if l == left and r == right:
            return True
    with pop_on_exit(assuming, left, right):
        for member in right.type.protocol_members:
            if member in members_not_to_check:
                continue
            ignore_names = member != '__call__'  # __call__ can be passed kwargs
            # The third argument below indicates to what self type is bound.
            # We always bind self to the subtype. (Similarly to nominal types).
            supertype = get_proper_type(find_member(member, right, left))
            assert supertype is not None
            subtype = get_proper_type(find_member(member, left, left))
            # Useful for debugging:
            # print(member, 'of', left, 'has type', subtype)
            # print(member, 'of', right, 'has type', supertype)
            if not subtype:
                return False
            if isinstance(subtype, PartialType):
                subtype = NoneType() if subtype.type is None else Instance(
                    subtype.type, [AnyType(TypeOfAny.unannotated)] * len(subtype.type.type_vars)
                )
            if not proper_subtype:
                # Nominal check currently ignores arg names
                # NOTE: If we ever change this, be sure to also change the call to
                # SubtypeVisitor.build_subtype_kind(...) down below.
                is_compat = is_subtype(subtype, supertype, ignore_pos_arg_names=ignore_names)
            else:
                is_compat = is_proper_subtype(subtype, supertype)
            if not is_compat:
                return False
            if isinstance(subtype, NoneType) and isinstance(supertype, CallableType):
                # We want __hash__ = None idiom to work even without --strict-optional
                return False
            subflags = get_member_flags(member, left.type)
            superflags = get_member_flags(member, right.type)
            if IS_SETTABLE in superflags:
                # Check opposite direction for settable attributes.
                if not is_subtype(supertype, subtype):
                    return False
            if (IS_CLASSVAR in subflags) != (IS_CLASSVAR in superflags):
                return False
            if IS_SETTABLE in superflags and IS_SETTABLE not in subflags:
                return False
            # This rule is copied from nominal check in checker.py
            if IS_CLASS_OR_STATIC in superflags and IS_CLASS_OR_STATIC not in subflags:
                return False

    if not proper_subtype:
        # Nominal check currently ignores arg names, but __call__ is special for protocols
        ignore_names = right.type.protocol_members != ['__call__']
        subtype_kind = SubtypeVisitor.build_subtype_kind(ignore_pos_arg_names=ignore_names)
    else:
        subtype_kind = ProperSubtypeVisitor.build_subtype_kind()
    TypeState.record_subtype_cache_entry(subtype_kind, left, right)
    return True


</t>
<t tx="ekr.20220525082935.858">def find_member(name: str,
                itype: Instance,
                subtype: Type,
                is_operator: bool = False) -&gt; Optional[Type]:
    """Find the type of member by 'name' in 'itype's TypeInfo.

    Find the member type after applying type arguments from 'itype', and binding
    'self' to 'subtype'. Return None if member was not found.
    """
    # TODO: this code shares some logic with checkmember.analyze_member_access,
    # consider refactoring.
    info = itype.type
    method = info.get_method(name)
    if method:
        if isinstance(method, Decorator):
            return find_node_type(method.var, itype, subtype)
        if method.is_property:
            assert isinstance(method, OverloadedFuncDef)
            dec = method.items[0]
            assert isinstance(dec, Decorator)
            return find_node_type(dec.var, itype, subtype)
        return find_node_type(method, itype, subtype)
    else:
        # don't have such method, maybe variable or decorator?
        node = info.get(name)
        v = node.node if node else None
        if isinstance(v, Var):
            return find_node_type(v, itype, subtype)
        if (not v and name not in ['__getattr__', '__setattr__', '__getattribute__'] and
                not is_operator):
            for method_name in ('__getattribute__', '__getattr__'):
                # Normally, mypy assumes that instances that define __getattr__ have all
                # attributes with the corresponding return type. If this will produce
                # many false negatives, then this could be prohibited for
                # structural subtyping.
                method = info.get_method(method_name)
                if method and method.info.fullname != 'builtins.object':
                    if isinstance(method, Decorator):
                        getattr_type = get_proper_type(find_node_type(method.var, itype, subtype))
                    else:
                        getattr_type = get_proper_type(find_node_type(method, itype, subtype))
                    if isinstance(getattr_type, CallableType):
                        return getattr_type.ret_type
                    return getattr_type
        if itype.type.fallback_to_any:
            return AnyType(TypeOfAny.special_form)
    return None


</t>
<t tx="ekr.20220525082935.859">def get_member_flags(name: str, info: TypeInfo) -&gt; Set[int]:
    """Detect whether a member 'name' is settable, whether it is an
    instance or class variable, and whether it is class or static method.

    The flags are defined as following:
    * IS_SETTABLE: whether this attribute can be set, not set for methods and
      non-settable properties;
    * IS_CLASSVAR: set if the variable is annotated as 'x: ClassVar[t]';
    * IS_CLASS_OR_STATIC: set for methods decorated with @classmethod or
      with @staticmethod.
    """
    method = info.get_method(name)
    setattr_meth = info.get_method('__setattr__')
    if method:
        if isinstance(method, Decorator):
            if method.var.is_staticmethod or method.var.is_classmethod:
                return {IS_CLASS_OR_STATIC}
        elif method.is_property:  # this could be settable property
            assert isinstance(method, OverloadedFuncDef)
            dec = method.items[0]
            assert isinstance(dec, Decorator)
            if dec.var.is_settable_property or setattr_meth:
                return {IS_SETTABLE}
        return set()
    node = info.get(name)
    if not node:
        if setattr_meth:
            return {IS_SETTABLE}
        return set()
    v = node.node
    # just a variable
    if isinstance(v, Var) and not v.is_property:
        flags = {IS_SETTABLE}
        if v.is_classvar:
            flags.add(IS_CLASSVAR)
        return flags
    return set()


</t>
<t tx="ekr.20220525082935.86">def analyze_namedtuple_assign(self, s: AssignmentStmt) -&gt; bool:
    """Check if s defines a namedtuple."""
    if isinstance(s.rvalue, CallExpr) and isinstance(s.rvalue.analyzed, NamedTupleExpr):
        return True  # This is a valid and analyzed named tuple definition, nothing to do here.
    if len(s.lvalues) != 1 or not isinstance(s.lvalues[0], (NameExpr, MemberExpr)):
        return False
    lvalue = s.lvalues[0]
    name = lvalue.name
    internal_name, info = self.named_tuple_analyzer.check_namedtuple(s.rvalue, name,
                                                                     self.is_func_scope())
    if internal_name is None:
        return False
    if isinstance(lvalue, MemberExpr):
        self.fail("NamedTuple type as an attribute is not supported", lvalue)
        return False
    if internal_name != name:
        self.fail('First argument to namedtuple() should be "{}", not "{}"'.format(
            name, internal_name), s.rvalue, code=codes.NAME_MATCH)
        return True
    # Yes, it's a valid namedtuple, but defer if it is not ready.
    if not info:
        self.mark_incomplete(name, lvalue, becomes_typeinfo=True)
    return True

</t>
<t tx="ekr.20220525082935.860">def find_node_type(node: Union[Var, FuncBase], itype: Instance, subtype: Type) -&gt; Type:
    """Find type of a variable or method 'node' (maybe also a decorated method).
    Apply type arguments from 'itype', and bind 'self' to 'subtype'.
    """
    from mypy.typeops import bind_self

    if isinstance(node, FuncBase):
        typ: Optional[Type] = mypy.typeops.function_type(
            node, fallback=Instance(itype.type.mro[-1], [])
        )
    else:
        typ = node.type
    typ = get_proper_type(typ)
    if typ is None:
        return AnyType(TypeOfAny.from_error)
    # We don't need to bind 'self' for static methods, since there is no 'self'.
    if (isinstance(node, FuncBase)
            or (isinstance(typ, FunctionLike)
                and node.is_initialized_in_class
                and not node.is_staticmethod)):
        assert isinstance(typ, FunctionLike)
        signature = bind_self(typ, subtype,
                              is_classmethod=isinstance(node, Var) and node.is_classmethod)
        if node.is_property:
            assert isinstance(signature, CallableType)
            typ = signature.ret_type
        else:
            typ = signature
    itype = map_instance_to_supertype(itype, node.info)
    typ = expand_type_by_instance(typ, itype)
    return typ


</t>
<t tx="ekr.20220525082935.861">def non_method_protocol_members(tp: TypeInfo) -&gt; List[str]:
    """Find all non-callable members of a protocol."""

    assert tp.is_protocol
    result: List[str] = []
    anytype = AnyType(TypeOfAny.special_form)
    instance = Instance(tp, [anytype] * len(tp.defn.type_vars))

    for member in tp.protocol_members:
        typ = get_proper_type(find_member(member, instance, instance))
        if not isinstance(typ, (Overloaded, CallableType)):
            result.append(member)
    return result


</t>
<t tx="ekr.20220525082935.862">def is_callable_compatible(left: CallableType, right: CallableType,
                           *,
                           is_compat: Callable[[Type, Type], bool],
                           is_compat_return: Optional[Callable[[Type, Type], bool]] = None,
                           ignore_return: bool = False,
                           ignore_pos_arg_names: bool = False,
                           check_args_covariantly: bool = False,
                           allow_partial_overlap: bool = False,
                           strict_concatenate: bool = False) -&gt; bool:
    """Is the left compatible with the right, using the provided compatibility check?

    is_compat:
        The check we want to run against the parameters.

    is_compat_return:
        The check we want to run against the return type.
        If None, use the 'is_compat' check.

    check_args_covariantly:
        If true, check if the left's args is compatible with the right's
        instead of the other way around (contravariantly).

        This function is mostly used to check if the left is a subtype of the right which
        is why the default is to check the args contravariantly. However, it's occasionally
        useful to check the args using some other check, so we leave the variance
        configurable.

        For example, when checking the validity of overloads, it's useful to see if
        the first overload alternative has more precise arguments then the second.
        We would want to check the arguments covariantly in that case.

        Note! The following two function calls are NOT equivalent:

            is_callable_compatible(f, g, is_compat=is_subtype, check_args_covariantly=False)
            is_callable_compatible(g, f, is_compat=is_subtype, check_args_covariantly=True)

        The two calls are similar in that they both check the function arguments in
        the same direction: they both run `is_subtype(argument_from_g, argument_from_f)`.

        However, the two calls differ in which direction they check things like
        keyword arguments. For example, suppose f and g are defined like so:

            def f(x: int, *y: int) -&gt; int: ...
            def g(x: int) -&gt; int: ...

        In this case, the first call will succeed and the second will fail: f is a
        valid stand-in for g but not vice-versa.

    allow_partial_overlap:
        By default this function returns True if and only if *all* calls to left are
        also calls to right (with respect to the provided 'is_compat' function).

        If this parameter is set to 'True', we return True if *there exists at least one*
        call to left that's also a call to right.

        In other words, we perform an existential check instead of a universal one;
        we require left to only overlap with right instead of being a subset.

        For example, suppose we set 'is_compat' to some subtype check and compare following:

            f(x: float, y: str = "...", *args: bool) -&gt; str
            g(*args: int) -&gt; str

        This function would normally return 'False': f is not a subtype of g.
        However, we would return True if this parameter is set to 'True': the two
        calls are compatible if the user runs "f_or_g(3)". In the context of that
        specific call, the two functions effectively have signatures of:

            f2(float) -&gt; str
            g2(int) -&gt; str

        Here, f2 is a valid subtype of g2 so we return True.

        Specifically, if this parameter is set this function will:

        -   Ignore optional arguments on either the left or right that have no
            corresponding match.
        -   No longer mandate optional arguments on either side are also optional
            on the other.
        -   No longer mandate that if right has a *arg or **kwarg that left must also
            have the same.

        Note: when this argument is set to True, this function becomes "symmetric" --
        the following calls are equivalent:

            is_callable_compatible(f, g,
                                   is_compat=some_check,
                                   check_args_covariantly=False,
                                   allow_partial_overlap=True)
            is_callable_compatible(g, f,
                                   is_compat=some_check,
                                   check_args_covariantly=True,
                                   allow_partial_overlap=True)

        If the 'some_check' function is also symmetric, the two calls would be equivalent
        whether or not we check the args covariantly.
    """
    if is_compat_return is None:
        is_compat_return = is_compat

    # If either function is implicitly typed, ignore positional arg names too
    if left.implicit or right.implicit:
        ignore_pos_arg_names = True

    # Non-type cannot be a subtype of type.
    if right.is_type_obj() and not left.is_type_obj():
        return False

    # A callable L is a subtype of a generic callable R if L is a
    # subtype of every type obtained from R by substituting types for
    # the variables of R. We can check this by simply leaving the
    # generic variables of R as type variables, effectively varying
    # over all possible values.

    # It's okay even if these variables share ids with generic
    # type variables of L, because generating and solving
    # constraints for the variables of L to make L a subtype of R
    # (below) treats type variables on the two sides as independent.
    if left.variables:
        # Apply generic type variables away in left via type inference.
        unified = unify_generic_callable(left, right, ignore_return=ignore_return)
        if unified is None:
            return False
        else:
            left = unified

    # If we allow partial overlaps, we don't need to leave R generic:
    # if we can find even just a single typevar assignment which
    # would make these callables compatible, we should return True.

    # So, we repeat the above checks in the opposite direction. This also
    # lets us preserve the 'symmetry' property of allow_partial_overlap.
    if allow_partial_overlap and right.variables:
        unified = unify_generic_callable(right, left, ignore_return=ignore_return)
        if unified is not None:
            right = unified

    # Check return types.
    if not ignore_return and not is_compat_return(left.ret_type, right.ret_type):
        return False

    if check_args_covariantly:
        is_compat = flip_compat_check(is_compat)

    if not strict_concatenate and (left.from_concatenate or right.from_concatenate):
        strict_concatenate_check = False
    else:
        strict_concatenate_check = True

    return are_parameters_compatible(left, right, is_compat=is_compat,
                                     ignore_pos_arg_names=ignore_pos_arg_names,
                                     check_args_covariantly=check_args_covariantly,
                                     allow_partial_overlap=allow_partial_overlap,
                                     strict_concatenate_check=strict_concatenate_check)


</t>
<t tx="ekr.20220525082935.863">def are_parameters_compatible(left: Union[Parameters, CallableType],
                              right: Union[Parameters, CallableType],
                              *,
                              is_compat: Callable[[Type, Type], bool],
                              ignore_pos_arg_names: bool = False,
                              check_args_covariantly: bool = False,
                              allow_partial_overlap: bool = False,
                              strict_concatenate_check: bool = True) -&gt; bool:
    """Helper function for is_callable_compatible, used for Parameter compatibility"""
    if right.is_ellipsis_args:
        return True

    left_star = left.var_arg()
    left_star2 = left.kw_arg()
    right_star = right.var_arg()
    right_star2 = right.kw_arg()

    # Match up corresponding arguments and check them for compatibility. In
    # every pair (argL, argR) of corresponding arguments from L and R, argL must
    # be "more general" than argR if L is to be a subtype of R.

    # Arguments are corresponding if they either share a name, share a position,
    # or both. If L's corresponding argument is ambiguous, L is not a subtype of R.

    # If left has one corresponding argument by name and another by position,
    # consider them to be one "merged" argument (and not ambiguous) if they're
    # both optional, they're name-only and position-only respectively, and they
    # have the same type.  This rule allows functions with (*args, **kwargs) to
    # properly stand in for the full domain of formal arguments that they're
    # used for in practice.

    # Every argument in R must have a corresponding argument in L, and every
    # required argument in L must have a corresponding argument in R.

    # Phase 1: Confirm every argument in R has a corresponding argument in L.

    @others
    if _incompatible(left_star, right_star) or _incompatible(left_star2, right_star2):
        return False

    # Phase 1b: Check non-star args: for every arg right can accept, left must
    #           also accept. The only exception is if we are allowing partial
    #           partial overlaps: in that case, we ignore optional args on the right.
    for right_arg in right.formal_arguments():
        left_arg = mypy.typeops.callable_corresponding_argument(left, right_arg)
        if left_arg is None:
            if allow_partial_overlap and not right_arg.required:
                continue
            return False
        if not are_args_compatible(left_arg, right_arg, ignore_pos_arg_names,
                                   allow_partial_overlap, is_compat):
            return False

    # Phase 1c: Check var args. Right has an infinite series of optional positional
    #           arguments. Get all further positional args of left, and make sure
    #           they're more general then the corresponding member in right.
    if right_star is not None:
        # Synthesize an anonymous formal argument for the right
        right_by_position = right.try_synthesizing_arg_from_vararg(None)
        assert right_by_position is not None

        i = right_star.pos
        assert i is not None
        while i &lt; len(left.arg_kinds) and left.arg_kinds[i].is_positional():
            if allow_partial_overlap and left.arg_kinds[i].is_optional():
                break

            left_by_position = left.argument_by_position(i)
            assert left_by_position is not None

            if not are_args_compatible(left_by_position, right_by_position,
                                       ignore_pos_arg_names, allow_partial_overlap,
                                       is_compat):
                return False
            i += 1

    # Phase 1d: Check kw args. Right has an infinite series of optional named
    #           arguments. Get all further named args of left, and make sure
    #           they're more general then the corresponding member in right.
    if right_star2 is not None:
        right_names = {name for name in right.arg_names if name is not None}
        left_only_names = set()
        for name, kind in zip(left.arg_names, left.arg_kinds):
            if (name is None or kind.is_star()
                    or name in right_names
                    or not strict_concatenate_check):
                continue
            left_only_names.add(name)

        # Synthesize an anonymous formal argument for the right
        right_by_name = right.try_synthesizing_arg_from_kwarg(None)
        assert right_by_name is not None

        for name in left_only_names:
            left_by_name = left.argument_by_name(name)
            assert left_by_name is not None

            if allow_partial_overlap and not left_by_name.required:
                continue

            if not are_args_compatible(left_by_name, right_by_name, ignore_pos_arg_names,
                                       allow_partial_overlap, is_compat):
                return False

    # Phase 2: Left must not impose additional restrictions.
    #          (Every required argument in L must have a corresponding argument in R)
    #          Note: we already checked the *arg and **kwarg arguments in phase 1a.
    for left_arg in left.formal_arguments():
        right_by_name = (right.argument_by_name(left_arg.name)
                         if left_arg.name is not None
                         else None)

        right_by_pos = (right.argument_by_position(left_arg.pos)
                        if left_arg.pos is not None
                        else None)

        # If the left hand argument corresponds to two right-hand arguments,
        # neither of them can be required.
        if (right_by_name is not None
                and right_by_pos is not None
                and right_by_name != right_by_pos
                and (right_by_pos.required or right_by_name.required)
                and strict_concatenate_check):
            return False

        # All *required* left-hand arguments must have a corresponding
        # right-hand argument.  Optional args do not matter.
        if left_arg.required and right_by_pos is None and right_by_name is None:
            return False

    return True


</t>
<t tx="ekr.20220525082935.864"># Phase 1a: If left and right can both accept an infinite number of args,
#           their types must be compatible.
#
#           Furthermore, if we're checking for compatibility in all cases,
#           we confirm that if R accepts an infinite number of arguments,
#           L must accept the same.
def _incompatible(left_arg: Optional[FormalArgument],
                  right_arg: Optional[FormalArgument]) -&gt; bool:
    if right_arg is None:
        return False
    if left_arg is None:
        return not allow_partial_overlap
    return not is_compat(right_arg.typ, left_arg.typ)

</t>
<t tx="ekr.20220525082935.865">def are_args_compatible(
        left: FormalArgument,
        right: FormalArgument,
        ignore_pos_arg_names: bool,
        allow_partial_overlap: bool,
        is_compat: Callable[[Type, Type], bool]) -&gt; bool:
    @others
    # If right has a specific name it wants this argument to be, left must
    # have the same.
    if is_different(left.name, right.name):
        # But pay attention to whether we're ignoring positional arg names
        if not ignore_pos_arg_names or right.pos is None:
            return False

    # If right is at a specific position, left must have the same:
    if is_different(left.pos, right.pos):
        return False

    # If right's argument is optional, left's must also be
    # (unless we're relaxing the checks to allow potential
    # rather then definite compatibility).
    if not allow_partial_overlap and not right.required and left.required:
        return False

    # If we're allowing partial overlaps and neither arg is required,
    # the types don't actually need to be the same
    if allow_partial_overlap and not left.required and not right.required:
        return True

    # Left must have a more general type
    return is_compat(right.typ, left.typ)


</t>
<t tx="ekr.20220525082935.866">def is_different(left_item: Optional[object], right_item: Optional[object]) -&gt; bool:
    """Checks if the left and right items are different.

    If the right item is unspecified (e.g. if the right callable doesn't care
    about what name or position its arg has), we default to returning False.

    If we're allowing partial overlap, we also default to returning False
    if the left callable also doesn't care."""
    if right_item is None:
        return False
    if allow_partial_overlap and left_item is None:
        return False
    return left_item != right_item

</t>
<t tx="ekr.20220525082935.867">def flip_compat_check(is_compat: Callable[[Type, Type], bool]) -&gt; Callable[[Type, Type], bool]:
    def new_is_compat(left: Type, right: Type) -&gt; bool:
        return is_compat(right, left)
    return new_is_compat


</t>
<t tx="ekr.20220525082935.868">def unify_generic_callable(type: CallableType, target: CallableType,
                           ignore_return: bool,
                           return_constraint_direction: Optional[int] = None,
                           ) -&gt; Optional[CallableType]:
    """Try to unify a generic callable type with another callable type.

    Return unified CallableType if successful; otherwise, return None.
    """
    import mypy.solve

    if return_constraint_direction is None:
        return_constraint_direction = mypy.constraints.SUBTYPE_OF

    constraints: List[mypy.constraints.Constraint] = []
    for arg_type, target_arg_type in zip(type.arg_types, target.arg_types):
        c = mypy.constraints.infer_constraints(
            arg_type, target_arg_type, mypy.constraints.SUPERTYPE_OF)
        constraints.extend(c)
    if not ignore_return:
        c = mypy.constraints.infer_constraints(
            type.ret_type, target.ret_type, return_constraint_direction)
        constraints.extend(c)
    type_var_ids = [tvar.id for tvar in type.variables]
    inferred_vars = mypy.solve.solve_constraints(type_var_ids, constraints)
    if None in inferred_vars:
        return None
    non_none_inferred_vars = cast(List[Type], inferred_vars)
    had_errors = False

    @others
    applied = mypy.applytype.apply_generic_arguments(type, non_none_inferred_vars, report,
                                                     context=target)
    if had_errors:
        return None
    return applied


</t>
<t tx="ekr.20220525082935.869">def report(*args: Any) -&gt; None:
    nonlocal had_errors
    had_errors = True

</t>
<t tx="ekr.20220525082935.87">def analyze_typeddict_assign(self, s: AssignmentStmt) -&gt; bool:
    """Check if s defines a typed dict."""
    if isinstance(s.rvalue, CallExpr) and isinstance(s.rvalue.analyzed, TypedDictExpr):
        return True  # This is a valid and analyzed typed dict definition, nothing to do here.
    if len(s.lvalues) != 1 or not isinstance(s.lvalues[0], (NameExpr, MemberExpr)):
        return False
    lvalue = s.lvalues[0]
    name = lvalue.name
    is_typed_dict, info = self.typed_dict_analyzer.check_typeddict(s.rvalue, name,
                                                                   self.is_func_scope())
    if not is_typed_dict:
        return False
    if isinstance(lvalue, MemberExpr):
        self.fail("TypedDict type as attribute is not supported", lvalue)
        return False
    # Yes, it's a valid typed dict, but defer if it is not ready.
    if not info:
        self.mark_incomplete(name, lvalue, becomes_typeinfo=True)
    return True

</t>
<t tx="ekr.20220525082935.870">def try_restrict_literal_union(t: UnionType, s: Type) -&gt; Optional[List[Type]]:
    """Return the items of t, excluding any occurrence of s, if and only if
      - t only contains simple literals
      - s is a simple literal

    Otherwise, returns None
    """
    ps = get_proper_type(s)
    if not mypy.typeops.is_simple_literal(ps):
        return None

    new_items: List[Type] = []
    for i in t.relevant_items():
        pi = get_proper_type(i)
        if not mypy.typeops.is_simple_literal(pi):
            return None
        if pi != ps:
            new_items.append(i)
    return new_items


</t>
<t tx="ekr.20220525082935.871">def restrict_subtype_away(t: Type, s: Type, *, ignore_promotions: bool = False) -&gt; Type:
    """Return t minus s for runtime type assertions.

    If we can't determine a precise result, return a supertype of the
    ideal result (just t is a valid result).

    This is used for type inference of runtime type checks such as
    isinstance(). Currently this just removes elements of a union type.
    """
    t = get_proper_type(t)
    s = get_proper_type(s)

    if isinstance(t, UnionType):
        new_items = try_restrict_literal_union(t, s)
        if new_items is None:
            new_items = [
                restrict_subtype_away(item, s, ignore_promotions=ignore_promotions)
                for item in t.relevant_items()
                if (isinstance(get_proper_type(item), AnyType) or
                    not covers_at_runtime(item, s, ignore_promotions))
            ]
        return UnionType.make_union(new_items)
    elif covers_at_runtime(t, s, ignore_promotions):
        return UninhabitedType()
    else:
        return t


</t>
<t tx="ekr.20220525082935.872">def covers_at_runtime(item: Type, supertype: Type, ignore_promotions: bool) -&gt; bool:
    """Will isinstance(item, supertype) always return True at runtime?"""
    item = get_proper_type(item)
    supertype = get_proper_type(supertype)

    # Since runtime type checks will ignore type arguments, erase the types.
    supertype = erase_type(supertype)
    if is_proper_subtype(erase_type(item), supertype, ignore_promotions=ignore_promotions,
                         erase_instances=True):
        return True
    if isinstance(supertype, Instance) and supertype.type.is_protocol:
        # TODO: Implement more robust support for runtime isinstance() checks, see issue #3827.
        if is_proper_subtype(item, supertype, ignore_promotions=ignore_promotions):
            return True
    if isinstance(item, TypedDictType) and isinstance(supertype, Instance):
        # Special case useful for selecting TypedDicts from unions using isinstance(x, dict).
        if supertype.type.fullname == 'builtins.dict':
            return True
    # TODO: Add more special cases.
    return False


</t>
<t tx="ekr.20220525082935.873">def is_proper_subtype(left: Type, right: Type, *,
                      ignore_promotions: bool = False,
                      erase_instances: bool = False,
                      keep_erased_types: bool = False) -&gt; bool:
    """Is left a proper subtype of right?

    For proper subtypes, there's no need to rely on compatibility due to
    Any types. Every usable type is a proper subtype of itself.

    If erase_instances is True, erase left instance *after* mapping it to supertype
    (this is useful for runtime isinstance() checks). If keep_erased_types is True,
    do not consider ErasedType a subtype of all types (used by type inference against unions).
    """
    if TypeState.is_assumed_proper_subtype(left, right):
        return True
    if (isinstance(left, TypeAliasType) and isinstance(right, TypeAliasType) and
            left.is_recursive and right.is_recursive):
        # This case requires special care because it may cause infinite recursion.
        # See is_subtype() for more info.
        with pop_on_exit(TypeState._assuming_proper, left, right):
            return _is_proper_subtype(left, right,
                                      ignore_promotions=ignore_promotions,
                                      erase_instances=erase_instances,
                                      keep_erased_types=keep_erased_types)
    return _is_proper_subtype(left, right,
                              ignore_promotions=ignore_promotions,
                              erase_instances=erase_instances,
                              keep_erased_types=keep_erased_types)


</t>
<t tx="ekr.20220525082935.874">def _is_proper_subtype(left: Type, right: Type, *,
                       ignore_promotions: bool = False,
                       erase_instances: bool = False,
                       keep_erased_types: bool = False) -&gt; bool:
    orig_left = left
    orig_right = right
    left = get_proper_type(left)
    right = get_proper_type(right)

    if isinstance(right, UnionType) and not isinstance(left, UnionType):
        return any(is_proper_subtype(orig_left, item,
                                     ignore_promotions=ignore_promotions,
                                     erase_instances=erase_instances,
                                     keep_erased_types=keep_erased_types)
                   for item in right.items)
    return left.accept(ProperSubtypeVisitor(orig_right,
                                            ignore_promotions=ignore_promotions,
                                            erase_instances=erase_instances,
                                            keep_erased_types=keep_erased_types))


</t>
<t tx="ekr.20220525082935.875">class ProperSubtypeVisitor(TypeVisitor[bool]):
    @others
</t>
<t tx="ekr.20220525082935.876">def __init__(self, right: Type, *,
             ignore_promotions: bool = False,
             erase_instances: bool = False,
             keep_erased_types: bool = False) -&gt; None:
    self.right = get_proper_type(right)
    self.orig_right = right
    self.ignore_promotions = ignore_promotions
    self.erase_instances = erase_instances
    self.keep_erased_types = keep_erased_types
    self._subtype_kind = ProperSubtypeVisitor.build_subtype_kind(
        ignore_promotions=ignore_promotions,
        erase_instances=erase_instances,
        keep_erased_types=keep_erased_types
    )

</t>
<t tx="ekr.20220525082935.877">@staticmethod
def build_subtype_kind(*,
                       ignore_promotions: bool = False,
                       erase_instances: bool = False,
                       keep_erased_types: bool = False) -&gt; SubtypeKind:
    return (state.strict_optional,
            True,
            ignore_promotions,
            erase_instances,
            keep_erased_types)

</t>
<t tx="ekr.20220525082935.878">def _is_proper_subtype(self, left: Type, right: Type) -&gt; bool:
    return is_proper_subtype(left, right,
                             ignore_promotions=self.ignore_promotions,
                             erase_instances=self.erase_instances,
                             keep_erased_types=self.keep_erased_types)

</t>
<t tx="ekr.20220525082935.879">def visit_unbound_type(self, left: UnboundType) -&gt; bool:
    # This can be called if there is a bad type annotation. The result probably
    # doesn't matter much but by returning True we simplify these bad types away
    # from unions, which could filter out some bogus messages.
    return True

</t>
<t tx="ekr.20220525082935.88">def analyze_lvalues(self, s: AssignmentStmt) -&gt; None:
    # We cannot use s.type, because analyze_simple_literal_type() will set it.
    explicit = s.unanalyzed_type is not None
    if self.is_final_type(s.unanalyzed_type):
        # We need to exclude bare Final.
        assert isinstance(s.unanalyzed_type, UnboundType)
        if not s.unanalyzed_type.args:
            explicit = False

    if s.rvalue:
        if isinstance(s.rvalue, TempNode):
            has_explicit_value = not s.rvalue.no_rhs
        else:
            has_explicit_value = True
    else:
        has_explicit_value = False

    for lval in s.lvalues:
        self.analyze_lvalue(lval,
                            explicit_type=explicit,
                            is_final=s.is_final_def,
                            has_explicit_value=has_explicit_value)

</t>
<t tx="ekr.20220525082935.880">def visit_any(self, left: AnyType) -&gt; bool:
    return isinstance(self.right, AnyType)

</t>
<t tx="ekr.20220525082935.881">def visit_none_type(self, left: NoneType) -&gt; bool:
    if state.strict_optional:
        return (isinstance(self.right, NoneType) or
                is_named_instance(self.right, 'builtins.object'))
    return True

</t>
<t tx="ekr.20220525082935.882">def visit_uninhabited_type(self, left: UninhabitedType) -&gt; bool:
    return True

</t>
<t tx="ekr.20220525082935.883">def visit_erased_type(self, left: ErasedType) -&gt; bool:
    # This may be encountered during type inference. The result probably doesn't
    # matter much.
    # TODO: it actually does matter, figure out more principled logic about this.
    if self.keep_erased_types:
        return False
    return True

</t>
<t tx="ekr.20220525082935.884">def visit_deleted_type(self, left: DeletedType) -&gt; bool:
    return True

</t>
<t tx="ekr.20220525082935.885">def visit_instance(self, left: Instance) -&gt; bool:
    right = self.right
    if isinstance(right, Instance):
        if TypeState.is_cached_subtype_check(self._subtype_kind, left, right):
            return True
        if not self.ignore_promotions:
            for base in left.type.mro:
                if base._promote and self._is_proper_subtype(base._promote, right):
                    TypeState.record_subtype_cache_entry(self._subtype_kind, left, right)
                    return True

        if left.type.has_base(right.type.fullname):
            # Map left type to corresponding right instances.
            left = map_instance_to_supertype(left, right.type)
            if self.erase_instances:
                erased = erase_type(left)
                assert isinstance(erased, Instance)
                left = erased

            nominal = True
            for ta, ra, tvar in zip(left.args, right.args, right.type.defn.type_vars):
                if isinstance(tvar, TypeVarType):
                    variance = tvar.variance
                    if variance == COVARIANT:
                        nominal = self._is_proper_subtype(ta, ra)
                    elif variance == CONTRAVARIANT:
                        nominal = self._is_proper_subtype(ra, ta)
                    else:
                        nominal = mypy.sametypes.is_same_type(ta, ra)
                else:
                    nominal = mypy.sametypes.is_same_type(ta, ra)
                if not nominal:
                    break

            if nominal:
                TypeState.record_subtype_cache_entry(self._subtype_kind, left, right)
            return nominal
        if (right.type.is_protocol and
                is_protocol_implementation(left, right, proper_subtype=True)):
            return True
        return False
    if isinstance(right, CallableType):
        call = find_member('__call__', left, left, is_operator=True)
        if call:
            return self._is_proper_subtype(call, right)
        return False
    return False

</t>
<t tx="ekr.20220525082935.886">def visit_type_var(self, left: TypeVarType) -&gt; bool:
    if isinstance(self.right, TypeVarType) and left.id == self.right.id:
        return True
    if left.values and self._is_proper_subtype(
            mypy.typeops.make_simplified_union(left.values), self.right):
        return True
    return self._is_proper_subtype(left.upper_bound, self.right)

</t>
<t tx="ekr.20220525082935.887">def visit_param_spec(self, left: ParamSpecType) -&gt; bool:
    right = self.right
    if (
        isinstance(right, ParamSpecType)
        and right.id == left.id
        and right.flavor == left.flavor
    ):
        return True
    return self._is_proper_subtype(left.upper_bound, self.right)

</t>
<t tx="ekr.20220525082935.888">def visit_type_var_tuple(self, left: TypeVarTupleType) -&gt; bool:
    right = self.right
    if (
        isinstance(right, TypeVarTupleType)
        and right.id == left.id
    ):
        return True
    return self._is_proper_subtype(left.upper_bound, self.right)

</t>
<t tx="ekr.20220525082935.889">def visit_unpack_type(self, left: UnpackType) -&gt; bool:
    if isinstance(self.right, UnpackType):
        return self._is_proper_subtype(left.type, self.right.type)
    return False

</t>
<t tx="ekr.20220525082935.89">def apply_dynamic_class_hook(self, s: AssignmentStmt) -&gt; None:
    if not isinstance(s.rvalue, CallExpr):
        return
    fname = None
    call = s.rvalue
    while True:
        if isinstance(call.callee, RefExpr):
            fname = call.callee.fullname
        # check if method call
        if fname is None and isinstance(call.callee, MemberExpr):
            callee_expr = call.callee.expr
            if isinstance(callee_expr, RefExpr) and callee_expr.fullname:
                method_name = call.callee.name
                fname = callee_expr.fullname + '.' + method_name
            elif isinstance(callee_expr, CallExpr):
                # check if chain call
                call = callee_expr
                continue
        break
    if not fname:
        return
    hook = self.plugin.get_dynamic_class_hook(fname)
    if not hook:
        return
    for lval in s.lvalues:
        if not isinstance(lval, NameExpr):
            continue
        hook(DynamicClassDefContext(call, lval.name, self))

</t>
<t tx="ekr.20220525082935.890">def visit_parameters(self, left: Parameters) -&gt; bool:
    right = self.right
    if isinstance(right, Parameters) or isinstance(right, CallableType):
        return are_parameters_compatible(left, right, is_compat=self._is_proper_subtype)
    else:
        return False

</t>
<t tx="ekr.20220525082935.891">def visit_callable_type(self, left: CallableType) -&gt; bool:
    right = self.right
    if isinstance(right, CallableType):
        return is_callable_compatible(left, right, is_compat=self._is_proper_subtype)
    elif isinstance(right, Overloaded):
        return all(self._is_proper_subtype(left, item)
                   for item in right.items)
    elif isinstance(right, Instance):
        return self._is_proper_subtype(left.fallback, right)
    elif isinstance(right, TypeType):
        # This is unsound, we don't check the __init__ signature.
        return left.is_type_obj() and self._is_proper_subtype(left.ret_type, right.item)
    return False

</t>
<t tx="ekr.20220525082935.892">def visit_tuple_type(self, left: TupleType) -&gt; bool:
    right = self.right
    if isinstance(right, Instance):
        if is_named_instance(right, TUPLE_LIKE_INSTANCE_NAMES):
            if not right.args:
                return False
            iter_type = get_proper_type(right.args[0])
            if is_named_instance(right, 'builtins.tuple') and isinstance(iter_type, AnyType):
                # TODO: We shouldn't need this special case. This is currently needed
                #       for isinstance(x, tuple), though it's unclear why.
                return True
            return all(self._is_proper_subtype(li, iter_type) for li in left.items)
        return self._is_proper_subtype(mypy.typeops.tuple_fallback(left), right)
    elif isinstance(right, TupleType):
        if len(left.items) != len(right.items):
            return False
        for l, r in zip(left.items, right.items):
            if not self._is_proper_subtype(l, r):
                return False
        return self._is_proper_subtype(mypy.typeops.tuple_fallback(left),
                                       mypy.typeops.tuple_fallback(right))
    return False

</t>
<t tx="ekr.20220525082935.893">def visit_typeddict_type(self, left: TypedDictType) -&gt; bool:
    right = self.right
    if isinstance(right, TypedDictType):
        for name, typ in left.items.items():
            if (name in right.items
                    and not mypy.sametypes.is_same_type(typ, right.items[name])):
                return False
        for name, typ in right.items.items():
            if name not in left.items:
                return False
        return True
    return self._is_proper_subtype(left.fallback, right)

</t>
<t tx="ekr.20220525082935.894">def visit_literal_type(self, left: LiteralType) -&gt; bool:
    if isinstance(self.right, LiteralType):
        return left == self.right
    else:
        return self._is_proper_subtype(left.fallback, self.right)

</t>
<t tx="ekr.20220525082935.895">def visit_overloaded(self, left: Overloaded) -&gt; bool:
    # TODO: What's the right thing to do here?
    return False

</t>
<t tx="ekr.20220525082935.896">def visit_union_type(self, left: UnionType) -&gt; bool:
    return all(self._is_proper_subtype(item, self.orig_right) for item in left.items)

</t>
<t tx="ekr.20220525082935.897">def visit_partial_type(self, left: PartialType) -&gt; bool:
    # TODO: What's the right thing to do here?
    return False

</t>
<t tx="ekr.20220525082935.898">def visit_type_type(self, left: TypeType) -&gt; bool:
    right = self.right
    if isinstance(right, TypeType):
        # This is unsound, we don't check the __init__ signature.
        return self._is_proper_subtype(left.item, right.item)
    if isinstance(right, CallableType):
        # This is also unsound because of __init__.
        return right.is_type_obj() and self._is_proper_subtype(left.item, right.ret_type)
    if isinstance(right, Instance):
        if right.type.fullname == 'builtins.type':
            # TODO: Strictly speaking, the type builtins.type is considered equivalent to
            #       Type[Any]. However, this would break the is_proper_subtype check in
            #       conditional_types for cases like isinstance(x, type) when the type
            #       of x is Type[int]. It's unclear what's the right way to address this.
            return True
        if right.type.fullname == 'builtins.object':
            return True
        item = left.item
        if isinstance(item, TypeVarType):
            item = get_proper_type(item.upper_bound)
        if isinstance(item, Instance):
            metaclass = item.type.metaclass_type
            return metaclass is not None and self._is_proper_subtype(metaclass, right)
    return False

</t>
<t tx="ekr.20220525082935.899">def visit_type_alias_type(self, left: TypeAliasType) -&gt; bool:
    assert False, f"This should be never called, got {left}"


</t>
<t tx="ekr.20220525082935.9">def prepare_builtins_namespace(self, file_node: MypyFile) -&gt; None:
    """Add certain special-cased definitions to the builtins module.

    Some definitions are too special or fundamental to be processed
    normally from the AST.
    """
    names = file_node.names

    # Add empty definition for core built-in classes, since they are required for basic
    # operation. These will be completed later on.
    for name in CORE_BUILTIN_CLASSES:
        cdef = ClassDef(name, Block([]))  # Dummy ClassDef, will be replaced later
        info = TypeInfo(SymbolTable(), cdef, 'builtins')
        info._fullname = f'builtins.{name}'
        names[name] = SymbolTableNode(GDEF, info)

    bool_info = names['bool'].node
    assert isinstance(bool_info, TypeInfo)
    bool_type = Instance(bool_info, [])

    special_var_types: List[Tuple[str, Type]] = [
        ('None', NoneType()),
        # reveal_type is a mypy-only function that gives an error with
        # the type of its arg.
        ('reveal_type', AnyType(TypeOfAny.special_form)),
        # reveal_locals is a mypy-only function that gives an error with the types of
        # locals
        ('reveal_locals', AnyType(TypeOfAny.special_form)),
        ('True', bool_type),
        ('False', bool_type),
        ('__debug__', bool_type),
    ]

    for name, typ in special_var_types:
        v = Var(name, typ)
        v._fullname = f'builtins.{name}'
        file_node.names[name] = SymbolTableNode(GDEF, v)

</t>
<t tx="ekr.20220525082935.90">def unwrap_final(self, s: AssignmentStmt) -&gt; bool:
    """Strip Final[...] if present in an assignment.

    This is done to invoke type inference during type checking phase for this
    assignment. Also, Final[...] doesn't affect type in any way -- it is rather an
    access qualifier for given `Var`.

    Also perform various consistency checks.

    Returns True if Final[...] was present.
    """
    if not s.unanalyzed_type or not self.is_final_type(s.unanalyzed_type):
        return False
    assert isinstance(s.unanalyzed_type, UnboundType)
    if len(s.unanalyzed_type.args) &gt; 1:
        self.fail("Final[...] takes at most one type argument", s.unanalyzed_type)
    invalid_bare_final = False
    if not s.unanalyzed_type.args:
        s.type = None
        if isinstance(s.rvalue, TempNode) and s.rvalue.no_rhs:
            invalid_bare_final = True
            self.fail("Type in Final[...] can only be omitted if there is an initializer", s)
    else:
        s.type = s.unanalyzed_type.args[0]

    if s.type is not None and self.is_classvar(s.type):
        self.fail("Variable should not be annotated with both ClassVar and Final", s)
        return False

    if len(s.lvalues) != 1 or not isinstance(s.lvalues[0], RefExpr):
        self.fail("Invalid final declaration", s)
        return False
    lval = s.lvalues[0]
    assert isinstance(lval, RefExpr)

    # Reset inferred status if it was set due to simple literal rvalue on previous iteration.
    # TODO: this is a best-effort quick fix, we should avoid the need to manually sync this,
    # see https://github.com/python/mypy/issues/6458.
    if lval.is_new_def:
        lval.is_inferred_def = s.type is None

    if self.loop_depth &gt; 0:
        self.fail("Cannot use Final inside a loop", s)
    if self.type and self.type.is_protocol:
        self.msg.protocol_members_cant_be_final(s)
    if (isinstance(s.rvalue, TempNode) and s.rvalue.no_rhs and
            not self.is_stub_file and not self.is_class_scope()):
        if not invalid_bare_final:  # Skip extra error messages.
            self.msg.final_without_value(s)
    return True

</t>
<t tx="ekr.20220525082935.900">def is_more_precise(left: Type, right: Type, *, ignore_promotions: bool = False) -&gt; bool:
    """Check if left is a more precise type than right.

    A left is a proper subtype of right, left is also more precise than
    right. Also, if right is Any, left is more precise than right, for
    any left.
    """
    # TODO Should List[int] be more precise than List[Any]?
    right = get_proper_type(right)
    if isinstance(right, AnyType):
        return True
    return is_proper_subtype(left, right, ignore_promotions=ignore_promotions)
</t>
<t tx="ekr.20220525082935.901">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Mechanisms for inferring function types based on callsites.

Currently works by collecting all argument types at callsites,
synthesizing a list of possible function types from that, trying them
all, and picking the one with the fewest errors that we think is the
"best".

Can return JSON that pyannotate can use to apply the annotations to code.

There are a bunch of TODOs here:
 * Maybe want a way to surface the choices not selected??
 * We can generate an exponential number of type suggestions, and probably want
   a way to not always need to check them all.
 * Our heuristics for what types to try are primitive and not yet
   supported by real practice.
 * More!

Other things:
 * This is super brute force. Could we integrate with the typechecker
   more to understand more about what is going on?
 * Like something with tracking constraints/unification variables?
 * No understanding of type variables at *all*
"""

from typing import (
    List, Optional, Tuple, Dict, Callable, Union, NamedTuple, TypeVar, Iterator, cast,
)
from typing_extensions import TypedDict

from mypy.state import state
from mypy.types import (
    Type, AnyType, TypeOfAny, CallableType, UnionType, NoneType, Instance, TupleType,
    TypeVarType, FunctionLike, UninhabitedType,
    TypeStrVisitor, TypeTranslator,
    is_optional, remove_optional, ProperType, get_proper_type,
    TypedDictType, TypeAliasType
)
from mypy.build import State, Graph
from mypy.nodes import (
    ArgKind, ARG_STAR, ARG_STAR2, FuncDef, MypyFile, SymbolTable,
    Decorator, RefExpr,
    SymbolNode, TypeInfo, Expression, ReturnStmt, CallExpr,
    reverse_builtin_aliases,
)
from mypy.server.update import FineGrainedBuildManager
from mypy.util import split_target
from mypy.find_sources import SourceFinder, InvalidSourceList
from mypy.modulefinder import PYTHON_EXTENSIONS
from mypy.plugin import Plugin, FunctionContext, MethodContext
from mypy.traverser import TraverserVisitor
from mypy.checkexpr import has_any_type, map_actuals_to_formals

from mypy.join import join_type_list
from mypy.meet import meet_type_list
from mypy.sametypes import is_same_type
from mypy.typeops import make_simplified_union

from contextlib import contextmanager

import itertools
import json
import os


@others
</t>
<t tx="ekr.20220525082935.902">class PyAnnotateSignature(TypedDict):
    return_type: str
    arg_types: List[str]


</t>
<t tx="ekr.20220525082935.903">class Callsite(NamedTuple):
    path: str
    line: int
    arg_kinds: List[List[ArgKind]]
    callee_arg_names: List[Optional[str]]
    arg_names: List[List[Optional[str]]]
    arg_types: List[List[Type]]


</t>
<t tx="ekr.20220525082935.904">class SuggestionPlugin(Plugin):
    """Plugin that records all calls to a given target."""

    @others
</t>
<t tx="ekr.20220525082935.905">def __init__(self, target: str) -&gt; None:
    if target.endswith(('.__new__', '.__init__')):
        target = target.rsplit('.', 1)[0]

    self.target = target
    # List of call sites found by dmypy suggest:
    # (path, line, &lt;arg kinds&gt;, &lt;arg names&gt;, &lt;arg types&gt;)
    self.mystery_hits: List[Callsite] = []

</t>
<t tx="ekr.20220525082935.906">def get_function_hook(self, fullname: str
                      ) -&gt; Optional[Callable[[FunctionContext], Type]]:
    if fullname == self.target:
        return self.log
    else:
        return None

</t>
<t tx="ekr.20220525082935.907">def get_method_hook(self, fullname: str
                    ) -&gt; Optional[Callable[[MethodContext], Type]]:
    if fullname == self.target:
        return self.log
    else:
        return None

</t>
<t tx="ekr.20220525082935.908">def log(self, ctx: Union[FunctionContext, MethodContext]) -&gt; Type:
    self.mystery_hits.append(Callsite(
        ctx.api.path,
        ctx.context.line,
        ctx.arg_kinds,
        ctx.callee_arg_names,
        ctx.arg_names,
        ctx.arg_types))
    return ctx.default_return_type


</t>
<t tx="ekr.20220525082935.909"># NOTE: We could make this a bunch faster by implementing a StatementVisitor that skips
# traversing into expressions
class ReturnFinder(TraverserVisitor):
    """Visitor for finding all types returned from a function."""
    @others
</t>
<t tx="ekr.20220525082935.91">def check_final_implicit_def(self, s: AssignmentStmt) -&gt; None:
    """Do basic checks for final declaration on self in __init__.

    Additional re-definition checks are performed by `analyze_lvalue`.
    """
    if not s.is_final_def:
        return
    lval = s.lvalues[0]
    assert isinstance(lval, RefExpr)
    if isinstance(lval, MemberExpr):
        if not self.is_self_member_ref(lval):
            self.fail("Final can be only applied to a name or an attribute on self", s)
            s.is_final_def = False
            return
        else:
            assert self.function_stack
            if self.function_stack[-1].name != '__init__':
                self.fail("Can only declare a final attribute in class body or __init__", s)
                s.is_final_def = False
                return

</t>
<t tx="ekr.20220525082935.910">def __init__(self, typemap: Dict[Expression, Type]) -&gt; None:
    self.typemap = typemap
    self.return_types: List[Type] = []

</t>
<t tx="ekr.20220525082935.911">def visit_return_stmt(self, o: ReturnStmt) -&gt; None:
    if o.expr is not None and o.expr in self.typemap:
        self.return_types.append(self.typemap[o.expr])

</t>
<t tx="ekr.20220525082935.912">def visit_func_def(self, o: FuncDef) -&gt; None:
    # Skip nested functions
    pass


</t>
<t tx="ekr.20220525082935.913">def get_return_types(typemap: Dict[Expression, Type], func: FuncDef) -&gt; List[Type]:
    """Find all the types returned by return statements in func."""
    finder = ReturnFinder(typemap)
    func.body.accept(finder)
    return finder.return_types


</t>
<t tx="ekr.20220525082935.914">class ArgUseFinder(TraverserVisitor):
    """Visitor for finding all the types of arguments that each arg is passed to.

    This is extremely simple minded but might be effective anyways.
    """
    @others
</t>
<t tx="ekr.20220525082935.915">def __init__(self, func: FuncDef, typemap: Dict[Expression, Type]) -&gt; None:
    self.typemap = typemap
    self.arg_types: Dict[SymbolNode, List[Type]] = {arg.variable: [] for arg in func.arguments}

</t>
<t tx="ekr.20220525082935.916">def visit_call_expr(self, o: CallExpr) -&gt; None:
    if not any(isinstance(e, RefExpr) and e.node in self.arg_types for e in o.args):
        return

    typ = get_proper_type(self.typemap.get(o.callee))
    if not isinstance(typ, CallableType):
        return

    formal_to_actual = map_actuals_to_formals(
        o.arg_kinds, o.arg_names, typ.arg_kinds, typ.arg_names,
        lambda n: AnyType(TypeOfAny.special_form))

    for i, args in enumerate(formal_to_actual):
        for arg_idx in args:
            arg = o.args[arg_idx]
            if isinstance(arg, RefExpr) and arg.node in self.arg_types:
                self.arg_types[arg.node].append(typ.arg_types[i])


</t>
<t tx="ekr.20220525082935.917">def get_arg_uses(typemap: Dict[Expression, Type], func: FuncDef) -&gt; List[List[Type]]:
    """Find all the types of arguments that each arg is passed to.

    For example, given
      def foo(x: int) -&gt; None: ...
      def bar(x: str) -&gt; None: ...
      def test(x, y):
          foo(x)
          bar(y)

    this will return [[int], [str]].
    """
    finder = ArgUseFinder(func, typemap)
    func.body.accept(finder)
    return [finder.arg_types[arg.variable] for arg in func.arguments]


</t>
<t tx="ekr.20220525082935.918">class SuggestionFailure(Exception):
    pass


</t>
<t tx="ekr.20220525082935.919">def is_explicit_any(typ: AnyType) -&gt; bool:
    # Originally I wanted to count as explicit anything derived from an explicit any, but that
    # seemed too strict in some testing.
    # return (typ.type_of_any == TypeOfAny.explicit
    #         or (typ.source_any is not None and typ.source_any.type_of_any == TypeOfAny.explicit))
    # Important question: what should we do with source_any stuff? Does that count?
    # And actually should explicit anys count at all?? Maybe not!
    return typ.type_of_any == TypeOfAny.explicit


</t>
<t tx="ekr.20220525082935.92">def store_final_status(self, s: AssignmentStmt) -&gt; None:
    """If this is a locally valid final declaration, set the corresponding flag on `Var`."""
    if s.is_final_def:
        if len(s.lvalues) == 1 and isinstance(s.lvalues[0], RefExpr):
            node = s.lvalues[0].node
            if isinstance(node, Var):
                node.is_final = True
                node.final_value = self.unbox_literal(s.rvalue)
                if (self.is_class_scope() and
                        (isinstance(s.rvalue, TempNode) and s.rvalue.no_rhs)):
                    node.final_unset_in_class = True
    else:
        for lval in self.flatten_lvalues(s.lvalues):
            # Special case: we are working with an `Enum`:
            #
            #   class MyEnum(Enum):
            #       key = 'some value'
            #
            # Here `key` is implicitly final. In runtime, code like
            #
            #     MyEnum.key = 'modified'
            #
            # will fail with `AttributeError: Cannot reassign members.`
            # That's why we need to replicate this.
            if (isinstance(lval, NameExpr) and
                    isinstance(self.type, TypeInfo) and
                    self.type.is_enum):
                cur_node = self.type.names.get(lval.name, None)
                if (cur_node and isinstance(cur_node.node, Var) and
                        not (isinstance(s.rvalue, TempNode) and s.rvalue.no_rhs)):
                    # Double underscored members are writable on an `Enum`.
                    # (Except read-only `__members__` but that is handled in type checker)
                    cur_node.node.is_final = s.is_final_def = not is_dunder(cur_node.node.name)

            # Special case: deferred initialization of a final attribute in __init__.
            # In this case we just pretend this is a valid final definition to suppress
            # errors about assigning to final attribute.
            if isinstance(lval, MemberExpr) and self.is_self_member_ref(lval):
                assert self.type, "Self member outside a class"
                cur_node = self.type.names.get(lval.name, None)
                if cur_node and isinstance(cur_node.node, Var) and cur_node.node.is_final:
                    assert self.function_stack
                    top_function = self.function_stack[-1]
                    if (top_function.name == '__init__' and
                            cur_node.node.final_unset_in_class and
                            not cur_node.node.final_set_in_init and
                            not (isinstance(s.rvalue, TempNode) and s.rvalue.no_rhs)):
                        cur_node.node.final_set_in_init = True
                        s.is_final_def = True

</t>
<t tx="ekr.20220525082935.920">def is_implicit_any(typ: Type) -&gt; bool:
    typ = get_proper_type(typ)
    return isinstance(typ, AnyType) and not is_explicit_any(typ)


</t>
<t tx="ekr.20220525082935.921">class SuggestionEngine:
    """Engine for finding call sites and suggesting signatures."""

    @others
</t>
<t tx="ekr.20220525082935.922">def __init__(self, fgmanager: FineGrainedBuildManager,
             *,
             json: bool,
             no_errors: bool = False,
             no_any: bool = False,
             try_text: bool = False,
             flex_any: Optional[float] = None,
             use_fixme: Optional[str] = None,
             max_guesses: Optional[int] = None
             ) -&gt; None:
    self.fgmanager = fgmanager
    self.manager = fgmanager.manager
    self.plugin = self.manager.plugin
    self.graph = fgmanager.graph
    self.finder = SourceFinder(self.manager.fscache, self.manager.options)

    self.give_json = json
    self.no_errors = no_errors
    self.try_text = try_text
    self.flex_any = flex_any
    if no_any:
        self.flex_any = 1.0

    self.max_guesses = max_guesses or 64
    self.use_fixme = use_fixme

</t>
<t tx="ekr.20220525082935.923">def suggest(self, function: str) -&gt; str:
    """Suggest an inferred type for function."""
    mod, func_name, node = self.find_node(function)

    with self.restore_after(mod):
        with self.with_export_types():
            suggestion = self.get_suggestion(mod, node)

    if self.give_json:
        return self.json_suggestion(mod, func_name, node, suggestion)
    else:
        return self.format_signature(suggestion)

</t>
<t tx="ekr.20220525082935.924">def suggest_callsites(self, function: str) -&gt; str:
    """Find a list of call sites of function."""
    mod, _, node = self.find_node(function)
    with self.restore_after(mod):
        callsites, _ = self.get_callsites(node)

    return '\n'.join(dedup(
        [f"{path}:{line}: {self.format_args(arg_kinds, arg_names, arg_types)}"
         for path, line, arg_kinds, _, arg_names, arg_types in callsites]
    ))

</t>
<t tx="ekr.20220525082935.925">@contextmanager
def restore_after(self, module: str) -&gt; Iterator[None]:
    """Context manager that reloads a module after executing the body.

    This should undo any damage done to the module state while mucking around.
    """
    try:
        yield
    finally:
        self.reload(self.graph[module])

</t>
<t tx="ekr.20220525082935.926">@contextmanager
def with_export_types(self) -&gt; Iterator[None]:
    """Context manager that enables the export_types flag in the body.

    This causes type information to be exported into the manager's all_types variable.
    """
    old = self.manager.options.export_types
    self.manager.options.export_types = True
    try:
        yield
    finally:
        self.manager.options.export_types = old

</t>
<t tx="ekr.20220525082935.927">def get_trivial_type(self, fdef: FuncDef) -&gt; CallableType:
    """Generate a trivial callable type from a func def, with all Anys"""
    # The Anys are marked as being from the suggestion engine
    # since they need some special treatment (specifically,
    # constraint generation ignores them.)
    return CallableType(
        [AnyType(TypeOfAny.suggestion_engine) for a in fdef.arg_kinds],
        fdef.arg_kinds,
        fdef.arg_names,
        AnyType(TypeOfAny.suggestion_engine),
        self.named_type('builtins.function'))

</t>
<t tx="ekr.20220525082935.928">def get_starting_type(self, fdef: FuncDef) -&gt; CallableType:
    if isinstance(fdef.type, CallableType):
        return make_suggestion_anys(fdef.type)
    else:
        return self.get_trivial_type(fdef)

</t>
<t tx="ekr.20220525082935.929">def get_args(self, is_method: bool,
             base: CallableType, defaults: List[Optional[Type]],
             callsites: List[Callsite],
             uses: List[List[Type]]) -&gt; List[List[Type]]:
    """Produce a list of type suggestions for each argument type."""
    types: List[List[Type]] = []
    for i in range(len(base.arg_kinds)):
        # Make self args Any but this will get overridden somewhere in the checker
        if i == 0 and is_method:
            types.append([AnyType(TypeOfAny.suggestion_engine)])
            continue

        all_arg_types = []
        for call in callsites:
            for typ in call.arg_types[i - is_method]:
                # Collect all the types except for implicit anys
                if not is_implicit_any(typ):
                    all_arg_types.append(typ)
        all_use_types = []
        for typ in uses[i]:
            # Collect all the types except for implicit anys
            if not is_implicit_any(typ):
                all_use_types.append(typ)
        # Add in any default argument types
        default = defaults[i]
        if default:
            all_arg_types.append(default)
            if all_use_types:
                all_use_types.append(default)

        arg_types = []

        if (all_arg_types
                and all(isinstance(get_proper_type(tp), NoneType) for tp in all_arg_types)):
            arg_types.append(
                UnionType.make_union([all_arg_types[0], AnyType(TypeOfAny.explicit)]))
        elif all_arg_types:
            arg_types.extend(generate_type_combinations(all_arg_types))
        else:
            arg_types.append(AnyType(TypeOfAny.explicit))

        if all_use_types:
            # This is a meet because the type needs to be compatible with all the uses
            arg_types.append(meet_type_list(all_use_types))

        types.append(arg_types)
    return types

</t>
<t tx="ekr.20220525082935.93">def flatten_lvalues(self, lvalues: List[Expression]) -&gt; List[Expression]:
    res: List[Expression] = []
    for lv in lvalues:
        if isinstance(lv, (TupleExpr, ListExpr)):
            res.extend(self.flatten_lvalues(lv.items))
        else:
            res.append(lv)
    return res

</t>
<t tx="ekr.20220525082935.930">def get_default_arg_types(self, fdef: FuncDef) -&gt; List[Optional[Type]]:
    return [
        self.manager.all_types[arg.initializer] if arg.initializer else None
        for arg in fdef.arguments
    ]

</t>
<t tx="ekr.20220525082935.931">def add_adjustments(self, typs: List[Type]) -&gt; List[Type]:
    if not self.try_text or self.manager.options.python_version[0] != 2:
        return typs
    translator = StrToText(self.named_type)
    return dedup(typs + [tp.accept(translator) for tp in typs])

</t>
<t tx="ekr.20220525082935.932">def get_guesses(self, is_method: bool, base: CallableType, defaults: List[Optional[Type]],
                callsites: List[Callsite],
                uses: List[List[Type]]) -&gt; List[CallableType]:
    """Compute a list of guesses for a function's type.

    This focuses just on the argument types, and doesn't change the provided return type.
    """
    options = self.get_args(is_method, base, defaults, callsites, uses)
    options = [self.add_adjustments(tps) for tps in options]

    # Take the first `max_guesses` guesses.
    product = itertools.islice(itertools.product(*options), 0, self.max_guesses)
    return [refine_callable(base, base.copy_modified(arg_types=list(x))) for x in product]

</t>
<t tx="ekr.20220525082935.933">def get_callsites(self, func: FuncDef) -&gt; Tuple[List[Callsite], List[str]]:
    """Find all call sites of a function."""
    new_type = self.get_starting_type(func)

    collector_plugin = SuggestionPlugin(func.fullname)

    self.plugin._plugins.insert(0, collector_plugin)
    try:
        errors = self.try_type(func, new_type)
    finally:
        self.plugin._plugins.pop(0)

    return collector_plugin.mystery_hits, errors

</t>
<t tx="ekr.20220525082935.934">def filter_options(
    self, guesses: List[CallableType], is_method: bool, ignore_return: bool
) -&gt; List[CallableType]:
    """Apply any configured filters to the possible guesses.

    Currently the only option is filtering based on Any prevalance."""
    return [
        t for t in guesses
        if self.flex_any is None
        or any_score_callable(t, is_method, ignore_return) &gt;= self.flex_any
    ]

</t>
<t tx="ekr.20220525082935.935">def find_best(self, func: FuncDef, guesses: List[CallableType]) -&gt; Tuple[CallableType, int]:
    """From a list of possible function types, find the best one.

    For best, we want the fewest errors, then the best "score" from score_callable.
    """
    if not guesses:
        raise SuggestionFailure("No guesses that match criteria!")
    errors = {guess: self.try_type(func, guess) for guess in guesses}
    best = min(guesses,
               key=lambda s: (count_errors(errors[s]), self.score_callable(s)))
    return best, count_errors(errors[best])

</t>
<t tx="ekr.20220525082935.936">def get_guesses_from_parent(self, node: FuncDef) -&gt; List[CallableType]:
    """Try to get a guess of a method type from a parent class."""
    if not node.info:
        return []

    for parent in node.info.mro[1:]:
        pnode = parent.names.get(node.name)
        if pnode and isinstance(pnode.node, (FuncDef, Decorator)):
            typ = get_proper_type(pnode.node.type)
            # FIXME: Doesn't work right with generic tyeps
            if isinstance(typ, CallableType) and len(typ.arg_types) == len(node.arguments):
                # Return the first thing we find, since it probably doesn't make sense
                # to grab things further up in the chain if an earlier parent has it.
                return [typ]

    return []

</t>
<t tx="ekr.20220525082935.937">def get_suggestion(self, mod: str, node: FuncDef) -&gt; PyAnnotateSignature:
    """Compute a suggestion for a function.

    Return the type and whether the first argument should be ignored.
    """
    graph = self.graph
    callsites, orig_errors = self.get_callsites(node)
    uses = get_arg_uses(self.manager.all_types, node)

    if self.no_errors and orig_errors:
        raise SuggestionFailure("Function does not typecheck.")

    is_method = bool(node.info) and not node.is_static

    with state.strict_optional_set(graph[mod].options.strict_optional):
        guesses = self.get_guesses(
            is_method,
            self.get_starting_type(node),
            self.get_default_arg_types(node),
            callsites,
            uses,
        )
    guesses += self.get_guesses_from_parent(node)
    guesses = self.filter_options(guesses, is_method, ignore_return=True)
    best, _ = self.find_best(node, guesses)

    # Now try to find the return type!
    self.try_type(node, best)
    returns = get_return_types(self.manager.all_types, node)
    with state.strict_optional_set(graph[mod].options.strict_optional):
        if returns:
            ret_types = generate_type_combinations(returns)
        else:
            ret_types = [NoneType()]

    guesses = [best.copy_modified(ret_type=refine_type(best.ret_type, t)) for t in ret_types]
    guesses = self.filter_options(guesses, is_method, ignore_return=False)
    best, errors = self.find_best(node, guesses)

    if self.no_errors and errors:
        raise SuggestionFailure("No annotation without errors")

    return self.pyannotate_signature(mod, is_method, best)

</t>
<t tx="ekr.20220525082935.938">def format_args(self,
                arg_kinds: List[List[ArgKind]],
                arg_names: List[List[Optional[str]]],
                arg_types: List[List[Type]]) -&gt; str:
    args: List[str] = []
    for i in range(len(arg_types)):
        for kind, name, typ in zip(arg_kinds[i], arg_names[i], arg_types[i]):
            arg = self.format_type(None, typ)
            if kind == ARG_STAR:
                arg = '*' + arg
            elif kind == ARG_STAR2:
                arg = '**' + arg
            elif kind.is_named():
                if name:
                    arg = f"{name}={arg}"
        args.append(arg)
    return f"({', '.join(args)})"

</t>
<t tx="ekr.20220525082935.939">def find_node(self, key: str) -&gt; Tuple[str, str, FuncDef]:
    """From a target name, return module/target names and the func def.

    The 'key' argument can be in one of two formats:
    * As the function full name, e.g., package.module.Cls.method
    * As the function location as file and line separated by column,
      e.g., path/to/file.py:42
    """
    # TODO: Also return OverloadedFuncDef -- currently these are ignored.
    node: Optional[SymbolNode] = None
    if ':' in key:
        if key.count(':') &gt; 1:
            raise SuggestionFailure(
                'Malformed location for function: {}. Must be either'
                ' package.module.Class.method or path/to/file.py:line'.format(key))
        file, line = key.split(':')
        if not line.isdigit():
            raise SuggestionFailure(f'Line number must be a number. Got {line}')
        line_number = int(line)
        modname, node = self.find_node_by_file_and_line(file, line_number)
        tail = node.fullname[len(modname) + 1:]  # add one to account for '.'
    else:
        target = split_target(self.fgmanager.graph, key)
        if not target:
            raise SuggestionFailure(f"Cannot find module for {key}")
        modname, tail = target
        node = self.find_node_by_module_and_name(modname, tail)

    if isinstance(node, Decorator):
        node = self.extract_from_decorator(node)
        if not node:
            raise SuggestionFailure(f"Object {key} is a decorator we can't handle")

    if not isinstance(node, FuncDef):
        raise SuggestionFailure(f"Object {key} is not a function")

    return modname, tail, node

</t>
<t tx="ekr.20220525082935.94">def unbox_literal(self, e: Expression) -&gt; Optional[Union[int, float, bool, str]]:
    if isinstance(e, (IntExpr, FloatExpr, StrExpr)):
        return e.value
    elif isinstance(e, NameExpr) and e.name in ('True', 'False'):
        return True if e.name == 'True' else False
    return None

</t>
<t tx="ekr.20220525082935.940">def find_node_by_module_and_name(self, modname: str, tail: str) -&gt; Optional[SymbolNode]:
    """Find symbol node by module id and qualified name.

    Raise SuggestionFailure if can't find one.
    """
    tree = self.ensure_loaded(self.fgmanager.graph[modname])

    # N.B. This is reimplemented from update's lookup_target
    # basically just to produce better error messages.

    names: SymbolTable = tree.names

    # Look through any classes
    components = tail.split('.')
    for i, component in enumerate(components[:-1]):
        if component not in names:
            raise SuggestionFailure("Unknown class %s.%s" %
                                    (modname, '.'.join(components[:i + 1])))
        node: Optional[SymbolNode] = names[component].node
        if not isinstance(node, TypeInfo):
            raise SuggestionFailure("Object %s.%s is not a class" %
                                    (modname, '.'.join(components[:i + 1])))
        names = node.names

    # Look for the actual function/method
    funcname = components[-1]
    if funcname not in names:
        key = modname + '.' + tail
        raise SuggestionFailure("Unknown %s %s" %
                                ("method" if len(components) &gt; 1 else "function", key))
    return names[funcname].node

</t>
<t tx="ekr.20220525082935.941">def find_node_by_file_and_line(self, file: str, line: int) -&gt; Tuple[str, SymbolNode]:
    """Find symbol node by path to file and line number.

    Find the first function declared *before or on* the line number.

    Return module id and the node found. Raise SuggestionFailure if can't find one.
    """
    if not any(file.endswith(ext) for ext in PYTHON_EXTENSIONS):
        raise SuggestionFailure('Source file is not a Python file')
    try:
        modname, _ = self.finder.crawl_up(os.path.normpath(file))
    except InvalidSourceList as e:
        raise SuggestionFailure('Invalid source file name: ' + file) from e
    if modname not in self.graph:
        raise SuggestionFailure('Unknown module: ' + modname)
    # We must be sure about any edits in this file as this might affect the line numbers.
    tree = self.ensure_loaded(self.fgmanager.graph[modname], force=True)
    node: Optional[SymbolNode] = None
    closest_line: Optional[int] = None
    # TODO: Handle nested functions.
    for _, sym, _ in tree.local_definitions():
        if isinstance(sym.node, (FuncDef, Decorator)):
            sym_line = sym.node.line
        # TODO: add support for OverloadedFuncDef.
        else:
            continue

        # We want the closest function above the specified line
        if sym_line &lt;= line and (closest_line is None or sym_line &gt; closest_line):
            closest_line = sym_line
            node = sym.node
    if not node:
        raise SuggestionFailure(f'Cannot find a function at line {line}')
    return modname, node

</t>
<t tx="ekr.20220525082935.942">def extract_from_decorator(self, node: Decorator) -&gt; Optional[FuncDef]:
    for dec in node.decorators:
        typ = None
        if (isinstance(dec, RefExpr)
                and isinstance(dec.node, FuncDef)):
            typ = dec.node.type
        elif (isinstance(dec, CallExpr)
                and isinstance(dec.callee, RefExpr)
                and isinstance(dec.callee.node, FuncDef)
                and isinstance(dec.callee.node.type, CallableType)):
            typ = get_proper_type(dec.callee.node.type.ret_type)

        if not isinstance(typ, FunctionLike):
            return None
        for ct in typ.items:
            if not (len(ct.arg_types) == 1
                    and isinstance(ct.arg_types[0], TypeVarType)
                    and ct.arg_types[0] == ct.ret_type):
                return None

    return node.func

</t>
<t tx="ekr.20220525082935.943">def try_type(self, func: FuncDef, typ: ProperType) -&gt; List[str]:
    """Recheck a function while assuming it has type typ.

    Return all error messages.
    """
    old = func.unanalyzed_type
    # During reprocessing, unanalyzed_type gets copied to type (by aststrip).
    # We set type to None to ensure that the type always changes during
    # reprocessing.
    func.type = None
    func.unanalyzed_type = typ
    try:
        res = self.fgmanager.trigger(func.fullname)
        # if res:
        #     print('===', typ)
        #     print('\n'.join(res))
        return res
    finally:
        func.unanalyzed_type = old

</t>
<t tx="ekr.20220525082935.944">def reload(self, state: State) -&gt; List[str]:
    """Recheck the module given by state."""
    assert state.path is not None
    self.fgmanager.flush_cache()
    return self.fgmanager.update([(state.id, state.path)], [])

</t>
<t tx="ekr.20220525082935.945">def ensure_loaded(self, state: State, force: bool = False) -&gt; MypyFile:
    """Make sure that the module represented by state is fully loaded."""
    if not state.tree or state.tree.is_cache_skeleton or force:
        self.reload(state)
    assert state.tree is not None
    return state.tree

</t>
<t tx="ekr.20220525082935.946">def named_type(self, s: str) -&gt; Instance:
    return self.manager.semantic_analyzer.named_type(s)

</t>
<t tx="ekr.20220525082935.947">def json_suggestion(self, mod: str, func_name: str, node: FuncDef,
                    suggestion: PyAnnotateSignature) -&gt; str:
    """Produce a json blob for a suggestion suitable for application by pyannotate."""
    # pyannotate irritatingly drops class names for class and static methods
    if node.is_class or node.is_static:
        func_name = func_name.split('.', 1)[-1]

    # pyannotate works with either paths relative to where the
    # module is rooted or with absolute paths. We produce absolute
    # paths because it is simpler.
    path = os.path.abspath(self.graph[mod].xpath)

    obj = {
        'signature': suggestion,
        'line': node.line,
        'path': path,
        'func_name': func_name,
        'samples': 0
    }
    return json.dumps([obj], sort_keys=True)

</t>
<t tx="ekr.20220525082935.948">def pyannotate_signature(
    self,
    cur_module: Optional[str],
    is_method: bool,
    typ: CallableType
) -&gt; PyAnnotateSignature:
    """Format a callable type as a pyannotate dict"""
    start = int(is_method)
    return {
        'arg_types': [self.format_type(cur_module, t) for t in typ.arg_types[start:]],
        'return_type': self.format_type(cur_module, typ.ret_type),
    }

</t>
<t tx="ekr.20220525082935.949">def format_signature(self, sig: PyAnnotateSignature) -&gt; str:
    """Format a callable type in a way suitable as an annotation... kind of"""
    return f"({', '.join(sig['arg_types'])}) -&gt; {sig['return_type']}"

</t>
<t tx="ekr.20220525082935.95">def process_type_annotation(self, s: AssignmentStmt) -&gt; None:
    """Analyze type annotation or infer simple literal type."""
    if s.type:
        lvalue = s.lvalues[-1]
        allow_tuple_literal = isinstance(lvalue, TupleExpr)
        analyzed = self.anal_type(s.type, allow_tuple_literal=allow_tuple_literal)
        # Don't store not ready types (including placeholders).
        if analyzed is None or has_placeholder(analyzed):
            return
        s.type = analyzed
        if (self.type and self.type.is_protocol and isinstance(lvalue, NameExpr) and
                isinstance(s.rvalue, TempNode) and s.rvalue.no_rhs):
            if isinstance(lvalue.node, Var):
                lvalue.node.is_abstract_var = True
    else:
        if (self.type and self.type.is_protocol and
                self.is_annotated_protocol_member(s) and not self.is_func_scope()):
            self.fail('All protocol members must have explicitly declared types', s)
        # Set the type if the rvalue is a simple literal (even if the above error occurred).
        if len(s.lvalues) == 1 and isinstance(s.lvalues[0], RefExpr):
            if s.lvalues[0].is_inferred_def:
                s.type = self.analyze_simple_literal_type(s.rvalue, s.is_final_def)
    if s.type:
        # Store type into nodes.
        for lvalue in s.lvalues:
            self.store_declared_types(lvalue, s.type)

</t>
<t tx="ekr.20220525082935.950">def format_type(self, cur_module: Optional[str], typ: Type) -&gt; str:
    if self.use_fixme and isinstance(get_proper_type(typ), AnyType):
        return self.use_fixme
    return typ.accept(TypeFormatter(cur_module, self.graph))

</t>
<t tx="ekr.20220525082935.951">def score_type(self, t: Type, arg_pos: bool) -&gt; int:
    """Generate a score for a type that we use to pick which type to use.

    Lower is better, prefer non-union/non-any types. Don't penalize optionals.
    """
    t = get_proper_type(t)
    if isinstance(t, AnyType):
        return 20
    if arg_pos and isinstance(t, NoneType):
        return 20
    if isinstance(t, UnionType):
        if any(isinstance(get_proper_type(x), AnyType) for x in t.items):
            return 20
        if any(has_any_type(x) for x in t.items):
            return 15
        if not is_optional(t):
            return 10
    if isinstance(t, CallableType) and (has_any_type(t) or is_tricky_callable(t)):
        return 10
    if self.try_text and isinstance(t, Instance) and t.type.fullname == 'builtins.str':
        return 1
    return 0

</t>
<t tx="ekr.20220525082935.952">def score_callable(self, t: CallableType) -&gt; int:
    return (sum(self.score_type(x, arg_pos=True) for x in t.arg_types) +
            self.score_type(t.ret_type, arg_pos=False))


</t>
<t tx="ekr.20220525082935.953">def any_score_type(ut: Type, arg_pos: bool) -&gt; float:
    """Generate a very made up number representing the Anyness of a type.

    Higher is better, 1.0 is max
    """
    t = get_proper_type(ut)
    if isinstance(t, AnyType) and t.type_of_any != TypeOfAny.suggestion_engine:
        return 0
    if isinstance(t, NoneType) and arg_pos:
        return 0.5
    if isinstance(t, UnionType):
        if any(isinstance(get_proper_type(x), AnyType) for x in t.items):
            return 0.5
        if any(has_any_type(x) for x in t.items):
            return 0.25
    if isinstance(t, CallableType) and is_tricky_callable(t):
        return 0.5
    if has_any_type(t):
        return 0.5

    return 1.0


</t>
<t tx="ekr.20220525082935.954">def any_score_callable(t: CallableType, is_method: bool, ignore_return: bool) -&gt; float:
    # Ignore the first argument of methods
    scores = [any_score_type(x, arg_pos=True) for x in t.arg_types[int(is_method):]]
    # Return type counts twice (since it spreads type information), unless it is
    # None in which case it does not count at all. (Though it *does* still count
    # if there are no arguments.)
    if not isinstance(get_proper_type(t.ret_type), NoneType) or not scores:
        ret = 1.0 if ignore_return else any_score_type(t.ret_type, arg_pos=False)
        scores += [ret, ret]

    return sum(scores) / len(scores)


</t>
<t tx="ekr.20220525082935.955">def is_tricky_callable(t: CallableType) -&gt; bool:
    """Is t a callable that we need to put a ... in for syntax reasons?"""
    return t.is_ellipsis_args or any(k.is_star() or k.is_named() for k in t.arg_kinds)


</t>
<t tx="ekr.20220525082935.956">class TypeFormatter(TypeStrVisitor):
    """Visitor used to format types
    """
    @others
</t>
<t tx="ekr.20220525082935.957"># TODO: Probably a lot
def __init__(self, module: Optional[str], graph: Graph) -&gt; None:
    super().__init__()
    self.module = module
    self.graph = graph

</t>
<t tx="ekr.20220525082935.958">def visit_any(self, t: AnyType) -&gt; str:
    if t.missing_import_name:
        return t.missing_import_name
    else:
        return "Any"

</t>
<t tx="ekr.20220525082935.959">def visit_instance(self, t: Instance) -&gt; str:
    s = t.type.fullname or t.type.name or None
    if s is None:
        return '&lt;???&gt;'
    if s in reverse_builtin_aliases:
        s = reverse_builtin_aliases[s]

    mod_obj = split_target(self.graph, s)
    assert mod_obj
    mod, obj = mod_obj

    # If a class is imported into the current module, rewrite the reference
    # to point to the current module. This helps the annotation tool avoid
    # inserting redundant imports when a type has been reexported.
    if self.module:
        parts = obj.split('.')  # need to split the object part if it is a nested class
        tree = self.graph[self.module].tree
        if tree and parts[0] in tree.names:
            mod = self.module

    if (mod, obj) == ('builtins', 'tuple'):
        mod, obj = 'typing', 'Tuple[' + t.args[0].accept(self) + ', ...]'
    elif t.args:
        obj += f'[{self.list_str(t.args)}]'

    if mod_obj == ('builtins', 'unicode'):
        return 'Text'
    elif mod == 'builtins':
        return obj
    else:
        delim = '.' if '.' not in obj else ':'
        return mod + delim + obj

</t>
<t tx="ekr.20220525082935.96">def is_annotated_protocol_member(self, s: AssignmentStmt) -&gt; bool:
    """Check whether a protocol member is annotated.

    There are some exceptions that can be left unannotated, like ``__slots__``."""
    return any(
        (
            isinstance(lv, NameExpr)
            and lv.name != '__slots__'
            and lv.is_inferred_def
        )
        for lv in s.lvalues
    )

</t>
<t tx="ekr.20220525082935.960">def visit_tuple_type(self, t: TupleType) -&gt; str:
    if t.partial_fallback and t.partial_fallback.type:
        fallback_name = t.partial_fallback.type.fullname
        if fallback_name != 'builtins.tuple':
            return t.partial_fallback.accept(self)
    s = self.list_str(t.items)
    return f'Tuple[{s}]'

</t>
<t tx="ekr.20220525082935.961">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; str:
    return "Any"

</t>
<t tx="ekr.20220525082935.962">def visit_typeddict_type(self, t: TypedDictType) -&gt; str:
    return t.fallback.accept(self)

</t>
<t tx="ekr.20220525082935.963">def visit_union_type(self, t: UnionType) -&gt; str:
    if len(t.items) == 2 and is_optional(t):
        return f"Optional[{remove_optional(t).accept(self)}]"
    else:
        return super().visit_union_type(t)

</t>
<t tx="ekr.20220525082935.964">def visit_callable_type(self, t: CallableType) -&gt; str:
    # TODO: use extended callables?
    if is_tricky_callable(t):
        arg_str = "..."
    else:
        # Note: for default arguments, we just assume that they
        # are required.  This isn't right, but neither is the
        # other thing, and I suspect this will produce more better
        # results than falling back to `...`
        args = [typ.accept(self) for typ in t.arg_types]
        arg_str = f"[{', '.join(args)}]"

    return f"Callable[{arg_str}, {t.ret_type.accept(self)}]"


</t>
<t tx="ekr.20220525082935.965">class StrToText(TypeTranslator):
    @others
</t>
<t tx="ekr.20220525082935.966">def __init__(self, named_type: Callable[[str], Instance]) -&gt; None:
    self.text_type = named_type('builtins.unicode')

</t>
<t tx="ekr.20220525082935.967">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    exp_t = get_proper_type(t)
    if isinstance(exp_t, Instance) and exp_t.type.fullname == 'builtins.str':
        return self.text_type
    return t.copy_modified(args=[a.accept(self) for a in t.args])

</t>
<t tx="ekr.20220525082935.968">def visit_instance(self, t: Instance) -&gt; Type:
    if t.type.fullname == 'builtins.str':
        return self.text_type
    else:
        return super().visit_instance(t)


</t>
<t tx="ekr.20220525082935.969">TType = TypeVar('TType', bound=Type)


</t>
<t tx="ekr.20220525082935.97">def analyze_simple_literal_type(self, rvalue: Expression, is_final: bool) -&gt; Optional[Type]:
    """Return builtins.int if rvalue is an int literal, etc.

    If this is a 'Final' context, we return "Literal[...]" instead."""
    if self.options.semantic_analysis_only or self.function_stack:
        # Skip this if we're only doing the semantic analysis pass.
        # This is mostly to avoid breaking unit tests.
        # Also skip inside a function; this is to avoid confusing
        # the code that handles dead code due to isinstance()
        # inside type variables with value restrictions (like
        # AnyStr).
        return None
    if isinstance(rvalue, FloatExpr):
        return self.named_type_or_none('builtins.float')

    value: Optional[LiteralValue] = None
    type_name: Optional[str] = None
    if isinstance(rvalue, IntExpr):
        value, type_name = rvalue.value, 'builtins.int'
    if isinstance(rvalue, StrExpr):
        value, type_name = rvalue.value, 'builtins.str'
    if isinstance(rvalue, BytesExpr):
        value, type_name = rvalue.value, 'builtins.bytes'
    if isinstance(rvalue, UnicodeExpr):
        value, type_name = rvalue.value, 'builtins.unicode'

    if type_name is not None:
        assert value is not None
        typ = self.named_type_or_none(type_name)
        if typ and is_final:
            return typ.copy_modified(last_known_value=LiteralType(
                value=value,
                fallback=typ,
                line=typ.line,
                column=typ.column,
            ))
        return typ

    return None

</t>
<t tx="ekr.20220525082935.970">def make_suggestion_anys(t: TType) -&gt; TType:
    """Make all anys in the type as coming from the suggestion engine.

    This keeps those Anys from influencing constraint generation,
    which allows us to do better when refining types.
    """
    return cast(TType, t.accept(MakeSuggestionAny()))


</t>
<t tx="ekr.20220525082935.971">class MakeSuggestionAny(TypeTranslator):
    @others
</t>
<t tx="ekr.20220525082935.972">def visit_any(self, t: AnyType) -&gt; Type:
    if not t.missing_import_name:
        return t.copy_modified(type_of_any=TypeOfAny.suggestion_engine)
    else:
        return t

</t>
<t tx="ekr.20220525082935.973">def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    return t.copy_modified(args=[a.accept(self) for a in t.args])


</t>
<t tx="ekr.20220525082935.974">def generate_type_combinations(types: List[Type]) -&gt; List[Type]:
    """Generate possible combinations of a list of types.

    mypy essentially supports two different ways to do this: joining the types
    and unioning the types. We try both.
    """
    joined_type = join_type_list(types)
    union_type = make_simplified_union(types)
    if is_same_type(joined_type, union_type):
        return [joined_type]
    else:
        return [joined_type, union_type]


</t>
<t tx="ekr.20220525082935.975">def count_errors(msgs: List[str]) -&gt; int:
    return len([x for x in msgs if ' error: ' in x])


</t>
<t tx="ekr.20220525082935.976">def refine_type(ti: Type, si: Type) -&gt; Type:
    """Refine `ti` by replacing Anys in it with information taken from `si`

    This basically works by, when the types have the same structure,
    traversing both of them in parallel and replacing Any on the left
    with whatever the type on the right is. If the types don't have the
    same structure (or aren't supported), the left type is chosen.

    For example:
      refine(Any, T) = T,  for all T
      refine(float, int) = float
      refine(List[Any], List[int]) = List[int]
      refine(Dict[int, Any], Dict[Any, int]) = Dict[int, int]
      refine(Tuple[int, Any], Tuple[Any, int]) = Tuple[int, int]

      refine(Callable[[Any], Any], Callable[[int], int]) = Callable[[int], int]
      refine(Callable[..., int], Callable[[int, float], Any]) = Callable[[int, float], int]

      refine(Optional[Any], int) = Optional[int]
      refine(Optional[Any], Optional[int]) = Optional[int]
      refine(Optional[Any], Union[int, str]) = Optional[Union[int, str]]
      refine(Optional[List[Any]], List[int]) = List[int]

    """
    t = get_proper_type(ti)
    s = get_proper_type(si)

    if isinstance(t, AnyType):
        # If s is also an Any, we return if it is a missing_import Any
        return t if isinstance(s, AnyType) and t.missing_import_name else s

    if isinstance(t, Instance) and isinstance(s, Instance) and t.type == s.type:
        return t.copy_modified(args=[refine_type(ta, sa) for ta, sa in zip(t.args, s.args)])

    if (
        isinstance(t, TupleType)
        and isinstance(s, TupleType)
        and t.partial_fallback == s.partial_fallback
        and len(t.items) == len(s.items)
    ):
        return t.copy_modified(items=[refine_type(ta, sa) for ta, sa in zip(t.items, s.items)])

    if isinstance(t, CallableType) and isinstance(s, CallableType):
        return refine_callable(t, s)

    if isinstance(t, UnionType):
        return refine_union(t, s)

    # TODO: Refining of builtins.tuple, Type?

    return t


</t>
<t tx="ekr.20220525082935.977">def refine_union(t: UnionType, s: ProperType) -&gt; Type:
    """Refine a union type based on another type.

    This is done by refining every component of the union against the
    right hand side type (or every component of its union if it is
    one). If an element of the union is successfully refined, we drop it
    from the union in favor of the refined versions.
    """
    # Don't try to do any union refining if the types are already the
    # same.  This prevents things like refining Optional[Any] against
    # itself and producing None.
    if t == s:
        return t

    rhs_items = s.items if isinstance(s, UnionType) else [s]

    new_items = []
    for lhs in t.items:
        refined = False
        for rhs in rhs_items:
            new = refine_type(lhs, rhs)
            if new != lhs:
                new_items.append(new)
                refined = True
        if not refined:
            new_items.append(lhs)

    # Turn strict optional on when simplifying the union since we
    # don't want to drop Nones.
    with state.strict_optional_set(True):
        return make_simplified_union(new_items)


</t>
<t tx="ekr.20220525082935.978">def refine_callable(t: CallableType, s: CallableType) -&gt; CallableType:
    """Refine a callable based on another.

    See comments for refine_type.
    """
    if t.fallback != s.fallback:
        return t

    if t.is_ellipsis_args and not is_tricky_callable(s):
        return s.copy_modified(ret_type=refine_type(t.ret_type, s.ret_type))

    if is_tricky_callable(t) or t.arg_kinds != s.arg_kinds:
        return t

    return t.copy_modified(
        arg_types=[refine_type(ta, sa) for ta, sa in zip(t.arg_types, s.arg_types)],
        ret_type=refine_type(t.ret_type, s.ret_type),
    )


</t>
<t tx="ekr.20220525082935.979">T = TypeVar('T')


</t>
<t tx="ekr.20220525082935.98">def analyze_alias(self, rvalue: Expression,
                  allow_placeholder: bool = False) -&gt; Tuple[Optional[Type], List[str],
                                                            Set[str], List[str]]:
    """Check if 'rvalue' is a valid type allowed for aliasing (e.g. not a type variable).

    If yes, return the corresponding type, a list of
    qualified type variable names for generic aliases, a set of names the alias depends on,
    and a list of type variables if the alias is generic.
    An schematic example for the dependencies:
        A = int
        B = str
        analyze_alias(Dict[A, B])[2] == {'__main__.A', '__main__.B'}
    """
    dynamic = bool(self.function_stack and self.function_stack[-1].is_dynamic())
    global_scope = not self.type and not self.function_stack
    res = analyze_type_alias(rvalue,
                             self,
                             self.tvar_scope,
                             self.plugin,
                             self.options,
                             self.is_typeshed_stub_file,
                             allow_placeholder=allow_placeholder,
                             in_dynamic_func=dynamic,
                             global_scope=global_scope)
    typ: Optional[Type] = None
    if res:
        typ, depends_on = res
        found_type_vars = typ.accept(TypeVarLikeQuery(self.lookup_qualified, self.tvar_scope))
        alias_tvars = [name for (name, node) in found_type_vars]
        qualified_tvars = [node.fullname for (name, node) in found_type_vars]
    else:
        alias_tvars = []
        depends_on = set()
        qualified_tvars = []
    return typ, alias_tvars, depends_on, qualified_tvars

</t>
<t tx="ekr.20220525082935.980">def dedup(old: List[T]) -&gt; List[T]:
    new: List[T] = []
    for x in old:
        if x not in new:
            new.append(x)
    return new
</t>
<t tx="ekr.20220525082935.981">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Generic node traverser visitor"""

from typing import List, Tuple
from mypy_extensions import mypyc_attr

from mypy.patterns import (
    AsPattern, OrPattern, ValuePattern, SequencePattern, StarredPattern, MappingPattern,
    ClassPattern
)
from mypy.visitor import NodeVisitor
from mypy.nodes import (
    AssertTypeExpr, Block, MypyFile, FuncBase, FuncItem, CallExpr, ClassDef, Decorator, FuncDef,
    ExpressionStmt, AssignmentStmt, OperatorAssignmentStmt, WhileStmt,
    ForStmt, ReturnStmt, AssertStmt, DelStmt, IfStmt, RaiseStmt,
    TryStmt, WithStmt, MatchStmt, NameExpr, MemberExpr, OpExpr, SliceExpr, CastExpr,
    RevealExpr, UnaryExpr, ListExpr, TupleExpr, DictExpr, SetExpr, IndexExpr, AssignmentExpr,
    GeneratorExpr, ListComprehension, SetComprehension, DictionaryComprehension,
    ConditionalExpr, TypeApplication, ExecStmt, Import, ImportFrom,
    LambdaExpr, ComparisonExpr, OverloadedFuncDef, YieldFromExpr,
    YieldExpr, StarExpr, BackquoteExpr, AwaitExpr, PrintStmt, SuperExpr, Node, REVEAL_TYPE,
)


@others
</t>
<t tx="ekr.20220525082935.982">@mypyc_attr(allow_interpreted_subclasses=True)
class TraverserVisitor(NodeVisitor[None]):
    """A parse tree visitor that traverses the parse tree during visiting.

    It does not perform any actions outside the traversal. Subclasses
    should override visit methods to perform actions during
    traversal. Calling the superclass method allows reusing the
    traversal implementation.
    """

    @others
</t>
<t tx="ekr.20220525082935.983">def __init__(self) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082935.984"># Visit methods

</t>
<t tx="ekr.20220525082935.985">def visit_mypy_file(self, o: MypyFile) -&gt; None:
    for d in o.defs:
        d.accept(self)

</t>
<t tx="ekr.20220525082935.986">def visit_block(self, block: Block) -&gt; None:
    for s in block.body:
        s.accept(self)

</t>
<t tx="ekr.20220525082935.987">def visit_func(self, o: FuncItem) -&gt; None:
    if o.arguments is not None:
        for arg in o.arguments:
            init = arg.initializer
            if init is not None:
                init.accept(self)

        for arg in o.arguments:
            self.visit_var(arg.variable)

    o.body.accept(self)

</t>
<t tx="ekr.20220525082935.988">def visit_func_def(self, o: FuncDef) -&gt; None:
    self.visit_func(o)

</t>
<t tx="ekr.20220525082935.989">def visit_overloaded_func_def(self, o: OverloadedFuncDef) -&gt; None:
    for item in o.items:
        item.accept(self)
    if o.impl:
        o.impl.accept(self)

</t>
<t tx="ekr.20220525082935.99">def check_and_set_up_type_alias(self, s: AssignmentStmt) -&gt; bool:
    """Check if assignment creates a type alias and set it up as needed.

    Return True if it is a type alias (even if the target is not ready),
    or False otherwise.

    Note: the resulting types for subscripted (including generic) aliases
    are also stored in rvalue.analyzed.
    """
    lvalue = s.lvalues[0]
    if len(s.lvalues) &gt; 1 or not isinstance(lvalue, NameExpr):
        # First rule: Only simple assignments like Alias = ... create aliases.
        return False

    pep_613 = False
    if s.unanalyzed_type is not None and isinstance(s.unanalyzed_type, UnboundType):
        lookup = self.lookup_qualified(s.unanalyzed_type.name, s, suppress_errors=True)
        if lookup and lookup.fullname in TYPE_ALIAS_NAMES:
            pep_613 = True
    if not pep_613 and s.unanalyzed_type is not None:
        # Second rule: Explicit type (cls: Type[A] = A) always creates variable, not alias.
        # unless using PEP 613 `cls: TypeAlias = A`
        return False

    existing = self.current_symbol_table().get(lvalue.name)
    # Third rule: type aliases can't be re-defined. For example:
    #     A: Type[float] = int
    #     A = float  # OK, but this doesn't define an alias
    #     B = int
    #     B = float  # Error!
    # Don't create an alias in these cases:
    if (existing
            and (isinstance(existing.node, Var)  # existing variable
                 or (isinstance(existing.node, TypeAlias)
                     and not s.is_alias_def)  # existing alias
                 or (isinstance(existing.node, PlaceholderNode)
                     and existing.node.node.line &lt; s.line))):  # previous incomplete definition
        # TODO: find a more robust way to track the order of definitions.
        # Note: if is_alias_def=True, this is just a node from previous iteration.
        if isinstance(existing.node, TypeAlias) and not s.is_alias_def:
            self.fail('Cannot assign multiple types to name "{}"'
                      ' without an explicit "Type[...]" annotation'
                      .format(lvalue.name), lvalue)
        return False

    non_global_scope = self.type or self.is_func_scope()
    if not pep_613 and isinstance(s.rvalue, RefExpr) and non_global_scope:
        # Fourth rule (special case): Non-subscripted right hand side creates a variable
        # at class and function scopes. For example:
        #
        #   class Model:
        #       ...
        #   class C:
        #       model = Model # this is automatically a variable with type 'Type[Model]'
        #
        # without this rule, this typical use case will require a lot of explicit
        # annotations (see the second rule).
        return False
    rvalue = s.rvalue
    if not pep_613 and not self.can_be_type_alias(rvalue):
        return False

    if existing and not isinstance(existing.node, (PlaceholderNode, TypeAlias)):
        # Cannot redefine existing node as type alias.
        return False

    res: Optional[Type] = None
    if self.is_none_alias(rvalue):
        res = NoneType()
        alias_tvars, depends_on, qualified_tvars = \
            [], set(), []  # type: List[str], Set[str], List[str]
    else:
        tag = self.track_incomplete_refs()
        res, alias_tvars, depends_on, qualified_tvars = \
            self.analyze_alias(rvalue, allow_placeholder=True)
        if not res:
            return False
        # TODO: Maybe we only need to reject top-level placeholders, similar
        #       to base classes.
        if self.found_incomplete_ref(tag) or has_placeholder(res):
            # Since we have got here, we know this must be a type alias (incomplete refs
            # may appear in nested positions), therefore use becomes_typeinfo=True.
            self.mark_incomplete(lvalue.name, rvalue, becomes_typeinfo=True)
            return True
    self.add_type_alias_deps(depends_on)
    # In addition to the aliases used, we add deps on unbound
    # type variables, since they are erased from target type.
    self.add_type_alias_deps(qualified_tvars)
    # The above are only direct deps on other aliases.
    # For subscripted aliases, type deps from expansion are added in deps.py
    # (because the type is stored).
    check_for_explicit_any(res, self.options, self.is_typeshed_stub_file, self.msg,
                           context=s)
    # When this type alias gets "inlined", the Any is not explicit anymore,
    # so we need to replace it with non-explicit Anys.
    if not has_placeholder(res):
        res = make_any_non_explicit(res)
    # Note: with the new (lazy) type alias representation we only need to set no_args to True
    # if the expected number of arguments is non-zero, so that aliases like A = List work.
    # However, eagerly expanding aliases like Text = str is a nice performance optimization.
    no_args = isinstance(res, Instance) and not res.args  # type: ignore[misc]
    fix_instance_types(res, self.fail, self.note, self.options.python_version)
    # Aliases defined within functions can't be accessed outside
    # the function, since the symbol table will no longer
    # exist. Work around by expanding them eagerly when used.
    eager = self.is_func_scope()
    alias_node = TypeAlias(res,
                           self.qualified_name(lvalue.name),
                           s.line,
                           s.column,
                           alias_tvars=alias_tvars,
                           no_args=no_args,
                           eager=eager)
    if isinstance(s.rvalue, (IndexExpr, CallExpr)):  # CallExpr is for `void = type(None)`
        s.rvalue.analyzed = TypeAliasExpr(alias_node)
        s.rvalue.analyzed.line = s.line
        # we use the column from resulting target, to get better location for errors
        s.rvalue.analyzed.column = res.column
    elif isinstance(s.rvalue, RefExpr):
        s.rvalue.is_alias_rvalue = True

    if existing:
        # An alias gets updated.
        updated = False
        if isinstance(existing.node, TypeAlias):
            if existing.node.target != res:
                # Copy expansion to the existing alias, this matches how we update base classes
                # for a TypeInfo _in place_ if there are nested placeholders.
                existing.node.target = res
                existing.node.alias_tvars = alias_tvars
                existing.node.no_args = no_args
                updated = True
        else:
            # Otherwise just replace existing placeholder with type alias.
            existing.node = alias_node
            updated = True
        if updated:
            if self.final_iteration:
                self.cannot_resolve_name(lvalue.name, 'name', s)
                return True
            else:
                self.progress = True
                # We need to defer so that this change can get propagated to base classes.
                self.defer(s)
    else:
        self.add_symbol(lvalue.name, alias_node, s)
    if isinstance(rvalue, RefExpr) and isinstance(rvalue.node, TypeAlias):
        alias_node.normalized = rvalue.node.normalized
    return True

</t>
<t tx="ekr.20220525082935.990">def visit_class_def(self, o: ClassDef) -&gt; None:
    for d in o.decorators:
        d.accept(self)
    for base in o.base_type_exprs:
        base.accept(self)
    if o.metaclass:
        o.metaclass.accept(self)
    for v in o.keywords.values():
        v.accept(self)
    o.defs.accept(self)
    if o.analyzed:
        o.analyzed.accept(self)

</t>
<t tx="ekr.20220525082935.991">def visit_decorator(self, o: Decorator) -&gt; None:
    o.func.accept(self)
    o.var.accept(self)
    for decorator in o.decorators:
        decorator.accept(self)

</t>
<t tx="ekr.20220525082935.992">def visit_expression_stmt(self, o: ExpressionStmt) -&gt; None:
    o.expr.accept(self)

</t>
<t tx="ekr.20220525082935.993">def visit_assignment_stmt(self, o: AssignmentStmt) -&gt; None:
    o.rvalue.accept(self)
    for l in o.lvalues:
        l.accept(self)

</t>
<t tx="ekr.20220525082935.994">def visit_operator_assignment_stmt(self, o: OperatorAssignmentStmt) -&gt; None:
    o.rvalue.accept(self)
    o.lvalue.accept(self)

</t>
<t tx="ekr.20220525082935.995">def visit_while_stmt(self, o: WhileStmt) -&gt; None:
    o.expr.accept(self)
    o.body.accept(self)
    if o.else_body:
        o.else_body.accept(self)

</t>
<t tx="ekr.20220525082935.996">def visit_for_stmt(self, o: ForStmt) -&gt; None:
    o.index.accept(self)
    o.expr.accept(self)
    o.body.accept(self)
    if o.else_body:
        o.else_body.accept(self)

</t>
<t tx="ekr.20220525082935.997">def visit_return_stmt(self, o: ReturnStmt) -&gt; None:
    if o.expr is not None:
        o.expr.accept(self)

</t>
<t tx="ekr.20220525082935.998">def visit_assert_stmt(self, o: AssertStmt) -&gt; None:
    if o.expr is not None:
        o.expr.accept(self)
    if o.msg is not None:
        o.msg.accept(self)

</t>
<t tx="ekr.20220525082935.999">def visit_del_stmt(self, o: DelStmt) -&gt; None:
    if o.expr is not None:
        o.expr.accept(self)

</t>
<t tx="ekr.20220525082936.1">def fill_typevars(typ: TypeInfo) -&gt; Union[Instance, TupleType]:
    """For a non-generic type, return instance type representing the type.

    For a generic G type with parameters T1, .., Tn, return G[T1, ..., Tn].
    """
    tvs: List[Type] = []
    # TODO: why do we need to keep both typ.type_vars and typ.defn.type_vars?
    for i in range(len(typ.defn.type_vars)):
        tv = typ.defn.type_vars[i]
        # Change the line number
        if isinstance(tv, TypeVarType):
            tv = TypeVarType(
                tv.name, tv.fullname, tv.id, tv.values,
                tv.upper_bound, tv.variance, line=-1, column=-1,
            )
        else:
            assert isinstance(tv, ParamSpecType)
            tv = ParamSpecType(tv.name, tv.fullname, tv.id, tv.flavor, tv.upper_bound,
                               line=-1, column=-1)
        tvs.append(tv)
    inst = Instance(typ, tvs)
    if typ.tuple_type is None:
        return inst
    return typ.tuple_type.copy_modified(fallback=inst)


</t>
<t tx="ekr.20220525082936.10">@abstractmethod
def visit_erased_type(self, t: ErasedType) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.100">def trim_source_line(line: str, max_len: int, col: int, min_width: int) -&gt; Tuple[str, int]:
    """Trim a line of source code to fit into max_len.

    Show 'min_width' characters on each side of 'col' (an error location). If either
    start or end is trimmed, this is indicated by adding '...' there.
    A typical result looks like this:
        ...some_variable = function_to_call(one_arg, other_arg) or...

    Return the trimmed string and the column offset to to adjust error location.
    """
    if max_len &lt; 2 * min_width + 1:
        # In case the window is too tiny it is better to still show something.
        max_len = 2 * min_width + 1

    # Trivial case: line already fits in.
    if len(line) &lt;= max_len:
        return line, 0

    # If column is not too large so that there is still min_width after it,
    # the line doesn't need to be trimmed at the start.
    if col + min_width &lt; max_len:
        return line[:max_len] + '...', 0

    # Otherwise, if the column is not too close to the end, trim both sides.
    if col &lt; len(line) - min_width - 1:
        offset = col - max_len + min_width + 1
        return '...' + line[offset:col + min_width + 1] + '...', offset - 3

    # Finally, if the column is near the end, just trim the start.
    return '...' + line[-max_len:], len(line) - max_len - 3


</t>
<t tx="ekr.20220525082936.1000">def build_increment(self, manager: FineGrainedBuildManager,
                    module_id: str, path: str) -&gt; Tuple[MypyFile,
                                                        Dict[Expression, Type]]:
    manager.flush_cache()
    manager.update([(module_id, path)], [])
    module = manager.manager.modules[module_id]
    type_map = manager.graph[module_id].type_map()
    return module, type_map

</t>
<t tx="ekr.20220525082936.1001">def dump(self,
         manager: FineGrainedBuildManager,
         kind: str) -&gt; List[str]:
    modules = manager.manager.modules
    if kind == AST:
        return self.dump_asts(modules)
    elif kind == TYPEINFO:
        return self.dump_typeinfos(modules)
    elif kind == SYMTABLE:
        return self.dump_symbol_tables(modules)
    elif kind == TYPES:
        return self.dump_types(manager)
    assert False, f'Invalid kind {kind}'

</t>
<t tx="ekr.20220525082936.1002">def dump_asts(self, modules: Dict[str, MypyFile]) -&gt; List[str]:
    a = []
    for m in sorted(modules):
        if m in NOT_DUMPED_MODULES:
            # We don't support incremental checking of changes to builtins, etc.
            continue
        s = modules[m].accept(self.str_conv)
        a.extend(s.splitlines())
    return a

</t>
<t tx="ekr.20220525082936.1003">def dump_symbol_tables(self, modules: Dict[str, MypyFile]) -&gt; List[str]:
    a = []
    for id in sorted(modules):
        if not is_dumped_module(id):
            # We don't support incremental checking of changes to builtins, etc.
            continue
        a.extend(self.dump_symbol_table(id, modules[id].names))
    return a

</t>
<t tx="ekr.20220525082936.1004">def dump_symbol_table(self, module_id: str, symtable: SymbolTable) -&gt; List[str]:
    a = [f'{module_id}:']
    for name in sorted(symtable):
        if name.startswith('__'):
            continue
        a.append(f'    {name}: {self.format_symbol_table_node(symtable[name])}')
    return a

</t>
<t tx="ekr.20220525082936.1005">def format_symbol_table_node(self, node: SymbolTableNode) -&gt; str:
    if node.node is None:
        if node.kind == UNBOUND_IMPORTED:
            return 'UNBOUND_IMPORTED'
        return 'None'
    if isinstance(node.node, Node):
        s = f'{str(type(node.node).__name__)}&lt;{self.id_mapper.id(node.node)}&gt;'
    else:
        s = f'? ({type(node.node)})'
    if (isinstance(node.node, Var) and node.node.type and
            not node.node.fullname.startswith('typing.')):
        typestr = self.format_type(node.node.type)
        s += f'({typestr})'
    return s

</t>
<t tx="ekr.20220525082936.1006">def dump_typeinfos(self, modules: Dict[str, MypyFile]) -&gt; List[str]:
    a = []
    for id in sorted(modules):
        if not is_dumped_module(id):
            continue
        a.extend(self.dump_typeinfos_recursive(modules[id].names))
    return a

</t>
<t tx="ekr.20220525082936.1007">def dump_typeinfos_recursive(self, names: SymbolTable) -&gt; List[str]:
    a = []
    for name, node in sorted(names.items(), key=lambda x: x[0]):
        if isinstance(node.node, TypeInfo):
            a.extend(self.dump_typeinfo(node.node))
            a.extend(self.dump_typeinfos_recursive(node.node.names))
    return a

</t>
<t tx="ekr.20220525082936.1008">def dump_typeinfo(self, info: TypeInfo) -&gt; List[str]:
    if info.fullname == 'enum.Enum':
        # Avoid noise
        return []
    s = info.dump(str_conv=self.str_conv,
                  type_str_conv=self.type_str_conv)
    return s.splitlines()

</t>
<t tx="ekr.20220525082936.1009">def dump_types(self, manager: FineGrainedBuildManager) -&gt; List[str]:
    a = []
    # To make the results repeatable, we try to generate unique and
    # deterministic sort keys.
    for module_id in sorted(manager.manager.modules):
        if not is_dumped_module(module_id):
            continue
        all_types = manager.manager.all_types
        # Compute a module type map from the global type map
        tree = manager.graph[module_id].tree
        assert tree is not None
        type_map = {node: all_types[node]
                    for node in get_subexpressions(tree)
                    if node in all_types}
        if type_map:
            a.append(f'## {module_id}')
            for expr in sorted(type_map, key=lambda n: (n.line, short_type(n),
                                                        str(n) + str(type_map[n]))):
                typ = type_map[expr]
                a.append(f'{short_type(expr)}:{expr.line}: {self.format_type(typ)}')
    return a

</t>
<t tx="ekr.20220525082936.101">def get_mypy_comments(source: str) -&gt; List[Tuple[int, str]]:
    PREFIX = '# mypy: '
    # Don't bother splitting up the lines unless we know it is useful
    if PREFIX not in source:
        return []
    lines = source.split('\n')
    results = []
    for i, line in enumerate(lines):
        if line.startswith(PREFIX):
            results.append((i + 1, line[len(PREFIX):]))

    return results


</t>
<t tx="ekr.20220525082936.1010">def format_type(self, typ: Type) -&gt; str:
    return typ.accept(self.type_str_conv)


</t>
<t tx="ekr.20220525082936.1011">def is_dumped_module(id: str) -&gt; bool:
    return id not in NOT_DUMPED_MODULES and (not id.startswith('_') or id == '__main__')
</t>
<t tx="ekr.20220525082936.1012">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
import os

from mypy.options import Options
from mypy.modulefinder import (
    FindModuleCache,
    SearchPaths,
    ModuleNotFoundReason,
    expand_site_packages
)

from mypy.test.helpers import Suite, assert_equal
from mypy.test.config import package_path
data_path = os.path.relpath(os.path.join(package_path, "modulefinder"))


@others
</t>
<t tx="ekr.20220525082936.1013">class ModuleFinderSuite(Suite):

    @others
</t>
<t tx="ekr.20220525082936.1014">def setUp(self) -&gt; None:
    self.search_paths = SearchPaths(
        python_path=(),
        mypy_path=(
            os.path.join(data_path, "nsx-pkg1"),
            os.path.join(data_path, "nsx-pkg2"),
            os.path.join(data_path, "nsx-pkg3"),
            os.path.join(data_path, "nsy-pkg1"),
            os.path.join(data_path, "nsy-pkg2"),
            os.path.join(data_path, "pkg1"),
            os.path.join(data_path, "pkg2"),
        ),
        package_path=(),
        typeshed_path=(),
    )
    options = Options()
    options.namespace_packages = True
    self.fmc_ns = FindModuleCache(self.search_paths, fscache=None, options=options)

    options = Options()
    options.namespace_packages = False
    self.fmc_nons = FindModuleCache(self.search_paths, fscache=None, options=options)

</t>
<t tx="ekr.20220525082936.1015">def test__no_namespace_packages__nsx(self) -&gt; None:
    """
    If namespace_packages is False, we shouldn't find nsx
    """
    found_module = self.fmc_nons.find_module("nsx")
    assert_equal(ModuleNotFoundReason.NOT_FOUND, found_module)

</t>
<t tx="ekr.20220525082936.1016">def test__no_namespace_packages__nsx_a(self) -&gt; None:
    """
    If namespace_packages is False, we shouldn't find nsx.a.
    """
    found_module = self.fmc_nons.find_module("nsx.a")
    assert_equal(ModuleNotFoundReason.NOT_FOUND, found_module)

</t>
<t tx="ekr.20220525082936.1017">def test__no_namespace_packages__find_a_in_pkg1(self) -&gt; None:
    """
    Find find pkg1/a.py for "a" with namespace_packages False.
    """
    found_module = self.fmc_nons.find_module("a")
    expected = os.path.join(data_path, "pkg1", "a.py")
    assert_equal(expected, found_module)

</t>
<t tx="ekr.20220525082936.1018">def test__no_namespace_packages__find_b_in_pkg2(self) -&gt; None:
    found_module = self.fmc_ns.find_module("b")
    expected = os.path.join(data_path, "pkg2", "b", "__init__.py")
    assert_equal(expected, found_module)

</t>
<t tx="ekr.20220525082936.1019">def test__find_nsx_as_namespace_pkg_in_pkg1(self) -&gt; None:
    """
    There's no __init__.py in any of the nsx dirs, return
    the path to the first one found in mypypath.
    """
    found_module = self.fmc_ns.find_module("nsx")
    expected = os.path.join(data_path, "nsx-pkg1", "nsx")
    assert_equal(expected, found_module)

</t>
<t tx="ekr.20220525082936.102">_python2_interpreter: Optional[str] = None


</t>
<t tx="ekr.20220525082936.1020">def test__find_nsx_a_init_in_pkg1(self) -&gt; None:
    """
    Find nsx-pkg1/nsx/a/__init__.py for "nsx.a" in namespace mode.
    """
    found_module = self.fmc_ns.find_module("nsx.a")
    expected = os.path.join(data_path, "nsx-pkg1", "nsx", "a", "__init__.py")
    assert_equal(expected, found_module)

</t>
<t tx="ekr.20220525082936.1021">def test__find_nsx_b_init_in_pkg2(self) -&gt; None:
    """
    Find nsx-pkg2/nsx/b/__init__.py for "nsx.b" in namespace mode.
    """
    found_module = self.fmc_ns.find_module("nsx.b")
    expected = os.path.join(data_path, "nsx-pkg2", "nsx", "b", "__init__.py")
    assert_equal(expected, found_module)

</t>
<t tx="ekr.20220525082936.1022">def test__find_nsx_c_c_in_pkg3(self) -&gt; None:
    """
    Find nsx-pkg3/nsx/c/c.py for "nsx.c.c" in namespace mode.
    """
    found_module = self.fmc_ns.find_module("nsx.c.c")
    expected = os.path.join(data_path, "nsx-pkg3", "nsx", "c", "c.py")
    assert_equal(expected, found_module)

</t>
<t tx="ekr.20220525082936.1023">def test__find_nsy_a__init_pyi(self) -&gt; None:
    """
    Prefer nsy-pkg1/a/__init__.pyi file over __init__.py.
    """
    found_module = self.fmc_ns.find_module("nsy.a")
    expected = os.path.join(data_path, "nsy-pkg1", "nsy", "a", "__init__.pyi")
    assert_equal(expected, found_module)

</t>
<t tx="ekr.20220525082936.1024">def test__find_nsy_b__init_py(self) -&gt; None:
    """
    There is a nsy-pkg2/nsy/b.pyi, but also a nsy-pkg2/nsy/b/__init__.py.
    We expect to find the latter when looking up "nsy.b" as
    a package is preferred over a module.
    """
    found_module = self.fmc_ns.find_module("nsy.b")
    expected = os.path.join(data_path, "nsy-pkg2", "nsy", "b", "__init__.py")
    assert_equal(expected, found_module)

</t>
<t tx="ekr.20220525082936.1025">def test__find_nsy_c_pyi(self) -&gt; None:
    """
    There is a nsy-pkg2/nsy/c.pyi and nsy-pkg2/nsy/c.py
    We expect to find the former when looking up "nsy.b" as
    .pyi is preferred over .py.
    """
    found_module = self.fmc_ns.find_module("nsy.c")
    expected = os.path.join(data_path, "nsy-pkg2", "nsy", "c.pyi")
    assert_equal(expected, found_module)

</t>
<t tx="ekr.20220525082936.1026">def test__find_a_in_pkg1(self) -&gt; None:
    found_module = self.fmc_ns.find_module("a")
    expected = os.path.join(data_path, "pkg1", "a.py")
    assert_equal(expected, found_module)

</t>
<t tx="ekr.20220525082936.1027">def test__find_b_init_in_pkg2(self) -&gt; None:
    found_module = self.fmc_ns.find_module("b")
    expected = os.path.join(data_path, "pkg2", "b", "__init__.py")
    assert_equal(expected, found_module)

</t>
<t tx="ekr.20220525082936.1028">def test__find_d_nowhere(self) -&gt; None:
    found_module = self.fmc_ns.find_module("d")
    assert_equal(ModuleNotFoundReason.NOT_FOUND, found_module)


</t>
<t tx="ekr.20220525082936.1029">class ModuleFinderSitePackagesSuite(Suite):

    @others
</t>
<t tx="ekr.20220525082936.103">def try_find_python2_interpreter() -&gt; Optional[str]:
    global _python2_interpreter
    if _python2_interpreter:
        return _python2_interpreter
    for interpreter in default_python2_interpreter:
        try:
            retcode = subprocess.Popen([
                interpreter, '-c',
                'import sys, typing; assert sys.version_info[:2] == (2, 7)'
            ]).wait()
            if not retcode:
                _python2_interpreter = interpreter
                return interpreter
        except OSError:
            pass
    return None


</t>
<t tx="ekr.20220525082936.1030">def setUp(self) -&gt; None:
    self.package_dir = os.path.relpath(os.path.join(
        package_path,
        "modulefinder-site-packages",
    ))

    egg_dirs, site_packages = expand_site_packages([self.package_dir])

    self.search_paths = SearchPaths(
        python_path=(),
        mypy_path=(os.path.join(data_path, "pkg1"),),
        package_path=tuple(egg_dirs + site_packages),
        typeshed_path=(),
    )
    options = Options()
    options.namespace_packages = True
    self.fmc_ns = FindModuleCache(self.search_paths, fscache=None, options=options)

    options = Options()
    options.namespace_packages = False
    self.fmc_nons = FindModuleCache(self.search_paths, fscache=None, options=options)

</t>
<t tx="ekr.20220525082936.1031">def path(self, *parts: str) -&gt; str:
    return os.path.join(self.package_dir, *parts)

</t>
<t tx="ekr.20220525082936.1032">def test__packages_with_ns(self) -&gt; None:
    cases = [
        # Namespace package with py.typed
        ("ns_pkg_typed", self.path("ns_pkg_typed")),
        ("ns_pkg_typed.a", self.path("ns_pkg_typed", "a.py")),
        ("ns_pkg_typed.b", self.path("ns_pkg_typed", "b")),
        ("ns_pkg_typed.b.c", self.path("ns_pkg_typed", "b", "c.py")),
        ("ns_pkg_typed.a.a_var", ModuleNotFoundReason.NOT_FOUND),

        # Namespace package without py.typed
        ("ns_pkg_untyped", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),
        ("ns_pkg_untyped.a", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),
        ("ns_pkg_untyped.b", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),
        ("ns_pkg_untyped.b.c", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),
        ("ns_pkg_untyped.a.a_var", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),

        # Namespace package without stub package
        ("ns_pkg_w_stubs", self.path("ns_pkg_w_stubs")),
        ("ns_pkg_w_stubs.typed", self.path("ns_pkg_w_stubs-stubs", "typed", "__init__.pyi")),
        ("ns_pkg_w_stubs.typed_inline",
            self.path("ns_pkg_w_stubs", "typed_inline", "__init__.py")),
        ("ns_pkg_w_stubs.untyped", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),

        # Regular package with py.typed
        ("pkg_typed", self.path("pkg_typed", "__init__.py")),
        ("pkg_typed.a", self.path("pkg_typed", "a.py")),
        ("pkg_typed.b", self.path("pkg_typed", "b", "__init__.py")),
        ("pkg_typed.b.c", self.path("pkg_typed", "b", "c.py")),
        ("pkg_typed.a.a_var", ModuleNotFoundReason.NOT_FOUND),

        # Regular package without py.typed
        ("pkg_untyped", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),
        ("pkg_untyped.a", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),
        ("pkg_untyped.b", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),
        ("pkg_untyped.b.c", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),
        ("pkg_untyped.a.a_var", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),

        # Top-level Python file in site-packages
        ("standalone", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),
        ("standalone.standalone_var", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),

        # Packages found by following .pth files
        ("baz_pkg", self.path("baz", "baz_pkg", "__init__.py")),
        ("ns_baz_pkg.a", self.path("baz", "ns_baz_pkg", "a.py")),
        ("neighbor_pkg", self.path("..", "modulefinder-src", "neighbor_pkg", "__init__.py")),
        ("ns_neighbor_pkg.a", self.path("..", "modulefinder-src", "ns_neighbor_pkg", "a.py")),

        # Something that doesn't exist
        ("does_not_exist", ModuleNotFoundReason.NOT_FOUND),

        # A regular package with an installed set of stubs
        ("foo.bar", self.path("foo-stubs", "bar.pyi")),

        # A regular, non-site-packages module
        ("a", os.path.join(data_path, "pkg1", "a.py")),
    ]
    for module, expected in cases:
        template = "Find(" + module + ") got {}; expected {}"

        actual = self.fmc_ns.find_module(module)
        assert_equal(actual, expected, template)

</t>
<t tx="ekr.20220525082936.1033">def test__packages_without_ns(self) -&gt; None:
    cases = [
        # Namespace package with py.typed
        ("ns_pkg_typed", ModuleNotFoundReason.NOT_FOUND),
        ("ns_pkg_typed.a", ModuleNotFoundReason.NOT_FOUND),
        ("ns_pkg_typed.b", ModuleNotFoundReason.NOT_FOUND),
        ("ns_pkg_typed.b.c", ModuleNotFoundReason.NOT_FOUND),
        ("ns_pkg_typed.a.a_var", ModuleNotFoundReason.NOT_FOUND),

        # Namespace package without py.typed
        ("ns_pkg_untyped", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),
        ("ns_pkg_untyped.a", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),
        ("ns_pkg_untyped.b", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),
        ("ns_pkg_untyped.b.c", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),
        ("ns_pkg_untyped.a.a_var", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),

        # Namespace package without stub package
        ("ns_pkg_w_stubs", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),
        ("ns_pkg_w_stubs.typed", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),
        ("ns_pkg_w_stubs.typed_inline",
            self.path("ns_pkg_w_stubs", "typed_inline", "__init__.py")),
        ("ns_pkg_w_stubs.untyped", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),

        # Regular package with py.typed
        ("pkg_typed", self.path("pkg_typed", "__init__.py")),
        ("pkg_typed.a", self.path("pkg_typed", "a.py")),
        ("pkg_typed.b", self.path("pkg_typed", "b", "__init__.py")),
        ("pkg_typed.b.c", self.path("pkg_typed", "b", "c.py")),
        ("pkg_typed.a.a_var", ModuleNotFoundReason.NOT_FOUND),

        # Regular package without py.typed
        ("pkg_untyped", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),
        ("pkg_untyped.a", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),
        ("pkg_untyped.b", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),
        ("pkg_untyped.b.c", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),
        ("pkg_untyped.a.a_var", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),

        # Top-level Python file in site-packages
        ("standalone", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),
        ("standalone.standalone_var", ModuleNotFoundReason.FOUND_WITHOUT_TYPE_HINTS),

        # Packages found by following .pth files
        ("baz_pkg", self.path("baz", "baz_pkg", "__init__.py")),
        ("ns_baz_pkg.a", ModuleNotFoundReason.NOT_FOUND),
        ("neighbor_pkg", self.path("..", "modulefinder-src", "neighbor_pkg", "__init__.py")),
        ("ns_neighbor_pkg.a", ModuleNotFoundReason.NOT_FOUND),

        # Something that doesn't exist
        ("does_not_exist", ModuleNotFoundReason.NOT_FOUND),

        # A regular package with an installed set of stubs
        ("foo.bar", self.path("foo-stubs", "bar.pyi")),

        # A regular, non-site-packages module
        ("a", os.path.join(data_path, "pkg1", "a.py")),
    ]
    for module, expected in cases:
        template = "Find(" + module + ") got {}; expected {}"

        actual = self.fmc_nons.find_module(module)
        assert_equal(actual, expected, template)
</t>
<t tx="ekr.20220525082936.1034">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""A basic check to make sure that we are using a mypyc-compiled version when expected."""

import mypy

from unittest import TestCase
import os


@others
</t>
<t tx="ekr.20220525082936.1035">class MypycTest(TestCase):
    def test_using_mypyc(self) -&gt; None:
        if os.getenv('TEST_MYPYC', None) == '1':
            assert not mypy.__file__.endswith('.py'), "Expected to find a mypyc-compiled version"
</t>
<t tx="ekr.20220525082936.1036">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Tests for the mypy parser."""

import sys

from pytest import skip

from mypy import defaults
from mypy.test.helpers import assert_string_arrays_equal, parse_options
from mypy.test.data import DataDrivenTestCase, DataSuite
from mypy.parse import parse
from mypy.errors import CompileError
from mypy.options import Options


@others
</t>
<t tx="ekr.20220525082936.1037">class ParserSuite(DataSuite):
    required_out_section = True
    base_path = '.'
    files = ['parse.test',
             'parse-python2.test']

    if sys.version_info &gt;= (3, 10):
        files.append('parse-python310.test')

    @others
</t>
<t tx="ekr.20220525082936.1038">def run_case(self, testcase: DataDrivenTestCase) -&gt; None:
    test_parser(testcase)


</t>
<t tx="ekr.20220525082936.1039">def test_parser(testcase: DataDrivenTestCase) -&gt; None:
    """Perform a single parser test case.

    The argument contains the description of the test case.
    """
    options = Options()

    if testcase.file.endswith('python2.test'):
        options.python_version = defaults.PYTHON2_VERSION
    elif testcase.file.endswith('python310.test'):
        options.python_version = (3, 10)
    else:
        options.python_version = defaults.PYTHON3_VERSION

    try:
        n = parse(bytes('\n'.join(testcase.input), 'ascii'),
                  fnam='main',
                  module='__main__',
                  errors=None,
                  options=options)
        a = str(n).split('\n')
    except CompileError as e:
        a = e.messages
    assert_string_arrays_equal(testcase.output, a,
                               'Invalid parser output ({}, line {})'.format(
                                   testcase.file, testcase.line))


</t>
<t tx="ekr.20220525082936.104">PASS_TEMPLATE: Final = """&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;testsuite errors="0" failures="0" name="mypy" skips="0" tests="1" time="{time:.3f}"&gt;
  &lt;testcase classname="mypy" file="mypy" line="1" name="mypy-py{ver}-{platform}" time="{time:.3f}"&gt;
  &lt;/testcase&gt;
&lt;/testsuite&gt;
"""

FAIL_TEMPLATE: Final = """&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;testsuite errors="0" failures="1" name="mypy" skips="0" tests="1" time="{time:.3f}"&gt;
  &lt;testcase classname="mypy" file="mypy" line="1" name="mypy-py{ver}-{platform}" time="{time:.3f}"&gt;
    &lt;failure message="mypy produced messages"&gt;{text}&lt;/failure&gt;
  &lt;/testcase&gt;
&lt;/testsuite&gt;
"""

ERROR_TEMPLATE: Final = """&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;testsuite errors="1" failures="0" name="mypy" skips="0" tests="1" time="{time:.3f}"&gt;
  &lt;testcase classname="mypy" file="mypy" line="1" name="mypy-py{ver}-{platform}" time="{time:.3f}"&gt;
    &lt;error message="mypy produced errors"&gt;{text}&lt;/error&gt;
  &lt;/testcase&gt;
&lt;/testsuite&gt;
"""


</t>
<t tx="ekr.20220525082936.1040"># The file name shown in test case output. This is displayed in error
# messages, and must match the file name in the test case descriptions.
INPUT_FILE_NAME = 'file'


</t>
<t tx="ekr.20220525082936.1041">class ParseErrorSuite(DataSuite):
    required_out_section = True
    base_path = '.'
    files = ['parse-errors.test']

    def run_case(self, testcase: DataDrivenTestCase) -&gt; None:
        test_parse_error(testcase)


</t>
<t tx="ekr.20220525082936.1042">def test_parse_error(testcase: DataDrivenTestCase) -&gt; None:
    try:
        options = parse_options('\n'.join(testcase.input), testcase, 0)
        if options.python_version != sys.version_info[:2]:
            skip()
        # Compile temporary file. The test file contains non-ASCII characters.
        parse(bytes('\n'.join(testcase.input), 'utf-8'), INPUT_FILE_NAME, '__main__', None,
              options)
        raise AssertionError('No errors reported')
    except CompileError as e:
        if e.module_with_blocker is not None:
            assert e.module_with_blocker == '__main__'
        # Verify that there was a compile error and that the error messages
        # are equivalent.
        assert_string_arrays_equal(
            testcase.output, e.messages,
            f'Invalid compiler output ({testcase.file}, line {testcase.line})')
</t>
<t tx="ekr.20220525082936.1043">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
from contextlib import contextmanager
import filelock
import os
import pytest
import re
import subprocess
from subprocess import PIPE
import sys
import tempfile
from typing import Tuple, List, Generator

import mypy.api
from mypy.test.config import package_path, pip_lock, pip_timeout
from mypy.util import try_find_python2_interpreter
from mypy.test.data import DataDrivenTestCase, DataSuite
from mypy.test.config import test_temp_dir
from mypy.test.helpers import assert_string_arrays_equal, perform_file_operations


# NOTE: options.use_builtins_fixtures should not be set in these
# tests, otherwise mypy will ignore installed third-party packages.


@others
</t>
<t tx="ekr.20220525082936.1044">class PEP561Suite(DataSuite):
    files = [
        'pep561.test',
    ]
    base_path = '.'

    @others
</t>
<t tx="ekr.20220525082936.1045">def run_case(self, test_case: DataDrivenTestCase) -&gt; None:
    test_pep561(test_case)


</t>
<t tx="ekr.20220525082936.1046">@contextmanager
def virtualenv(
                python_executable: str = sys.executable
                ) -&gt; Generator[Tuple[str, str], None, None]:
    """Context manager that creates a virtualenv in a temporary directory

    returns the path to the created Python executable"""
    # Sadly, we need virtualenv, as the Python 3 venv module does not support creating a venv
    # for Python 2, and Python 2 does not have its own venv.
    with tempfile.TemporaryDirectory() as venv_dir:
        proc = subprocess.run([sys.executable,
                               '-m',
                               'virtualenv',
                               f'-p{python_executable}',
                               venv_dir], cwd=os.getcwd(), stdout=PIPE, stderr=PIPE)
        if proc.returncode != 0:
            err = proc.stdout.decode('utf-8') + proc.stderr.decode('utf-8')
            raise Exception("Failed to create venv. Do you have virtualenv installed?\n" + err)
        if sys.platform == 'win32':
            yield venv_dir, os.path.abspath(os.path.join(venv_dir, 'Scripts', 'python'))
        else:
            yield venv_dir, os.path.abspath(os.path.join(venv_dir, 'bin', 'python'))


</t>
<t tx="ekr.20220525082936.1047">def install_package(pkg: str,
                    python_executable: str = sys.executable,
                    use_pip: bool = True,
                    editable: bool = False) -&gt; None:
    """Install a package from test-data/packages/pkg/"""
    working_dir = os.path.join(package_path, pkg)
    with tempfile.TemporaryDirectory() as dir:
        if use_pip:
            install_cmd = [python_executable, '-m', 'pip', 'install']
            if editable:
                install_cmd.append('-e')
            install_cmd.append('.')
        else:
            install_cmd = [python_executable, 'setup.py']
            if editable:
                install_cmd.append('develop')
            else:
                install_cmd.append('install')
        # Note that newer versions of pip (21.3+) don't
        # follow this env variable, but this is for compatibility
        env = {'PIP_BUILD': dir}
        # Inherit environment for Windows
        env.update(os.environ)
        try:
            with filelock.FileLock(pip_lock, timeout=pip_timeout):
                proc = subprocess.run(install_cmd,
                                      cwd=working_dir,
                                      stdout=PIPE,
                                      stderr=PIPE,
                                      env=env)
        except filelock.Timeout as err:
            raise Exception("Failed to acquire {}".format(pip_lock)) from err
    if proc.returncode != 0:
        raise Exception(proc.stdout.decode('utf-8') + proc.stderr.decode('utf-8'))


</t>
<t tx="ekr.20220525082936.1048">def test_pep561(testcase: DataDrivenTestCase) -&gt; None:
    """Test running mypy on files that depend on PEP 561 packages."""
    assert testcase.old_cwd is not None, "test was not properly set up"
    if 'python2' in testcase.name.lower():
        python = try_find_python2_interpreter()
        if python is None:
            pytest.skip()
    else:
        python = sys.executable

    assert python is not None, "Should be impossible"
    pkgs, pip_args = parse_pkgs(testcase.input[0])
    mypy_args = parse_mypy_args(testcase.input[1])
    use_pip = True
    editable = False
    for arg in pip_args:
        if arg == 'no-pip':
            use_pip = False
        elif arg == 'editable':
            editable = True
    assert pkgs != [], "No packages to install for PEP 561 test?"
    with virtualenv(python) as venv:
        venv_dir, python_executable = venv
        for pkg in pkgs:
            install_package(pkg, python_executable, use_pip, editable)

        cmd_line = list(mypy_args)
        has_program = not ('-p' in cmd_line or '--package' in cmd_line)
        if has_program:
            program = testcase.name + '.py'
            with open(program, 'w', encoding='utf-8') as f:
                for s in testcase.input:
                    f.write(f'{s}\n')
            cmd_line.append(program)

        cmd_line.extend(['--no-error-summary'])
        if python_executable != sys.executable:
            cmd_line.append(f'--python-executable={python_executable}')

        steps = testcase.find_steps()
        if steps != [[]]:
            steps = [[]] + steps  # type: ignore[operator,assignment]

        for i, operations in enumerate(steps):
            perform_file_operations(operations)

            output = []
            # Type check the module
            out, err, returncode = mypy.api.run(cmd_line)

            # split lines, remove newlines, and remove directory of test case
            for line in (out + err).splitlines():
                if line.startswith(test_temp_dir + os.sep):
                    output.append(line[len(test_temp_dir + os.sep):].rstrip("\r\n"))
                else:
                    # Normalize paths so that the output is the same on Windows and Linux/macOS.
                    line = line.replace(test_temp_dir + os.sep, test_temp_dir + '/')
                    output.append(line.rstrip("\r\n"))
            iter_count = '' if i == 0 else f' on iteration {i + 1}'
            expected = testcase.output if i == 0 else testcase.output2.get(i + 1, [])

            assert_string_arrays_equal(expected, output,
                               'Invalid output ({}, line {}){}'.format(
                                   testcase.file, testcase.line, iter_count))

        if has_program:
            os.remove(program)


</t>
<t tx="ekr.20220525082936.1049">def parse_pkgs(comment: str) -&gt; Tuple[List[str], List[str]]:
    if not comment.startswith('# pkgs:'):
        return ([], [])
    else:
        pkgs_str, *args = comment[7:].split(';')
        return ([pkg.strip() for pkg in pkgs_str.split(',')], [arg.strip() for arg in args])


</t>
<t tx="ekr.20220525082936.105">def write_junit_xml(dt: float, serious: bool, messages: List[str], path: str,
                    version: str, platform: str) -&gt; None:
    from xml.sax.saxutils import escape
    if not messages and not serious:
        xml = PASS_TEMPLATE.format(time=dt, ver=version, platform=platform)
    elif not serious:
        xml = FAIL_TEMPLATE.format(text=escape('\n'.join(messages)), time=dt,
                                   ver=version, platform=platform)
    else:
        xml = ERROR_TEMPLATE.format(text=escape('\n'.join(messages)), time=dt,
                                    ver=version, platform=platform)

    # checks for a directory structure in path and creates folders if needed
    xml_dirs = os.path.dirname(os.path.abspath(path))
    if not os.path.isdir(xml_dirs):
        os.makedirs(xml_dirs)

    with open(path, 'wb') as f:
        f.write(xml.encode('utf-8'))


</t>
<t tx="ekr.20220525082936.1050">def parse_mypy_args(line: str) -&gt; List[str]:
    m = re.match('# flags: (.*)$', line)
    if not m:
        return []  # No args; mypy will spit out an error.
    return m.group(1).split()


</t>
<t tx="ekr.20220525082936.1051">def test_mypy_path_is_respected() -&gt; None:
    assert False
    packages = 'packages'
    pkg_name = 'a'
    with tempfile.TemporaryDirectory() as temp_dir:
        old_dir = os.getcwd()
        os.chdir(temp_dir)
        try:
            # Create the pkg for files to go into
            full_pkg_name = os.path.join(temp_dir, packages, pkg_name)
            os.makedirs(full_pkg_name)

            # Create the empty __init__ file to declare a package
            pkg_init_name = os.path.join(temp_dir, packages, pkg_name, '__init__.py')
            open(pkg_init_name, 'w', encoding='utf8').close()

            mypy_config_path = os.path.join(temp_dir, 'mypy.ini')
            with open(mypy_config_path, 'w') as mypy_file:
                mypy_file.write('[mypy]\n')
                mypy_file.write(f'mypy_path = ./{packages}\n')

            with virtualenv() as venv:
                venv_dir, python_executable = venv

                cmd_line_args = []
                if python_executable != sys.executable:
                    cmd_line_args.append(f'--python-executable={python_executable}')
                cmd_line_args.extend(['--config-file', mypy_config_path,
                                      '--package', pkg_name])

                out, err, returncode = mypy.api.run(cmd_line_args)
                assert returncode == 0
        finally:
            os.chdir(old_dir)
</t>
<t tx="ekr.20220525082936.1052">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Test cases for running mypy programs using a Python interpreter.

Each test case type checks a program then runs it using Python. The
output (stdout) of the program is compared to expected output. Type checking
uses full builtins and other stubs.

Note: Currently Python interpreter paths are hard coded.

Note: These test cases are *not* included in the main test suite, as including
      this suite would slow down the main suite too much.
"""

import os
import os.path
import re
import subprocess
from subprocess import PIPE
import sys
from tempfile import TemporaryDirectory

import pytest

from typing import List

from mypy.defaults import PYTHON3_VERSION
from mypy.test.config import test_temp_dir
from mypy.test.data import DataDrivenTestCase, DataSuite
from mypy.test.helpers import assert_string_arrays_equal, split_lines
from mypy.util import try_find_python2_interpreter
from mypy import api

# Path to Python 3 interpreter
python3_path = sys.executable
program_re = re.compile(r'\b_program.py\b')


@others
</t>
<t tx="ekr.20220525082936.1053">class PythonEvaluationSuite(DataSuite):
    files = ['pythoneval.test',
             'python2eval.test',
             'pythoneval-asyncio.test']
    cache_dir = TemporaryDirectory()

    @others
</t>
<t tx="ekr.20220525082936.1054">def run_case(self, testcase: DataDrivenTestCase) -&gt; None:
    test_python_evaluation(testcase, os.path.join(self.cache_dir.name, '.mypy_cache'))


</t>
<t tx="ekr.20220525082936.1055">def test_python_evaluation(testcase: DataDrivenTestCase, cache_dir: str) -&gt; None:
    """Runs Mypy in a subprocess.

    If this passes without errors, executes the script again with a given Python
    version.
    """
    assert testcase.old_cwd is not None, "test was not properly set up"
    # We must enable site packages to get access to installed stubs.
    # TODO: Enable strict optional for these tests
    mypy_cmdline = [
        '--show-traceback',
        '--no-strict-optional',
        '--no-silence-site-packages',
        '--no-error-summary',
    ]
    py2 = testcase.name.lower().endswith('python2')
    if py2:
        mypy_cmdline.append('--py2')
        interpreter = try_find_python2_interpreter()
        if interpreter is None:
            # Skip, can't find a Python 2 interpreter.
            pytest.skip()
            # placate the type checker
            return
    else:
        interpreter = python3_path
        mypy_cmdline.append(f"--python-version={'.'.join(map(str, PYTHON3_VERSION))}")

    m = re.search('# flags: (.*)$', '\n'.join(testcase.input), re.MULTILINE)
    if m:
        mypy_cmdline.extend(m.group(1).split())

    # Write the program to a file.
    program = '_' + testcase.name + '.py'
    program_path = os.path.join(test_temp_dir, program)
    mypy_cmdline.append(program_path)
    with open(program_path, 'w', encoding='utf8') as file:
        for s in testcase.input:
            file.write(f'{s}\n')
    mypy_cmdline.append(f'--cache-dir={cache_dir}')
    output = []
    # Type check the program.
    out, err, returncode = api.run(mypy_cmdline)
    # split lines, remove newlines, and remove directory of test case
    for line in (out + err).splitlines():
        if line.startswith(test_temp_dir + os.sep):
            output.append(line[len(test_temp_dir + os.sep):].rstrip("\r\n"))
        else:
            # Normalize paths so that the output is the same on Windows and Linux/macOS.
            line = line.replace(test_temp_dir + os.sep, test_temp_dir + '/')
            output.append(line.rstrip("\r\n"))
    if returncode == 0:
        # Execute the program.
        proc = subprocess.run([interpreter, '-Wignore', program],
                              cwd=test_temp_dir, stdout=PIPE, stderr=PIPE)
        output.extend(split_lines(proc.stdout, proc.stderr))
    # Remove temp file.
    os.remove(program_path)
    for i, line in enumerate(output):
        if os.path.sep + 'typeshed' + os.path.sep in line:
            output[i] = line.split(os.path.sep)[-1]
    assert_string_arrays_equal(adapt_output(testcase), output,
                               'Invalid output ({}, line {})'.format(
                                   testcase.file, testcase.line))


</t>
<t tx="ekr.20220525082936.1056">def adapt_output(testcase: DataDrivenTestCase) -&gt; List[str]:
    """Translates the generic _program.py into the actual filename."""
    program = '_' + testcase.name + '.py'
    return [program_re.sub(program, line) for line in testcase.output]
</t>
<t tx="ekr.20220525082936.1057">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Test cases for reports generated by mypy."""
import textwrap

from mypy.test.helpers import Suite, assert_equal
from mypy.report import CoberturaPackage, get_line_rate


try:
    import lxml  # type: ignore
except ImportError:
    lxml = None

import pytest


@others
</t>
<t tx="ekr.20220525082936.1058">class CoberturaReportSuite(Suite):
    @others
</t>
<t tx="ekr.20220525082936.1059">@pytest.mark.skipif(lxml is None, reason="Cannot import lxml. Is it installed?")
def test_get_line_rate(self) -&gt; None:
    assert_equal('1.0', get_line_rate(0, 0))
    assert_equal('0.3333', get_line_rate(1, 3))

</t>
<t tx="ekr.20220525082936.106">class IdMapper:
    """Generate integer ids for objects.

    Unlike id(), these start from 0 and increment by 1, and ids won't
    get reused across the life-time of IdMapper.

    Assume objects don't redefine __eq__ or __hash__.
    """

    @others
</t>
<t tx="ekr.20220525082936.1060">@pytest.mark.skipif(lxml is None, reason="Cannot import lxml. Is it installed?")
def test_as_xml(self) -&gt; None:
    import lxml.etree as etree  # type: ignore

    cobertura_package = CoberturaPackage('foobar')
    cobertura_package.covered_lines = 21
    cobertura_package.total_lines = 42

    child_package = CoberturaPackage('raz')
    child_package.covered_lines = 10
    child_package.total_lines = 10
    child_package.classes['class'] = etree.Element('class')

    cobertura_package.packages['raz'] = child_package

    expected_output = textwrap.dedent('''\
        &lt;package complexity="1.0" name="foobar" branch-rate="0" line-rate="0.5000"&gt;
          &lt;classes/&gt;
          &lt;packages&gt;
            &lt;package complexity="1.0" name="raz" branch-rate="0" line-rate="1.0000"&gt;
              &lt;classes&gt;
                &lt;class/&gt;
              &lt;/classes&gt;
            &lt;/package&gt;
          &lt;/packages&gt;
        &lt;/package&gt;
    ''').encode('ascii')
    assert_equal(expected_output,
                 etree.tostring(cobertura_package.as_xml(), pretty_print=True))
</t>
<t tx="ekr.20220525082936.1061">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Semantic analyzer test cases"""

import os.path
import sys

from typing import Dict, List

from mypy import build
from mypy.modulefinder import BuildSource
from mypy.defaults import PYTHON3_VERSION
from mypy.test.helpers import (
    assert_string_arrays_equal, normalize_error_messages, testfile_pyversion, parse_options
)
from mypy.test.data import DataDrivenTestCase, DataSuite
from mypy.test.config import test_temp_dir
from mypy.errors import CompileError
from mypy.nodes import TypeInfo
from mypy.options import Options


# Semantic analyzer test cases: dump parse tree

# Semantic analysis test case description files.
semanal_files = [
    'semanal-basic.test',
    'semanal-expressions.test',
    'semanal-classes.test',
    'semanal-types.test',
    'semanal-typealiases.test',
    'semanal-modules.test',
    'semanal-statements.test',
    'semanal-abstractclasses.test',
    'semanal-namedtuple.test',
    'semanal-typeddict.test',
    'semenal-literal.test',
    'semanal-classvar.test',
    'semanal-python2.test',
    'semanal-lambda.test',
]


if sys.version_info &gt;= (3, 10):
    semanal_files.append('semanal-python310.test')


@others
</t>
<t tx="ekr.20220525082936.1062">def get_semanal_options(program_text: str, testcase: DataDrivenTestCase) -&gt; Options:
    options = parse_options(program_text, testcase, 1)
    options.use_builtins_fixtures = True
    options.semantic_analysis_only = True
    options.show_traceback = True
    options.python_version = PYTHON3_VERSION
    options.enable_incomplete_features = True
    return options


</t>
<t tx="ekr.20220525082936.1063">class SemAnalSuite(DataSuite):
    files = semanal_files
    native_sep = True

    def run_case(self, testcase: DataDrivenTestCase) -&gt; None:
        test_semanal(testcase)


</t>
<t tx="ekr.20220525082936.1064">def test_semanal(testcase: DataDrivenTestCase) -&gt; None:
    """Perform a semantic analysis test case.

    The testcase argument contains a description of the test case
    (inputs and output).
    """

    try:
        src = '\n'.join(testcase.input)
        options = get_semanal_options(src, testcase)
        options.python_version = testfile_pyversion(testcase.file)
        result = build.build(sources=[BuildSource('main', None, src)],
                             options=options,
                             alt_lib_path=test_temp_dir)
        a = result.errors
        if a:
            raise CompileError(a)
        # Include string representations of the source files in the actual
        # output.
        for fnam in sorted(result.files.keys()):
            f = result.files[fnam]
            # Omit the builtins module and files with a special marker in the
            # path.
            # TODO the test is not reliable
            if (not f.path.endswith((os.sep + 'builtins.pyi',
                                     'typing.pyi',
                                     'mypy_extensions.pyi',
                                     'typing_extensions.pyi',
                                     'abc.pyi',
                                     'collections.pyi',
                                     'sys.pyi'))
                    and not os.path.basename(f.path).startswith('_')
                    and not os.path.splitext(
                        os.path.basename(f.path))[0].endswith('_')):
                a += str(f).split('\n')
    except CompileError as e:
        a = e.messages
    if testcase.normalize_output:
        a = normalize_error_messages(a)
    assert_string_arrays_equal(
        testcase.output, a,
        f'Invalid semantic analyzer output ({testcase.file}, line {testcase.line})')


</t>
<t tx="ekr.20220525082936.1065"># Semantic analyzer error test cases

</t>
<t tx="ekr.20220525082936.1066">class SemAnalErrorSuite(DataSuite):
    files = ['semanal-errors.test']
    if sys.version_info &gt;= (3, 10):
        semanal_files.append('semanal-errors-python310.test')

    def run_case(self, testcase: DataDrivenTestCase) -&gt; None:
        test_semanal_error(testcase)


</t>
<t tx="ekr.20220525082936.1067">def test_semanal_error(testcase: DataDrivenTestCase) -&gt; None:
    """Perform a test case."""

    try:
        src = '\n'.join(testcase.input)
        res = build.build(sources=[BuildSource('main', None, src)],
                          options=get_semanal_options(src, testcase),
                          alt_lib_path=test_temp_dir)
        a = res.errors
        assert a, f'No errors reported in {testcase.file}, line {testcase.line}'
    except CompileError as e:
        # Verify that there was a compile error and that the error messages
        # are equivalent.
        a = e.messages
    if testcase.normalize_output:
        a = normalize_error_messages(a)
    assert_string_arrays_equal(
        testcase.output, a,
        f'Invalid compiler output ({testcase.file}, line {testcase.line})')


</t>
<t tx="ekr.20220525082936.1068"># SymbolNode table export test cases

</t>
<t tx="ekr.20220525082936.1069">class SemAnalSymtableSuite(DataSuite):
    required_out_section = True
    files = ['semanal-symtable.test']

    @others
</t>
<t tx="ekr.20220525082936.107">def __init__(self) -&gt; None:
    self.id_map: Dict[object, int] = {}
    self.next_id = 0

</t>
<t tx="ekr.20220525082936.1070">def run_case(self, testcase: DataDrivenTestCase) -&gt; None:
    """Perform a test case."""
    try:
        # Build test case input.
        src = '\n'.join(testcase.input)
        result = build.build(sources=[BuildSource('main', None, src)],
                             options=get_semanal_options(src, testcase),
                             alt_lib_path=test_temp_dir)
        # The output is the symbol table converted into a string.
        a = result.errors
        if a:
            raise CompileError(a)
        for f in sorted(result.files.keys()):
            if f not in ('builtins', 'typing', 'abc'):
                a.append(f'{f}:')
                for s in str(result.files[f].names).split('\n'):
                    a.append('  ' + s)
    except CompileError as e:
        a = e.messages
    assert_string_arrays_equal(
        testcase.output, a,
        f'Invalid semantic analyzer output ({testcase.file}, line {testcase.line})')


</t>
<t tx="ekr.20220525082936.1071"># Type info export test cases
class SemAnalTypeInfoSuite(DataSuite):
    required_out_section = True
    files = ['semanal-typeinfo.test']

    @others
</t>
<t tx="ekr.20220525082936.1072">def run_case(self, testcase: DataDrivenTestCase) -&gt; None:
    """Perform a test case."""
    try:
        # Build test case input.
        src = '\n'.join(testcase.input)
        result = build.build(sources=[BuildSource('main', None, src)],
                             options=get_semanal_options(src, testcase),
                             alt_lib_path=test_temp_dir)
        a = result.errors
        if a:
            raise CompileError(a)

        # Collect all TypeInfos in top-level modules.
        typeinfos = TypeInfoMap()
        for f in result.files.values():
            for n in f.names.values():
                if isinstance(n.node, TypeInfo):
                    assert n.fullname is not None
                    typeinfos[n.fullname] = n.node

        # The output is the symbol table converted into a string.
        a = str(typeinfos).split('\n')
    except CompileError as e:
        a = e.messages
    assert_string_arrays_equal(
        testcase.output, a,
        f'Invalid semantic analyzer output ({testcase.file}, line {testcase.line})')


</t>
<t tx="ekr.20220525082936.1073">class TypeInfoMap(Dict[str, TypeInfo]):
    @others
</t>
<t tx="ekr.20220525082936.1074">def __str__(self) -&gt; str:
    a: List[str] = ["TypeInfoMap("]
    for x, y in sorted(self.items()):
        if isinstance(x, str) and (not x.startswith('builtins.') and
                                   not x.startswith('typing.') and
                                   not x.startswith('abc.')):
            ti = ('\n' + '  ').join(str(y).split('\n'))
            a.append(f'  {x} : {ti}')
    a[-1] += ')'
    return '\n'.join(a)
</t>
<t tx="ekr.20220525082936.1075">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Test cases for the constraint solver used in type inference."""

from typing import List, Union, Tuple, Optional

from mypy.test.helpers import Suite, assert_equal
from mypy.constraints import SUPERTYPE_OF, SUBTYPE_OF, Constraint
from mypy.solve import solve_constraints
from mypy.test.typefixture import TypeFixture
from mypy.types import Type, TypeVarType, TypeVarId


@others
</t>
<t tx="ekr.20220525082936.1076">class SolveSuite(Suite):
    @others
</t>
<t tx="ekr.20220525082936.1077">def setUp(self) -&gt; None:
    self.fx = TypeFixture()

</t>
<t tx="ekr.20220525082936.1078">def test_empty_input(self) -&gt; None:
    self.assert_solve([], [], [])

</t>
<t tx="ekr.20220525082936.1079">def test_simple_supertype_constraints(self) -&gt; None:
    self.assert_solve([self.fx.t.id],
                      [self.supc(self.fx.t, self.fx.a)],
                      [(self.fx.a, self.fx.o)])
    self.assert_solve([self.fx.t.id],
                      [self.supc(self.fx.t, self.fx.a),
                       self.supc(self.fx.t, self.fx.b)],
                      [(self.fx.a, self.fx.o)])

</t>
<t tx="ekr.20220525082936.108">def id(self, o: object) -&gt; int:
    if o not in self.id_map:
        self.id_map[o] = self.next_id
        self.next_id += 1
    return self.id_map[o]


</t>
<t tx="ekr.20220525082936.1080">def test_simple_subtype_constraints(self) -&gt; None:
    self.assert_solve([self.fx.t.id],
                      [self.subc(self.fx.t, self.fx.a)],
                      [self.fx.a])
    self.assert_solve([self.fx.t.id],
                      [self.subc(self.fx.t, self.fx.a),
                       self.subc(self.fx.t, self.fx.b)],
                      [self.fx.b])

</t>
<t tx="ekr.20220525082936.1081">def test_both_kinds_of_constraints(self) -&gt; None:
    self.assert_solve([self.fx.t.id],
                      [self.supc(self.fx.t, self.fx.b),
                       self.subc(self.fx.t, self.fx.a)],
                      [(self.fx.b, self.fx.a)])

</t>
<t tx="ekr.20220525082936.1082">def test_unsatisfiable_constraints(self) -&gt; None:
    # The constraints are impossible to satisfy.
    self.assert_solve([self.fx.t.id],
                      [self.supc(self.fx.t, self.fx.a),
                       self.subc(self.fx.t, self.fx.b)],
                      [None])

</t>
<t tx="ekr.20220525082936.1083">def test_exactly_specified_result(self) -&gt; None:
    self.assert_solve([self.fx.t.id],
                      [self.supc(self.fx.t, self.fx.b),
                       self.subc(self.fx.t, self.fx.b)],
                      [(self.fx.b, self.fx.b)])

</t>
<t tx="ekr.20220525082936.1084">def test_multiple_variables(self) -&gt; None:
    self.assert_solve([self.fx.t.id, self.fx.s.id],
                      [self.supc(self.fx.t, self.fx.b),
                       self.supc(self.fx.s, self.fx.c),
                       self.subc(self.fx.t, self.fx.a)],
                      [(self.fx.b, self.fx.a), (self.fx.c, self.fx.o)])

</t>
<t tx="ekr.20220525082936.1085">def test_no_constraints_for_var(self) -&gt; None:
    self.assert_solve([self.fx.t.id],
                      [],
                      [self.fx.uninhabited])
    self.assert_solve([self.fx.t.id, self.fx.s.id],
                      [],
                      [self.fx.uninhabited, self.fx.uninhabited])
    self.assert_solve([self.fx.t.id, self.fx.s.id],
                      [self.supc(self.fx.s, self.fx.a)],
                      [self.fx.uninhabited, (self.fx.a, self.fx.o)])

</t>
<t tx="ekr.20220525082936.1086">def test_simple_constraints_with_dynamic_type(self) -&gt; None:
    self.assert_solve([self.fx.t.id],
                      [self.supc(self.fx.t, self.fx.anyt)],
                      [(self.fx.anyt, self.fx.anyt)])
    self.assert_solve([self.fx.t.id],
                      [self.supc(self.fx.t, self.fx.anyt),
                       self.supc(self.fx.t, self.fx.anyt)],
                      [(self.fx.anyt, self.fx.anyt)])
    self.assert_solve([self.fx.t.id],
                      [self.supc(self.fx.t, self.fx.anyt),
                       self.supc(self.fx.t, self.fx.a)],
                      [(self.fx.anyt, self.fx.anyt)])

    self.assert_solve([self.fx.t.id],
                      [self.subc(self.fx.t, self.fx.anyt)],
                      [(self.fx.anyt, self.fx.anyt)])
    self.assert_solve([self.fx.t.id],
                      [self.subc(self.fx.t, self.fx.anyt),
                       self.subc(self.fx.t, self.fx.anyt)],
                      [(self.fx.anyt, self.fx.anyt)])
    # self.assert_solve([self.fx.t.id],
    #                   [self.subc(self.fx.t, self.fx.anyt),
    #                    self.subc(self.fx.t, self.fx.a)],
    #                   [(self.fx.anyt, self.fx.anyt)])
    # TODO: figure out what this should be after changes to meet(any, X)

</t>
<t tx="ekr.20220525082936.1087">def test_both_normal_and_any_types_in_results(self) -&gt; None:
    # If one of the bounds is any, we promote the other bound to
    # any as well, since otherwise the type range does not make sense.
    self.assert_solve([self.fx.t.id],
                      [self.supc(self.fx.t, self.fx.a),
                       self.subc(self.fx.t, self.fx.anyt)],
                      [(self.fx.anyt, self.fx.anyt)])

    self.assert_solve([self.fx.t.id],
                      [self.supc(self.fx.t, self.fx.anyt),
                       self.subc(self.fx.t, self.fx.a)],
                      [(self.fx.anyt, self.fx.anyt)])

</t>
<t tx="ekr.20220525082936.1088">def assert_solve(self,
                 vars: List[TypeVarId],
                 constraints: List[Constraint],
                 results: List[Union[None, Type, Tuple[Type, Type]]],
                 ) -&gt; None:
    res: List[Optional[Type]] = []
    for r in results:
        if isinstance(r, tuple):
            res.append(r[0])
        else:
            res.append(r)
    actual = solve_constraints(vars, constraints)
    assert_equal(str(actual), str(res))

</t>
<t tx="ekr.20220525082936.1089">def supc(self, type_var: TypeVarType, bound: Type) -&gt; Constraint:
    return Constraint(type_var.id, SUPERTYPE_OF, bound)

</t>
<t tx="ekr.20220525082936.109">def get_prefix(fullname: str) -&gt; str:
    """Drop the final component of a qualified name (e.g. ('x.y' -&gt; 'x')."""
    return fullname.rsplit('.', 1)[0]


</t>
<t tx="ekr.20220525082936.1090">def subc(self, type_var: TypeVarType, bound: Type) -&gt; Constraint:
    return Constraint(type_var.id, SUBTYPE_OF, bound)
</t>
<t tx="ekr.20220525082936.1091">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
import io
import os.path
import shutil
import sys
import tempfile
import re
import unittest
from types import ModuleType

from typing import Any, List, Tuple, Optional

from mypy.test.helpers import (
    assert_equal, assert_string_arrays_equal, local_sys_path_set
)
from mypy.test.data import DataSuite, DataDrivenTestCase
from mypy.errors import CompileError
from mypy.stubgen import (
    generate_stubs, parse_options, Options, collect_build_targets,
    mypy_options, is_blacklisted_path, is_non_library_module
)
from mypy.stubutil import walk_packages, remove_misplaced_type_comments, common_dir_prefix
from mypy.stubgenc import (
    generate_c_type_stub, infer_method_sig, generate_c_function_stub, generate_c_property_stub,
    is_c_property_readonly
)
from mypy.stubdoc import (
    parse_signature, parse_all_signatures, build_signature, find_unique_signatures,
    infer_sig_from_docstring, infer_prop_type_from_docstring, FunctionSig, ArgSig,
    infer_arg_sig_from_anon_docstring, is_valid_type
)
from mypy.moduleinspect import ModuleInspect, InspectError


@others
</t>
<t tx="ekr.20220525082936.1092">class StubgenCmdLineSuite(unittest.TestCase):
    """Test cases for processing command-line options and finding files."""

    @others
</t>
<t tx="ekr.20220525082936.1093">@unittest.skipIf(sys.platform == 'win32', "clean up fails on Windows")
def test_files_found(self) -&gt; None:
    current = os.getcwd()
    with tempfile.TemporaryDirectory() as tmp:
        try:
            os.chdir(tmp)
            os.mkdir('subdir')
            self.make_file('subdir', 'a.py')
            self.make_file('subdir', 'b.py')
            os.mkdir(os.path.join('subdir', 'pack'))
            self.make_file('subdir', 'pack', '__init__.py')
            opts = parse_options(['subdir'])
            py_mods, c_mods = collect_build_targets(opts, mypy_options(opts))
            assert_equal(c_mods, [])
            files = {mod.path for mod in py_mods}
            assert_equal(files, {os.path.join('subdir', 'pack', '__init__.py'),
                                 os.path.join('subdir', 'a.py'),
                                 os.path.join('subdir', 'b.py')})
        finally:
            os.chdir(current)

</t>
<t tx="ekr.20220525082936.1094">@unittest.skipIf(sys.platform == 'win32', "clean up fails on Windows")
def test_packages_found(self) -&gt; None:
    current = os.getcwd()
    with tempfile.TemporaryDirectory() as tmp:
        try:
            os.chdir(tmp)
            os.mkdir('pack')
            self.make_file('pack', '__init__.py', content='from . import a, b')
            self.make_file('pack', 'a.py')
            self.make_file('pack', 'b.py')
            opts = parse_options(['-p', 'pack'])
            py_mods, c_mods = collect_build_targets(opts, mypy_options(opts))
            assert_equal(c_mods, [])
            files = {os.path.relpath(mod.path or 'FAIL') for mod in py_mods}
            assert_equal(files, {os.path.join('pack', '__init__.py'),
                                 os.path.join('pack', 'a.py'),
                                 os.path.join('pack', 'b.py')})
        finally:
            os.chdir(current)

</t>
<t tx="ekr.20220525082936.1095">@unittest.skipIf(sys.platform == 'win32', "clean up fails on Windows")
def test_module_not_found(self) -&gt; None:
    current = os.getcwd()
    captured_output = io.StringIO()
    sys.stdout = captured_output
    with tempfile.TemporaryDirectory() as tmp:
        try:
            os.chdir(tmp)
            self.make_file(tmp, 'mymodule.py', content='import a')
            opts = parse_options(['-m', 'mymodule'])
            py_mods, c_mods = collect_build_targets(opts, mypy_options(opts))
            assert captured_output.getvalue() == ''
        finally:
            sys.stdout = sys.__stdout__
            os.chdir(current)

</t>
<t tx="ekr.20220525082936.1096">def make_file(self, *path: str, content: str = '') -&gt; None:
    file = os.path.join(*path)
    with open(file, 'w') as f:
        f.write(content)

</t>
<t tx="ekr.20220525082936.1097">def run(self, result: Optional[Any] = None) -&gt; Optional[Any]:
    with local_sys_path_set():
        return super().run(result)


</t>
<t tx="ekr.20220525082936.1098">class StubgenCliParseSuite(unittest.TestCase):
    @others
</t>
<t tx="ekr.20220525082936.1099">def test_walk_packages(self) -&gt; None:
    with ModuleInspect() as m:
        assert_equal(
            set(walk_packages(m, ["mypy.errors"])),
            {"mypy.errors"})

        assert_equal(
            set(walk_packages(m, ["mypy.errors", "mypy.stubgen"])),
            {"mypy.errors", "mypy.stubgen"})

        all_mypy_packages = set(walk_packages(m, ["mypy"]))
        self.assertTrue(all_mypy_packages.issuperset({
            "mypy",
            "mypy.errors",
            "mypy.stubgen",
            "mypy.test",
            "mypy.test.helpers",
        }))


</t>
<t tx="ekr.20220525082936.11">@abstractmethod
def visit_deleted_type(self, t: DeletedType) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.110">def get_top_two_prefixes(fullname: str) -&gt; Tuple[str, str]:
    """Return one and two component prefixes of a fully qualified name.

    Given 'a.b.c.d', return ('a', 'a.b').

    If fullname has only one component, return (fullname, fullname).
    """
    components = fullname.split('.', 3)
    return components[0], '.'.join(components[:2])


</t>
<t tx="ekr.20220525082936.1100">class StubgenUtilSuite(unittest.TestCase):
    """Unit tests for stubgen utility functions."""

    @others
</t>
<t tx="ekr.20220525082936.1101">def test_parse_signature(self) -&gt; None:
    self.assert_parse_signature('func()', ('func', [], []))

</t>
<t tx="ekr.20220525082936.1102">def test_parse_signature_with_args(self) -&gt; None:
    self.assert_parse_signature('func(arg)', ('func', ['arg'], []))
    self.assert_parse_signature('do(arg, arg2)', ('do', ['arg', 'arg2'], []))

</t>
<t tx="ekr.20220525082936.1103">def test_parse_signature_with_optional_args(self) -&gt; None:
    self.assert_parse_signature('func([arg])', ('func', [], ['arg']))
    self.assert_parse_signature('func(arg[, arg2])', ('func', ['arg'], ['arg2']))
    self.assert_parse_signature('func([arg[, arg2]])', ('func', [], ['arg', 'arg2']))

</t>
<t tx="ekr.20220525082936.1104">def test_parse_signature_with_default_arg(self) -&gt; None:
    self.assert_parse_signature('func(arg=None)', ('func', [], ['arg']))
    self.assert_parse_signature('func(arg, arg2=None)', ('func', ['arg'], ['arg2']))
    self.assert_parse_signature('func(arg=1, arg2="")', ('func', [], ['arg', 'arg2']))

</t>
<t tx="ekr.20220525082936.1105">def test_parse_signature_with_qualified_function(self) -&gt; None:
    self.assert_parse_signature('ClassName.func(arg)', ('func', ['arg'], []))

</t>
<t tx="ekr.20220525082936.1106">def test_parse_signature_with_kw_only_arg(self) -&gt; None:
    self.assert_parse_signature('ClassName.func(arg, *, arg2=1)',
                                ('func', ['arg', '*'], ['arg2']))

</t>
<t tx="ekr.20220525082936.1107">def test_parse_signature_with_star_arg(self) -&gt; None:
    self.assert_parse_signature('ClassName.func(arg, *args)',
                                ('func', ['arg', '*args'], []))

</t>
<t tx="ekr.20220525082936.1108">def test_parse_signature_with_star_star_arg(self) -&gt; None:
    self.assert_parse_signature('ClassName.func(arg, **args)',
                                ('func', ['arg', '**args'], []))

</t>
<t tx="ekr.20220525082936.1109">def assert_parse_signature(self, sig: str, result: Tuple[str, List[str], List[str]]) -&gt; None:
    assert_equal(parse_signature(sig), result)

</t>
<t tx="ekr.20220525082936.111">def correct_relative_import(cur_mod_id: str,
                            relative: int,
                            target: str,
                            is_cur_package_init_file: bool) -&gt; Tuple[str, bool]:
    if relative == 0:
        return target, True
    parts = cur_mod_id.split(".")
    rel = relative
    if is_cur_package_init_file:
        rel -= 1
    ok = len(parts) &gt;= rel
    if rel != 0:
        cur_mod_id = ".".join(parts[:-rel])
    return cur_mod_id + (("." + target) if target else ""), ok


</t>
<t tx="ekr.20220525082936.1110">def test_build_signature(self) -&gt; None:
    assert_equal(build_signature([], []), '()')
    assert_equal(build_signature(['arg'], []), '(arg)')
    assert_equal(build_signature(['arg', 'arg2'], []), '(arg, arg2)')
    assert_equal(build_signature(['arg'], ['arg2']), '(arg, arg2=...)')
    assert_equal(build_signature(['arg'], ['arg2', '**x']), '(arg, arg2=..., **x)')

</t>
<t tx="ekr.20220525082936.1111">def test_parse_all_signatures(self) -&gt; None:
    assert_equal(parse_all_signatures(['random text',
                                       '.. function:: fn(arg',
                                       '.. function:: fn()',
                                       '  .. method:: fn2(arg)']),
                 ([('fn', '()'),
                   ('fn2', '(arg)')], []))

</t>
<t tx="ekr.20220525082936.1112">def test_find_unique_signatures(self) -&gt; None:
    assert_equal(find_unique_signatures(
        [('func', '()'),
         ('func', '()'),
         ('func2', '()'),
         ('func2', '(arg)'),
         ('func3', '(arg, arg2)')]),
        [('func', '()'),
         ('func3', '(arg, arg2)')])

</t>
<t tx="ekr.20220525082936.1113">def test_infer_sig_from_docstring(self) -&gt; None:
    assert_equal(infer_sig_from_docstring('\nfunc(x) - y', 'func'),
                 [FunctionSig(name='func', args=[ArgSig(name='x')], ret_type='Any')])
    assert_equal(infer_sig_from_docstring('\nfunc(x)', 'func'),
                 [FunctionSig(name='func', args=[ArgSig(name='x')], ret_type='Any')])

    assert_equal(infer_sig_from_docstring('\nfunc(x, Y_a=None)', 'func'),
                 [FunctionSig(name='func',
                              args=[ArgSig(name='x'), ArgSig(name='Y_a', default=True)],
                              ret_type='Any')])

    assert_equal(infer_sig_from_docstring('\nfunc(x, Y_a=3)', 'func'),
                 [FunctionSig(name='func',
                              args=[ArgSig(name='x'), ArgSig(name='Y_a', default=True)],
                              ret_type='Any')])

    assert_equal(infer_sig_from_docstring('\nfunc(x, Y_a=[1, 2, 3])', 'func'),
                 [FunctionSig(name='func',
                              args=[ArgSig(name='x'), ArgSig(name='Y_a', default=True)],
                              ret_type='Any')])

    assert_equal(infer_sig_from_docstring('\nafunc(x) - y', 'func'), [])
    assert_equal(infer_sig_from_docstring('\nfunc(x, y', 'func'), [])
    assert_equal(infer_sig_from_docstring('\nfunc(x=z(y))', 'func'),
                 [FunctionSig(name='func', args=[ArgSig(name='x', default=True)],
                              ret_type='Any')])

    assert_equal(infer_sig_from_docstring('\nfunc x', 'func'), [])
    # Try to infer signature from type annotation.
    assert_equal(infer_sig_from_docstring('\nfunc(x: int)', 'func'),
                 [FunctionSig(name='func', args=[ArgSig(name='x', type='int')],
                              ret_type='Any')])
    assert_equal(infer_sig_from_docstring('\nfunc(x: int=3)', 'func'),
                 [FunctionSig(name='func', args=[ArgSig(name='x', type='int', default=True)],
                              ret_type='Any')])

    assert_equal(infer_sig_from_docstring('\nfunc(x=3)', 'func'),
                 [FunctionSig(name='func', args=[ArgSig(name='x', type=None, default=True)],
                              ret_type='Any')])

    assert_equal(infer_sig_from_docstring('\nfunc() -&gt; int', 'func'),
                 [FunctionSig(name='func', args=[], ret_type='int')])

    assert_equal(infer_sig_from_docstring('\nfunc(x: int=3) -&gt; int', 'func'),
                 [FunctionSig(name='func', args=[ArgSig(name='x', type='int', default=True)],
                              ret_type='int')])

    assert_equal(infer_sig_from_docstring('\nfunc(x: int=3) -&gt; int   \n', 'func'),
                 [FunctionSig(name='func', args=[ArgSig(name='x', type='int', default=True)],
                              ret_type='int')])

    assert_equal(infer_sig_from_docstring('\nfunc(x: Tuple[int, str]) -&gt; str', 'func'),
                 [FunctionSig(name='func', args=[ArgSig(name='x', type='Tuple[int,str]')],
                              ret_type='str')])

    assert_equal(
        infer_sig_from_docstring('\nfunc(x: Tuple[int, Tuple[str, int], str], y: int) -&gt; str',
                                 'func'),
        [FunctionSig(name='func',
                     args=[ArgSig(name='x', type='Tuple[int,Tuple[str,int],str]'),
                           ArgSig(name='y', type='int')],
                     ret_type='str')])

    assert_equal(infer_sig_from_docstring('\nfunc(x: foo.bar)', 'func'),
                 [FunctionSig(name='func', args=[ArgSig(name='x', type='foo.bar')],
                              ret_type='Any')])

    assert_equal(infer_sig_from_docstring('\nfunc(x: list=[1,2,[3,4]])', 'func'),
                 [FunctionSig(name='func', args=[ArgSig(name='x', type='list', default=True)],
                              ret_type='Any')])

    assert_equal(infer_sig_from_docstring('\nfunc(x: str="nasty[")', 'func'),
                 [FunctionSig(name='func', args=[ArgSig(name='x', type='str', default=True)],
                              ret_type='Any')])

    assert_equal(infer_sig_from_docstring('\nfunc[(x: foo.bar, invalid]', 'func'), [])

    assert_equal(infer_sig_from_docstring('\nfunc(x: invalid::type&lt;with_template&gt;)', 'func'),
                 [FunctionSig(name='func', args=[ArgSig(name='x', type=None)],
                              ret_type='Any')])

    assert_equal(infer_sig_from_docstring('\nfunc(x: str="")', 'func'),
                 [FunctionSig(name='func', args=[ArgSig(name='x', type='str', default=True)],
                              ret_type='Any')])

</t>
<t tx="ekr.20220525082936.1114">def test_infer_sig_from_docstring_duplicate_args(self) -&gt; None:
    assert_equal(infer_sig_from_docstring('\nfunc(x, x) -&gt; str\nfunc(x, y) -&gt; int', 'func'),
                 [FunctionSig(name='func', args=[ArgSig(name='x'), ArgSig(name='y')],
                              ret_type='int')])

</t>
<t tx="ekr.20220525082936.1115">def test_infer_sig_from_docstring_bad_indentation(self) -&gt; None:
    assert_equal(infer_sig_from_docstring("""
        x
          x
         x
        """, 'func'), None)

</t>
<t tx="ekr.20220525082936.1116">def test_infer_arg_sig_from_anon_docstring(self) -&gt; None:
    assert_equal(infer_arg_sig_from_anon_docstring("(*args, **kwargs)"),
                 [ArgSig(name='*args'), ArgSig(name='**kwargs')])

    assert_equal(
        infer_arg_sig_from_anon_docstring(
            "(x: Tuple[int, Tuple[str, int], str]=(1, ('a', 2), 'y'), y: int=4)"),
        [ArgSig(name='x', type='Tuple[int,Tuple[str,int],str]', default=True),
         ArgSig(name='y', type='int', default=True)])

</t>
<t tx="ekr.20220525082936.1117">def test_infer_prop_type_from_docstring(self) -&gt; None:
    assert_equal(infer_prop_type_from_docstring('str: A string.'), 'str')
    assert_equal(infer_prop_type_from_docstring('Optional[int]: An int.'), 'Optional[int]')
    assert_equal(infer_prop_type_from_docstring('Tuple[int, int]: A tuple.'),
                 'Tuple[int, int]')
    assert_equal(infer_prop_type_from_docstring('\nstr: A string.'), None)

</t>
<t tx="ekr.20220525082936.1118">def test_infer_sig_from_docstring_square_brackets(self) -&gt; None:
    assert infer_sig_from_docstring(
        'fetch_row([maxrows, how]) -- Fetches stuff',
        'fetch_row',
    ) == []

</t>
<t tx="ekr.20220525082936.1119">def test_remove_misplaced_type_comments_1(self) -&gt; None:
    good = """
    \u1234
    def f(x):  # type: (int) -&gt; int

    def g(x):
        # type: (int) -&gt; int

    def h():

        # type: () int

    x = 1  # type: int
    """

    assert_equal(remove_misplaced_type_comments(good), good)

</t>
<t tx="ekr.20220525082936.112">fields_cache: Final[Dict[Type[object], List[str]]] = {}


</t>
<t tx="ekr.20220525082936.1120">def test_remove_misplaced_type_comments_2(self) -&gt; None:
    bad = """
    def f(x):
        # type: Callable[[int], int]
        pass

    #  type:  "foo"
    #  type:  'bar'
    x = 1
    # type: int
    """
    bad_fixed = """
    def f(x):

        pass



    x = 1

    """
    assert_equal(remove_misplaced_type_comments(bad), bad_fixed)

</t>
<t tx="ekr.20220525082936.1121">def test_remove_misplaced_type_comments_3(self) -&gt; None:
    bad = '''
    def f(x):
        """docstring"""
        # type: (int) -&gt; int
        pass

    def g(x):
        """docstring
        """
        # type: (int) -&gt; int
        pass
    '''
    bad_fixed = '''
    def f(x):
        """docstring"""

        pass

    def g(x):
        """docstring
        """

        pass
    '''
    assert_equal(remove_misplaced_type_comments(bad), bad_fixed)

</t>
<t tx="ekr.20220525082936.1122">def test_remove_misplaced_type_comments_4(self) -&gt; None:
    bad = """
    def f(x):
        '''docstring'''
        # type: (int) -&gt; int
        pass

    def g(x):
        '''docstring
        '''
        # type: (int) -&gt; int
        pass
    """
    bad_fixed = """
    def f(x):
        '''docstring'''

        pass

    def g(x):
        '''docstring
        '''

        pass
    """
    assert_equal(remove_misplaced_type_comments(bad), bad_fixed)

</t>
<t tx="ekr.20220525082936.1123">def test_remove_misplaced_type_comments_5(self) -&gt; None:
    bad = """
    def f(x):
        # type: (int, List[Any],
        #        float, bool) -&gt; int
        pass

    def g(x):
        # type: (int, List[Any])
        pass
    """
    bad_fixed = """
    def f(x):

        #        float, bool) -&gt; int
        pass

    def g(x):

        pass
    """
    assert_equal(remove_misplaced_type_comments(bad), bad_fixed)

</t>
<t tx="ekr.20220525082936.1124">def test_remove_misplaced_type_comments_bytes(self) -&gt; None:
    original = b"""
    \xbf
    def f(x):  # type: (int) -&gt; int

    def g(x):
        # type: (int) -&gt; int
        pass

    def h():
        # type: int
        pass

    x = 1  # type: int
    """

    dest = b"""
    \xbf
    def f(x):  # type: (int) -&gt; int

    def g(x):
        # type: (int) -&gt; int
        pass

    def h():

        pass

    x = 1  # type: int
    """

    assert_equal(remove_misplaced_type_comments(original), dest)

</t>
<t tx="ekr.20220525082936.1125">@unittest.skipIf(sys.platform == 'win32',
                 'Tests building the paths common ancestor on *nix')
</t>
<t tx="ekr.20220525082936.1126">def test_common_dir_prefix_unix(self) -&gt; None:
    assert common_dir_prefix([]) == '.'
    assert common_dir_prefix(['x.pyi']) == '.'
    assert common_dir_prefix(['./x.pyi']) == '.'
    assert common_dir_prefix(['foo/bar/x.pyi']) == 'foo/bar'
    assert common_dir_prefix(['foo/bar/x.pyi',
                              'foo/bar/y.pyi']) == 'foo/bar'
    assert common_dir_prefix(['foo/bar/x.pyi', 'foo/y.pyi']) == 'foo'
    assert common_dir_prefix(['foo/x.pyi', 'foo/bar/y.pyi']) == 'foo'
    assert common_dir_prefix(['foo/bar/zar/x.pyi', 'foo/y.pyi']) == 'foo'
    assert common_dir_prefix(['foo/x.pyi', 'foo/bar/zar/y.pyi']) == 'foo'
    assert common_dir_prefix(['foo/bar/zar/x.pyi', 'foo/bar/y.pyi']) == 'foo/bar'
    assert common_dir_prefix(['foo/bar/x.pyi', 'foo/bar/zar/y.pyi']) == 'foo/bar'
    assert common_dir_prefix([r'foo/bar\x.pyi']) == 'foo'
    assert common_dir_prefix([r'foo\bar/x.pyi']) == r'foo\bar'

</t>
<t tx="ekr.20220525082936.1127">@unittest.skipIf(sys.platform != 'win32',
                 'Tests building the paths common ancestor on Windows')
</t>
<t tx="ekr.20220525082936.1128">def test_common_dir_prefix_win(self) -&gt; None:
    assert common_dir_prefix(['x.pyi']) == '.'
    assert common_dir_prefix([r'.\x.pyi']) == '.'
    assert common_dir_prefix([r'foo\bar\x.pyi']) == r'foo\bar'
    assert common_dir_prefix([r'foo\bar\x.pyi',
                              r'foo\bar\y.pyi']) == r'foo\bar'
    assert common_dir_prefix([r'foo\bar\x.pyi', r'foo\y.pyi']) == 'foo'
    assert common_dir_prefix([r'foo\x.pyi', r'foo\bar\y.pyi']) == 'foo'
    assert common_dir_prefix([r'foo\bar\zar\x.pyi', r'foo\y.pyi']) == 'foo'
    assert common_dir_prefix([r'foo\x.pyi', r'foo\bar\zar\y.pyi']) == 'foo'
    assert common_dir_prefix([r'foo\bar\zar\x.pyi', r'foo\bar\y.pyi']) == r'foo\bar'
    assert common_dir_prefix([r'foo\bar\x.pyi', r'foo\bar\zar\y.pyi']) == r'foo\bar'
    assert common_dir_prefix([r'foo/bar\x.pyi']) == r'foo\bar'
    assert common_dir_prefix([r'foo\bar/x.pyi']) == r'foo\bar'
    assert common_dir_prefix([r'foo/bar/x.pyi']) == r'foo\bar'


</t>
<t tx="ekr.20220525082936.1129">class StubgenHelpersSuite(unittest.TestCase):
    @others
</t>
<t tx="ekr.20220525082936.113">def get_class_descriptors(cls: 'Type[object]') -&gt; Sequence[str]:
    import inspect  # Lazy import for minor startup speed win
    # Maintain a cache of type -&gt; attributes defined by descriptors in the class
    # (that is, attributes from __slots__ and C extension classes)
    if cls not in fields_cache:
        members = inspect.getmembers(
            cls,
            lambda o: inspect.isgetsetdescriptor(o) or inspect.ismemberdescriptor(o))
        fields_cache[cls] = [x for x, y in members if x != '__weakref__' and x != '__dict__']
    return fields_cache[cls]


</t>
<t tx="ekr.20220525082936.1130">def test_is_blacklisted_path(self) -&gt; None:
    assert not is_blacklisted_path('foo/bar.py')
    assert not is_blacklisted_path('foo.py')
    assert not is_blacklisted_path('foo/xvendor/bar.py')
    assert not is_blacklisted_path('foo/vendorx/bar.py')
    assert is_blacklisted_path('foo/vendor/bar.py')
    assert is_blacklisted_path('foo/vendored/bar.py')
    assert is_blacklisted_path('foo/vendored/bar/thing.py')
    assert is_blacklisted_path('foo/six.py')

</t>
<t tx="ekr.20220525082936.1131">def test_is_non_library_module(self) -&gt; None:
    assert not is_non_library_module('foo')
    assert not is_non_library_module('foo.bar')

    # The following could be test modules, but we are very conservative and
    # don't treat them as such since they could plausibly be real modules.
    assert not is_non_library_module('foo.bartest')
    assert not is_non_library_module('foo.bartests')
    assert not is_non_library_module('foo.testbar')

    assert is_non_library_module('foo.test')
    assert is_non_library_module('foo.test.foo')
    assert is_non_library_module('foo.tests')
    assert is_non_library_module('foo.tests.foo')
    assert is_non_library_module('foo.testing.foo')
    assert is_non_library_module('foo.SelfTest.foo')

    assert is_non_library_module('foo.test_bar')
    assert is_non_library_module('foo.bar_tests')
    assert is_non_library_module('foo.testing')
    assert is_non_library_module('foo.conftest')
    assert is_non_library_module('foo.bar_test_util')
    assert is_non_library_module('foo.bar_test_utils')
    assert is_non_library_module('foo.bar_test_base')

    assert is_non_library_module('foo.setup')

    assert is_non_library_module('foo.__main__')


</t>
<t tx="ekr.20220525082936.1132">class StubgenPythonSuite(DataSuite):
    """Data-driven end-to-end test cases that generate stub files.

    You can use these magic test case name suffixes:

    *_semanal
        Run semantic analysis (slow as this uses real stubs -- only use
        when necessary)
    *_import
        Import module and perform runtime introspection (in the current
        process!)

    You can use these magic comments:

    # flags: --some-stubgen-option ...
        Specify custom stubgen options

    # modules: module1 module2 ...
        Specify which modules to output (by default only 'main')
    """

    required_out_section = True
    base_path = '.'
    files = ['stubgen.test']

    @others
</t>
<t tx="ekr.20220525082936.1133">def run_case(self, testcase: DataDrivenTestCase) -&gt; None:
    with local_sys_path_set():
        self.run_case_inner(testcase)

</t>
<t tx="ekr.20220525082936.1134">def run_case_inner(self, testcase: DataDrivenTestCase) -&gt; None:
    extra = []  # Extra command-line args
    mods = []  # Module names to process
    source = '\n'.join(testcase.input)
    for file, content in testcase.files + [('./main.py', source)]:
        # Strip ./ prefix and .py suffix.
        mod = file[2:-3].replace('/', '.')
        if mod.endswith('.__init__'):
            mod, _, _ = mod.rpartition('.')
        mods.append(mod)
        if '-p ' not in source:
            extra.extend(['-m', mod])
        with open(file, 'w') as f:
            f.write(content)

    options = self.parse_flags(source, extra)
    modules = self.parse_modules(source)
    out_dir = 'out'
    try:
        try:
            if not testcase.name.endswith('_import'):
                options.no_import = True
            if not testcase.name.endswith('_semanal'):
                options.parse_only = True
            generate_stubs(options)
            a: List[str] = []
            for module in modules:
                fnam = module_to_path(out_dir, module)
                self.add_file(fnam, a, header=len(modules) &gt; 1)
        except CompileError as e:
            a = e.messages
        assert_string_arrays_equal(testcase.output, a,
                                   'Invalid output ({}, line {})'.format(
                                       testcase.file, testcase.line))
    finally:
        for mod in mods:
            if mod in sys.modules:
                del sys.modules[mod]
        shutil.rmtree(out_dir)

</t>
<t tx="ekr.20220525082936.1135">def parse_flags(self, program_text: str, extra: List[str]) -&gt; Options:
    flags = re.search('# flags: (.*)$', program_text, flags=re.MULTILINE)
    if flags:
        flag_list = flags.group(1).split()
    else:
        flag_list = []
    options = parse_options(flag_list + extra)
    if '--verbose' not in flag_list:
        options.quiet = True
    else:
        options.verbose = True
    return options

</t>
<t tx="ekr.20220525082936.1136">def parse_modules(self, program_text: str) -&gt; List[str]:
    modules = re.search('# modules: (.*)$', program_text, flags=re.MULTILINE)
    if modules:
        return modules.group(1).split()
    else:
        return ['main']

</t>
<t tx="ekr.20220525082936.1137">def add_file(self, path: str, result: List[str], header: bool) -&gt; None:
    if not os.path.exists(path):
        result.append('&lt;%s was not generated&gt;' % path.replace('\\', '/'))
        return
    if header:
        result.append(f'# {path[4:]}')
    with open(path, encoding='utf8') as file:
        result.extend(file.read().splitlines())


</t>
<t tx="ekr.20220525082936.1138">self_arg = ArgSig(name='self')


</t>
<t tx="ekr.20220525082936.1139">class TestBaseClass:
    pass


</t>
<t tx="ekr.20220525082936.114">def replace_object_state(new: object, old: object, copy_dict: bool = False) -&gt; None:
    """Copy state of old node to the new node.

    This handles cases where there is __dict__ and/or attribute descriptors
    (either from slots or because the type is defined in a C extension module).

    Assume that both objects have the same __class__.
    """
    if hasattr(old, '__dict__'):
        if copy_dict:
            new.__dict__ = dict(old.__dict__)
        else:
            new.__dict__ = old.__dict__

    for attr in get_class_descriptors(old.__class__):
        try:
            if hasattr(old, attr):
                setattr(new, attr, getattr(old, attr))
            elif hasattr(new, attr):
                delattr(new, attr)
        # There is no way to distinguish getsetdescriptors that allow
        # writes from ones that don't (I think?), so we just ignore
        # AttributeErrors if we need to.
        # TODO: What about getsetdescriptors that act like properties???
        except AttributeError:
            pass


</t>
<t tx="ekr.20220525082936.1140">class TestClass(TestBaseClass):
    pass


</t>
<t tx="ekr.20220525082936.1141">class StubgencSuite(unittest.TestCase):
    """Unit tests for stub generation from C modules using introspection.

    Note that these don't cover a lot!
    """

    @others
</t>
<t tx="ekr.20220525082936.1142">def test_infer_hash_sig(self) -&gt; None:
    assert_equal(infer_method_sig('__hash__'), [self_arg])

</t>
<t tx="ekr.20220525082936.1143">def test_infer_getitem_sig(self) -&gt; None:
    assert_equal(infer_method_sig('__getitem__'), [self_arg, ArgSig(name='index')])

</t>
<t tx="ekr.20220525082936.1144">def test_infer_setitem_sig(self) -&gt; None:
    assert_equal(infer_method_sig('__setitem__'),
                 [self_arg, ArgSig(name='index'), ArgSig(name='object')])

</t>
<t tx="ekr.20220525082936.1145">def test_infer_binary_op_sig(self) -&gt; None:
    for op in ('eq', 'ne', 'lt', 'le', 'gt', 'ge',
               'add', 'radd', 'sub', 'rsub', 'mul', 'rmul'):
        assert_equal(infer_method_sig(f'__{op}__'), [self_arg, ArgSig(name='other')])

</t>
<t tx="ekr.20220525082936.1146">def test_infer_unary_op_sig(self) -&gt; None:
    for op in ('neg', 'pos'):
        assert_equal(infer_method_sig(f'__{op}__'), [self_arg])

</t>
<t tx="ekr.20220525082936.1147">def test_generate_c_type_stub_no_crash_for_object(self) -&gt; None:
    output: List[str] = []
    mod = ModuleType('module', '')  # any module is fine
    imports: List[str] = []
    generate_c_type_stub(mod, 'alias', object, output, imports)
    assert_equal(imports, [])
    assert_equal(output[0], 'class alias:')

</t>
<t tx="ekr.20220525082936.1148">def test_generate_c_type_stub_variable_type_annotation(self) -&gt; None:
    # This class mimics the stubgen unit test 'testClassVariable'
    class TestClassVariableCls:
        x = 1

    output: List[str] = []
    imports: List[str] = []
    mod = ModuleType('module', '')  # any module is fine
    generate_c_type_stub(mod, 'C', TestClassVariableCls, output, imports)
    assert_equal(imports, [])
    assert_equal(output, ['class C:', '    x: ClassVar[int] = ...'])

</t>
<t tx="ekr.20220525082936.1149">def test_generate_c_type_inheritance(self) -&gt; None:
    class TestClass(KeyError):
        pass

    output: List[str] = []
    imports: List[str] = []
    mod = ModuleType('module, ')
    generate_c_type_stub(mod, 'C', TestClass, output, imports)
    assert_equal(output, ['class C(KeyError): ...', ])
    assert_equal(imports, [])

</t>
<t tx="ekr.20220525082936.115">def is_sub_path(path1: str, path2: str) -&gt; bool:
    """Given two paths, return if path1 is a sub-path of path2."""
    return pathlib.Path(path2) in pathlib.Path(path1).parents


</t>
<t tx="ekr.20220525082936.1150">def test_generate_c_type_inheritance_same_module(self) -&gt; None:
    output: List[str] = []
    imports: List[str] = []
    mod = ModuleType(TestBaseClass.__module__, '')
    generate_c_type_stub(mod, 'C', TestClass, output, imports)
    assert_equal(output, ['class C(TestBaseClass): ...', ])
    assert_equal(imports, [])

</t>
<t tx="ekr.20220525082936.1151">def test_generate_c_type_inheritance_other_module(self) -&gt; None:
    import argparse

    class TestClass(argparse.Action):
        pass

    output: List[str] = []
    imports: List[str] = []
    mod = ModuleType('module', '')
    generate_c_type_stub(mod, 'C', TestClass, output, imports)
    assert_equal(output, ['class C(argparse.Action): ...', ])
    assert_equal(imports, ['import argparse'])

</t>
<t tx="ekr.20220525082936.1152">def test_generate_c_type_inheritance_builtin_type(self) -&gt; None:
    class TestClass(type):
        pass

    output: List[str] = []
    imports: List[str] = []
    mod = ModuleType('module', '')
    generate_c_type_stub(mod, 'C', TestClass, output, imports)
    assert_equal(output, ['class C(type): ...', ])
    assert_equal(imports, [])

</t>
<t tx="ekr.20220525082936.1153">def test_generate_c_type_with_docstring(self) -&gt; None:
    class TestClass:
        @others
    output: List[str] = []
    imports: List[str] = []
    mod = ModuleType(TestClass.__module__, '')
    generate_c_function_stub(mod, 'test', TestClass.test, output, imports,
                             self_var='self', class_name='TestClass')
    assert_equal(output, ['def test(self, arg0: int) -&gt; Any: ...'])
    assert_equal(imports, [])

</t>
<t tx="ekr.20220525082936.1154">def test(self, arg0: str) -&gt; None:
    """
    test(self: TestClass, arg0: int)
    """
    pass

</t>
<t tx="ekr.20220525082936.1155">def test_generate_c_type_with_docstring_no_self_arg(self) -&gt; None:
    class TestClass:
        @others
    output = []  # type: List[str]
    imports = []  # type: List[str]
    mod = ModuleType(TestClass.__module__, '')
    generate_c_function_stub(mod, 'test', TestClass.test, output, imports,
                             self_var='self', class_name='TestClass')
    assert_equal(output, ['def test(self, arg0: int) -&gt; Any: ...'])
    assert_equal(imports, [])

</t>
<t tx="ekr.20220525082936.1156">def test(self, arg0: str) -&gt; None:
    """
    test(arg0: int)
    """
    pass
</t>
<t tx="ekr.20220525082936.1157">def test_generate_c_type_classmethod(self) -&gt; None:
    class TestClass:
        @others
    output = []  # type: List[str]
    imports = []  # type: List[str]
    mod = ModuleType(TestClass.__module__, '')
    generate_c_function_stub(mod, 'test', TestClass.test, output, imports,
                             self_var='cls', class_name='TestClass')
    assert_equal(output, ['def test(cls, *args, **kwargs) -&gt; Any: ...'])
    assert_equal(imports, [])

</t>
<t tx="ekr.20220525082936.1158">@classmethod
def test(cls, arg0: str) -&gt; None:
    pass
</t>
<t tx="ekr.20220525082936.1159">def test_generate_c_type_with_docstring_empty_default(self) -&gt; None:
    class TestClass:
        @others
    output: List[str] = []
    imports: List[str] = []
    mod = ModuleType(TestClass.__module__, '')
    generate_c_function_stub(mod, 'test', TestClass.test, output, imports,
                             self_var='self', class_name='TestClass')
    assert_equal(output, ['def test(self, arg0: str = ...) -&gt; Any: ...'])
    assert_equal(imports, [])

</t>
<t tx="ekr.20220525082936.116">def hard_exit(status: int = 0) -&gt; None:
    """Kill the current process without fully cleaning up.

    This can be quite a bit faster than a normal exit() since objects are not freed.
    """
    sys.stdout.flush()
    sys.stderr.flush()
    os._exit(status)


</t>
<t tx="ekr.20220525082936.1160">def test(self, arg0: str = "") -&gt; None:
    """
    test(self: TestClass, arg0: str = "")
    """
    pass

</t>
<t tx="ekr.20220525082936.1161">def test_generate_c_function_other_module_arg(self) -&gt; None:
    """Test that if argument references type from other module, module will be imported."""
    # Provide different type in python spec than in docstring to make sure, that docstring
    # information is used.
    def test(arg0: str) -&gt; None:
        """
        test(arg0: argparse.Action)
        """
        pass

    output: List[str] = []
    imports: List[str] = []
    mod = ModuleType(self.__module__, '')
    generate_c_function_stub(mod, 'test', test, output, imports)
    assert_equal(output, ['def test(arg0: argparse.Action) -&gt; Any: ...'])
    assert_equal(imports, ['import argparse'])

</t>
<t tx="ekr.20220525082936.1162">def test_generate_c_function_same_module_arg(self) -&gt; None:
    """Test that if argument references type from same module but using full path, no module
    will be imported, and type specification will be striped to local reference.
    """
    # Provide different type in python spec than in docstring to make sure, that docstring
    # information is used.
    def test(arg0: str) -&gt; None:
        """
        test(arg0: argparse.Action)
        """
        pass

    output: List[str] = []
    imports: List[str] = []
    mod = ModuleType('argparse', '')
    generate_c_function_stub(mod, 'test', test, output, imports)
    assert_equal(output, ['def test(arg0: Action) -&gt; Any: ...'])
    assert_equal(imports, [])

</t>
<t tx="ekr.20220525082936.1163">def test_generate_c_function_other_module_ret(self) -&gt; None:
    """Test that if return type references type from other module, module will be imported."""
    def test(arg0: str) -&gt; None:
        """
        test(arg0: str) -&gt; argparse.Action
        """
        pass

    output: List[str] = []
    imports: List[str] = []
    mod = ModuleType(self.__module__, '')
    generate_c_function_stub(mod, 'test', test, output, imports)
    assert_equal(output, ['def test(arg0: str) -&gt; argparse.Action: ...'])
    assert_equal(imports, ['import argparse'])

</t>
<t tx="ekr.20220525082936.1164">def test_generate_c_function_same_module_ret(self) -&gt; None:
    """Test that if return type references type from same module but using full path,
    no module will be imported, and type specification will be striped to local reference.
    """
    def test(arg0: str) -&gt; None:
        """
        test(arg0: str) -&gt; argparse.Action
        """
        pass

    output: List[str] = []
    imports: List[str] = []
    mod = ModuleType('argparse', '')
    generate_c_function_stub(mod, 'test', test, output, imports)
    assert_equal(output, ['def test(arg0: str) -&gt; Action: ...'])
    assert_equal(imports, [])

</t>
<t tx="ekr.20220525082936.1165">def test_generate_c_property_with_pybind11(self) -&gt; None:
    """Signatures included by PyBind11 inside property.fget are read."""
    class TestClass:
        @others
        attribute = property(get_attribute, doc="")

    readwrite_properties: List[str] = []
    readonly_properties: List[str] = []
    generate_c_property_stub('attribute', TestClass.attribute, [],
                             readwrite_properties, readonly_properties,
                             is_c_property_readonly(TestClass.attribute))
    assert_equal(readwrite_properties, [])
    assert_equal(readonly_properties, ['@property', 'def attribute(self) -&gt; str: ...'])

</t>
<t tx="ekr.20220525082936.1166">def get_attribute(self) -&gt; None:
    """
    (self: TestClass) -&gt; str
    """
    pass
</t>
<t tx="ekr.20220525082936.1167">def test_generate_c_property_with_rw_property(self) -&gt; None:
    class TestClass:
        @others
    readwrite_properties: List[str] = []
    readonly_properties: List[str] = []
    generate_c_property_stub("attribute", type(TestClass.attribute), [],
                             readwrite_properties, readonly_properties,
                             is_c_property_readonly(TestClass.attribute))
    assert_equal(readwrite_properties, ['attribute: Any'])
    assert_equal(readonly_properties, [])

</t>
<t tx="ekr.20220525082936.1168">def __init__(self) -&gt; None:
    self._attribute = 0

</t>
<t tx="ekr.20220525082936.1169">@property
def attribute(self) -&gt; int:
    return self._attribute

</t>
<t tx="ekr.20220525082936.117">def unmangle(name: str) -&gt; str:
    """Remove internal suffixes from a short name."""
    return name.rstrip("'")


</t>
<t tx="ekr.20220525082936.1170">@attribute.setter
def attribute(self, value: int) -&gt; None:
    self._attribute = value

</t>
<t tx="ekr.20220525082936.1171">def test_generate_c_type_with_single_arg_generic(self) -&gt; None:
    class TestClass:
        @others
    output: List[str] = []
    imports: List[str] = []
    mod = ModuleType(TestClass.__module__, '')
    generate_c_function_stub(mod, 'test', TestClass.test, output, imports,
                             self_var='self', class_name='TestClass')
    assert_equal(output, ['def test(self, arg0: List[int]) -&gt; Any: ...'])
    assert_equal(imports, [])

</t>
<t tx="ekr.20220525082936.1172">def test(self, arg0: str) -&gt; None:
    """
    test(self: TestClass, arg0: List[int])
    """
    pass

</t>
<t tx="ekr.20220525082936.1173">def test_generate_c_type_with_double_arg_generic(self) -&gt; None:
    class TestClass:
        @others
    output: List[str] = []
    imports: List[str] = []
    mod = ModuleType(TestClass.__module__, '')
    generate_c_function_stub(mod, 'test', TestClass.test, output, imports,
                             self_var='self', class_name='TestClass')
    assert_equal(output, ['def test(self, arg0: Dict[str,int]) -&gt; Any: ...'])
    assert_equal(imports, [])

</t>
<t tx="ekr.20220525082936.1174">def test(self, arg0: str) -&gt; None:
    """
    test(self: TestClass, arg0: Dict[str, int])
    """
    pass

</t>
<t tx="ekr.20220525082936.1175">def test_generate_c_type_with_nested_generic(self) -&gt; None:
    class TestClass:
        @others
    output: List[str] = []
    imports: List[str] = []
    mod = ModuleType(TestClass.__module__, '')
    generate_c_function_stub(mod, 'test', TestClass.test, output, imports,
                             self_var='self', class_name='TestClass')
    assert_equal(output, ['def test(self, arg0: Dict[str,List[int]]) -&gt; Any: ...'])
    assert_equal(imports, [])

</t>
<t tx="ekr.20220525082936.1176">def test(self, arg0: str) -&gt; None:
    """
    test(self: TestClass, arg0: Dict[str, List[int]])
    """
    pass

</t>
<t tx="ekr.20220525082936.1177">def test_generate_c_type_with_generic_using_other_module_first(self) -&gt; None:
    class TestClass:
        @others
    output: List[str] = []
    imports: List[str] = []
    mod = ModuleType(TestClass.__module__, '')
    generate_c_function_stub(mod, 'test', TestClass.test, output, imports,
                             self_var='self', class_name='TestClass')
    assert_equal(output, ['def test(self, arg0: Dict[argparse.Action,int]) -&gt; Any: ...'])
    assert_equal(imports, ['import argparse'])

</t>
<t tx="ekr.20220525082936.1178">def test(self, arg0: str) -&gt; None:
    """
    test(self: TestClass, arg0: Dict[argparse.Action, int])
    """
    pass

</t>
<t tx="ekr.20220525082936.1179">def test_generate_c_type_with_generic_using_other_module_last(self) -&gt; None:
    class TestClass:
        @others
    output: List[str] = []
    imports: List[str] = []
    mod = ModuleType(TestClass.__module__, '')
    generate_c_function_stub(mod, 'test', TestClass.test, output, imports,
                             self_var='self', class_name='TestClass')
    assert_equal(output, ['def test(self, arg0: Dict[str,argparse.Action]) -&gt; Any: ...'])
    assert_equal(imports, ['import argparse'])

</t>
<t tx="ekr.20220525082936.118">def get_unique_redefinition_name(name: str, existing: Container[str]) -&gt; str:
    """Get a simple redefinition name not present among existing.

    For example, for name 'foo' we try 'foo-redefinition', 'foo-redefinition2',
    'foo-redefinition3', etc. until we find one that is not in existing.
    """
    r_name = name + '-redefinition'
    if r_name not in existing:
        return r_name

    i = 2
    while r_name + str(i) in existing:
        i += 1
    return r_name + str(i)


</t>
<t tx="ekr.20220525082936.1180">def test(self, arg0: str) -&gt; None:
    """
    test(self: TestClass, arg0: Dict[str, argparse.Action])
    """
    pass

</t>
<t tx="ekr.20220525082936.1181">def test_generate_c_type_with_overload_pybind11(self) -&gt; None:
    class TestClass:
        @others
    output: List[str] = []
    imports: List[str] = []
    mod = ModuleType(TestClass.__module__, '')
    generate_c_function_stub(mod, '__init__', TestClass.__init__, output, imports,
                             self_var='self', class_name='TestClass')
    assert_equal(output, [
        '@overload',
        'def __init__(self, arg0: str) -&gt; None: ...',
        '@overload',
        'def __init__(self, arg0: str, arg1: str) -&gt; None: ...',
        '@overload',
        'def __init__(*args, **kwargs) -&gt; Any: ...'])
    assert_equal(set(imports), {'from typing import overload'})


</t>
<t tx="ekr.20220525082936.1182">def __init__(self, arg0: str) -&gt; None:
    """
    __init__(*args, **kwargs)
    Overloaded function.

    1. __init__(self: TestClass, arg0: str) -&gt; None

    2. __init__(self: TestClass, arg0: str, arg1: str) -&gt; None
    """
    pass

</t>
<t tx="ekr.20220525082936.1183">class ArgSigSuite(unittest.TestCase):
    @others
</t>
<t tx="ekr.20220525082936.1184">def test_repr(self) -&gt; None:
    assert_equal(repr(ArgSig(name='asd"dsa')),
                 "ArgSig(name='asd\"dsa', type=None, default=False)")
    assert_equal(repr(ArgSig(name="asd'dsa")),
                 'ArgSig(name="asd\'dsa", type=None, default=False)')
    assert_equal(repr(ArgSig("func", 'str')),
                 "ArgSig(name='func', type='str', default=False)")
    assert_equal(repr(ArgSig("func", 'str', default=True)),
                 "ArgSig(name='func', type='str', default=True)")


</t>
<t tx="ekr.20220525082936.1185">class IsValidTypeSuite(unittest.TestCase):
    @others
</t>
<t tx="ekr.20220525082936.1186">def test_is_valid_type(self) -&gt; None:
    assert is_valid_type('int')
    assert is_valid_type('str')
    assert is_valid_type('Foo_Bar234')
    assert is_valid_type('foo.bar')
    assert is_valid_type('List[int]')
    assert is_valid_type('Dict[str, int]')
    assert is_valid_type('None')
    assert not is_valid_type('foo-bar')
    assert not is_valid_type('x-&gt;y')
    assert not is_valid_type('True')
    assert not is_valid_type('False')
    assert not is_valid_type('x,y')
    assert not is_valid_type('x, y')


</t>
<t tx="ekr.20220525082936.1187">class ModuleInspectSuite(unittest.TestCase):
    @others
</t>
<t tx="ekr.20220525082936.1188">def test_python_module(self) -&gt; None:
    with ModuleInspect() as m:
        p = m.get_package_properties('inspect')
        assert p is not None
        assert p.name == 'inspect'
        assert p.file
        assert p.path is None
        assert p.is_c_module is False
        assert p.subpackages == []

</t>
<t tx="ekr.20220525082936.1189">def test_python_package(self) -&gt; None:
    with ModuleInspect() as m:
        p = m.get_package_properties('unittest')
        assert p is not None
        assert p.name == 'unittest'
        assert p.file
        assert p.path
        assert p.is_c_module is False
        assert p.subpackages
        assert all(sub.startswith('unittest.') for sub in p.subpackages)

</t>
<t tx="ekr.20220525082936.119">def check_python_version(program: str) -&gt; None:
    """Report issues with the Python used to run mypy, dmypy, or stubgen"""
    # Check for known bad Python versions.
    if sys.version_info[:2] &lt; (3, 6):
        sys.exit("Running {name} with Python 3.5 or lower is not supported; "
                 "please upgrade to 3.6 or newer".format(name=program))


</t>
<t tx="ekr.20220525082936.1190">def test_c_module(self) -&gt; None:
    with ModuleInspect() as m:
        p = m.get_package_properties('_socket')
        assert p is not None
        assert p.name == '_socket'
        assert p.path is None
        assert p.is_c_module is True
        assert p.subpackages == []

</t>
<t tx="ekr.20220525082936.1191">def test_non_existent(self) -&gt; None:
    with ModuleInspect() as m:
        with self.assertRaises(InspectError) as e:
            m.get_package_properties('foobar-non-existent')
        assert str(e.exception) == "No module named 'foobar-non-existent'"


</t>
<t tx="ekr.20220525082936.1192">def module_to_path(out_dir: str, module: str) -&gt; str:
    fnam = os.path.join(out_dir, f"{module.replace('.', '/')}.pyi")
    if not os.path.exists(fnam):
        alt_fnam = fnam.replace('.pyi', '/__init__.pyi')
        if os.path.exists(alt_fnam):
            return alt_fnam
    return fnam
</t>
<t tx="ekr.20220525082936.1193">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
import unittest

from mypy.stubinfo import is_legacy_bundled_package


@others
</t>
<t tx="ekr.20220525082936.1194">class TestStubInfo(unittest.TestCase):
    @others
</t>
<t tx="ekr.20220525082936.1195">def test_is_legacy_bundled_packages(self) -&gt; None:
    assert not is_legacy_bundled_package('foobar_asdf', 2)
    assert not is_legacy_bundled_package('foobar_asdf', 3)

    assert is_legacy_bundled_package('certifi', 2)
    assert is_legacy_bundled_package('certifi', 3)

    assert is_legacy_bundled_package('scribe', 2)
    assert not is_legacy_bundled_package('scribe', 3)

    assert not is_legacy_bundled_package('dataclasses', 2)
    assert is_legacy_bundled_package('dataclasses', 3)
</t>
<t tx="ekr.20220525082936.1196">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
import contextlib
import inspect
import io
import os
import re
import sys
import tempfile
import textwrap
import unittest
from typing import Any, Callable, Iterator, List, Optional

import mypy.stubtest
from mypy.stubtest import parse_options, test_stubs
from mypy.test.data import root_dir


@others
</t>
<t tx="ekr.20220525082936.1197">@contextlib.contextmanager
def use_tmp_dir(mod_name: str) -&gt; Iterator[str]:
    current = os.getcwd()
    current_syspath = sys.path[:]
    with tempfile.TemporaryDirectory() as tmp:
        try:
            os.chdir(tmp)
            if sys.path[0] != tmp:
                sys.path.insert(0, tmp)
            yield tmp
        finally:
            sys.path = current_syspath[:]
            if mod_name in sys.modules:
                del sys.modules[mod_name]

            os.chdir(current)


</t>
<t tx="ekr.20220525082936.1198">TEST_MODULE_NAME = "test_module"


stubtest_typing_stub = """
Any = object()

class _SpecialForm:
    def __getitem__(self, typeargs: Any) -&gt; object: ...

Callable: _SpecialForm = ...
Generic: _SpecialForm = ...
Protocol: _SpecialForm = ...

class TypeVar:
    def __init__(self, name, covariant: bool = ..., contravariant: bool = ...) -&gt; None: ...

class ParamSpec:
    def __init__(self, name: str) -&gt; None: ...

_T = TypeVar("_T")
_T_co = TypeVar("_T_co", covariant=True)
_K = TypeVar("_K")
_V = TypeVar("_V")
_S = TypeVar("_S", contravariant=True)
_R = TypeVar("_R", covariant=True)

class Coroutine(Generic[_T_co, _S, _R]): ...
class Iterable(Generic[_T_co]): ...
class Mapping(Generic[_K, _V]): ...
class Sequence(Iterable[_T_co]): ...
class Tuple(Sequence[_T_co]): ...
def overload(func: _T) -&gt; _T: ...
"""

stubtest_builtins_stub = """
from typing import Generic, Mapping, Sequence, TypeVar, overload

T = TypeVar('T')
T_co = TypeVar('T_co', covariant=True)
KT = TypeVar('KT')
VT = TypeVar('VT')

class object:
    __module__: str
    def __init__(self) -&gt; None: pass
class type: ...

class tuple(Sequence[T_co], Generic[T_co]): ...
class dict(Mapping[KT, VT]): ...

class function: pass
class ellipsis: pass

class int: ...
class float: ...
class bool(int): ...
class str: ...
class bytes: ...

class list(Sequence[T]): ...

def property(f: T) -&gt; T: ...
def classmethod(f: T) -&gt; T: ...
def staticmethod(f: T) -&gt; T: ...
"""


</t>
<t tx="ekr.20220525082936.1199">def run_stubtest(
    stub: str, runtime: str, options: List[str], config_file: Optional[str] = None,
) -&gt; str:
    with use_tmp_dir(TEST_MODULE_NAME) as tmp_dir:
        with open("builtins.pyi", "w") as f:
            f.write(stubtest_builtins_stub)
        with open("typing.pyi", "w") as f:
            f.write(stubtest_typing_stub)
        with open(f"{TEST_MODULE_NAME}.pyi", "w") as f:
            f.write(stub)
        with open(f"{TEST_MODULE_NAME}.py", "w") as f:
            f.write(runtime)
        if config_file:
            with open(f"{TEST_MODULE_NAME}_config.ini", "w") as f:
                f.write(config_file)
            options = options + ["--mypy-config-file", f"{TEST_MODULE_NAME}_config.ini"]
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            test_stubs(
                parse_options([TEST_MODULE_NAME] + options),
                use_builtins_fixtures=True
            )
        # remove cwd as it's not available from outside
        return output.getvalue().replace(tmp_dir + os.sep, "")


</t>
<t tx="ekr.20220525082936.12">@abstractmethod
def visit_type_var(self, t: TypeVarType) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.120">def count_stats(messages: List[str]) -&gt; Tuple[int, int, int]:
    """Count total number of errors, notes and error_files in message list."""
    errors = [e for e in messages if ': error:' in e]
    error_files = {e.split(':')[0] for e in errors}
    notes = [e for e in messages if ': note:' in e]
    return len(errors), len(notes), len(error_files)


</t>
<t tx="ekr.20220525082936.1200">class Case:
    def __init__(self, stub: str, runtime: str, error: Optional[str]):
        self.stub = stub
        self.runtime = runtime
        self.error = error


</t>
<t tx="ekr.20220525082936.1201">def collect_cases(fn: Callable[..., Iterator[Case]]) -&gt; Callable[..., None]:
    """run_stubtest used to be slow, so we used this decorator to combine cases.

    If you're reading this and bored, feel free to refactor this and make it more like
    other mypy tests.

    """

    @others
    return test


</t>
<t tx="ekr.20220525082936.1202">def test(*args: Any, **kwargs: Any) -&gt; None:
    cases = list(fn(*args, **kwargs))
    expected_errors = set()
    for c in cases:
        if c.error is None:
            continue
        expected_error = c.error
        if expected_error == "":
            expected_error = TEST_MODULE_NAME
        elif not expected_error.startswith(f"{TEST_MODULE_NAME}."):
            expected_error = f"{TEST_MODULE_NAME}.{expected_error}"
        assert expected_error not in expected_errors, (
            "collect_cases merges cases into a single stubtest invocation; we already "
            "expect an error for {}".format(expected_error)
        )
        expected_errors.add(expected_error)
    output = run_stubtest(
        stub="\n\n".join(textwrap.dedent(c.stub.lstrip("\n")) for c in cases),
        runtime="\n\n".join(textwrap.dedent(c.runtime.lstrip("\n")) for c in cases),
        options=["--generate-allowlist"],
    )

    actual_errors = set(output.splitlines())
    assert actual_errors == expected_errors, output

</t>
<t tx="ekr.20220525082936.1203">class StubtestUnit(unittest.TestCase):
    @others
</t>
<t tx="ekr.20220525082936.1204">@collect_cases
def test_basic_good(self) -&gt; Iterator[Case]:
    yield Case(
        stub="def f(number: int, text: str) -&gt; None: ...",
        runtime="def f(number, text): pass",
        error=None,
    )
    yield Case(
        stub="""
        class X:
            def f(self, number: int, text: str) -&gt; None: ...
        """,
        runtime="""
        class X:
            def f(self, number, text): pass
        """,
        error=None,
    )

</t>
<t tx="ekr.20220525082936.1205">@collect_cases
def test_types(self) -&gt; Iterator[Case]:
    yield Case(
        stub="def mistyped_class() -&gt; None: ...",
        runtime="class mistyped_class: pass",
        error="mistyped_class",
    )
    yield Case(
        stub="class mistyped_fn: ...", runtime="def mistyped_fn(): pass", error="mistyped_fn"
    )
    yield Case(
        stub="""
        class X:
            def mistyped_var(self) -&gt; int: ...
        """,
        runtime="""
        class X:
            mistyped_var = 1
        """,
        error="X.mistyped_var",
    )

</t>
<t tx="ekr.20220525082936.1206">@collect_cases
def test_coroutines(self) -&gt; Iterator[Case]:
    yield Case(
        stub="def bar() -&gt; int: ...",
        runtime="async def bar(): return 5",
        error="bar",
    )
    # Don't error for this one -- we get false positives otherwise
    yield Case(
        stub="async def foo() -&gt; int: ...",
        runtime="def foo(): return 5",
        error=None,
    )
    yield Case(
        stub="def baz() -&gt; int: ...",
        runtime="def baz(): return 5",
        error=None,
    )
    yield Case(
        stub="async def bingo() -&gt; int: ...",
        runtime="async def bingo(): return 5",
        error=None,
    )

</t>
<t tx="ekr.20220525082936.1207">@collect_cases
def test_arg_name(self) -&gt; Iterator[Case]:
    yield Case(
        stub="def bad(number: int, text: str) -&gt; None: ...",
        runtime="def bad(num, text) -&gt; None: pass",
        error="bad",
    )
    if sys.version_info &gt;= (3, 8):
        yield Case(
            stub="def good_posonly(__number: int, text: str) -&gt; None: ...",
            runtime="def good_posonly(num, /, text): pass",
            error=None,
        )
        yield Case(
            stub="def bad_posonly(__number: int, text: str) -&gt; None: ...",
            runtime="def bad_posonly(flag, /, text): pass",
            error="bad_posonly",
        )
    yield Case(
        stub="""
        class BadMethod:
            def f(self, number: int, text: str) -&gt; None: ...
        """,
        runtime="""
        class BadMethod:
            def f(self, n, text): pass
        """,
        error="BadMethod.f",
    )
    yield Case(
        stub="""
        class GoodDunder:
            def __exit__(self, t, v, tb) -&gt; None: ...
        """,
        runtime="""
        class GoodDunder:
            def __exit__(self, exc_type, exc_val, exc_tb): pass
        """,
        error=None,
    )

</t>
<t tx="ekr.20220525082936.1208">@collect_cases
def test_arg_kind(self) -&gt; Iterator[Case]:
    yield Case(
        stub="def runtime_kwonly(number: int, text: str) -&gt; None: ...",
        runtime="def runtime_kwonly(number, *, text): pass",
        error="runtime_kwonly",
    )
    yield Case(
        stub="def stub_kwonly(number: int, *, text: str) -&gt; None: ...",
        runtime="def stub_kwonly(number, text): pass",
        error="stub_kwonly",
    )
    yield Case(
        stub="def stub_posonly(__number: int, text: str) -&gt; None: ...",
        runtime="def stub_posonly(number, text): pass",
        error="stub_posonly",
    )
    if sys.version_info &gt;= (3, 8):
        yield Case(
            stub="def good_posonly(__number: int, text: str) -&gt; None: ...",
            runtime="def good_posonly(number, /, text): pass",
            error=None,
        )
        yield Case(
            stub="def runtime_posonly(number: int, text: str) -&gt; None: ...",
            runtime="def runtime_posonly(number, /, text): pass",
            error="runtime_posonly",
        )

</t>
<t tx="ekr.20220525082936.1209">@collect_cases
def test_default_value(self) -&gt; Iterator[Case]:
    yield Case(
        stub="def f1(text: str = ...) -&gt; None: ...",
        runtime="def f1(text = 'asdf'): pass",
        error=None,
    )
    yield Case(
        stub="def f2(text: str = ...) -&gt; None: ...", runtime="def f2(text): pass", error="f2"
    )
    yield Case(
        stub="def f3(text: str) -&gt; None: ...",
        runtime="def f3(text = 'asdf'): pass",
        error="f3",
    )
    yield Case(
        stub="def f4(text: str = ...) -&gt; None: ...",
        runtime="def f4(text = None): pass",
        error="f4",
    )
    yield Case(
        stub="def f5(data: bytes = ...) -&gt; None: ...",
        runtime="def f5(data = 'asdf'): pass",
        error="f5",
    )
    yield Case(
        stub="""
        from typing import TypeVar
        _T = TypeVar("_T", bound=str)
        def f6(text: _T = ...) -&gt; None: ...
        """,
        runtime="def f6(text = None): pass",
        error="f6",
    )

</t>
<t tx="ekr.20220525082936.121">def split_words(msg: str) -&gt; List[str]:
    """Split line of text into words (but not within quoted groups)."""
    next_word = ''
    res: List[str] = []
    allow_break = True
    for c in msg:
        if c == ' ' and allow_break:
            res.append(next_word)
            next_word = ''
            continue
        if c == '"':
            allow_break = not allow_break
        next_word += c
    res.append(next_word)
    return res


</t>
<t tx="ekr.20220525082936.1210">@collect_cases
def test_static_class_method(self) -&gt; Iterator[Case]:
    yield Case(
        stub="""
        class Good:
            @classmethod
            def f(cls, number: int, text: str) -&gt; None: ...
        """,
        runtime="""
        class Good:
            @classmethod
            def f(cls, number, text): pass
        """,
        error=None,
    )
    yield Case(
        stub="""
        class Bad1:
            def f(cls, number: int, text: str) -&gt; None: ...
        """,
        runtime="""
        class Bad1:
            @classmethod
            def f(cls, number, text): pass
        """,
        error="Bad1.f",
    )
    yield Case(
        stub="""
        class Bad2:
            @classmethod
            def f(cls, number: int, text: str) -&gt; None: ...
        """,
        runtime="""
        class Bad2:
            @staticmethod
            def f(self, number, text): pass
        """,
        error="Bad2.f",
    )
    yield Case(
        stub="""
        class Bad3:
            @staticmethod
            def f(cls, number: int, text: str) -&gt; None: ...
        """,
        runtime="""
        class Bad3:
            @classmethod
            def f(self, number, text): pass
        """,
        error="Bad3.f",
    )
    yield Case(
        stub="""
        class GoodNew:
            def __new__(cls, *args, **kwargs): ...
        """,
        runtime="""
        class GoodNew:
            def __new__(cls, *args, **kwargs): pass
        """,
        error=None,
    )

</t>
<t tx="ekr.20220525082936.1211">@collect_cases
def test_arg_mismatch(self) -&gt; Iterator[Case]:
    yield Case(
        stub="def f1(a, *, b, c) -&gt; None: ...", runtime="def f1(a, *, b, c): pass", error=None
    )
    yield Case(
        stub="def f2(a, *, b) -&gt; None: ...", runtime="def f2(a, *, b, c): pass", error="f2"
    )
    yield Case(
        stub="def f3(a, *, b, c) -&gt; None: ...", runtime="def f3(a, *, b): pass", error="f3"
    )
    yield Case(
        stub="def f4(a, *, b, c) -&gt; None: ...", runtime="def f4(a, b, *, c): pass", error="f4"
    )
    yield Case(
        stub="def f5(a, b, *, c) -&gt; None: ...", runtime="def f5(a, *, b, c): pass", error="f5"
    )

</t>
<t tx="ekr.20220525082936.1212">@collect_cases
def test_varargs_varkwargs(self) -&gt; Iterator[Case]:
    yield Case(
        stub="def f1(*args, **kwargs) -&gt; None: ...",
        runtime="def f1(*args, **kwargs): pass",
        error=None,
    )
    yield Case(
        stub="def f2(*args, **kwargs) -&gt; None: ...",
        runtime="def f2(**kwargs): pass",
        error="f2",
    )
    yield Case(
        stub="def g1(a, b, c, d) -&gt; None: ...", runtime="def g1(a, *args): pass", error=None
    )
    yield Case(
        stub="def g2(a, b, c, d, *args) -&gt; None: ...", runtime="def g2(a): pass", error="g2"
    )
    yield Case(
        stub="def g3(a, b, c, d, *args) -&gt; None: ...",
        runtime="def g3(a, *args): pass",
        error=None,
    )
    yield Case(
        stub="def h1(a) -&gt; None: ...", runtime="def h1(a, b, c, d, *args): pass", error="h1"
    )
    yield Case(
        stub="def h2(a, *args) -&gt; None: ...", runtime="def h2(a, b, c, d): pass", error="h2"
    )
    yield Case(
        stub="def h3(a, *args) -&gt; None: ...",
        runtime="def h3(a, b, c, d, *args): pass",
        error="h3",
    )
    yield Case(
        stub="def j1(a: int, *args) -&gt; None: ...", runtime="def j1(a): pass", error="j1"
    )
    yield Case(
        stub="def j2(a: int) -&gt; None: ...", runtime="def j2(a, *args): pass", error="j2"
    )
    yield Case(
        stub="def j3(a, b, c) -&gt; None: ...", runtime="def j3(a, *args, c): pass", error="j3"
    )
    yield Case(stub="def k1(a, **kwargs) -&gt; None: ...", runtime="def k1(a): pass", error="k1")
    yield Case(
        # In theory an error, but led to worse results in practice
        stub="def k2(a) -&gt; None: ...",
        runtime="def k2(a, **kwargs): pass",
        error=None,
    )
    yield Case(
        stub="def k3(a, b) -&gt; None: ...", runtime="def k3(a, **kwargs): pass", error="k3"
    )
    yield Case(
        stub="def k4(a, *, b) -&gt; None: ...", runtime="def k4(a, **kwargs): pass", error=None
    )
    yield Case(
        stub="def k5(a, *, b) -&gt; None: ...",
        runtime="def k5(a, *, b, c, **kwargs): pass",
        error="k5",
    )
    yield Case(
        stub="def k6(a, *, b, **kwargs) -&gt; None: ...",
        runtime="def k6(a, *, b, c, **kwargs): pass",
        error="k6",
    )

</t>
<t tx="ekr.20220525082936.1213">@collect_cases
def test_overload(self) -&gt; Iterator[Case]:
    yield Case(
        stub="""
        from typing import overload

        @overload
        def f1(a: int, *, c: int = ...) -&gt; int: ...
        @overload
        def f1(a: int, b: int, c: int = ...) -&gt; str: ...
        """,
        runtime="def f1(a, b = 0, c = 0): pass",
        error=None,
    )
    yield Case(
        stub="""
        @overload
        def f2(a: int, *, c: int = ...) -&gt; int: ...
        @overload
        def f2(a: int, b: int, c: int = ...) -&gt; str: ...
        """,
        runtime="def f2(a, b, c = 0): pass",
        error="f2",
    )
    yield Case(
        stub="""
        @overload
        def f3(a: int) -&gt; int: ...
        @overload
        def f3(a: int, b: str) -&gt; str: ...
        """,
        runtime="def f3(a, b = None): pass",
        error="f3",
    )
    yield Case(
        stub="""
        @overload
        def f4(a: int, *args, b: int, **kwargs) -&gt; int: ...
        @overload
        def f4(a: str, *args, b: int, **kwargs) -&gt; str: ...
        """,
        runtime="def f4(a, *args, b, **kwargs): pass",
        error=None,
    )
    if sys.version_info &gt;= (3, 8):
        yield Case(
            stub="""
            @overload
            def f5(__a: int) -&gt; int: ...
            @overload
            def f5(__b: str) -&gt; str: ...
            """,
            runtime="def f5(x, /): pass",
            error=None,
        )

</t>
<t tx="ekr.20220525082936.1214">@collect_cases
def test_property(self) -&gt; Iterator[Case]:
    yield Case(
        stub="""
        class Good:
            @property
            def read_only_attr(self) -&gt; int: ...
        """,
        runtime="""
        class Good:
            @property
            def read_only_attr(self): return 1
        """,
        error=None,
    )
    yield Case(
        stub="""
        class Bad:
            @property
            def f(self) -&gt; int: ...
        """,
        runtime="""
        class Bad:
            def f(self) -&gt; int: return 1
        """,
        error="Bad.f",
    )
    yield Case(
        stub="""
        class GoodReadOnly:
            @property
            def f(self) -&gt; int: ...
        """,
        runtime="""
        class GoodReadOnly:
            f = 1
        """,
        error=None,
    )
    yield Case(
        stub="""
        class BadReadOnly:
            @property
            def f(self) -&gt; str: ...
        """,
        runtime="""
        class BadReadOnly:
            f = 1
        """,
        error="BadReadOnly.f",
    )
    yield Case(
        stub="""
        class Y:
            @property
            def read_only_attr(self) -&gt; int: ...
            @read_only_attr.setter
            def read_only_attr(self, val: int) -&gt; None: ...
        """,
        runtime="""
        class Y:
            @property
            def read_only_attr(self): return 5
        """,
        error="Y.read_only_attr",
    )
    yield Case(
        stub="""
        class Z:
            @property
            def read_write_attr(self) -&gt; int: ...
            @read_write_attr.setter
            def read_write_attr(self, val: int) -&gt; None: ...
        """,
        runtime="""
        class Z:
            @property
            def read_write_attr(self): return self._val
            @read_write_attr.setter
            def read_write_attr(self, val): self._val = val
        """,
        error=None,
    )

</t>
<t tx="ekr.20220525082936.1215">@collect_cases
def test_var(self) -&gt; Iterator[Case]:
    yield Case(stub="x1: int", runtime="x1 = 5", error=None)
    yield Case(stub="x2: str", runtime="x2 = 5", error="x2")
    yield Case("from typing import Tuple", "", None)  # dummy case
    yield Case(
        stub="""
        x3: Tuple[int, int]
        """,
        runtime="x3 = (1, 3)",
        error=None,
    )
    yield Case(
        stub="""
        x4: Tuple[int, int]
        """,
        runtime="x4 = (1, 3, 5)",
        error="x4",
    )
    yield Case(stub="x5: int", runtime="def x5(a, b): pass", error="x5")
    yield Case(
        stub="def foo(a: int, b: int) -&gt; None: ...\nx6 = foo",
        runtime="def foo(a, b): pass\ndef x6(c, d): pass",
        error="x6",
    )
    yield Case(
        stub="""
        class X:
            f: int
        """,
        runtime="""
        class X:
            def __init__(self):
                self.f = "asdf"
        """,
        error=None,
    )
    yield Case(
        stub="""
        class Y:
            read_only_attr: int
        """,
        runtime="""
        class Y:
            @property
            def read_only_attr(self): return 5
        """,
        error="Y.read_only_attr",
    )
    yield Case(
        stub="""
        class Z:
            read_write_attr: int
        """,
        runtime="""
        class Z:
            @property
            def read_write_attr(self): return self._val
            @read_write_attr.setter
            def read_write_attr(self, val): self._val = val
        """,
        error=None,
    )

</t>
<t tx="ekr.20220525082936.1216">@collect_cases
def test_type_alias(self) -&gt; Iterator[Case]:
    yield Case(
        stub="""
        class X:
            def f(self) -&gt; None: ...
        Y = X
        """,
        runtime="""
        class X:
            def f(self) -&gt; None: ...
        class Y: ...
        """,
        error="Y.f",
    )
    yield Case(
        stub="""
        from typing import Tuple
        A = Tuple[int, str]
        """,
        runtime="A = (int, str)",
        error="A",
    )
    # Error if an alias isn't present at runtime...
    yield Case(
        stub="B = str",
        runtime="",
        error="B"
    )
    # ... but only if the alias isn't private
    yield Case(
        stub="_C = int",
        runtime="",
        error=None
    )

</t>
<t tx="ekr.20220525082936.1217">@collect_cases
def test_enum(self) -&gt; Iterator[Case]:
    yield Case(
        stub="""
        import enum
        class X(enum.Enum):
            a: int
            b: str
            c: str
        """,
        runtime="""
        import enum
        class X(enum.Enum):
            a = 1
            b = "asdf"
            c = 2
        """,
        error="X.c",
    )

</t>
<t tx="ekr.20220525082936.1218">@collect_cases
def test_decorator(self) -&gt; Iterator[Case]:
    yield Case(
        stub="""
        from typing import Any, Callable
        def decorator(f: Callable[[], int]) -&gt; Callable[..., Any]: ...
        @decorator
        def f() -&gt; Any: ...
        """,
        runtime="""
        def decorator(f): return f
        @decorator
        def f(): return 3
        """,
        error=None,
    )

</t>
<t tx="ekr.20220525082936.1219">@collect_cases
def test_missing(self) -&gt; Iterator[Case]:
    yield Case(stub="x = 5", runtime="", error="x")
    yield Case(stub="def f(): ...", runtime="", error="f")
    yield Case(stub="class X: ...", runtime="", error="X")
    yield Case(
        stub="""
        from typing import overload
        @overload
        def h(x: int): ...
        @overload
        def h(x: str): ...
        """,
        runtime="",
        error="h",
    )
    yield Case(stub="", runtime="__all__ = []", error=None)  # dummy case
    yield Case(stub="", runtime="__all__ += ['y']\ny = 5", error="y")
    yield Case(stub="", runtime="__all__ += ['g']\ndef g(): pass", error="g")
    # Here we should only check that runtime has B, since the stub explicitly re-exports it
    yield Case(
        stub="from mystery import A, B as B, C as D  # type: ignore", runtime="", error="B"
    )
    yield Case(
        stub="class Y: ...",
        runtime="__all__ += ['Y']\nclass Y:\n  def __or__(self, other): return self|other",
        error="Y.__or__"
    )
    yield Case(
        stub="class Z: ...",
        runtime="__all__ += ['Z']\nclass Z:\n  def __reduce__(self): return (Z,)",
        error=None
    )

</t>
<t tx="ekr.20220525082936.122">def get_terminal_width() -&gt; int:
    """Get current terminal width if possible, otherwise return the default one."""
    return (int(os.getenv('MYPY_FORCE_TERMINAL_WIDTH', '0'))
            or shutil.get_terminal_size().columns
            or DEFAULT_COLUMNS)


</t>
<t tx="ekr.20220525082936.1220">@collect_cases
def test_missing_no_runtime_all(self) -&gt; Iterator[Case]:
    yield Case(stub="", runtime="import sys", error=None)
    yield Case(stub="", runtime="def g(): ...", error="g")
    yield Case(stub="", runtime="CONSTANT = 0", error="CONSTANT")

</t>
<t tx="ekr.20220525082936.1221">@collect_cases
def test_non_public_1(self) -&gt; Iterator[Case]:
    yield Case(
        stub="__all__: list[str]", runtime="", error=f"{TEST_MODULE_NAME}.__all__"
    )  # dummy case
    yield Case(stub="_f: int", runtime="def _f(): ...", error="_f")

</t>
<t tx="ekr.20220525082936.1222">@collect_cases
def test_non_public_2(self) -&gt; Iterator[Case]:
    yield Case(
        stub="__all__: list[str] = ['f']", runtime="__all__ = ['f']", error=None
    )
    yield Case(stub="f: int", runtime="def f(): ...", error="f")
    yield Case(stub="g: int", runtime="def g(): ...", error="g")

</t>
<t tx="ekr.20220525082936.1223">@collect_cases
def test_dunders(self) -&gt; Iterator[Case]:
    yield Case(
        stub="class A:\n  def __init__(self, a: int, b: int) -&gt; None: ...",
        runtime="class A:\n  def __init__(self, a, bx): pass",
        error="A.__init__",
    )
    yield Case(
        stub="class B:\n  def __call__(self, c: int, d: int) -&gt; None: ...",
        runtime="class B:\n  def __call__(self, c, dx): pass",
        error="B.__call__",
    )
    yield Case(
        stub=(
            "class C:\n"
            "  def __init_subclass__(\n"
            "    cls, e: int = ..., **kwargs: int\n"
            "  ) -&gt; None: ...\n"
        ),
        runtime="class C:\n  def __init_subclass__(cls, e=1, **kwargs): pass",
        error=None,
    )
    if sys.version_info &gt;= (3, 9):
        yield Case(
            stub="class D:\n  def __class_getitem__(cls, type: type) -&gt; type: ...",
            runtime="class D:\n  def __class_getitem__(cls, type): ...",
            error=None,
        )

</t>
<t tx="ekr.20220525082936.1224">@collect_cases
def test_not_subclassable(self) -&gt; Iterator[Case]:
    yield Case(
        stub="class CanBeSubclassed: ...",
        runtime="class CanBeSubclassed: ...",
        error=None,
    )
    yield Case(
        stub="class CannotBeSubclassed:\n  def __init_subclass__(cls) -&gt; None: ...",
        runtime="class CannotBeSubclassed:\n  def __init_subclass__(cls): raise TypeError",
        error="CannotBeSubclassed",
    )

</t>
<t tx="ekr.20220525082936.1225">@collect_cases
def test_name_mangling(self) -&gt; Iterator[Case]:
    yield Case(
        stub="""
        class X:
            def __mangle_good(self, text: str) -&gt; None: ...
            def __mangle_bad(self, number: int) -&gt; None: ...
        """,
        runtime="""
        class X:
            def __mangle_good(self, text): pass
            def __mangle_bad(self, text): pass
        """,
        error="X.__mangle_bad"
    )

</t>
<t tx="ekr.20220525082936.1226">@collect_cases
def test_mro(self) -&gt; Iterator[Case]:
    yield Case(
        stub="""
        class A:
            def foo(self, x: int) -&gt; None: ...
        class B(A):
            pass
        class C(A):
            pass
        """,
        runtime="""
        class A:
            def foo(self, x: int) -&gt; None: ...
        class B(A):
            def foo(self, x: int) -&gt; None: ...
        class C(A):
            def foo(self, y: int) -&gt; None: ...
        """,
        error="C.foo"
    )
    yield Case(
        stub="""
        class X: ...
        """,
        runtime="""
        class X:
            def __init__(self, x): pass
        """,
        error="X.__init__"
    )

</t>
<t tx="ekr.20220525082936.1227">@collect_cases
def test_good_literal(self) -&gt; Iterator[Case]:
    yield Case(
        stub=r"""
        from typing_extensions import Literal

        import enum
        class Color(enum.Enum):
            RED: int

        NUM: Literal[1]
        CHAR: Literal['a']
        FLAG: Literal[True]
        NON: Literal[None]
        BYT1: Literal[b'abc']
        BYT2: Literal[b'\x90']
        ENUM: Literal[Color.RED]
        """,
        runtime=r"""
        import enum
        class Color(enum.Enum):
            RED = 3

        NUM = 1
        CHAR = 'a'
        NON = None
        FLAG = True
        BYT1 = b"abc"
        BYT2 = b'\x90'
        ENUM = Color.RED
        """,
        error=None,
    )

</t>
<t tx="ekr.20220525082936.1228">@collect_cases
def test_bad_literal(self) -&gt; Iterator[Case]:
    yield Case("from typing_extensions import Literal", "", None)  # dummy case
    yield Case(
        stub="INT_FLOAT_MISMATCH: Literal[1]",
        runtime="INT_FLOAT_MISMATCH = 1.0",
        error="INT_FLOAT_MISMATCH",
    )
    yield Case(
        stub="WRONG_INT: Literal[1]",
        runtime="WRONG_INT = 2",
        error="WRONG_INT",
    )
    yield Case(
        stub="WRONG_STR: Literal['a']",
        runtime="WRONG_STR = 'b'",
        error="WRONG_STR",
    )
    yield Case(
        stub="BYTES_STR_MISMATCH: Literal[b'value']",
        runtime="BYTES_STR_MISMATCH = 'value'",
        error="BYTES_STR_MISMATCH",
    )
    yield Case(
        stub="STR_BYTES_MISMATCH: Literal['value']",
        runtime="STR_BYTES_MISMATCH = b'value'",
        error="STR_BYTES_MISMATCH",
    )
    yield Case(
        stub="WRONG_BYTES: Literal[b'abc']",
        runtime="WRONG_BYTES = b'xyz'",
        error="WRONG_BYTES",
    )
    yield Case(
        stub="WRONG_BOOL_1: Literal[True]",
        runtime="WRONG_BOOL_1 = False",
        error='WRONG_BOOL_1',
    )
    yield Case(
        stub="WRONG_BOOL_2: Literal[False]",
        runtime="WRONG_BOOL_2 = True",
        error='WRONG_BOOL_2',
    )

</t>
<t tx="ekr.20220525082936.1229">@collect_cases
def test_special_subtype(self) -&gt; Iterator[Case]:
    yield Case(
        stub="""
        b1: bool
        b2: bool
        b3: bool
        """,
        runtime="""
        b1 = 0
        b2 = 1
        b3 = 2
        """,
        error="b3",
    )
    yield Case(
        stub="""
        from typing_extensions import TypedDict

        class _Options(TypedDict):
            a: str
            b: int

        opt1: _Options
        opt2: _Options
        opt3: _Options
        """,
        runtime="""
        opt1 = {"a": "3.", "b": 14}
        opt2 = {"some": "stuff"}  # false negative
        opt3 = 0
        """,
        error="opt3",
    )

</t>
<t tx="ekr.20220525082936.123">def soft_wrap(msg: str, max_len: int, first_offset: int,
              num_indent: int = 0) -&gt; str:
    """Wrap a long error message into few lines.

    Breaks will only happen between words, and never inside a quoted group
    (to avoid breaking types such as "Union[int, str]"). The 'first_offset' is
    the width before the start of first line.

    Pad every next line with 'num_indent' spaces. Every line will be at most 'max_len'
    characters, except if it is a single word or quoted group.

    For example:
               first_offset
        ------------------------
        path/to/file: error: 58: Some very long error message
            that needs to be split in separate lines.
            "Long[Type, Names]" are never split.
        ^^^^--------------------------------------------------
        num_indent           max_len
    """
    words = split_words(msg)
    next_line = words.pop(0)
    lines: List[str] = []
    while words:
        next_word = words.pop(0)
        max_line_len = max_len - num_indent if lines else max_len - first_offset
        # Add 1 to account for space between words.
        if len(next_line) + len(next_word) + 1 &lt;= max_line_len:
            next_line += ' ' + next_word
        else:
            lines.append(next_line)
            next_line = next_word
    lines.append(next_line)
    padding = '\n' + ' ' * num_indent
    return padding.join(lines)


</t>
<t tx="ekr.20220525082936.1230">@collect_cases
def test_protocol(self) -&gt; Iterator[Case]:
    if sys.version_info &lt; (3, 7):
        return
    yield Case(
        stub="""
        from typing_extensions import Protocol

        class X(Protocol):
            def foo(self, x: int, y: bytes = ...) -&gt; str: ...
        """,
        runtime="""
        from typing_extensions import Protocol

        class X(Protocol):
            def foo(self, x: int, y: bytes = ...) -&gt; str: ...
        """,
        # TODO: this should not be an error, #12820
        error="X.__init__"
    )

@collect_cases
def test_type_var(self) -&gt; Iterator[Case]:
    yield Case(
        stub="from typing import TypeVar", runtime="from typing import TypeVar", error=None
    )
    yield Case(
        stub="A = TypeVar('A')",
        runtime="A = TypeVar('A')",
        error=None,
    )
    yield Case(
        stub="B = TypeVar('B')",
        runtime="B = 5",
        error="B",
    )
    if sys.version_info &gt;= (3, 10):
        yield Case(
            stub="from typing import ParamSpec",
            runtime="from typing import ParamSpec",
            error=None
        )
        yield Case(
            stub="C = ParamSpec('C')",
            runtime="C = ParamSpec('C')",
            error=None,
        )


</t>
<t tx="ekr.20220525082936.1231">def remove_color_code(s: str) -&gt; str:
    return re.sub("\\x1b.*?m", "", s)  # this works!


</t>
<t tx="ekr.20220525082936.1232">class StubtestMiscUnit(unittest.TestCase):
    @others
</t>
<t tx="ekr.20220525082936.1233">def test_output(self) -&gt; None:
    output = run_stubtest(
        stub="def bad(number: int, text: str) -&gt; None: ...",
        runtime="def bad(num, text): pass",
        options=[],
    )
    expected = (
        f'error: {TEST_MODULE_NAME}.bad is inconsistent, stub argument "number" differs '
        'from runtime argument "num"\n'
        'Stub: at line 1\ndef (number: builtins.int, text: builtins.str)\n'
        f"Runtime: at line 1 in file {TEST_MODULE_NAME}.py\ndef (num, text)\n\n"
        'Found 1 error (checked 1 module)\n'
    )
    assert remove_color_code(output) == expected

    output = run_stubtest(
        stub="def bad(number: int, text: str) -&gt; None: ...",
        runtime="def bad(num, text): pass",
        options=["--concise"],
    )
    expected = (
        "{}.bad is inconsistent, "
        'stub argument "number" differs from runtime argument "num"\n'.format(TEST_MODULE_NAME)
    )
    assert remove_color_code(output) == expected

</t>
<t tx="ekr.20220525082936.1234">def test_ignore_flags(self) -&gt; None:
    output = run_stubtest(
        stub="", runtime="__all__ = ['f']\ndef f(): pass", options=["--ignore-missing-stub"]
    )
    assert output == 'Success: no issues found in 1 module\n'

    output = run_stubtest(
        stub="", runtime="def f(): pass", options=["--ignore-missing-stub"]
    )
    assert output == 'Success: no issues found in 1 module\n'

    output = run_stubtest(
        stub="def f(__a): ...", runtime="def f(a): pass", options=["--ignore-positional-only"]
    )
    assert output == 'Success: no issues found in 1 module\n'

</t>
<t tx="ekr.20220525082936.1235">def test_allowlist(self) -&gt; None:
    # Can't use this as a context because Windows
    allowlist = tempfile.NamedTemporaryFile(mode="w+", delete=False)
    try:
        with allowlist:
            allowlist.write(f"{TEST_MODULE_NAME}.bad  # comment\n# comment")

        output = run_stubtest(
            stub="def bad(number: int, text: str) -&gt; None: ...",
            runtime="def bad(asdf, text): pass",
            options=["--allowlist", allowlist.name],
        )
        assert output == 'Success: no issues found in 1 module\n'

        # test unused entry detection
        output = run_stubtest(stub="", runtime="", options=["--allowlist", allowlist.name])
        assert output == (
            f"note: unused allowlist entry {TEST_MODULE_NAME}.bad\n"
            "Found 1 error (checked 1 module)\n"
        )

        output = run_stubtest(
            stub="",
            runtime="",
            options=["--allowlist", allowlist.name, "--ignore-unused-allowlist"],
        )
        assert output == 'Success: no issues found in 1 module\n'

        # test regex matching
        with open(allowlist.name, mode="w+") as f:
            f.write(f"{TEST_MODULE_NAME}.b.*\n")
            f.write("(unused_missing)?\n")
            f.write("unused.*\n")

        output = run_stubtest(
            stub=textwrap.dedent(
                """
                def good() -&gt; None: ...
                def bad(number: int) -&gt; None: ...
                def also_bad(number: int) -&gt; None: ...
                """.lstrip("\n")
            ),
            runtime=textwrap.dedent(
                """
                def good(): pass
                def bad(asdf): pass
                def also_bad(asdf): pass
                """.lstrip("\n")
            ),
            options=["--allowlist", allowlist.name, "--generate-allowlist"],
        )
        assert output == (
            f"note: unused allowlist entry unused.*\n"
            f"{TEST_MODULE_NAME}.also_bad\n"
        )
    finally:
        os.unlink(allowlist.name)

</t>
<t tx="ekr.20220525082936.1236">def test_mypy_build(self) -&gt; None:
    output = run_stubtest(stub="+", runtime="", options=[])
    assert remove_color_code(output) == (
        "error: not checking stubs due to failed mypy compile:\n{}.pyi:1: "
        "error: invalid syntax\n".format(TEST_MODULE_NAME)
    )

    output = run_stubtest(stub="def f(): ...\ndef f(): ...", runtime="", options=[])
    assert remove_color_code(output) == (
        'error: not checking stubs due to mypy build errors:\n{}.pyi:2: '
        'error: Name "f" already defined on line 1\n'.format(TEST_MODULE_NAME)
    )

</t>
<t tx="ekr.20220525082936.1237">def test_missing_stubs(self) -&gt; None:
    output = io.StringIO()
    with contextlib.redirect_stdout(output):
        test_stubs(parse_options(["not_a_module"]))
    assert remove_color_code(output.getvalue()) == (
        "error: not_a_module failed to find stubs\n"
        "Stub:\nMISSING\nRuntime:\nN/A\n\n"
        "Found 1 error (checked 1 module)\n"
    )

def test_only_py(self) -&gt; None:
    # in this case, stubtest will check the py against itself
    # this is useful to support packages with a mix of stubs and inline types
    with use_tmp_dir(TEST_MODULE_NAME):
        with open(f"{TEST_MODULE_NAME}.py", "w") as f:
            f.write("a = 1")
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            test_stubs(parse_options([TEST_MODULE_NAME]))
        output_str = remove_color_code(output.getvalue())
        assert output_str == 'Success: no issues found in 1 module\n'

</t>
<t tx="ekr.20220525082936.1238">def test_get_typeshed_stdlib_modules(self) -&gt; None:
    stdlib = mypy.stubtest.get_typeshed_stdlib_modules(None, (3, 6))
    assert "builtins" in stdlib
    assert "os" in stdlib
    assert "os.path" in stdlib
    assert "asyncio" in stdlib
    assert "graphlib" not in stdlib
    assert "formatter" in stdlib
    assert "importlib.metadata" not in stdlib

    stdlib = mypy.stubtest.get_typeshed_stdlib_modules(None, (3, 10))
    assert "graphlib" in stdlib
    assert "formatter" not in stdlib
    assert "importlib.metadata" in stdlib

</t>
<t tx="ekr.20220525082936.1239">def test_signature(self) -&gt; None:
    def f(a: int, b: int, *, c: int, d: int = 0, **kwargs: Any) -&gt; None:
        pass

    assert (
        str(mypy.stubtest.Signature.from_inspect_signature(inspect.signature(f)))
        == "def (a, b, *, c, d = ..., **kwargs)"
    )

</t>
<t tx="ekr.20220525082936.124">def hash_digest(data: bytes) -&gt; str:
    """Compute a hash digest of some data.

    We use a cryptographic hash because we want a low probability of
    accidental collision, but we don't really care about any of the
    cryptographic properties.
    """
    # Once we drop Python 3.5 support, we should consider using
    # blake2b, which is faster.
    return hashlib.sha256(data).hexdigest()


</t>
<t tx="ekr.20220525082936.1240">def test_config_file(self) -&gt; None:
    runtime = "temp = 5\n"
    stub = "from decimal import Decimal\ntemp: Decimal\n"
    config_file = (
        f"[mypy]\nplugins={root_dir}/test-data/unit/plugins/decimal_to_int.py\n"
    )
    output = run_stubtest(stub=stub, runtime=runtime, options=[])
    assert remove_color_code(output) == (
        f"error: {TEST_MODULE_NAME}.temp variable differs from runtime type Literal[5]\n"
        "Stub: at line 2\n_decimal.Decimal\nRuntime:\n5\n\n"
        "Found 1 error (checked 1 module)\n"
    )
    output = run_stubtest(stub=stub, runtime=runtime, options=[], config_file=config_file)
    assert output == "Success: no issues found in 1 module\n"

def test_no_modules(self) -&gt; None:
    output = io.StringIO()
    with contextlib.redirect_stdout(output):
        test_stubs(parse_options([]))
    assert remove_color_code(output.getvalue()) == "error: no modules to check\n"

def test_module_and_typeshed(self) -&gt; None:
    output = io.StringIO()
    with contextlib.redirect_stdout(output):
        test_stubs(parse_options(["--check-typeshed", "some_module"]))
    assert remove_color_code(output.getvalue()) == (
        "error: cannot pass both --check-typeshed and a list of modules\n"
    )
</t>
<t tx="ekr.20220525082936.1241">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
from mypy.test.helpers import Suite, skip
from mypy.nodes import CONTRAVARIANT, INVARIANT, COVARIANT
from mypy.subtypes import is_subtype
from mypy.test.typefixture import TypeFixture, InterfaceTypeFixture
from mypy.types import Type


@others
</t>
<t tx="ekr.20220525082936.1242">class SubtypingSuite(Suite):
    @others
</t>
<t tx="ekr.20220525082936.1243">def setUp(self) -&gt; None:
    self.fx = TypeFixture(INVARIANT)
    self.fx_contra = TypeFixture(CONTRAVARIANT)
    self.fx_co = TypeFixture(COVARIANT)

</t>
<t tx="ekr.20220525082936.1244">def test_trivial_cases(self) -&gt; None:
    for simple in self.fx_co.a, self.fx_co.o, self.fx_co.b:
        self.assert_subtype(simple, simple)

</t>
<t tx="ekr.20220525082936.1245">def test_instance_subtyping(self) -&gt; None:
    self.assert_strict_subtype(self.fx.a, self.fx.o)
    self.assert_strict_subtype(self.fx.b, self.fx.o)
    self.assert_strict_subtype(self.fx.b, self.fx.a)

    self.assert_not_subtype(self.fx.a, self.fx.d)
    self.assert_not_subtype(self.fx.b, self.fx.c)

</t>
<t tx="ekr.20220525082936.1246">def test_simple_generic_instance_subtyping_invariant(self) -&gt; None:
    self.assert_subtype(self.fx.ga, self.fx.ga)
    self.assert_subtype(self.fx.hab, self.fx.hab)

    self.assert_not_subtype(self.fx.ga, self.fx.g2a)
    self.assert_not_subtype(self.fx.ga, self.fx.gb)
    self.assert_not_subtype(self.fx.gb, self.fx.ga)

</t>
<t tx="ekr.20220525082936.1247">def test_simple_generic_instance_subtyping_covariant(self) -&gt; None:
    self.assert_subtype(self.fx_co.ga, self.fx_co.ga)
    self.assert_subtype(self.fx_co.hab, self.fx_co.hab)

    self.assert_not_subtype(self.fx_co.ga, self.fx_co.g2a)
    self.assert_not_subtype(self.fx_co.ga, self.fx_co.gb)
    self.assert_subtype(self.fx_co.gb, self.fx_co.ga)

</t>
<t tx="ekr.20220525082936.1248">def test_simple_generic_instance_subtyping_contravariant(self) -&gt; None:
    self.assert_subtype(self.fx_contra.ga, self.fx_contra.ga)
    self.assert_subtype(self.fx_contra.hab, self.fx_contra.hab)

    self.assert_not_subtype(self.fx_contra.ga, self.fx_contra.g2a)
    self.assert_subtype(self.fx_contra.ga, self.fx_contra.gb)
    self.assert_not_subtype(self.fx_contra.gb, self.fx_contra.ga)

</t>
<t tx="ekr.20220525082936.1249">def test_generic_subtyping_with_inheritance_invariant(self) -&gt; None:
    self.assert_subtype(self.fx.gsab, self.fx.gb)
    self.assert_not_subtype(self.fx.gsab, self.fx.ga)
    self.assert_not_subtype(self.fx.gsaa, self.fx.gb)

</t>
<t tx="ekr.20220525082936.125">def parse_gray_color(cup: bytes) -&gt; str:
    """Reproduce a gray color in ANSI escape sequence"""
    if sys.platform == "win32":
        assert False, "curses is not available on Windows"
    set_color = ''.join([cup[:-1].decode(), 'm'])
    gray = curses.tparm(set_color.encode('utf-8'), 1, 89).decode()
    return gray


</t>
<t tx="ekr.20220525082936.1250">def test_generic_subtyping_with_inheritance_covariant(self) -&gt; None:
    self.assert_subtype(self.fx_co.gsab, self.fx_co.gb)
    self.assert_subtype(self.fx_co.gsab, self.fx_co.ga)
    self.assert_not_subtype(self.fx_co.gsaa, self.fx_co.gb)

</t>
<t tx="ekr.20220525082936.1251">def test_generic_subtyping_with_inheritance_contravariant(self) -&gt; None:
    self.assert_subtype(self.fx_contra.gsab, self.fx_contra.gb)
    self.assert_not_subtype(self.fx_contra.gsab, self.fx_contra.ga)
    self.assert_subtype(self.fx_contra.gsaa, self.fx_contra.gb)

</t>
<t tx="ekr.20220525082936.1252">def test_interface_subtyping(self) -&gt; None:
    self.assert_subtype(self.fx.e, self.fx.f)
    self.assert_equivalent(self.fx.f, self.fx.f)
    self.assert_not_subtype(self.fx.a, self.fx.f)

</t>
<t tx="ekr.20220525082936.1253">@skip
def test_generic_interface_subtyping(self) -&gt; None:
    # TODO make this work
    fx2 = InterfaceTypeFixture()

    self.assert_subtype(fx2.m1, fx2.gfa)
    self.assert_not_subtype(fx2.m1, fx2.gfb)

    self.assert_equivalent(fx2.gfa, fx2.gfa)

</t>
<t tx="ekr.20220525082936.1254">def test_basic_callable_subtyping(self) -&gt; None:
    self.assert_strict_subtype(self.fx.callable(self.fx.o, self.fx.d),
                               self.fx.callable(self.fx.a, self.fx.d))
    self.assert_strict_subtype(self.fx.callable(self.fx.d, self.fx.b),
                               self.fx.callable(self.fx.d, self.fx.a))

    self.assert_strict_subtype(self.fx.callable(self.fx.a, self.fx.nonet),
                               self.fx.callable(self.fx.a, self.fx.a))

    self.assert_unrelated(
        self.fx.callable(self.fx.a, self.fx.a, self.fx.a),
        self.fx.callable(self.fx.a, self.fx.a))

</t>
<t tx="ekr.20220525082936.1255">def test_default_arg_callable_subtyping(self) -&gt; None:
    self.assert_strict_subtype(
        self.fx.callable_default(1, self.fx.a, self.fx.d, self.fx.a),
        self.fx.callable(self.fx.a, self.fx.d, self.fx.a))

    self.assert_strict_subtype(
        self.fx.callable_default(1, self.fx.a, self.fx.d, self.fx.a),
        self.fx.callable(self.fx.a, self.fx.a))

    self.assert_strict_subtype(
        self.fx.callable_default(0, self.fx.a, self.fx.d, self.fx.a),
        self.fx.callable_default(1, self.fx.a, self.fx.d, self.fx.a))

    self.assert_unrelated(
        self.fx.callable_default(1, self.fx.a, self.fx.d, self.fx.a),
        self.fx.callable(self.fx.d, self.fx.d, self.fx.a))

    self.assert_unrelated(
        self.fx.callable_default(0, self.fx.a, self.fx.d, self.fx.a),
        self.fx.callable_default(1, self.fx.a, self.fx.a, self.fx.a))

    self.assert_unrelated(
        self.fx.callable_default(1, self.fx.a, self.fx.a),
        self.fx.callable(self.fx.a, self.fx.a, self.fx.a))

</t>
<t tx="ekr.20220525082936.1256">def test_var_arg_callable_subtyping_1(self) -&gt; None:
    self.assert_strict_subtype(
        self.fx.callable_var_arg(0, self.fx.a, self.fx.a),
        self.fx.callable_var_arg(0, self.fx.b, self.fx.a))

</t>
<t tx="ekr.20220525082936.1257">def test_var_arg_callable_subtyping_2(self) -&gt; None:
    self.assert_strict_subtype(
        self.fx.callable_var_arg(0, self.fx.a, self.fx.a),
        self.fx.callable(self.fx.b, self.fx.a))

</t>
<t tx="ekr.20220525082936.1258">def test_var_arg_callable_subtyping_3(self) -&gt; None:
    self.assert_strict_subtype(
        self.fx.callable_var_arg(0, self.fx.a, self.fx.a),
        self.fx.callable(self.fx.a))

</t>
<t tx="ekr.20220525082936.1259">def test_var_arg_callable_subtyping_4(self) -&gt; None:
    self.assert_strict_subtype(
        self.fx.callable_var_arg(1, self.fx.a, self.fx.d, self.fx.a),
        self.fx.callable(self.fx.b, self.fx.a))

</t>
<t tx="ekr.20220525082936.126">class FancyFormatter:
    """Apply color and bold font to terminal output.

    This currently only works on Linux and Mac.
    """
    @others
</t>
<t tx="ekr.20220525082936.1260">def test_var_arg_callable_subtyping_5(self) -&gt; None:
    self.assert_strict_subtype(
        self.fx.callable_var_arg(0, self.fx.a, self.fx.d, self.fx.a),
        self.fx.callable(self.fx.b, self.fx.a))

</t>
<t tx="ekr.20220525082936.1261">def test_var_arg_callable_subtyping_6(self) -&gt; None:
    self.assert_strict_subtype(
        self.fx.callable_var_arg(0, self.fx.a, self.fx.f, self.fx.d),
        self.fx.callable_var_arg(0, self.fx.b, self.fx.e, self.fx.d))

</t>
<t tx="ekr.20220525082936.1262">def test_var_arg_callable_subtyping_7(self) -&gt; None:
    self.assert_not_subtype(
        self.fx.callable_var_arg(0, self.fx.b, self.fx.d),
        self.fx.callable(self.fx.a, self.fx.d))

</t>
<t tx="ekr.20220525082936.1263">def test_var_arg_callable_subtyping_8(self) -&gt; None:
    self.assert_not_subtype(
        self.fx.callable_var_arg(0, self.fx.b, self.fx.d),
        self.fx.callable_var_arg(0, self.fx.a, self.fx.a, self.fx.d))
    self.assert_subtype(
        self.fx.callable_var_arg(0, self.fx.a, self.fx.d),
        self.fx.callable_var_arg(0, self.fx.b, self.fx.b, self.fx.d))

</t>
<t tx="ekr.20220525082936.1264">def test_var_arg_callable_subtyping_9(self) -&gt; None:
    self.assert_not_subtype(
        self.fx.callable_var_arg(0, self.fx.b, self.fx.b, self.fx.d),
        self.fx.callable_var_arg(0, self.fx.a, self.fx.d))
    self.assert_subtype(
        self.fx.callable_var_arg(0, self.fx.a, self.fx.a, self.fx.d),
        self.fx.callable_var_arg(0, self.fx.b, self.fx.d))

</t>
<t tx="ekr.20220525082936.1265">def test_type_callable_subtyping(self) -&gt; None:
    self.assert_subtype(
        self.fx.callable_type(self.fx.d, self.fx.a), self.fx.type_type)

    self.assert_strict_subtype(
        self.fx.callable_type(self.fx.d, self.fx.b),
        self.fx.callable(self.fx.d, self.fx.a))

    self.assert_strict_subtype(self.fx.callable_type(self.fx.a, self.fx.b),
                               self.fx.callable(self.fx.a, self.fx.b))

</t>
<t tx="ekr.20220525082936.1266"># IDEA: Maybe add these test cases (they are tested pretty well in type
#       checker tests already):
#  * more interface subtyping test cases
#  * more generic interface subtyping test cases
#  * type variables
#  * tuple types
#  * None type
#  * any type
#  * generic function types

</t>
<t tx="ekr.20220525082936.1267">def assert_subtype(self, s: Type, t: Type) -&gt; None:
    assert is_subtype(s, t), f'{s} not subtype of {t}'

</t>
<t tx="ekr.20220525082936.1268">def assert_not_subtype(self, s: Type, t: Type) -&gt; None:
    assert not is_subtype(s, t), f'{s} subtype of {t}'

</t>
<t tx="ekr.20220525082936.1269">def assert_strict_subtype(self, s: Type, t: Type) -&gt; None:
    self.assert_subtype(s, t)
    self.assert_not_subtype(t, s)

</t>
<t tx="ekr.20220525082936.127">def __init__(self, f_out: IO[str], f_err: IO[str], show_error_codes: bool) -&gt; None:
    self.show_error_codes = show_error_codes
    # Check if we are in a human-facing terminal on a supported platform.
    if sys.platform not in ('linux', 'darwin', 'win32'):
        self.dummy_term = True
        return
    force_color = int(os.getenv('MYPY_FORCE_COLOR', '0'))
    if not force_color and (not f_out.isatty() or not f_err.isatty()):
        self.dummy_term = True
        return
    if sys.platform == 'win32':
        self.dummy_term = not self.initialize_win_colors()
    else:
        self.dummy_term = not self.initialize_unix_colors()
    if not self.dummy_term:
        self.colors = {'red': self.RED, 'green': self.GREEN,
                       'blue': self.BLUE, 'yellow': self.YELLOW,
                       'none': ''}

</t>
<t tx="ekr.20220525082936.1270">def assert_equivalent(self, s: Type, t: Type) -&gt; None:
    self.assert_subtype(s, t)
    self.assert_subtype(t, s)

</t>
<t tx="ekr.20220525082936.1271">def assert_unrelated(self, s: Type, t: Type) -&gt; None:
    self.assert_not_subtype(s, t)
    self.assert_not_subtype(t, s)
</t>
<t tx="ekr.20220525082936.1272">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Identity AST transform test cases"""

import os.path

from mypy import build
from mypy.modulefinder import BuildSource
from mypy.test.helpers import (
    assert_string_arrays_equal, normalize_error_messages, parse_options
)
from mypy.test.data import DataDrivenTestCase, DataSuite
from mypy.test.config import test_temp_dir
from mypy.test.visitors import TypeAssertTransformVisitor
from mypy.errors import CompileError


@others
</t>
<t tx="ekr.20220525082936.1273">class TransformSuite(DataSuite):
    required_out_section = True
    # Reuse semantic analysis test cases.
    files = ['semanal-basic.test',
             'semanal-expressions.test',
             'semanal-classes.test',
             'semanal-types.test',
             'semanal-modules.test',
             'semanal-statements.test',
             'semanal-abstractclasses.test',
             'semanal-python2.test']
    native_sep = True

    @others
</t>
<t tx="ekr.20220525082936.1274">def run_case(self, testcase: DataDrivenTestCase) -&gt; None:
    test_transform(testcase)


</t>
<t tx="ekr.20220525082936.1275">def test_transform(testcase: DataDrivenTestCase) -&gt; None:
    """Perform an identity transform test case."""

    try:
        src = '\n'.join(testcase.input)
        options = parse_options(src, testcase, 1)
        options.use_builtins_fixtures = True
        options.semantic_analysis_only = True
        options.enable_incomplete_features = True
        options.show_traceback = True
        result = build.build(sources=[BuildSource('main', None, src)],
                             options=options,
                             alt_lib_path=test_temp_dir)
        a = result.errors
        if a:
            raise CompileError(a)
        # Include string representations of the source files in the actual
        # output.
        for fnam in sorted(result.files.keys()):
            f = result.files[fnam]

            # Omit the builtins module and files with a special marker in the
            # path.
            # TODO the test is not reliable
            if (not f.path.endswith((os.sep + 'builtins.pyi',
                                     'typing_extensions.pyi',
                                     'typing.pyi',
                                     'abc.pyi',
                                     'sys.pyi'))
                    and not os.path.basename(f.path).startswith('_')
                    and not os.path.splitext(
                        os.path.basename(f.path))[0].endswith('_')):
                t = TypeAssertTransformVisitor()
                t.test_only = True
                f = t.mypyfile(f)
                a += str(f).split('\n')
    except CompileError as e:
        a = e.messages
    if testcase.normalize_output:
        a = normalize_error_messages(a)
    assert_string_arrays_equal(
        testcase.output, a,
        f'Invalid semantic analyzer output ({testcase.file}, line {testcase.line})')
</t>
<t tx="ekr.20220525082936.1276">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Test cases for the type checker: exporting inferred types"""

import re

from mypy import build
from mypy.modulefinder import BuildSource
from mypy.test.config import test_temp_dir
from mypy.test.data import DataDrivenTestCase, DataSuite
from mypy.test.helpers import assert_string_arrays_equal
from mypy.test.visitors import SkippedNodeSearcher, ignore_node
from mypy.util import short_type
from mypy.nodes import NameExpr
from mypy.errors import CompileError
from mypy.options import Options


@others
</t>
<t tx="ekr.20220525082936.1277">class TypeExportSuite(DataSuite):
    required_out_section = True
    files = ['typexport-basic.test']

    @others
</t>
<t tx="ekr.20220525082936.1278">def run_case(self, testcase: DataDrivenTestCase) -&gt; None:
    try:
        line = testcase.input[0]
        mask = ''
        if line.startswith('##'):
            mask = '(' + line[2:].strip() + ')$'

        src = '\n'.join(testcase.input)
        options = Options()
        options.strict_optional = False  # TODO: Enable strict optional checking
        options.use_builtins_fixtures = True
        options.show_traceback = True
        options.export_types = True
        options.preserve_asts = True
        result = build.build(sources=[BuildSource('main', None, src)],
                             options=options,
                             alt_lib_path=test_temp_dir)
        a = result.errors
        map = result.types
        nodes = map.keys()

        # Ignore NameExpr nodes of variables with explicit (trivial) types
        # to simplify output.
        searcher = SkippedNodeSearcher()
        for file in result.files.values():
            file.accept(searcher)
        ignored = searcher.nodes

        # Filter nodes that should be included in the output.
        keys = []
        for node in nodes:
            if node.line is not None and node.line != -1 and map[node]:
                if ignore_node(node) or node in ignored:
                    continue
                if (re.match(mask, short_type(node))
                        or (isinstance(node, NameExpr)
                            and re.match(mask, node.name))):
                    # Include node in output.
                    keys.append(node)

        for key in sorted(keys,
                          key=lambda n: (n.line, short_type(n),
                                         str(n) + str(map[n]))):
            ts = str(map[key]).replace('*', '')  # Remove erased tags
            ts = ts.replace('__main__.', '')
            a.append(f'{short_type(key)}({key.line}) : {ts}')
    except CompileError as e:
        a = e.messages
    assert_string_arrays_equal(
        testcase.output, a,
        f'Invalid type checker output ({testcase.file}, line {testcase.line})')
</t>
<t tx="ekr.20220525082936.1279">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Test cases for mypy types and type operations."""

from typing import List, Tuple

from mypy.test.helpers import Suite, assert_equal, assert_type, skip
from mypy.erasetype import erase_type, remove_instance_last_known_values
from mypy.expandtype import expand_type
from mypy.join import join_types, join_simple
from mypy.meet import meet_types, narrow_declared_type
from mypy.sametypes import is_same_type
from mypy.indirection import TypeIndirectionVisitor
from mypy.types import (
    UnboundType, AnyType, CallableType, TupleType, TypeVarType, Type, Instance, NoneType,
    Overloaded, TypeType, UnionType, UninhabitedType, TypeVarId, TypeOfAny, ProperType,
    LiteralType, get_proper_type
)
from mypy.nodes import ARG_POS, ARG_OPT, ARG_STAR, ARG_STAR2, CONTRAVARIANT, INVARIANT, COVARIANT
from mypy.subtypes import is_subtype, is_more_precise, is_proper_subtype
from mypy.test.typefixture import TypeFixture, InterfaceTypeFixture
from mypy.state import state
from mypy.typeops import true_only, false_only, make_simplified_union


@others
</t>
<t tx="ekr.20220525082936.128">def initialize_win_colors(self) -&gt; bool:
    """Return True if initialization was successful and we can use colors, False otherwise"""
    # Windows ANSI escape sequences are only supported on Threshold 2 and above.
    # we check with an assert at runtime and an if check for mypy, as asserts do not
    # yet narrow platform
    assert sys.platform == 'win32'
    if sys.platform == 'win32':
        winver = sys.getwindowsversion()
        if (winver.major &lt; MINIMUM_WINDOWS_MAJOR_VT100
        or winver.build &lt; MINIMUM_WINDOWS_BUILD_VT100):
            return False
        import ctypes
        kernel32 = ctypes.windll.kernel32
        ENABLE_PROCESSED_OUTPUT = 0x1
        ENABLE_WRAP_AT_EOL_OUTPUT = 0x2
        ENABLE_VIRTUAL_TERMINAL_PROCESSING = 0x4
        STD_OUTPUT_HANDLE = -11
        kernel32.SetConsoleMode(kernel32.GetStdHandle(STD_OUTPUT_HANDLE),
                                ENABLE_PROCESSED_OUTPUT
                                | ENABLE_WRAP_AT_EOL_OUTPUT
                                | ENABLE_VIRTUAL_TERMINAL_PROCESSING)
        self.BOLD = '\033[1m'
        self.UNDER = '\033[4m'
        self.BLUE = '\033[94m'
        self.GREEN = '\033[92m'
        self.RED = '\033[91m'
        self.YELLOW = '\033[93m'
        self.NORMAL = '\033[0m'
        self.DIM = '\033[2m'
        return True
    return False

</t>
<t tx="ekr.20220525082936.1280">class TypesSuite(Suite):
    @others
</t>
<t tx="ekr.20220525082936.1281">def setUp(self) -&gt; None:
    self.x = UnboundType('X')  # Helpers
    self.y = UnboundType('Y')
    self.fx = TypeFixture()
    self.function = self.fx.function

</t>
<t tx="ekr.20220525082936.1282">def test_any(self) -&gt; None:
    assert_equal(str(AnyType(TypeOfAny.special_form)), 'Any')

</t>
<t tx="ekr.20220525082936.1283">def test_simple_unbound_type(self) -&gt; None:
    u = UnboundType('Foo')
    assert_equal(str(u), 'Foo?')

</t>
<t tx="ekr.20220525082936.1284">def test_generic_unbound_type(self) -&gt; None:
    u = UnboundType('Foo', [UnboundType('T'), AnyType(TypeOfAny.special_form)])
    assert_equal(str(u), 'Foo?[T?, Any]')

</t>
<t tx="ekr.20220525082936.1285">def test_callable_type(self) -&gt; None:
    c = CallableType([self.x, self.y],
                     [ARG_POS, ARG_POS],
                     [None, None],
                     AnyType(TypeOfAny.special_form), self.function)
    assert_equal(str(c), 'def (X?, Y?) -&gt; Any')

    c2 = CallableType([], [], [], NoneType(), self.fx.function)
    assert_equal(str(c2), 'def ()')

</t>
<t tx="ekr.20220525082936.1286">def test_callable_type_with_default_args(self) -&gt; None:
    c = CallableType([self.x, self.y], [ARG_POS, ARG_OPT], [None, None],
                 AnyType(TypeOfAny.special_form), self.function)
    assert_equal(str(c), 'def (X?, Y? =) -&gt; Any')

    c2 = CallableType([self.x, self.y], [ARG_OPT, ARG_OPT], [None, None],
                  AnyType(TypeOfAny.special_form), self.function)
    assert_equal(str(c2), 'def (X? =, Y? =) -&gt; Any')

</t>
<t tx="ekr.20220525082936.1287">def test_callable_type_with_var_args(self) -&gt; None:
    c = CallableType([self.x], [ARG_STAR], [None], AnyType(TypeOfAny.special_form),
                     self.function)
    assert_equal(str(c), 'def (*X?) -&gt; Any')

    c2 = CallableType([self.x, self.y], [ARG_POS, ARG_STAR],
                  [None, None], AnyType(TypeOfAny.special_form), self.function)
    assert_equal(str(c2), 'def (X?, *Y?) -&gt; Any')

    c3 = CallableType([self.x, self.y], [ARG_OPT, ARG_STAR], [None, None],
                  AnyType(TypeOfAny.special_form), self.function)
    assert_equal(str(c3), 'def (X? =, *Y?) -&gt; Any')

</t>
<t tx="ekr.20220525082936.1288">def test_tuple_type(self) -&gt; None:
    assert_equal(str(TupleType([], self.fx.std_tuple)), 'Tuple[]')
    assert_equal(str(TupleType([self.x], self.fx.std_tuple)), 'Tuple[X?]')
    assert_equal(str(TupleType([self.x, AnyType(TypeOfAny.special_form)],
                               self.fx.std_tuple)), 'Tuple[X?, Any]')

</t>
<t tx="ekr.20220525082936.1289">def test_type_variable_binding(self) -&gt; None:
    assert_equal(str(TypeVarType('X', 'X', 1, [], self.fx.o)), 'X`1')
    assert_equal(str(TypeVarType('X', 'X', 1, [self.x, self.y], self.fx.o)),
                 'X`1')

</t>
<t tx="ekr.20220525082936.129">def initialize_unix_colors(self) -&gt; bool:
    """Return True if initialization was successful and we can use colors, False otherwise"""
    if sys.platform == "win32" or not CURSES_ENABLED:
        return False
    try:
        # setupterm wants a fd to potentially write an "initialization sequence".
        # We override sys.stdout for the daemon API so if stdout doesn't have an fd,
        # just give it /dev/null.
        try:
            fd = sys.stdout.fileno()
        except io.UnsupportedOperation:
            with open("/dev/null", "rb") as f:
                curses.setupterm(fd=f.fileno())
        else:
            curses.setupterm(fd=fd)
    except curses.error:
        # Most likely terminfo not found.
        return False
    bold = curses.tigetstr('bold')
    under = curses.tigetstr('smul')
    set_color = curses.tigetstr('setaf')
    set_eseq = curses.tigetstr('cup')
    normal = curses.tigetstr('sgr0')

    if not (bold and under and set_color and set_eseq and normal):
        return False

    self.NORMAL = normal.decode()
    self.BOLD = bold.decode()
    self.UNDER = under.decode()
    self.DIM = parse_gray_color(set_eseq)
    self.BLUE = curses.tparm(set_color, curses.COLOR_BLUE).decode()
    self.GREEN = curses.tparm(set_color, curses.COLOR_GREEN).decode()
    self.RED = curses.tparm(set_color, curses.COLOR_RED).decode()
    self.YELLOW = curses.tparm(set_color, curses.COLOR_YELLOW).decode()
    return True

</t>
<t tx="ekr.20220525082936.1290">def test_generic_function_type(self) -&gt; None:
    c = CallableType([self.x, self.y], [ARG_POS, ARG_POS], [None, None],
                 self.y, self.function, name=None,
                 variables=[TypeVarType('X', 'X', -1, [], self.fx.o)])
    assert_equal(str(c), 'def [X] (X?, Y?) -&gt; Y?')

    v = [TypeVarType('Y', 'Y', -1, [], self.fx.o),
         TypeVarType('X', 'X', -2, [], self.fx.o)]
    c2 = CallableType([], [], [], NoneType(), self.function, name=None, variables=v)
    assert_equal(str(c2), 'def [Y, X] ()')

</t>
<t tx="ekr.20220525082936.1291">def test_type_alias_expand_once(self) -&gt; None:
    A, target = self.fx.def_alias_1(self.fx.a)
    assert get_proper_type(A) == target
    assert get_proper_type(target) == target

    A, target = self.fx.def_alias_2(self.fx.a)
    assert get_proper_type(A) == target
    assert get_proper_type(target) == target

</t>
<t tx="ekr.20220525082936.1292">def test_type_alias_expand_all(self) -&gt; None:
    A, _ = self.fx.def_alias_1(self.fx.a)
    assert A.expand_all_if_possible() is None
    A, _ = self.fx.def_alias_2(self.fx.a)
    assert A.expand_all_if_possible() is None

    B = self.fx.non_rec_alias(self.fx.a)
    C = self.fx.non_rec_alias(TupleType([B, B], Instance(self.fx.std_tuplei,
                                                         [B])))
    assert C.expand_all_if_possible() == TupleType([self.fx.a, self.fx.a],
                                                   Instance(self.fx.std_tuplei,
                                                            [self.fx.a]))

</t>
<t tx="ekr.20220525082936.1293">def test_indirection_no_infinite_recursion(self) -&gt; None:
    A, _ = self.fx.def_alias_1(self.fx.a)
    visitor = TypeIndirectionVisitor()
    modules = A.accept(visitor)
    assert modules == {'__main__', 'builtins'}

    A, _ = self.fx.def_alias_2(self.fx.a)
    visitor = TypeIndirectionVisitor()
    modules = A.accept(visitor)
    assert modules == {'__main__', 'builtins'}


</t>
<t tx="ekr.20220525082936.1294">class TypeOpsSuite(Suite):
    @others
</t>
<t tx="ekr.20220525082936.1295">def setUp(self) -&gt; None:
    self.fx = TypeFixture(INVARIANT)
    self.fx_co = TypeFixture(COVARIANT)
    self.fx_contra = TypeFixture(CONTRAVARIANT)

</t>
<t tx="ekr.20220525082936.1296"># expand_type

</t>
<t tx="ekr.20220525082936.1297">def test_trivial_expand(self) -&gt; None:
    for t in (self.fx.a, self.fx.o, self.fx.t, self.fx.nonet,
              self.tuple(self.fx.a),
              self.callable([], self.fx.a, self.fx.a), self.fx.anyt):
        self.assert_expand(t, [], t)
        self.assert_expand(t, [], t)
        self.assert_expand(t, [], t)

</t>
<t tx="ekr.20220525082936.1298">def test_trivial_expand_recursive(self) -&gt; None:
    A, _ = self.fx.def_alias_1(self.fx.a)
    self.assert_expand(A, [], A)
    A, _ = self.fx.def_alias_2(self.fx.a)
    self.assert_expand(A, [], A)

</t>
<t tx="ekr.20220525082936.1299">def test_expand_naked_type_var(self) -&gt; None:
    self.assert_expand(self.fx.t, [(self.fx.t.id, self.fx.a)], self.fx.a)
    self.assert_expand(self.fx.t, [(self.fx.s.id, self.fx.a)], self.fx.t)

</t>
<t tx="ekr.20220525082936.13">@abstractmethod
def visit_param_spec(self, t: ParamSpecType) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.130">def style(self, text: str, color: Literal['red', 'green', 'blue', 'yellow', 'none'],
          bold: bool = False, underline: bool = False, dim: bool = False) -&gt; str:
    """Apply simple color and style (underlined or bold)."""
    if self.dummy_term:
        return text
    if bold:
        start = self.BOLD
    else:
        start = ''
    if underline:
        start += self.UNDER
    if dim:
        start += self.DIM
    return start + self.colors[color] + text + self.NORMAL

</t>
<t tx="ekr.20220525082936.1300">def test_expand_basic_generic_types(self) -&gt; None:
    self.assert_expand(self.fx.gt, [(self.fx.t.id, self.fx.a)], self.fx.ga)

</t>
<t tx="ekr.20220525082936.1301"># IDEA: Add test cases for
#   tuple types
#   callable types
#   multiple arguments

</t>
<t tx="ekr.20220525082936.1302">def assert_expand(self,
                  orig: Type,
                  map_items: List[Tuple[TypeVarId, Type]],
                  result: Type,
                  ) -&gt; None:
    lower_bounds = {}

    for id, t in map_items:
        lower_bounds[id] = t

    exp = expand_type(orig, lower_bounds)
    # Remove erased tags (asterisks).
    assert_equal(str(exp).replace('*', ''), str(result))

</t>
<t tx="ekr.20220525082936.1303"># erase_type

</t>
<t tx="ekr.20220525082936.1304">def test_trivial_erase(self) -&gt; None:
    for t in (self.fx.a, self.fx.o, self.fx.nonet, self.fx.anyt):
        self.assert_erase(t, t)

</t>
<t tx="ekr.20220525082936.1305">def test_erase_with_type_variable(self) -&gt; None:
    self.assert_erase(self.fx.t, self.fx.anyt)

</t>
<t tx="ekr.20220525082936.1306">def test_erase_with_generic_type(self) -&gt; None:
    self.assert_erase(self.fx.ga, self.fx.gdyn)
    self.assert_erase(self.fx.hab,
                      Instance(self.fx.hi, [self.fx.anyt, self.fx.anyt]))

</t>
<t tx="ekr.20220525082936.1307">def test_erase_with_generic_type_recursive(self) -&gt; None:
    tuple_any = Instance(self.fx.std_tuplei, [AnyType(TypeOfAny.explicit)])
    A, _ = self.fx.def_alias_1(self.fx.a)
    self.assert_erase(A, tuple_any)
    A, _ = self.fx.def_alias_2(self.fx.a)
    self.assert_erase(A, UnionType([self.fx.a, tuple_any]))

</t>
<t tx="ekr.20220525082936.1308">def test_erase_with_tuple_type(self) -&gt; None:
    self.assert_erase(self.tuple(self.fx.a), self.fx.std_tuple)

</t>
<t tx="ekr.20220525082936.1309">def test_erase_with_function_type(self) -&gt; None:
    self.assert_erase(self.fx.callable(self.fx.a, self.fx.b),
                      CallableType(arg_types=[self.fx.anyt, self.fx.anyt],
                                   arg_kinds=[ARG_STAR, ARG_STAR2],
                                   arg_names=[None, None],
                                   ret_type=self.fx.anyt,
                                   fallback=self.fx.function))

</t>
<t tx="ekr.20220525082936.131">def fit_in_terminal(self, messages: List[str],
                    fixed_terminal_width: Optional[int] = None) -&gt; List[str]:
    """Improve readability by wrapping error messages and trimming source code."""
    width = fixed_terminal_width or get_terminal_width()
    new_messages = messages.copy()
    for i, error in enumerate(messages):
        if ': error:' in error:
            loc, msg = error.split('error:', maxsplit=1)
            msg = soft_wrap(msg, width, first_offset=len(loc) + len('error: '))
            new_messages[i] = loc + 'error:' + msg
        if error.startswith(' ' * DEFAULT_SOURCE_OFFSET) and '^' not in error:
            # TODO: detecting source code highlights through an indent can be surprising.
            # Restore original error message and error location.
            error = error[DEFAULT_SOURCE_OFFSET:]
            column = messages[i+1].index('^') - DEFAULT_SOURCE_OFFSET

            # Let source have some space also on the right side, plus 6
            # to accommodate ... on each side.
            max_len = width - DEFAULT_SOURCE_OFFSET - 6
            source_line, offset = trim_source_line(error, max_len, column, MINIMUM_WIDTH)

            new_messages[i] = ' ' * DEFAULT_SOURCE_OFFSET + source_line
            # Also adjust the error marker position.
            new_messages[i+1] = ' ' * (DEFAULT_SOURCE_OFFSET + column - offset) + '^'
    return new_messages

</t>
<t tx="ekr.20220525082936.1310">def test_erase_with_type_object(self) -&gt; None:
    self.assert_erase(self.fx.callable_type(self.fx.a, self.fx.b),
                      CallableType(arg_types=[self.fx.anyt, self.fx.anyt],
                                   arg_kinds=[ARG_STAR, ARG_STAR2],
                                   arg_names=[None, None],
                                   ret_type=self.fx.anyt,
                                   fallback=self.fx.type_type))

</t>
<t tx="ekr.20220525082936.1311">def test_erase_with_type_type(self) -&gt; None:
    self.assert_erase(self.fx.type_a, self.fx.type_a)
    self.assert_erase(self.fx.type_t, self.fx.type_any)

</t>
<t tx="ekr.20220525082936.1312">def assert_erase(self, orig: Type, result: Type) -&gt; None:
    assert_equal(str(erase_type(orig)), str(result))

</t>
<t tx="ekr.20220525082936.1313"># is_more_precise

</t>
<t tx="ekr.20220525082936.1314">def test_is_more_precise(self) -&gt; None:
    fx = self.fx
    assert is_more_precise(fx.b, fx.a)
    assert is_more_precise(fx.b, fx.b)
    assert is_more_precise(fx.b, fx.b)
    assert is_more_precise(fx.b, fx.anyt)
    assert is_more_precise(self.tuple(fx.b, fx.a),
                           self.tuple(fx.b, fx.a))
    assert is_more_precise(self.tuple(fx.b, fx.b),
                           self.tuple(fx.b, fx.a))

    assert not is_more_precise(fx.a, fx.b)
    assert not is_more_precise(fx.anyt, fx.b)

</t>
<t tx="ekr.20220525082936.1315"># is_proper_subtype

</t>
<t tx="ekr.20220525082936.1316">def test_is_proper_subtype(self) -&gt; None:
    fx = self.fx

    assert is_proper_subtype(fx.a, fx.a)
    assert is_proper_subtype(fx.b, fx.a)
    assert is_proper_subtype(fx.b, fx.o)
    assert is_proper_subtype(fx.b, fx.o)

    assert not is_proper_subtype(fx.a, fx.b)
    assert not is_proper_subtype(fx.o, fx.b)

    assert is_proper_subtype(fx.anyt, fx.anyt)
    assert not is_proper_subtype(fx.a, fx.anyt)
    assert not is_proper_subtype(fx.anyt, fx.a)

    assert is_proper_subtype(fx.ga, fx.ga)
    assert is_proper_subtype(fx.gdyn, fx.gdyn)
    assert not is_proper_subtype(fx.ga, fx.gdyn)
    assert not is_proper_subtype(fx.gdyn, fx.ga)

    assert is_proper_subtype(fx.t, fx.t)
    assert not is_proper_subtype(fx.t, fx.s)

    assert is_proper_subtype(fx.a, UnionType([fx.a, fx.b]))
    assert is_proper_subtype(UnionType([fx.a, fx.b]),
                             UnionType([fx.a, fx.b, fx.c]))
    assert not is_proper_subtype(UnionType([fx.a, fx.b]),
                                 UnionType([fx.b, fx.c]))

</t>
<t tx="ekr.20220525082936.1317">def test_is_proper_subtype_covariance(self) -&gt; None:
    fx_co = self.fx_co

    assert is_proper_subtype(fx_co.gsab, fx_co.gb)
    assert is_proper_subtype(fx_co.gsab, fx_co.ga)
    assert not is_proper_subtype(fx_co.gsaa, fx_co.gb)
    assert is_proper_subtype(fx_co.gb, fx_co.ga)
    assert not is_proper_subtype(fx_co.ga, fx_co.gb)

</t>
<t tx="ekr.20220525082936.1318">def test_is_proper_subtype_contravariance(self) -&gt; None:
    fx_contra = self.fx_contra

    assert is_proper_subtype(fx_contra.gsab, fx_contra.gb)
    assert not is_proper_subtype(fx_contra.gsab, fx_contra.ga)
    assert is_proper_subtype(fx_contra.gsaa, fx_contra.gb)
    assert not is_proper_subtype(fx_contra.gb, fx_contra.ga)
    assert is_proper_subtype(fx_contra.ga, fx_contra.gb)

</t>
<t tx="ekr.20220525082936.1319">def test_is_proper_subtype_invariance(self) -&gt; None:
    fx = self.fx

    assert is_proper_subtype(fx.gsab, fx.gb)
    assert not is_proper_subtype(fx.gsab, fx.ga)
    assert not is_proper_subtype(fx.gsaa, fx.gb)
    assert not is_proper_subtype(fx.gb, fx.ga)
    assert not is_proper_subtype(fx.ga, fx.gb)

</t>
<t tx="ekr.20220525082936.132">def colorize(self, error: str) -&gt; str:
    """Colorize an output line by highlighting the status and error code."""
    if ': error:' in error:
        loc, msg = error.split('error:', maxsplit=1)
        if not self.show_error_codes:
            return (loc + self.style('error:', 'red', bold=True) +
                    self.highlight_quote_groups(msg))
        codepos = msg.rfind('[')
        if codepos != -1:
            code = msg[codepos:]
            msg = msg[:codepos]
        else:
            code = ""  # no error code specified
        return (loc + self.style('error:', 'red', bold=True) +
                self.highlight_quote_groups(msg) + self.style(code, 'yellow'))
    elif ': note:' in error:
        loc, msg = error.split('note:', maxsplit=1)
        formatted = self.highlight_quote_groups(self.underline_link(msg))
        return loc + self.style('note:', 'blue') + formatted
    elif error.startswith(' ' * DEFAULT_SOURCE_OFFSET):
        # TODO: detecting source code highlights through an indent can be surprising.
        if '^' not in error:
            return self.style(error, 'none', dim=True)
        return self.style(error, 'red')
    else:
        return error

</t>
<t tx="ekr.20220525082936.1320">def test_is_proper_subtype_and_subtype_literal_types(self) -&gt; None:
    fx = self.fx

    lit1 = fx.lit1
    lit2 = fx.lit2
    lit3 = fx.lit3

    assert is_proper_subtype(lit1, fx.a)
    assert not is_proper_subtype(lit1, fx.d)
    assert not is_proper_subtype(fx.a, lit1)
    assert is_proper_subtype(fx.uninhabited, lit1)
    assert not is_proper_subtype(lit1, fx.uninhabited)
    assert is_proper_subtype(lit1, lit1)
    assert not is_proper_subtype(lit1, lit2)
    assert not is_proper_subtype(lit2, lit3)

    assert is_subtype(lit1, fx.a)
    assert not is_subtype(lit1, fx.d)
    assert not is_subtype(fx.a, lit1)
    assert is_subtype(fx.uninhabited, lit1)
    assert not is_subtype(lit1, fx.uninhabited)
    assert is_subtype(lit1, lit1)
    assert not is_subtype(lit1, lit2)
    assert not is_subtype(lit2, lit3)

    assert not is_proper_subtype(lit1, fx.anyt)
    assert not is_proper_subtype(fx.anyt, lit1)

    assert is_subtype(lit1, fx.anyt)
    assert is_subtype(fx.anyt, lit1)

</t>
<t tx="ekr.20220525082936.1321">def test_subtype_aliases(self) -&gt; None:
    A1, _ = self.fx.def_alias_1(self.fx.a)
    AA1, _ = self.fx.def_alias_1(self.fx.a)
    assert is_subtype(A1, AA1)
    assert is_subtype(AA1, A1)

    A2, _ = self.fx.def_alias_2(self.fx.a)
    AA2, _ = self.fx.def_alias_2(self.fx.a)
    assert is_subtype(A2, AA2)
    assert is_subtype(AA2, A2)

    B1, _ = self.fx.def_alias_1(self.fx.b)
    B2, _ = self.fx.def_alias_2(self.fx.b)
    assert is_subtype(B1, A1)
    assert is_subtype(B2, A2)
    assert not is_subtype(A1, B1)
    assert not is_subtype(A2, B2)

    assert not is_subtype(A2, A1)
    assert is_subtype(A1, A2)

</t>
<t tx="ekr.20220525082936.1322"># can_be_true / can_be_false

</t>
<t tx="ekr.20220525082936.1323">def test_empty_tuple_always_false(self) -&gt; None:
    tuple_type = self.tuple()
    assert tuple_type.can_be_false
    assert not tuple_type.can_be_true

</t>
<t tx="ekr.20220525082936.1324">def test_nonempty_tuple_always_true(self) -&gt; None:
    tuple_type = self.tuple(AnyType(TypeOfAny.special_form),
                            AnyType(TypeOfAny.special_form))
    assert tuple_type.can_be_true
    assert not tuple_type.can_be_false

</t>
<t tx="ekr.20220525082936.1325">def test_union_can_be_true_if_any_true(self) -&gt; None:
    union_type = UnionType([self.fx.a, self.tuple()])
    assert union_type.can_be_true

</t>
<t tx="ekr.20220525082936.1326">def test_union_can_not_be_true_if_none_true(self) -&gt; None:
    union_type = UnionType([self.tuple(), self.tuple()])
    assert not union_type.can_be_true

</t>
<t tx="ekr.20220525082936.1327">def test_union_can_be_false_if_any_false(self) -&gt; None:
    union_type = UnionType([self.fx.a, self.tuple()])
    assert union_type.can_be_false

</t>
<t tx="ekr.20220525082936.1328">def test_union_can_not_be_false_if_none_false(self) -&gt; None:
    union_type = UnionType([self.tuple(self.fx.a), self.tuple(self.fx.d)])
    assert not union_type.can_be_false

</t>
<t tx="ekr.20220525082936.1329"># true_only / false_only

</t>
<t tx="ekr.20220525082936.133">def highlight_quote_groups(self, msg: str) -&gt; str:
    """Make groups quoted with double quotes bold (including quotes).

    This is used to highlight types, attribute names etc.
    """
    if msg.count('"') % 2:
        # Broken error message, don't do any formatting.
        return msg
    parts = msg.split('"')
    out = ''
    for i, part in enumerate(parts):
        if i % 2 == 0:
            out += self.style(part, 'none')
        else:
            out += self.style('"' + part + '"', 'none', bold=True)
    return out

</t>
<t tx="ekr.20220525082936.1330">def test_true_only_of_false_type_is_uninhabited(self) -&gt; None:
    to = true_only(NoneType())
    assert_type(UninhabitedType, to)

</t>
<t tx="ekr.20220525082936.1331">def test_true_only_of_true_type_is_idempotent(self) -&gt; None:
    always_true = self.tuple(AnyType(TypeOfAny.special_form))
    to = true_only(always_true)
    assert always_true is to

</t>
<t tx="ekr.20220525082936.1332">def test_true_only_of_instance(self) -&gt; None:
    to = true_only(self.fx.a)
    assert_equal(str(to), "A")
    assert to.can_be_true
    assert not to.can_be_false
    assert_type(Instance, to)
    # The original class still can be false
    assert self.fx.a.can_be_false

</t>
<t tx="ekr.20220525082936.1333">def test_true_only_of_union(self) -&gt; None:
    tup_type = self.tuple(AnyType(TypeOfAny.special_form))
    # Union of something that is unknown, something that is always true, something
    # that is always false
    union_type = UnionType([self.fx.a, tup_type, self.tuple()])
    to = true_only(union_type)
    assert isinstance(to, UnionType)
    assert_equal(len(to.items), 2)
    assert to.items[0].can_be_true
    assert not to.items[0].can_be_false
    assert to.items[1] is tup_type

</t>
<t tx="ekr.20220525082936.1334">def test_false_only_of_true_type_is_uninhabited(self) -&gt; None:
    with state.strict_optional_set(True):
        fo = false_only(self.tuple(AnyType(TypeOfAny.special_form)))
        assert_type(UninhabitedType, fo)

</t>
<t tx="ekr.20220525082936.1335">def test_false_only_tuple(self) -&gt; None:
    with state.strict_optional_set(False):
        fo = false_only(self.tuple(self.fx.a))
        assert_equal(fo, NoneType())
    with state.strict_optional_set(True):
        fo = false_only(self.tuple(self.fx.a))
        assert_equal(fo, UninhabitedType())

</t>
<t tx="ekr.20220525082936.1336">def test_false_only_of_false_type_is_idempotent(self) -&gt; None:
    always_false = NoneType()
    fo = false_only(always_false)
    assert always_false is fo

</t>
<t tx="ekr.20220525082936.1337">def test_false_only_of_instance(self) -&gt; None:
    fo = false_only(self.fx.a)
    assert_equal(str(fo), "A")
    assert not fo.can_be_true
    assert fo.can_be_false
    assert_type(Instance, fo)
    # The original class still can be true
    assert self.fx.a.can_be_true

</t>
<t tx="ekr.20220525082936.1338">def test_false_only_of_union(self) -&gt; None:
    with state.strict_optional_set(True):
        tup_type = self.tuple()
        # Union of something that is unknown, something that is always true, something
        # that is always false
        union_type = UnionType([self.fx.a, self.tuple(AnyType(TypeOfAny.special_form)),
                                tup_type])
        assert_equal(len(union_type.items), 3)
        fo = false_only(union_type)
        assert isinstance(fo, UnionType)
        assert_equal(len(fo.items), 2)
        assert not fo.items[0].can_be_true
        assert fo.items[0].can_be_false
        assert fo.items[1] is tup_type

</t>
<t tx="ekr.20220525082936.1339">def test_simplified_union(self) -&gt; None:
    fx = self.fx

    self.assert_simplified_union([fx.a, fx.a], fx.a)
    self.assert_simplified_union([fx.a, fx.b], fx.a)
    self.assert_simplified_union([fx.a, fx.d], UnionType([fx.a, fx.d]))
    self.assert_simplified_union([fx.a, fx.uninhabited], fx.a)
    self.assert_simplified_union([fx.ga, fx.gs2a], fx.ga)
    self.assert_simplified_union([fx.ga, fx.gsab], UnionType([fx.ga, fx.gsab]))
    self.assert_simplified_union([fx.ga, fx.gsba], fx.ga)
    self.assert_simplified_union([fx.a, UnionType([fx.d])], UnionType([fx.a, fx.d]))
    self.assert_simplified_union([fx.a, UnionType([fx.a])], fx.a)
    self.assert_simplified_union([fx.b, UnionType([fx.c, UnionType([fx.d])])],
                                 UnionType([fx.b, fx.c, fx.d]))

</t>
<t tx="ekr.20220525082936.134">def underline_link(self, note: str) -&gt; str:
    """Underline a link in a note message (if any).

    This assumes there is at most one link in the message.
    """
    match = re.search(r'https?://\S*', note)
    if not match:
        return note
    start = match.start()
    end = match.end()
    return (note[:start] +
            self.style(note[start:end], 'none', underline=True) +
            note[end:])

</t>
<t tx="ekr.20220525082936.1340">def test_simplified_union_with_literals(self) -&gt; None:
    fx = self.fx

    self.assert_simplified_union([fx.lit1, fx.a], fx.a)
    self.assert_simplified_union([fx.lit1, fx.lit2, fx.a], fx.a)
    self.assert_simplified_union([fx.lit1, fx.lit1], fx.lit1)
    self.assert_simplified_union([fx.lit1, fx.lit2], UnionType([fx.lit1, fx.lit2]))
    self.assert_simplified_union([fx.lit1, fx.lit3], UnionType([fx.lit1, fx.lit3]))
    self.assert_simplified_union([fx.lit1, fx.uninhabited], fx.lit1)
    self.assert_simplified_union([fx.lit1_inst, fx.a], fx.a)
    self.assert_simplified_union([fx.lit1_inst, fx.lit1_inst], fx.lit1_inst)
    self.assert_simplified_union([fx.lit1_inst, fx.lit2_inst],
                                 UnionType([fx.lit1_inst, fx.lit2_inst]))
    self.assert_simplified_union([fx.lit1_inst, fx.lit3_inst],
                                 UnionType([fx.lit1_inst, fx.lit3_inst]))
    self.assert_simplified_union([fx.lit1_inst, fx.uninhabited], fx.lit1_inst)
    self.assert_simplified_union([fx.lit1, fx.lit1_inst], UnionType([fx.lit1, fx.lit1_inst]))
    self.assert_simplified_union([fx.lit1, fx.lit2_inst], UnionType([fx.lit1, fx.lit2_inst]))
    self.assert_simplified_union([fx.lit1, fx.lit3_inst], UnionType([fx.lit1, fx.lit3_inst]))

</t>
<t tx="ekr.20220525082936.1341">def test_simplified_union_with_str_literals(self) -&gt; None:
    fx = self.fx

    self.assert_simplified_union([fx.lit_str1, fx.lit_str2, fx.str_type], fx.str_type)
    self.assert_simplified_union([fx.lit_str1, fx.lit_str1, fx.lit_str1], fx.lit_str1)
    self.assert_simplified_union([fx.lit_str1, fx.lit_str2, fx.lit_str3],
                                 UnionType([fx.lit_str1, fx.lit_str2, fx.lit_str3]))
    self.assert_simplified_union([fx.lit_str1, fx.lit_str2, fx.uninhabited],
                                 UnionType([fx.lit_str1, fx.lit_str2]))

</t>
<t tx="ekr.20220525082936.1342">def test_simplify_very_large_union(self) -&gt; None:
    fx = self.fx
    literals = []
    for i in range(5000):
        literals.append(LiteralType("v%d" % i, fx.str_type))
    # This shouldn't be very slow, even if the union is big.
    self.assert_simplified_union([*literals, fx.str_type], fx.str_type)

</t>
<t tx="ekr.20220525082936.1343">def test_simplified_union_with_str_instance_literals(self) -&gt; None:
    fx = self.fx

    self.assert_simplified_union([fx.lit_str1_inst, fx.lit_str2_inst, fx.str_type],
                                 fx.str_type)
    self.assert_simplified_union([fx.lit_str1_inst, fx.lit_str1_inst, fx.lit_str1_inst],
                                 fx.lit_str1_inst)
    self.assert_simplified_union([fx.lit_str1_inst, fx.lit_str2_inst, fx.lit_str3_inst],
                                 UnionType([fx.lit_str1_inst,
                                            fx.lit_str2_inst,
                                            fx.lit_str3_inst]))
    self.assert_simplified_union([fx.lit_str1_inst, fx.lit_str2_inst, fx.uninhabited],
                                 UnionType([fx.lit_str1_inst, fx.lit_str2_inst]))

</t>
<t tx="ekr.20220525082936.1344">def test_simplified_union_with_mixed_str_literals(self) -&gt; None:
    fx = self.fx

    self.assert_simplified_union([fx.lit_str1, fx.lit_str2, fx.lit_str3_inst],
                                 UnionType([fx.lit_str1,
                                            fx.lit_str2,
                                            fx.lit_str3_inst]))
    self.assert_simplified_union([fx.lit_str1, fx.lit_str1, fx.lit_str1_inst],
                                 UnionType([fx.lit_str1, fx.lit_str1_inst]))

</t>
<t tx="ekr.20220525082936.1345">def assert_simplified_union(self, original: List[Type], union: Type) -&gt; None:
    assert_equal(make_simplified_union(original), union)
    assert_equal(make_simplified_union(list(reversed(original))), union)

</t>
<t tx="ekr.20220525082936.1346"># Helpers

</t>
<t tx="ekr.20220525082936.1347">def tuple(self, *a: Type) -&gt; TupleType:
    return TupleType(list(a), self.fx.std_tuple)

</t>
<t tx="ekr.20220525082936.1348">def callable(self, vars: List[str], *a: Type) -&gt; CallableType:
    """callable(args, a1, ..., an, r) constructs a callable with
    argument types a1, ... an and return type r and type arguments
    vars.
    """
    tv: List[TypeVarType] = []
    n = -1
    for v in vars:
        tv.append(TypeVarType(v, v, n, [], self.fx.o))
        n -= 1
    return CallableType(list(a[:-1]),
                        [ARG_POS] * (len(a) - 1),
                        [None] * (len(a) - 1),
                        a[-1],
                        self.fx.function,
                        name=None,
                        variables=tv)


</t>
<t tx="ekr.20220525082936.1349">class JoinSuite(Suite):
    @others
</t>
<t tx="ekr.20220525082936.135">def format_success(self, n_sources: int, use_color: bool = True) -&gt; str:
    """Format short summary in case of success.

    n_sources is total number of files passed directly on command line,
    i.e. excluding stubs and followed imports.
    """
    msg = f'Success: no issues found in {n_sources} source file{plural_s(n_sources)}'
    if not use_color:
        return msg
    return self.style(msg, 'green', bold=True)

</t>
<t tx="ekr.20220525082936.1350">def setUp(self) -&gt; None:
    self.fx = TypeFixture(INVARIANT)
    self.fx_co = TypeFixture(COVARIANT)
    self.fx_contra = TypeFixture(CONTRAVARIANT)

</t>
<t tx="ekr.20220525082936.1351">def test_trivial_cases(self) -&gt; None:
    for simple in self.fx.a, self.fx.o, self.fx.b:
        self.assert_join(simple, simple, simple)

</t>
<t tx="ekr.20220525082936.1352">def test_class_subtyping(self) -&gt; None:
    self.assert_join(self.fx.a, self.fx.o, self.fx.o)
    self.assert_join(self.fx.b, self.fx.o, self.fx.o)
    self.assert_join(self.fx.a, self.fx.d, self.fx.o)
    self.assert_join(self.fx.b, self.fx.c, self.fx.a)
    self.assert_join(self.fx.b, self.fx.d, self.fx.o)

</t>
<t tx="ekr.20220525082936.1353">def test_tuples(self) -&gt; None:
    self.assert_join(self.tuple(), self.tuple(), self.tuple())
    self.assert_join(self.tuple(self.fx.a),
                     self.tuple(self.fx.a),
                     self.tuple(self.fx.a))
    self.assert_join(self.tuple(self.fx.b, self.fx.c),
                     self.tuple(self.fx.a, self.fx.d),
                     self.tuple(self.fx.a, self.fx.o))

    self.assert_join(self.tuple(self.fx.a, self.fx.a),
                     self.fx.std_tuple,
                     self.var_tuple(self.fx.anyt))
    self.assert_join(self.tuple(self.fx.a),
                     self.tuple(self.fx.a, self.fx.a),
                     self.var_tuple(self.fx.a))
    self.assert_join(self.tuple(self.fx.b),
                     self.tuple(self.fx.a, self.fx.c),
                     self.var_tuple(self.fx.a))
    self.assert_join(self.tuple(),
                     self.tuple(self.fx.a),
                     self.var_tuple(self.fx.a))

</t>
<t tx="ekr.20220525082936.1354">def test_var_tuples(self) -&gt; None:
    self.assert_join(self.tuple(self.fx.a),
                     self.var_tuple(self.fx.a),
                     self.var_tuple(self.fx.a))
    self.assert_join(self.var_tuple(self.fx.a),
                     self.tuple(self.fx.a),
                     self.var_tuple(self.fx.a))
    self.assert_join(self.var_tuple(self.fx.a),
                     self.tuple(),
                     self.var_tuple(self.fx.a))

</t>
<t tx="ekr.20220525082936.1355">def test_function_types(self) -&gt; None:
    self.assert_join(self.callable(self.fx.a, self.fx.b),
                     self.callable(self.fx.a, self.fx.b),
                     self.callable(self.fx.a, self.fx.b))

    self.assert_join(self.callable(self.fx.a, self.fx.b),
                     self.callable(self.fx.b, self.fx.b),
                     self.callable(self.fx.b, self.fx.b))
    self.assert_join(self.callable(self.fx.a, self.fx.b),
                     self.callable(self.fx.a, self.fx.a),
                     self.callable(self.fx.a, self.fx.a))
    self.assert_join(self.callable(self.fx.a, self.fx.b),
                     self.fx.function,
                     self.fx.function)
    self.assert_join(self.callable(self.fx.a, self.fx.b),
                     self.callable(self.fx.d, self.fx.b),
                     self.fx.function)

</t>
<t tx="ekr.20220525082936.1356">def test_type_vars(self) -&gt; None:
    self.assert_join(self.fx.t, self.fx.t, self.fx.t)
    self.assert_join(self.fx.s, self.fx.s, self.fx.s)
    self.assert_join(self.fx.t, self.fx.s, self.fx.o)

</t>
<t tx="ekr.20220525082936.1357">def test_none(self) -&gt; None:
    # Any type t joined with None results in t.
    for t in [NoneType(), self.fx.a, self.fx.o, UnboundType('x'),
              self.fx.t, self.tuple(),
              self.callable(self.fx.a, self.fx.b), self.fx.anyt]:
        self.assert_join(t, NoneType(), t)

</t>
<t tx="ekr.20220525082936.1358">def test_unbound_type(self) -&gt; None:
    self.assert_join(UnboundType('x'), UnboundType('x'), self.fx.anyt)
    self.assert_join(UnboundType('x'), UnboundType('y'), self.fx.anyt)

    # Any type t joined with an unbound type results in dynamic. Unbound
    # type means that there is an error somewhere in the program, so this
    # does not affect type safety (whatever the result).
    for t in [self.fx.a, self.fx.o, self.fx.ga, self.fx.t, self.tuple(),
              self.callable(self.fx.a, self.fx.b)]:
        self.assert_join(t, UnboundType('X'), self.fx.anyt)

</t>
<t tx="ekr.20220525082936.1359">def test_any_type(self) -&gt; None:
    # Join against 'Any' type always results in 'Any'.
    for t in [self.fx.anyt, self.fx.a, self.fx.o, NoneType(),
              UnboundType('x'), self.fx.t, self.tuple(),
              self.callable(self.fx.a, self.fx.b)]:
        self.assert_join(t, self.fx.anyt, self.fx.anyt)

</t>
<t tx="ekr.20220525082936.136">def format_error(
    self, n_errors: int, n_files: int, n_sources: int, *,
    blockers: bool = False, use_color: bool = True
) -&gt; str:
    """Format a short summary in case of errors."""
    msg = f'Found {n_errors} error{plural_s(n_errors)} in {n_files} file{plural_s(n_files)}'
    if blockers:
        msg += ' (errors prevented further checking)'
    else:
        msg += f" (checked {n_sources} source file{plural_s(n_sources)})"
    if not use_color:
        return msg
    return self.style(msg, 'red', bold=True)


</t>
<t tx="ekr.20220525082936.1360">def test_mixed_truth_restricted_type_simple(self) -&gt; None:
    # join_simple against differently restricted truthiness types drops restrictions.
    true_a = true_only(self.fx.a)
    false_o = false_only(self.fx.o)
    j = join_simple(self.fx.o, true_a, false_o)
    assert j.can_be_true
    assert j.can_be_false

</t>
<t tx="ekr.20220525082936.1361">def test_mixed_truth_restricted_type(self) -&gt; None:
    # join_types against differently restricted truthiness types drops restrictions.
    true_any = true_only(AnyType(TypeOfAny.special_form))
    false_o = false_only(self.fx.o)
    j = join_types(true_any, false_o)
    assert j.can_be_true
    assert j.can_be_false

</t>
<t tx="ekr.20220525082936.1362">def test_other_mixed_types(self) -&gt; None:
    # In general, joining unrelated types produces object.
    for t1 in [self.fx.a, self.fx.t, self.tuple(),
               self.callable(self.fx.a, self.fx.b)]:
        for t2 in [self.fx.a, self.fx.t, self.tuple(),
                   self.callable(self.fx.a, self.fx.b)]:
            if str(t1) != str(t2):
                self.assert_join(t1, t2, self.fx.o)

</t>
<t tx="ekr.20220525082936.1363">def test_simple_generics(self) -&gt; None:
    self.assert_join(self.fx.ga, self.fx.nonet, self.fx.ga)
    self.assert_join(self.fx.ga, self.fx.anyt, self.fx.anyt)

    for t in [self.fx.a, self.fx.o, self.fx.t, self.tuple(),
              self.callable(self.fx.a, self.fx.b)]:
        self.assert_join(t, self.fx.ga, self.fx.o)

</t>
<t tx="ekr.20220525082936.1364">def test_generics_invariant(self) -&gt; None:
    self.assert_join(self.fx.ga, self.fx.ga, self.fx.ga)
    self.assert_join(self.fx.ga, self.fx.gb, self.fx.o)
    self.assert_join(self.fx.ga, self.fx.gd, self.fx.o)
    self.assert_join(self.fx.ga, self.fx.g2a, self.fx.o)

</t>
<t tx="ekr.20220525082936.1365">def test_generics_covariant(self) -&gt; None:
    self.assert_join(self.fx_co.ga, self.fx_co.ga, self.fx_co.ga)
    self.assert_join(self.fx_co.ga, self.fx_co.gb, self.fx_co.ga)
    self.assert_join(self.fx_co.ga, self.fx_co.gd, self.fx_co.go)
    self.assert_join(self.fx_co.ga, self.fx_co.g2a, self.fx_co.o)

</t>
<t tx="ekr.20220525082936.1366">def test_generics_contravariant(self) -&gt; None:
    self.assert_join(self.fx_contra.ga, self.fx_contra.ga, self.fx_contra.ga)
    # TODO: this can be more precise than "object", see a comment in mypy/join.py
    self.assert_join(self.fx_contra.ga, self.fx_contra.gb, self.fx_contra.o)
    self.assert_join(self.fx_contra.ga, self.fx_contra.g2a, self.fx_contra.o)

</t>
<t tx="ekr.20220525082936.1367">def test_generics_with_multiple_args(self) -&gt; None:
    self.assert_join(self.fx_co.hab, self.fx_co.hab, self.fx_co.hab)
    self.assert_join(self.fx_co.hab, self.fx_co.hbb, self.fx_co.hab)
    self.assert_join(self.fx_co.had, self.fx_co.haa, self.fx_co.hao)

</t>
<t tx="ekr.20220525082936.1368">def test_generics_with_inheritance(self) -&gt; None:
    self.assert_join(self.fx_co.gsab, self.fx_co.gb, self.fx_co.gb)
    self.assert_join(self.fx_co.gsba, self.fx_co.gb, self.fx_co.ga)
    self.assert_join(self.fx_co.gsab, self.fx_co.gd, self.fx_co.go)

</t>
<t tx="ekr.20220525082936.1369">def test_generics_with_inheritance_and_shared_supertype(self) -&gt; None:
    self.assert_join(self.fx_co.gsba, self.fx_co.gs2a, self.fx_co.ga)
    self.assert_join(self.fx_co.gsab, self.fx_co.gs2a, self.fx_co.ga)
    self.assert_join(self.fx_co.gsab, self.fx_co.gs2d, self.fx_co.go)

</t>
<t tx="ekr.20220525082936.137">def is_typeshed_file(file: str) -&gt; bool:
    # gross, but no other clear way to tell
    return 'typeshed' in os.path.abspath(file).split(os.sep)


</t>
<t tx="ekr.20220525082936.1370">def test_generic_types_and_any(self) -&gt; None:
    self.assert_join(self.fx.gdyn, self.fx.ga, self.fx.gdyn)
    self.assert_join(self.fx_co.gdyn, self.fx_co.ga, self.fx_co.gdyn)
    self.assert_join(self.fx_contra.gdyn, self.fx_contra.ga, self.fx_contra.gdyn)

</t>
<t tx="ekr.20220525082936.1371">def test_callables_with_any(self) -&gt; None:
    self.assert_join(self.callable(self.fx.a, self.fx.a, self.fx.anyt,
                                   self.fx.a),
                     self.callable(self.fx.a, self.fx.anyt, self.fx.a,
                                   self.fx.anyt),
                     self.callable(self.fx.a, self.fx.anyt, self.fx.anyt,
                                   self.fx.anyt))

</t>
<t tx="ekr.20220525082936.1372">def test_overloaded(self) -&gt; None:
    c = self.callable

    def ov(*items: CallableType) -&gt; Overloaded:
        return Overloaded(list(items))

    fx = self.fx
    func = fx.function
    c1 = c(fx.a, fx.a)
    c2 = c(fx.b, fx.b)
    c3 = c(fx.c, fx.c)
    self.assert_join(ov(c1, c2), c1, c1)
    self.assert_join(ov(c1, c2), c2, c2)
    self.assert_join(ov(c1, c2), ov(c1, c2), ov(c1, c2))
    self.assert_join(ov(c1, c2), ov(c1, c3), c1)
    self.assert_join(ov(c2, c1), ov(c3, c1), c1)
    self.assert_join(ov(c1, c2), c3, func)

</t>
<t tx="ekr.20220525082936.1373">def test_overloaded_with_any(self) -&gt; None:
    c = self.callable

    def ov(*items: CallableType) -&gt; Overloaded:
        return Overloaded(list(items))

    fx = self.fx
    any = fx.anyt
    self.assert_join(ov(c(fx.a, fx.a), c(fx.b, fx.b)), c(any, fx.b), c(any, fx.b))
    self.assert_join(ov(c(fx.a, fx.a), c(any, fx.b)), c(fx.b, fx.b), c(any, fx.b))

</t>
<t tx="ekr.20220525082936.1374">@skip
def test_join_interface_types(self) -&gt; None:
    self.assert_join(self.fx.f, self.fx.f, self.fx.f)
    self.assert_join(self.fx.f, self.fx.f2, self.fx.o)
    self.assert_join(self.fx.f, self.fx.f3, self.fx.f)

</t>
<t tx="ekr.20220525082936.1375">@skip
def test_join_interface_and_class_types(self) -&gt; None:
    self.assert_join(self.fx.o, self.fx.f, self.fx.o)
    self.assert_join(self.fx.a, self.fx.f, self.fx.o)

    self.assert_join(self.fx.e, self.fx.f, self.fx.f)

</t>
<t tx="ekr.20220525082936.1376">@skip
def test_join_class_types_with_interface_result(self) -&gt; None:
    # Unique result
    self.assert_join(self.fx.e, self.fx.e2, self.fx.f)

    # Ambiguous result
    self.assert_join(self.fx.e2, self.fx.e3, self.fx.anyt)

</t>
<t tx="ekr.20220525082936.1377">@skip
def test_generic_interfaces(self) -&gt; None:
    fx = InterfaceTypeFixture()

    self.assert_join(fx.gfa, fx.gfa, fx.gfa)
    self.assert_join(fx.gfa, fx.gfb, fx.o)

    self.assert_join(fx.m1, fx.gfa, fx.gfa)

    self.assert_join(fx.m1, fx.gfb, fx.o)

</t>
<t tx="ekr.20220525082936.1378">def test_simple_type_objects(self) -&gt; None:
    t1 = self.type_callable(self.fx.a, self.fx.a)
    t2 = self.type_callable(self.fx.b, self.fx.b)
    tr = self.type_callable(self.fx.b, self.fx.a)

    self.assert_join(t1, t1, t1)
    j = join_types(t1, t1)
    assert isinstance(j, CallableType)
    assert j.is_type_obj()

    self.assert_join(t1, t2, tr)
    self.assert_join(t1, self.fx.type_type, self.fx.type_type)
    self.assert_join(self.fx.type_type, self.fx.type_type,
                     self.fx.type_type)

</t>
<t tx="ekr.20220525082936.1379">def test_type_type(self) -&gt; None:
    self.assert_join(self.fx.type_a, self.fx.type_b, self.fx.type_a)
    self.assert_join(self.fx.type_b, self.fx.type_any, self.fx.type_any)
    self.assert_join(self.fx.type_b, self.fx.type_type, self.fx.type_type)
    self.assert_join(self.fx.type_b, self.fx.type_c, self.fx.type_a)
    self.assert_join(self.fx.type_c, self.fx.type_d, TypeType.make_normalized(self.fx.o))
    self.assert_join(self.fx.type_type, self.fx.type_any, self.fx.type_type)
    self.assert_join(self.fx.type_b, self.fx.anyt, self.fx.anyt)

</t>
<t tx="ekr.20220525082936.138">def is_stub_package_file(file: str) -&gt; bool:
    # Use hacky heuristics to check whether file is part of a PEP 561 stub package.
    if not file.endswith('.pyi'):
        return False
    return any(component.endswith('-stubs')
               for component in os.path.abspath(file).split(os.sep))


</t>
<t tx="ekr.20220525082936.1380">def test_literal_type(self) -&gt; None:
    a = self.fx.a
    d = self.fx.d
    lit1 = self.fx.lit1
    lit2 = self.fx.lit2
    lit3 = self.fx.lit3

    self.assert_join(lit1, lit1, lit1)
    self.assert_join(lit1, a, a)
    self.assert_join(lit1, d, self.fx.o)
    self.assert_join(lit1, lit2, a)
    self.assert_join(lit1, lit3, self.fx.o)
    self.assert_join(lit1, self.fx.anyt, self.fx.anyt)
    self.assert_join(UnionType([lit1, lit2]), lit2, UnionType([lit1, lit2]))
    self.assert_join(UnionType([lit1, lit2]), a, a)
    self.assert_join(UnionType([lit1, lit3]), a, UnionType([a, lit3]))
    self.assert_join(UnionType([d, lit3]), lit3, d)
    self.assert_join(UnionType([d, lit3]), d, UnionType([d, lit3]))
    self.assert_join(UnionType([a, lit1]), lit1, a)
    self.assert_join(UnionType([a, lit1]), lit2, a)
    self.assert_join(UnionType([lit1, lit2]),
                     UnionType([lit1, lit2]),
                     UnionType([lit1, lit2]))

    # The order in which we try joining two unions influences the
    # ordering of the items in the final produced unions. So, we
    # manually call 'assert_simple_join' and tune the output
    # after swapping the arguments here.
    self.assert_simple_join(UnionType([lit1, lit2]),
                            UnionType([lit2, lit3]),
                            UnionType([lit1, lit2, lit3]))
    self.assert_simple_join(UnionType([lit2, lit3]),
                            UnionType([lit1, lit2]),
                            UnionType([lit2, lit3, lit1]))

</t>
<t tx="ekr.20220525082936.1381"># There are additional test cases in check-inference.test.

# TODO: Function types + varargs and default args.

</t>
<t tx="ekr.20220525082936.1382">def assert_join(self, s: Type, t: Type, join: Type) -&gt; None:
    self.assert_simple_join(s, t, join)
    self.assert_simple_join(t, s, join)

</t>
<t tx="ekr.20220525082936.1383">def assert_simple_join(self, s: Type, t: Type, join: Type) -&gt; None:
    result = join_types(s, t)
    actual = str(result)
    expected = str(join)
    assert_equal(actual, expected,
                 f'join({s}, {t}) == {{}} ({{}} expected)')
    assert is_subtype(s, result), f'{s} not subtype of {result}'
    assert is_subtype(t, result), f'{t} not subtype of {result}'

</t>
<t tx="ekr.20220525082936.1384">def tuple(self, *a: Type) -&gt; TupleType:
    return TupleType(list(a), self.fx.std_tuple)

</t>
<t tx="ekr.20220525082936.1385">def var_tuple(self, t: Type) -&gt; Instance:
    """Construct a variable-length tuple type"""
    return Instance(self.fx.std_tuplei, [t])

</t>
<t tx="ekr.20220525082936.1386">def callable(self, *a: Type) -&gt; CallableType:
    """callable(a1, ..., an, r) constructs a callable with argument types
    a1, ... an and return type r.
    """
    n = len(a) - 1
    return CallableType(list(a[:-1]), [ARG_POS] * n, [None] * n,
                    a[-1], self.fx.function)

</t>
<t tx="ekr.20220525082936.1387">def type_callable(self, *a: Type) -&gt; CallableType:
    """type_callable(a1, ..., an, r) constructs a callable with
    argument types a1, ... an and return type r, and which
    represents a type.
    """
    n = len(a) - 1
    return CallableType(list(a[:-1]), [ARG_POS] * n, [None] * n,
                    a[-1], self.fx.type_type)


</t>
<t tx="ekr.20220525082936.1388">class MeetSuite(Suite):
    @others
</t>
<t tx="ekr.20220525082936.1389">def setUp(self) -&gt; None:
    self.fx = TypeFixture()

</t>
<t tx="ekr.20220525082936.139">def unnamed_function(name: Optional[str]) -&gt; bool:
    return name is not None and name == "_"


</t>
<t tx="ekr.20220525082936.1390">def test_trivial_cases(self) -&gt; None:
    for simple in self.fx.a, self.fx.o, self.fx.b:
        self.assert_meet(simple, simple, simple)

</t>
<t tx="ekr.20220525082936.1391">def test_class_subtyping(self) -&gt; None:
    self.assert_meet(self.fx.a, self.fx.o, self.fx.a)
    self.assert_meet(self.fx.a, self.fx.b, self.fx.b)
    self.assert_meet(self.fx.b, self.fx.o, self.fx.b)
    self.assert_meet(self.fx.a, self.fx.d, NoneType())
    self.assert_meet(self.fx.b, self.fx.c, NoneType())

</t>
<t tx="ekr.20220525082936.1392">def test_tuples(self) -&gt; None:
    self.assert_meet(self.tuple(), self.tuple(), self.tuple())
    self.assert_meet(self.tuple(self.fx.a),
                     self.tuple(self.fx.a),
                     self.tuple(self.fx.a))
    self.assert_meet(self.tuple(self.fx.b, self.fx.c),
                     self.tuple(self.fx.a, self.fx.d),
                     self.tuple(self.fx.b, NoneType()))

    self.assert_meet(self.tuple(self.fx.a, self.fx.a),
                     self.fx.std_tuple,
                     self.tuple(self.fx.a, self.fx.a))
    self.assert_meet(self.tuple(self.fx.a),
                     self.tuple(self.fx.a, self.fx.a),
                     NoneType())

</t>
<t tx="ekr.20220525082936.1393">def test_function_types(self) -&gt; None:
    self.assert_meet(self.callable(self.fx.a, self.fx.b),
                     self.callable(self.fx.a, self.fx.b),
                     self.callable(self.fx.a, self.fx.b))

    self.assert_meet(self.callable(self.fx.a, self.fx.b),
                     self.callable(self.fx.b, self.fx.b),
                     self.callable(self.fx.a, self.fx.b))
    self.assert_meet(self.callable(self.fx.a, self.fx.b),
                     self.callable(self.fx.a, self.fx.a),
                     self.callable(self.fx.a, self.fx.b))

</t>
<t tx="ekr.20220525082936.1394">def test_type_vars(self) -&gt; None:
    self.assert_meet(self.fx.t, self.fx.t, self.fx.t)
    self.assert_meet(self.fx.s, self.fx.s, self.fx.s)
    self.assert_meet(self.fx.t, self.fx.s, NoneType())

</t>
<t tx="ekr.20220525082936.1395">def test_none(self) -&gt; None:
    self.assert_meet(NoneType(), NoneType(), NoneType())

    self.assert_meet(NoneType(), self.fx.anyt, NoneType())

    # Any type t joined with None results in None, unless t is Any.
    for t in [self.fx.a, self.fx.o, UnboundType('x'), self.fx.t,
              self.tuple(), self.callable(self.fx.a, self.fx.b)]:
        self.assert_meet(t, NoneType(), NoneType())

</t>
<t tx="ekr.20220525082936.1396">def test_unbound_type(self) -&gt; None:
    self.assert_meet(UnboundType('x'), UnboundType('x'), self.fx.anyt)
    self.assert_meet(UnboundType('x'), UnboundType('y'), self.fx.anyt)

    self.assert_meet(UnboundType('x'), self.fx.anyt, UnboundType('x'))

    # The meet of any type t with an unbound type results in dynamic.
    # Unbound type means that there is an error somewhere in the program,
    # so this does not affect type safety.
    for t in [self.fx.a, self.fx.o, self.fx.t, self.tuple(),
              self.callable(self.fx.a, self.fx.b)]:
        self.assert_meet(t, UnboundType('X'), self.fx.anyt)

</t>
<t tx="ekr.20220525082936.1397">def test_dynamic_type(self) -&gt; None:
    # Meet against dynamic type always results in dynamic.
    for t in [self.fx.anyt, self.fx.a, self.fx.o, NoneType(),
              UnboundType('x'), self.fx.t, self.tuple(),
              self.callable(self.fx.a, self.fx.b)]:
        self.assert_meet(t, self.fx.anyt, t)

</t>
<t tx="ekr.20220525082936.1398">def test_simple_generics(self) -&gt; None:
    self.assert_meet(self.fx.ga, self.fx.ga, self.fx.ga)
    self.assert_meet(self.fx.ga, self.fx.o, self.fx.ga)
    self.assert_meet(self.fx.ga, self.fx.gb, self.fx.gb)
    self.assert_meet(self.fx.ga, self.fx.gd, self.fx.nonet)
    self.assert_meet(self.fx.ga, self.fx.g2a, self.fx.nonet)

    self.assert_meet(self.fx.ga, self.fx.nonet, self.fx.nonet)
    self.assert_meet(self.fx.ga, self.fx.anyt, self.fx.ga)

    for t in [self.fx.a, self.fx.t, self.tuple(),
              self.callable(self.fx.a, self.fx.b)]:
        self.assert_meet(t, self.fx.ga, self.fx.nonet)

</t>
<t tx="ekr.20220525082936.1399">def test_generics_with_multiple_args(self) -&gt; None:
    self.assert_meet(self.fx.hab, self.fx.hab, self.fx.hab)
    self.assert_meet(self.fx.hab, self.fx.haa, self.fx.hab)
    self.assert_meet(self.fx.hab, self.fx.had, self.fx.nonet)
    self.assert_meet(self.fx.hab, self.fx.hbb, self.fx.hbb)

</t>
<t tx="ekr.20220525082936.14">@abstractmethod
def visit_parameters(self, t: Parameters) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.140"># TODO: replace with uses of perf_counter_ns when support for py3.6 is dropped
# (or when mypy properly handles alternate definitions based on python version check
time_ref = time.perf_counter


</t>
<t tx="ekr.20220525082936.1400">def test_generics_with_inheritance(self) -&gt; None:
    self.assert_meet(self.fx.gsab, self.fx.gb, self.fx.gsab)
    self.assert_meet(self.fx.gsba, self.fx.gb, self.fx.nonet)

</t>
<t tx="ekr.20220525082936.1401">def test_generics_with_inheritance_and_shared_supertype(self) -&gt; None:
    self.assert_meet(self.fx.gsba, self.fx.gs2a, self.fx.nonet)
    self.assert_meet(self.fx.gsab, self.fx.gs2a, self.fx.nonet)

</t>
<t tx="ekr.20220525082936.1402">def test_generic_types_and_dynamic(self) -&gt; None:
    self.assert_meet(self.fx.gdyn, self.fx.ga, self.fx.ga)

</t>
<t tx="ekr.20220525082936.1403">def test_callables_with_dynamic(self) -&gt; None:
    self.assert_meet(self.callable(self.fx.a, self.fx.a, self.fx.anyt,
                                   self.fx.a),
                     self.callable(self.fx.a, self.fx.anyt, self.fx.a,
                                   self.fx.anyt),
                     self.callable(self.fx.a, self.fx.anyt, self.fx.anyt,
                                   self.fx.anyt))

</t>
<t tx="ekr.20220525082936.1404">def test_meet_interface_types(self) -&gt; None:
    self.assert_meet(self.fx.f, self.fx.f, self.fx.f)
    self.assert_meet(self.fx.f, self.fx.f2, self.fx.nonet)
    self.assert_meet(self.fx.f, self.fx.f3, self.fx.f3)

</t>
<t tx="ekr.20220525082936.1405">def test_meet_interface_and_class_types(self) -&gt; None:
    self.assert_meet(self.fx.o, self.fx.f, self.fx.f)
    self.assert_meet(self.fx.a, self.fx.f, self.fx.nonet)

    self.assert_meet(self.fx.e, self.fx.f, self.fx.e)

</t>
<t tx="ekr.20220525082936.1406">def test_meet_class_types_with_shared_interfaces(self) -&gt; None:
    # These have nothing special with respect to meets, unlike joins. These
    # are for completeness only.
    self.assert_meet(self.fx.e, self.fx.e2, self.fx.nonet)
    self.assert_meet(self.fx.e2, self.fx.e3, self.fx.nonet)

</t>
<t tx="ekr.20220525082936.1407">@skip
def test_meet_with_generic_interfaces(self) -&gt; None:
    fx = InterfaceTypeFixture()
    self.assert_meet(fx.gfa, fx.m1, fx.m1)
    self.assert_meet(fx.gfa, fx.gfa, fx.gfa)
    self.assert_meet(fx.gfb, fx.m1, fx.nonet)

</t>
<t tx="ekr.20220525082936.1408">def test_type_type(self) -&gt; None:
    self.assert_meet(self.fx.type_a, self.fx.type_b, self.fx.type_b)
    self.assert_meet(self.fx.type_b, self.fx.type_any, self.fx.type_b)
    self.assert_meet(self.fx.type_b, self.fx.type_type, self.fx.type_b)
    self.assert_meet(self.fx.type_b, self.fx.type_c, self.fx.nonet)
    self.assert_meet(self.fx.type_c, self.fx.type_d, self.fx.nonet)
    self.assert_meet(self.fx.type_type, self.fx.type_any, self.fx.type_any)
    self.assert_meet(self.fx.type_b, self.fx.anyt, self.fx.type_b)

</t>
<t tx="ekr.20220525082936.1409">def test_literal_type(self) -&gt; None:
    a = self.fx.a
    lit1 = self.fx.lit1
    lit2 = self.fx.lit2
    lit3 = self.fx.lit3

    self.assert_meet(lit1, lit1, lit1)
    self.assert_meet(lit1, a, lit1)
    self.assert_meet_uninhabited(lit1, lit3)
    self.assert_meet_uninhabited(lit1, lit2)
    self.assert_meet(UnionType([lit1, lit2]), lit1, lit1)
    self.assert_meet(UnionType([lit1, lit2]), UnionType([lit2, lit3]), lit2)
    self.assert_meet(UnionType([lit1, lit2]), UnionType([lit1, lit2]), UnionType([lit1, lit2]))
    self.assert_meet(lit1, self.fx.anyt, lit1)
    self.assert_meet(lit1, self.fx.o, lit1)

    assert is_same_type(lit1, narrow_declared_type(lit1, a))
    assert is_same_type(lit2, narrow_declared_type(lit2, a))

</t>
<t tx="ekr.20220525082936.141">def time_spent_us(t0: float) -&gt; int:
    return int((time.perf_counter() - t0) * 1e6)


def plural_s(s: Union[int, Sized]) -&gt; str:
    count = s if isinstance(s, int) else len(s)
    if count &gt; 1:
        return 's'
    else:
        return ''
</t>
<t tx="ekr.20220525082936.1410"># FIX generic interfaces + ranges

</t>
<t tx="ekr.20220525082936.1411">def assert_meet_uninhabited(self, s: Type, t: Type) -&gt; None:
    with state.strict_optional_set(False):
        self.assert_meet(s, t, self.fx.nonet)
    with state.strict_optional_set(True):
        self.assert_meet(s, t, self.fx.uninhabited)

</t>
<t tx="ekr.20220525082936.1412">def assert_meet(self, s: Type, t: Type, meet: Type) -&gt; None:
    self.assert_simple_meet(s, t, meet)
    self.assert_simple_meet(t, s, meet)

</t>
<t tx="ekr.20220525082936.1413">def assert_simple_meet(self, s: Type, t: Type, meet: Type) -&gt; None:
    result = meet_types(s, t)
    actual = str(result)
    expected = str(meet)
    assert_equal(actual, expected,
                 f'meet({s}, {t}) == {{}} ({{}} expected)')
    assert is_subtype(result, s), f'{result} not subtype of {s}'
    assert is_subtype(result, t), f'{result} not subtype of {t}'

</t>
<t tx="ekr.20220525082936.1414">def tuple(self, *a: Type) -&gt; TupleType:
    return TupleType(list(a), self.fx.std_tuple)

</t>
<t tx="ekr.20220525082936.1415">def callable(self, *a: Type) -&gt; CallableType:
    """callable(a1, ..., an, r) constructs a callable with argument types
    a1, ... an and return type r.
    """
    n = len(a) - 1
    return CallableType(list(a[:-1]),
                        [ARG_POS] * n, [None] * n,
                        a[-1], self.fx.function)


</t>
<t tx="ekr.20220525082936.1416">class SameTypeSuite(Suite):
    @others
</t>
<t tx="ekr.20220525082936.1417">def setUp(self) -&gt; None:
    self.fx = TypeFixture()

</t>
<t tx="ekr.20220525082936.1418">def test_literal_type(self) -&gt; None:
    a = self.fx.a
    b = self.fx.b  # Reminder: b is a subclass of a

    lit1 = self.fx.lit1
    lit2 = self.fx.lit2
    lit3 = self.fx.lit3

    self.assert_same(lit1, lit1)
    self.assert_same(UnionType([lit1, lit2]), UnionType([lit1, lit2]))
    self.assert_same(UnionType([lit1, lit2]), UnionType([lit2, lit1]))
    self.assert_same(UnionType([a, b]), UnionType([b, a]))
    self.assert_not_same(lit1, b)
    self.assert_not_same(lit1, lit2)
    self.assert_not_same(lit1, lit3)

    self.assert_not_same(lit1, self.fx.anyt)
    self.assert_not_same(lit1, self.fx.nonet)

</t>
<t tx="ekr.20220525082936.1419">def assert_same(self, s: Type, t: Type, strict: bool = True) -&gt; None:
    self.assert_simple_is_same(s, t, expected=True, strict=strict)
    self.assert_simple_is_same(t, s, expected=True, strict=strict)

</t>
<t tx="ekr.20220525082936.142">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
import os
from mypy import git

# Base version.
# - Release versions have the form "0.NNN".
# - Dev versions have the form "0.NNN+dev" (PLUS sign to conform to PEP 440).
# - For 1.0 we'll switch back to 1.2.3 form.
__version__ = '0.970+dev'
base_version = __version__

mypy_dir = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))
if __version__.endswith('+dev') and git.is_git_repo(mypy_dir) and git.have_git():
    __version__ += '.' + git.git_revision(mypy_dir).decode('utf-8')
    if git.is_dirty(mypy_dir):
        __version__ += '.dirty'
del mypy_dir
</t>
<t tx="ekr.20220525082936.1420">def assert_not_same(self, s: Type, t: Type, strict: bool = True) -&gt; None:
    self.assert_simple_is_same(s, t, False, strict=strict)
    self.assert_simple_is_same(t, s, False, strict=strict)

</t>
<t tx="ekr.20220525082936.1421">def assert_simple_is_same(self, s: Type, t: Type, expected: bool, strict: bool) -&gt; None:
    actual = is_same_type(s, t)
    assert_equal(actual, expected,
                 f'is_same_type({s}, {t}) is {{}} ({{}} expected)')

    if strict:
        actual2 = (s == t)
        assert_equal(actual2, expected,
                     f'({s} == {t}) is {{}} ({{}} expected)')
        assert_equal(hash(s) == hash(t), expected,
                     f'(hash({s}) == hash({t}) is {{}} ({{}} expected)')


</t>
<t tx="ekr.20220525082936.1422">class RemoveLastKnownValueSuite(Suite):
    @others
</t>
<t tx="ekr.20220525082936.1423">def setUp(self) -&gt; None:
    self.fx = TypeFixture()

</t>
<t tx="ekr.20220525082936.1424">def test_optional(self) -&gt; None:
    t = UnionType.make_union([self.fx.a, self.fx.nonet])
    self.assert_union_result(t, [self.fx.a, self.fx.nonet])

</t>
<t tx="ekr.20220525082936.1425">def test_two_instances(self) -&gt; None:
    t = UnionType.make_union([self.fx.a, self.fx.b])
    self.assert_union_result(t, [self.fx.a, self.fx.b])

</t>
<t tx="ekr.20220525082936.1426">def test_multiple_same_instances(self) -&gt; None:
    t = UnionType.make_union([self.fx.a, self.fx.a])
    assert remove_instance_last_known_values(t) == self.fx.a
    t = UnionType.make_union([self.fx.a, self.fx.a, self.fx.b])
    self.assert_union_result(t, [self.fx.a, self.fx.b])
    t = UnionType.make_union([self.fx.a, self.fx.nonet, self.fx.a, self.fx.b])
    self.assert_union_result(t, [self.fx.a, self.fx.nonet, self.fx.b])

</t>
<t tx="ekr.20220525082936.1427">def test_single_last_known_value(self) -&gt; None:
    t = UnionType.make_union([self.fx.lit1_inst, self.fx.nonet])
    self.assert_union_result(t, [self.fx.a, self.fx.nonet])

</t>
<t tx="ekr.20220525082936.1428">def test_last_known_values_with_merge(self) -&gt; None:
    t = UnionType.make_union([self.fx.lit1_inst, self.fx.lit2_inst, self.fx.lit4_inst])
    assert remove_instance_last_known_values(t) == self.fx.a
    t = UnionType.make_union([self.fx.lit1_inst,
                              self.fx.b,
                              self.fx.lit2_inst,
                              self.fx.lit4_inst])
    self.assert_union_result(t, [self.fx.a, self.fx.b])

</t>
<t tx="ekr.20220525082936.1429">def test_generics(self) -&gt; None:
    t = UnionType.make_union([self.fx.ga, self.fx.gb])
    self.assert_union_result(t, [self.fx.ga, self.fx.gb])

</t>
<t tx="ekr.20220525082936.143">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Generic abstract syntax tree node visitor"""

from abc import abstractmethod
from typing import TypeVar, Generic
from typing_extensions import TYPE_CHECKING
from mypy_extensions import trait, mypyc_attr

if TYPE_CHECKING:
    # break import cycle only needed for mypy
    import mypy.nodes
    import mypy.patterns


T = TypeVar('T')


@others
</t>
<t tx="ekr.20220525082936.1430">def assert_union_result(self, t: ProperType, expected: List[Type]) -&gt; None:
    t2 = remove_instance_last_known_values(t)
    assert type(t2) is UnionType
    assert t2.items == expected
</t>
<t tx="ekr.20220525082936.1431">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
import os
from unittest import mock, TestCase

from mypy.util import get_terminal_width


@others
</t>
<t tx="ekr.20220525082936.1432">class TestGetTerminalSize(TestCase):
    def test_get_terminal_size_in_pty_defaults_to_80(self) -&gt; None:
        # when run using a pty, `os.get_terminal_size()` returns `0, 0`
        ret = os.terminal_size((0, 0))
        mock_environ = os.environ.copy()
        mock_environ.pop('COLUMNS', None)
        with mock.patch.object(os, 'get_terminal_size', return_value=ret):
            with mock.patch.dict(os.environ, values=mock_environ, clear=True):
                assert get_terminal_width() == 80
</t>
<t tx="ekr.20220525082936.1433">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
import os
import pytest
import shutil
import tempfile
import unittest
from typing import List, Optional, Set, Tuple

from mypy.find_sources import InvalidSourceList, SourceFinder, create_source_list
from mypy.fscache import FileSystemCache
from mypy.options import Options
from mypy.modulefinder import BuildSource


@others
</t>
<t tx="ekr.20220525082936.1434">class FakeFSCache(FileSystemCache):
    @others
</t>
<t tx="ekr.20220525082936.1435">def __init__(self, files: Set[str]) -&gt; None:
    self.files = {os.path.abspath(f) for f in files}

</t>
<t tx="ekr.20220525082936.1436">def isfile(self, file: str) -&gt; bool:
    return file in self.files

</t>
<t tx="ekr.20220525082936.1437">def isdir(self, dir: str) -&gt; bool:
    if not dir.endswith(os.sep):
        dir += os.sep
    return any(f.startswith(dir) for f in self.files)

</t>
<t tx="ekr.20220525082936.1438">def listdir(self, dir: str) -&gt; List[str]:
    if not dir.endswith(os.sep):
        dir += os.sep
    return list({f[len(dir):].split(os.sep)[0] for f in self.files if f.startswith(dir)})

</t>
<t tx="ekr.20220525082936.1439">def init_under_package_root(self, file: str) -&gt; bool:
    return False


</t>
<t tx="ekr.20220525082936.144">@trait
@mypyc_attr(allow_interpreted_subclasses=True)
class ExpressionVisitor(Generic[T]):
    @others
</t>
<t tx="ekr.20220525082936.1440">def normalise_path(path: str) -&gt; str:
    path = os.path.splitdrive(path)[1]
    path = path.replace(os.sep, "/")
    return path


</t>
<t tx="ekr.20220525082936.1441">def normalise_build_source_list(sources: List[BuildSource]) -&gt; List[Tuple[str, Optional[str]]]:
    return sorted(
        (s.module, (normalise_path(s.base_dir) if s.base_dir is not None else None))
        for s in sources
    )


</t>
<t tx="ekr.20220525082936.1442">def crawl(finder: SourceFinder, f: str) -&gt; Tuple[str, str]:
    module, base_dir = finder.crawl_up(f)
    return module, normalise_path(base_dir)


</t>
<t tx="ekr.20220525082936.1443">def find_sources_in_dir(finder: SourceFinder, f: str) -&gt; List[Tuple[str, Optional[str]]]:
    return normalise_build_source_list(finder.find_sources_in_dir(os.path.abspath(f)))


</t>
<t tx="ekr.20220525082936.1444">def find_sources(
    paths: List[str], options: Options, fscache: FileSystemCache
) -&gt; List[Tuple[str, Optional[str]]]:
    paths = [os.path.abspath(p) for p in paths]
    return normalise_build_source_list(create_source_list(paths, options, fscache))


</t>
<t tx="ekr.20220525082936.1445">class SourceFinderSuite(unittest.TestCase):
    @others
</t>
<t tx="ekr.20220525082936.1446">def setUp(self) -&gt; None:
    self.tempdir = tempfile.mkdtemp()
    self.oldcwd = os.getcwd()
    os.chdir(self.tempdir)

</t>
<t tx="ekr.20220525082936.1447">def tearDown(self) -&gt; None:
    os.chdir(self.oldcwd)
    shutil.rmtree(self.tempdir)

</t>
<t tx="ekr.20220525082936.1448">def test_crawl_no_namespace(self) -&gt; None:
    options = Options()
    options.namespace_packages = False

    finder = SourceFinder(FakeFSCache({"/setup.py"}), options)
    assert crawl(finder, "/setup.py") == ("setup", "/")

    finder = SourceFinder(FakeFSCache({"/a/setup.py"}), options)
    assert crawl(finder, "/a/setup.py") == ("setup", "/a")

    finder = SourceFinder(FakeFSCache({"/a/b/setup.py"}), options)
    assert crawl(finder, "/a/b/setup.py") == ("setup", "/a/b")

    finder = SourceFinder(FakeFSCache({"/a/setup.py", "/a/__init__.py"}), options)
    assert crawl(finder, "/a/setup.py") == ("a.setup", "/")

    finder = SourceFinder(
        FakeFSCache({"/a/invalid-name/setup.py", "/a/__init__.py"}),
        options,
    )
    assert crawl(finder, "/a/invalid-name/setup.py") == ("setup", "/a/invalid-name")

    finder = SourceFinder(FakeFSCache({"/a/b/setup.py", "/a/__init__.py"}), options)
    assert crawl(finder, "/a/b/setup.py") == ("setup", "/a/b")

    finder = SourceFinder(
        FakeFSCache({"/a/b/c/setup.py", "/a/__init__.py", "/a/b/c/__init__.py"}),
        options,
    )
    assert crawl(finder, "/a/b/c/setup.py") == ("c.setup", "/a/b")

</t>
<t tx="ekr.20220525082936.1449">def test_crawl_namespace(self) -&gt; None:
    options = Options()
    options.namespace_packages = True

    finder = SourceFinder(FakeFSCache({"/setup.py"}), options)
    assert crawl(finder, "/setup.py") == ("setup", "/")

    finder = SourceFinder(FakeFSCache({"/a/setup.py"}), options)
    assert crawl(finder, "/a/setup.py") == ("setup", "/a")

    finder = SourceFinder(FakeFSCache({"/a/b/setup.py"}), options)
    assert crawl(finder, "/a/b/setup.py") == ("setup", "/a/b")

    finder = SourceFinder(FakeFSCache({"/a/setup.py", "/a/__init__.py"}), options)
    assert crawl(finder, "/a/setup.py") == ("a.setup", "/")

    finder = SourceFinder(
        FakeFSCache({"/a/invalid-name/setup.py", "/a/__init__.py"}),
        options,
    )
    assert crawl(finder, "/a/invalid-name/setup.py") == ("setup", "/a/invalid-name")

    finder = SourceFinder(FakeFSCache({"/a/b/setup.py", "/a/__init__.py"}), options)
    assert crawl(finder, "/a/b/setup.py") == ("a.b.setup", "/")

    finder = SourceFinder(
        FakeFSCache({"/a/b/c/setup.py", "/a/__init__.py", "/a/b/c/__init__.py"}),
        options,
    )
    assert crawl(finder, "/a/b/c/setup.py") == ("a.b.c.setup", "/")

</t>
<t tx="ekr.20220525082936.145">@abstractmethod
def visit_int_expr(self, o: 'mypy.nodes.IntExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.1450">def test_crawl_namespace_explicit_base(self) -&gt; None:
    options = Options()
    options.namespace_packages = True
    options.explicit_package_bases = True

    finder = SourceFinder(FakeFSCache({"/setup.py"}), options)
    assert crawl(finder, "/setup.py") == ("setup", "/")

    finder = SourceFinder(FakeFSCache({"/a/setup.py"}), options)
    assert crawl(finder, "/a/setup.py") == ("setup", "/a")

    finder = SourceFinder(FakeFSCache({"/a/b/setup.py"}), options)
    assert crawl(finder, "/a/b/setup.py") == ("setup", "/a/b")

    finder = SourceFinder(FakeFSCache({"/a/setup.py", "/a/__init__.py"}), options)
    assert crawl(finder, "/a/setup.py") == ("a.setup", "/")

    finder = SourceFinder(
        FakeFSCache({"/a/invalid-name/setup.py", "/a/__init__.py"}),
        options,
    )
    assert crawl(finder, "/a/invalid-name/setup.py") == ("setup", "/a/invalid-name")

    finder = SourceFinder(FakeFSCache({"/a/b/setup.py", "/a/__init__.py"}), options)
    assert crawl(finder, "/a/b/setup.py") == ("a.b.setup", "/")

    finder = SourceFinder(
        FakeFSCache({"/a/b/c/setup.py", "/a/__init__.py", "/a/b/c/__init__.py"}),
        options,
    )
    assert crawl(finder, "/a/b/c/setup.py") == ("a.b.c.setup", "/")

    # set mypy path, so we actually have some explicit base dirs
    options.mypy_path = ["/a/b"]

    finder = SourceFinder(FakeFSCache({"/a/b/c/setup.py"}), options)
    assert crawl(finder, "/a/b/c/setup.py") == ("c.setup", "/a/b")

    finder = SourceFinder(
        FakeFSCache({"/a/b/c/setup.py", "/a/__init__.py", "/a/b/c/__init__.py"}),
        options,
    )
    assert crawl(finder, "/a/b/c/setup.py") == ("c.setup", "/a/b")

    options.mypy_path = ["/a/b", "/a/b/c"]
    finder = SourceFinder(FakeFSCache({"/a/b/c/setup.py"}), options)
    assert crawl(finder, "/a/b/c/setup.py") == ("setup", "/a/b/c")

</t>
<t tx="ekr.20220525082936.1451">def test_crawl_namespace_multi_dir(self) -&gt; None:
    options = Options()
    options.namespace_packages = True
    options.explicit_package_bases = True
    options.mypy_path = ["/a", "/b"]

    finder = SourceFinder(FakeFSCache({"/a/pkg/a.py", "/b/pkg/b.py"}), options)
    assert crawl(finder, "/a/pkg/a.py") == ("pkg.a", "/a")
    assert crawl(finder, "/b/pkg/b.py") == ("pkg.b", "/b")

</t>
<t tx="ekr.20220525082936.1452">def test_find_sources_in_dir_no_namespace(self) -&gt; None:
    options = Options()
    options.namespace_packages = False

    files = {
        "/pkg/a1/b/c/d/e.py",
        "/pkg/a1/b/f.py",
        "/pkg/a2/__init__.py",
        "/pkg/a2/b/c/d/e.py",
        "/pkg/a2/b/f.py",
    }
    finder = SourceFinder(FakeFSCache(files), options)
    assert find_sources_in_dir(finder, "/") == [
        ("a2", "/pkg"),
        ("e", "/pkg/a1/b/c/d"),
        ("e", "/pkg/a2/b/c/d"),
        ("f", "/pkg/a1/b"),
        ("f", "/pkg/a2/b"),
    ]

</t>
<t tx="ekr.20220525082936.1453">def test_find_sources_in_dir_namespace(self) -&gt; None:
    options = Options()
    options.namespace_packages = True

    files = {
        "/pkg/a1/b/c/d/e.py",
        "/pkg/a1/b/f.py",
        "/pkg/a2/__init__.py",
        "/pkg/a2/b/c/d/e.py",
        "/pkg/a2/b/f.py",
    }
    finder = SourceFinder(FakeFSCache(files), options)
    assert find_sources_in_dir(finder, "/") == [
        ("a2", "/pkg"),
        ("a2.b.c.d.e", "/pkg"),
        ("a2.b.f", "/pkg"),
        ("e", "/pkg/a1/b/c/d"),
        ("f", "/pkg/a1/b"),
    ]

</t>
<t tx="ekr.20220525082936.1454">def test_find_sources_in_dir_namespace_explicit_base(self) -&gt; None:
    options = Options()
    options.namespace_packages = True
    options.explicit_package_bases = True
    options.mypy_path = ["/"]

    files = {
        "/pkg/a1/b/c/d/e.py",
        "/pkg/a1/b/f.py",
        "/pkg/a2/__init__.py",
        "/pkg/a2/b/c/d/e.py",
        "/pkg/a2/b/f.py",
    }
    finder = SourceFinder(FakeFSCache(files), options)
    assert find_sources_in_dir(finder, "/") == [
        ("pkg.a1.b.c.d.e", "/"),
        ("pkg.a1.b.f", "/"),
        ("pkg.a2", "/"),
        ("pkg.a2.b.c.d.e", "/"),
        ("pkg.a2.b.f", "/"),
    ]

    options.mypy_path = ["/pkg"]
    finder = SourceFinder(FakeFSCache(files), options)
    assert find_sources_in_dir(finder, "/") == [
        ("a1.b.c.d.e", "/pkg"),
        ("a1.b.f", "/pkg"),
        ("a2", "/pkg"),
        ("a2.b.c.d.e", "/pkg"),
        ("a2.b.f", "/pkg"),
    ]

</t>
<t tx="ekr.20220525082936.1455">def test_find_sources_in_dir_namespace_multi_dir(self) -&gt; None:
    options = Options()
    options.namespace_packages = True
    options.explicit_package_bases = True
    options.mypy_path = ["/a", "/b"]

    finder = SourceFinder(FakeFSCache({"/a/pkg/a.py", "/b/pkg/b.py"}), options)
    assert find_sources_in_dir(finder, "/") == [("pkg.a", "/a"), ("pkg.b", "/b")]

</t>
<t tx="ekr.20220525082936.1456">def test_find_sources_exclude(self) -&gt; None:
    options = Options()
    options.namespace_packages = True

    # default
    for excluded_dir in ["site-packages", ".whatever", "node_modules", ".x/.z"]:
        fscache = FakeFSCache({"/dir/a.py", f"/dir/venv/{excluded_dir}/b.py"})
        assert find_sources(["/"], options, fscache) == [("a", "/dir")]
        with pytest.raises(InvalidSourceList):
            find_sources(["/dir/venv/"], options, fscache)
        assert find_sources([f"/dir/venv/{excluded_dir}"], options, fscache) == [
            ("b", f"/dir/venv/{excluded_dir}")
        ]
        assert find_sources([f"/dir/venv/{excluded_dir}/b.py"], options, fscache) == [
            ("b", f"/dir/venv/{excluded_dir}")
        ]

    files = {
        "/pkg/a1/b/c/d/e.py",
        "/pkg/a1/b/f.py",
        "/pkg/a2/__init__.py",
        "/pkg/a2/b/c/d/e.py",
        "/pkg/a2/b/f.py",
    }

    # file name
    options.exclude = [r"/f\.py$"]
    fscache = FakeFSCache(files)
    assert find_sources(["/"], options, fscache) == [
        ("a2", "/pkg"),
        ("a2.b.c.d.e", "/pkg"),
        ("e", "/pkg/a1/b/c/d"),
    ]
    assert find_sources(["/pkg/a1/b/f.py"], options, fscache) == [('f', '/pkg/a1/b')]
    assert find_sources(["/pkg/a2/b/f.py"], options, fscache) == [('a2.b.f', '/pkg')]

    # directory name
    options.exclude = ["/a1/"]
    fscache = FakeFSCache(files)
    assert find_sources(["/"], options, fscache) == [
        ("a2", "/pkg"),
        ("a2.b.c.d.e", "/pkg"),
        ("a2.b.f", "/pkg"),
    ]
    with pytest.raises(InvalidSourceList):
        find_sources(["/pkg/a1"], options, fscache)
    with pytest.raises(InvalidSourceList):
        find_sources(["/pkg/a1/"], options, fscache)
    with pytest.raises(InvalidSourceList):
        find_sources(["/pkg/a1/b"], options, fscache)

    options.exclude = ["/a1/$"]
    assert find_sources(["/pkg/a1"], options, fscache) == [
        ('e', '/pkg/a1/b/c/d'), ('f', '/pkg/a1/b')
    ]

    # paths
    options.exclude = ["/pkg/a1/"]
    fscache = FakeFSCache(files)
    assert find_sources(["/"], options, fscache) == [
        ("a2", "/pkg"),
        ("a2.b.c.d.e", "/pkg"),
        ("a2.b.f", "/pkg"),
    ]
    with pytest.raises(InvalidSourceList):
        find_sources(["/pkg/a1"], options, fscache)

    # OR two patterns together
    for orred in [["/(a1|a3)/"], ["a1", "a3"], ["a3", "a1"]]:
        options.exclude = orred
        fscache = FakeFSCache(files)
        assert find_sources(["/"], options, fscache) == [
            ("a2", "/pkg"),
            ("a2.b.c.d.e", "/pkg"),
            ("a2.b.f", "/pkg"),
        ]

    options.exclude = ["b/c/"]
    fscache = FakeFSCache(files)
    assert find_sources(["/"], options, fscache) == [
        ("a2", "/pkg"),
        ("a2.b.f", "/pkg"),
        ("f", "/pkg/a1/b"),
    ]

    # nothing should be ignored as a result of this
    big_exclude1 = [
        "/pkg/a/", "/2", "/1", "/pk/", "/kg", "/g.py", "/bc", "/xxx/pkg/a2/b/f.py"
        "xxx/pkg/a2/b/f.py",
    ]
    big_exclude2 = ["|".join(big_exclude1)]
    for big_exclude in [big_exclude1, big_exclude2]:
        options.exclude = big_exclude
        fscache = FakeFSCache(files)
        assert len(find_sources(["/"], options, fscache)) == len(files)

        files = {
            "pkg/a1/b/c/d/e.py",
            "pkg/a1/b/f.py",
            "pkg/a2/__init__.py",
            "pkg/a2/b/c/d/e.py",
            "pkg/a2/b/f.py",
        }
        fscache = FakeFSCache(files)
        assert len(find_sources(["."], options, fscache)) == len(files)
</t>
<t tx="ekr.20220525082936.1457">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Fixture used in type-related test cases.

It contains class TypeInfos and Type objects.
"""

from typing import List, Optional, Tuple

from mypy.semanal_shared import set_callable_name
from mypy.types import (
    Type, AnyType, NoneType, Instance, CallableType, TypeVarType, TypeType,
    UninhabitedType, TypeOfAny, TypeAliasType, UnionType, LiteralType,
    TypeVarLikeType
)
from mypy.nodes import (
    TypeInfo, ClassDef, FuncDef, Block, ARG_POS, ARG_OPT, ARG_STAR, SymbolTable,
    COVARIANT, TypeAlias, SymbolTableNode, MDEF,
)


@others
</t>
<t tx="ekr.20220525082936.1458">class TypeFixture:
    """Helper class that is used as a fixture in type-related unit tests.

    The members are initialized to contain various type-related values.
    """

    @others
</t>
<t tx="ekr.20220525082936.1459">def __init__(self, variance: int = COVARIANT) -&gt; None:
    # The 'object' class
    self.oi = self.make_type_info('builtins.object')               # class object
    self.o = Instance(self.oi, [])                        # object

    # Type variables (these are effectively global)

    def make_type_var(name: str, id: int, values: List[Type], upper_bound: Type,
                      variance: int) -&gt; TypeVarType:
        return TypeVarType(name, name, id, values, upper_bound, variance)

    self.t = make_type_var('T', 1, [], self.o, variance)     # T`1 (type variable)
    self.tf = make_type_var('T', -1, [], self.o, variance)   # T`-1 (type variable)
    self.tf2 = make_type_var('T', -2, [], self.o, variance)  # T`-2 (type variable)
    self.s = make_type_var('S', 2, [], self.o, variance)     # S`2 (type variable)
    self.s1 = make_type_var('S', 1, [], self.o, variance)    # S`1 (type variable)
    self.sf = make_type_var('S', -2, [], self.o, variance)   # S`-2 (type variable)
    self.sf1 = make_type_var('S', -1, [], self.o, variance)  # S`-1 (type variable)

    # Simple types
    self.anyt = AnyType(TypeOfAny.special_form)
    self.nonet = NoneType()
    self.uninhabited = UninhabitedType()

    # Abstract class TypeInfos

    # class F
    self.fi = self.make_type_info('F', is_abstract=True)

    # class F2
    self.f2i = self.make_type_info('F2', is_abstract=True)

    # class F3(F)
    self.f3i = self.make_type_info('F3', is_abstract=True, mro=[self.fi])

    # Class TypeInfos
    self.std_tuplei = self.make_type_info('builtins.tuple',
                                          mro=[self.oi],
                                          typevars=['T'],
                                          variances=[COVARIANT])   # class tuple
    self.type_typei = self.make_type_info('builtins.type')         # class type
    self.bool_type_info = self.make_type_info('builtins.bool')
    self.str_type_info = self.make_type_info('builtins.str')
    self.functioni = self.make_type_info('builtins.function')  # function TODO
    self.ai = self.make_type_info('A', mro=[self.oi])              # class A
    self.bi = self.make_type_info('B', mro=[self.ai, self.oi])     # class B(A)
    self.ci = self.make_type_info('C', mro=[self.ai, self.oi])     # class C(A)
    self.di = self.make_type_info('D', mro=[self.oi])              # class D
    # class E(F)
    self.ei = self.make_type_info('E', mro=[self.fi, self.oi])
    # class E2(F2, F)
    self.e2i = self.make_type_info('E2', mro=[self.f2i, self.fi, self.oi])
    # class E3(F, F2)
    self.e3i = self.make_type_info('E3', mro=[self.fi, self.f2i, self.oi])

    # Generic class TypeInfos
    # G[T]
    self.gi = self.make_type_info('G', mro=[self.oi],
                                  typevars=['T'],
                                  variances=[variance])
    # G2[T]
    self.g2i = self.make_type_info('G2', mro=[self.oi],
                                   typevars=['T'],
                                   variances=[variance])
    # H[S, T]
    self.hi = self.make_type_info('H', mro=[self.oi],
                                  typevars=['S', 'T'],
                                  variances=[variance, variance])
    # GS[T, S] &lt;: G[S]
    self.gsi = self.make_type_info('GS', mro=[self.gi, self.oi],
                                   typevars=['T', 'S'],
                                   variances=[variance, variance],
                                   bases=[Instance(self.gi, [self.s])])
    # GS2[S] &lt;: G[S]
    self.gs2i = self.make_type_info('GS2', mro=[self.gi, self.oi],
                                    typevars=['S'],
                                    variances=[variance],
                                    bases=[Instance(self.gi, [self.s1])])
    # list[T]
    self.std_listi = self.make_type_info('builtins.list', mro=[self.oi],
                                         typevars=['T'],
                                         variances=[variance])

    # Instance types
    self.std_tuple = Instance(self.std_tuplei, [self.anyt])        # tuple
    self.type_type = Instance(self.type_typei, [])        # type
    self.function = Instance(self.functioni, [])  # function TODO
    self.str_type = Instance(self.str_type_info, [])
    self.a = Instance(self.ai, [])          # A
    self.b = Instance(self.bi, [])          # B
    self.c = Instance(self.ci, [])          # C
    self.d = Instance(self.di, [])          # D

    self.e = Instance(self.ei, [])          # E
    self.e2 = Instance(self.e2i, [])        # E2
    self.e3 = Instance(self.e3i, [])        # E3

    self.f = Instance(self.fi, [])          # F
    self.f2 = Instance(self.f2i, [])        # F2
    self.f3 = Instance(self.f3i, [])        # F3

    # Generic instance types
    self.ga = Instance(self.gi, [self.a])        # G[A]
    self.gb = Instance(self.gi, [self.b])        # G[B]
    self.gd = Instance(self.gi, [self.d])        # G[D]
    self.go = Instance(self.gi, [self.o])        # G[object]
    self.gt = Instance(self.gi, [self.t])        # G[T`1]
    self.gtf = Instance(self.gi, [self.tf])      # G[T`-1]
    self.gtf2 = Instance(self.gi, [self.tf2])    # G[T`-2]
    self.gs = Instance(self.gi, [self.s])        # G[S]
    self.gdyn = Instance(self.gi, [self.anyt])    # G[Any]
    self.gn = Instance(self.gi, [NoneType()])    # G[None]

    self.g2a = Instance(self.g2i, [self.a])      # G2[A]

    self.gsaa = Instance(self.gsi, [self.a, self.a])  # GS[A, A]
    self.gsab = Instance(self.gsi, [self.a, self.b])  # GS[A, B]
    self.gsba = Instance(self.gsi, [self.b, self.a])  # GS[B, A]

    self.gs2a = Instance(self.gs2i, [self.a])    # GS2[A]
    self.gs2b = Instance(self.gs2i, [self.b])    # GS2[B]
    self.gs2d = Instance(self.gs2i, [self.d])    # GS2[D]

    self.hab = Instance(self.hi, [self.a, self.b])    # H[A, B]
    self.haa = Instance(self.hi, [self.a, self.a])    # H[A, A]
    self.hbb = Instance(self.hi, [self.b, self.b])    # H[B, B]
    self.hts = Instance(self.hi, [self.t, self.s])    # H[T, S]
    self.had = Instance(self.hi, [self.a, self.d])    # H[A, D]
    self.hao = Instance(self.hi, [self.a, self.o])    # H[A, object]

    self.lsta = Instance(self.std_listi, [self.a])  # List[A]
    self.lstb = Instance(self.std_listi, [self.b])  # List[B]

    self.lit1 = LiteralType(1, self.a)
    self.lit2 = LiteralType(2, self.a)
    self.lit3 = LiteralType("foo", self.d)
    self.lit4 = LiteralType(4, self.a)
    self.lit1_inst = Instance(self.ai, [], last_known_value=self.lit1)
    self.lit2_inst = Instance(self.ai, [], last_known_value=self.lit2)
    self.lit3_inst = Instance(self.di, [], last_known_value=self.lit3)
    self.lit4_inst = Instance(self.ai, [], last_known_value=self.lit4)

    self.lit_str1 = LiteralType("x", self.str_type)
    self.lit_str2 = LiteralType("y", self.str_type)
    self.lit_str3 = LiteralType("z", self.str_type)
    self.lit_str1_inst = Instance(self.str_type_info, [], last_known_value=self.lit_str1)
    self.lit_str2_inst = Instance(self.str_type_info, [], last_known_value=self.lit_str2)
    self.lit_str3_inst = Instance(self.str_type_info, [], last_known_value=self.lit_str3)

    self.type_a = TypeType.make_normalized(self.a)
    self.type_b = TypeType.make_normalized(self.b)
    self.type_c = TypeType.make_normalized(self.c)
    self.type_d = TypeType.make_normalized(self.d)
    self.type_t = TypeType.make_normalized(self.t)
    self.type_any = TypeType.make_normalized(self.anyt)

    self._add_bool_dunder(self.bool_type_info)
    self._add_bool_dunder(self.ai)

</t>
<t tx="ekr.20220525082936.146">@abstractmethod
def visit_str_expr(self, o: 'mypy.nodes.StrExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.1460">def _add_bool_dunder(self, type_info: TypeInfo) -&gt; None:
    signature = CallableType([], [], [], Instance(self.bool_type_info, []), self.function)
    bool_func = FuncDef('__bool__', [], Block([]))
    bool_func.type = set_callable_name(signature, bool_func)
    type_info.names[bool_func.name] = SymbolTableNode(MDEF, bool_func)

</t>
<t tx="ekr.20220525082936.1461"># Helper methods

</t>
<t tx="ekr.20220525082936.1462">def callable(self, *a: Type) -&gt; CallableType:
    """callable(a1, ..., an, r) constructs a callable with argument types
    a1, ... an and return type r.
    """
    return CallableType(list(a[:-1]), [ARG_POS] * (len(a) - 1),
                    [None] * (len(a) - 1), a[-1], self.function)

</t>
<t tx="ekr.20220525082936.1463">def callable_type(self, *a: Type) -&gt; CallableType:
    """callable_type(a1, ..., an, r) constructs a callable with
    argument types a1, ... an and return type r, and which
    represents a type.
    """
    return CallableType(list(a[:-1]), [ARG_POS] * (len(a) - 1),
                    [None] * (len(a) - 1), a[-1], self.type_type)

</t>
<t tx="ekr.20220525082936.1464">def callable_default(self, min_args: int, *a: Type) -&gt; CallableType:
    """callable_default(min_args, a1, ..., an, r) constructs a
    callable with argument types a1, ... an and return type r,
    with min_args mandatory fixed arguments.
    """
    n = len(a) - 1
    return CallableType(list(a[:-1]),
                        [ARG_POS] * min_args + [ARG_OPT] * (n - min_args),
                        [None] * n,
                        a[-1], self.function)

</t>
<t tx="ekr.20220525082936.1465">def callable_var_arg(self, min_args: int, *a: Type) -&gt; CallableType:
    """callable_var_arg(min_args, a1, ..., an, r) constructs a callable
    with argument types a1, ... *an and return type r.
    """
    n = len(a) - 1
    return CallableType(list(a[:-1]),
                        [ARG_POS] * min_args +
                        [ARG_OPT] * (n - 1 - min_args) +
                        [ARG_STAR], [None] * n,
                        a[-1], self.function)

</t>
<t tx="ekr.20220525082936.1466">def make_type_info(self, name: str,
                   module_name: Optional[str] = None,
                   is_abstract: bool = False,
                   mro: Optional[List[TypeInfo]] = None,
                   bases: Optional[List[Instance]] = None,
                   typevars: Optional[List[str]] = None,
                   variances: Optional[List[int]] = None) -&gt; TypeInfo:
    """Make a TypeInfo suitable for use in unit tests."""

    class_def = ClassDef(name, Block([]), None, [])
    class_def.fullname = name

    if module_name is None:
        if '.' in name:
            module_name = name.rsplit('.', 1)[0]
        else:
            module_name = '__main__'

    if typevars:
        v: List[TypeVarLikeType] = []
        for id, n in enumerate(typevars, 1):
            if variances:
                variance = variances[id - 1]
            else:
                variance = COVARIANT
            v.append(TypeVarType(n, n, id, [], self.o, variance=variance))
        class_def.type_vars = v

    info = TypeInfo(SymbolTable(), class_def, module_name)
    if mro is None:
        mro = []
        if name != 'builtins.object':
            mro.append(self.oi)
    info.mro = [info] + mro
    if bases is None:
        if mro:
            # By default, assume that there is a single non-generic base.
            bases = [Instance(mro[0], [])]
        else:
            bases = []
    info.bases = bases

    return info

</t>
<t tx="ekr.20220525082936.1467">def def_alias_1(self, base: Instance) -&gt; Tuple[TypeAliasType, Type]:
    A = TypeAliasType(None, [])
    target = Instance(self.std_tuplei,
                      [UnionType([base, A])])  # A = Tuple[Union[base, A], ...]
    AN = TypeAlias(target, '__main__.A', -1, -1)
    A.alias = AN
    return A, target

</t>
<t tx="ekr.20220525082936.1468">def def_alias_2(self, base: Instance) -&gt; Tuple[TypeAliasType, Type]:
    A = TypeAliasType(None, [])
    target = UnionType([base,
                        Instance(self.std_tuplei, [A])])  # A = Union[base, Tuple[A, ...]]
    AN = TypeAlias(target, '__main__.A', -1, -1)
    A.alias = AN
    return A, target

</t>
<t tx="ekr.20220525082936.1469">def non_rec_alias(self, target: Type) -&gt; TypeAliasType:
    AN = TypeAlias(target, '__main__.A', -1, -1)
    return TypeAliasType(AN, [])


</t>
<t tx="ekr.20220525082936.147">@abstractmethod
def visit_bytes_expr(self, o: 'mypy.nodes.BytesExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.1470">class InterfaceTypeFixture(TypeFixture):
    """Extension of TypeFixture that contains additional generic
    interface types."""

    @others
</t>
<t tx="ekr.20220525082936.1471">def __init__(self) -&gt; None:
    super().__init__()
    # GF[T]
    self.gfi = self.make_type_info('GF', typevars=['T'], is_abstract=True)

    # M1 &lt;: GF[A]
    self.m1i = self.make_type_info('M1',
                                   is_abstract=True,
                                   mro=[self.gfi, self.oi],
                                   bases=[Instance(self.gfi, [self.a])])

    self.gfa = Instance(self.gfi, [self.a])  # GF[A]
    self.gfb = Instance(self.gfi, [self.b])  # GF[B]

    self.m1 = Instance(self.m1i, [])  # M1
</t>
<t tx="ekr.20220525082936.1472"></t>
<t tx="ekr.20220525082936.1473">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Visitor classes pulled out from different tests

These are here because we don't currently support having interpreted
classes subtype compiled ones but pytest grabs the python file
even if the test was compiled.

"""

from typing import Set

from mypy.nodes import (
    NameExpr, TypeVarExpr, CallExpr, Expression, MypyFile, AssignmentStmt, IntExpr
)
from mypy.traverser import TraverserVisitor

from mypy.treetransform import TransformVisitor
from mypy.types import Type


@others
</t>
<t tx="ekr.20220525082936.1474"># from testtypegen
class SkippedNodeSearcher(TraverserVisitor):
    @others
</t>
<t tx="ekr.20220525082936.1475">def __init__(self) -&gt; None:
    self.nodes: Set[Expression] = set()
    self.is_typing = False

</t>
<t tx="ekr.20220525082936.1476">def visit_mypy_file(self, f: MypyFile) -&gt; None:
    self.is_typing = f.fullname == 'typing' or f.fullname == 'builtins'
    super().visit_mypy_file(f)

</t>
<t tx="ekr.20220525082936.1477">def visit_assignment_stmt(self, s: AssignmentStmt) -&gt; None:
    if s.type or ignore_node(s.rvalue):
        for lvalue in s.lvalues:
            if isinstance(lvalue, NameExpr):
                self.nodes.add(lvalue)
    super().visit_assignment_stmt(s)

</t>
<t tx="ekr.20220525082936.1478">def visit_name_expr(self, n: NameExpr) -&gt; None:
    self.skip_if_typing(n)

</t>
<t tx="ekr.20220525082936.1479">def visit_int_expr(self, n: IntExpr) -&gt; None:
    self.skip_if_typing(n)

</t>
<t tx="ekr.20220525082936.148">@abstractmethod
def visit_unicode_expr(self, o: 'mypy.nodes.UnicodeExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.1480">def skip_if_typing(self, n: Expression) -&gt; None:
    if self.is_typing:
        self.nodes.add(n)


</t>
<t tx="ekr.20220525082936.1481">def ignore_node(node: Expression) -&gt; bool:
    """Return True if node is to be omitted from test case output."""

    # We want to get rid of object() expressions in the typing module stub
    # and also TypeVar(...) expressions. Since detecting whether a node comes
    # from the typing module is not easy, we just to strip them all away.
    if isinstance(node, TypeVarExpr):
        return True
    if isinstance(node, NameExpr) and node.fullname == 'builtins.object':
        return True
    if isinstance(node, NameExpr) and node.fullname == 'builtins.None':
        return True
    if isinstance(node, CallExpr) and (ignore_node(node.callee) or
                                       node.analyzed):
        return True

    return False


</t>
<t tx="ekr.20220525082936.1482"># from testtransform
class TypeAssertTransformVisitor(TransformVisitor):
    def type(self, type: Type) -&gt; Type:
        assert type is not None
        return type
</t>
<t tx="ekr.20220525082936.1484"></t>
<t tx="ekr.20220525082936.1485"></t>
<t tx="ekr.20220525082936.1486"></t>
<t tx="ekr.20220525082936.149">@abstractmethod
def visit_float_expr(self, o: 'mypy.nodes.FloatExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.1491"></t>
<t tx="ekr.20220525082936.1493"></t>
<t tx="ekr.20220525082936.1497"></t>
<t tx="ekr.20220525082936.15">@abstractmethod
def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.150">@abstractmethod
def visit_complex_expr(self, o: 'mypy.nodes.ComplexExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.1501"></t>
<t tx="ekr.20220525082936.1508"></t>
<t tx="ekr.20220525082936.151">@abstractmethod
def visit_ellipsis(self, o: 'mypy.nodes.EllipsisExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.1511"></t>
<t tx="ekr.20220525082936.1517"></t>
<t tx="ekr.20220525082936.152">@abstractmethod
def visit_star_expr(self, o: 'mypy.nodes.StarExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.1522"></t>
<t tx="ekr.20220525082936.1524"></t>
<t tx="ekr.20220525082936.153">@abstractmethod
def visit_name_expr(self, o: 'mypy.nodes.NameExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.1530"></t>
<t tx="ekr.20220525082936.1533"></t>
<t tx="ekr.20220525082936.1537"></t>
<t tx="ekr.20220525082936.154">@abstractmethod
def visit_member_expr(self, o: 'mypy.nodes.MemberExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.1548"></t>
<t tx="ekr.20220525082936.155">@abstractmethod
def visit_yield_from_expr(self, o: 'mypy.nodes.YieldFromExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.1551"></t>
<t tx="ekr.20220525082936.1556"></t>
<t tx="ekr.20220525082936.1557"></t>
<t tx="ekr.20220525082936.156">@abstractmethod
def visit_yield_expr(self, o: 'mypy.nodes.YieldExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.157">@abstractmethod
def visit_call_expr(self, o: 'mypy.nodes.CallExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.158">@abstractmethod
def visit_op_expr(self, o: 'mypy.nodes.OpExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.159">@abstractmethod
def visit_comparison_expr(self, o: 'mypy.nodes.ComparisonExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.16">@abstractmethod
def visit_instance(self, t: Instance) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.160">@abstractmethod
def visit_cast_expr(self, o: 'mypy.nodes.CastExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.161">@abstractmethod
def visit_assert_type_expr(self, o: 'mypy.nodes.AssertTypeExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.162">@abstractmethod
def visit_reveal_expr(self, o: 'mypy.nodes.RevealExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.163">@abstractmethod
def visit_super_expr(self, o: 'mypy.nodes.SuperExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.164">@abstractmethod
def visit_unary_expr(self, o: 'mypy.nodes.UnaryExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.165">@abstractmethod
def visit_assignment_expr(self, o: 'mypy.nodes.AssignmentExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.166">@abstractmethod
def visit_list_expr(self, o: 'mypy.nodes.ListExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.167">@abstractmethod
def visit_dict_expr(self, o: 'mypy.nodes.DictExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.168">@abstractmethod
def visit_tuple_expr(self, o: 'mypy.nodes.TupleExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.169">@abstractmethod
def visit_set_expr(self, o: 'mypy.nodes.SetExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.17">@abstractmethod
def visit_callable_type(self, t: CallableType) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.170">@abstractmethod
def visit_index_expr(self, o: 'mypy.nodes.IndexExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.171">@abstractmethod
def visit_type_application(self, o: 'mypy.nodes.TypeApplication') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.172">@abstractmethod
def visit_lambda_expr(self, o: 'mypy.nodes.LambdaExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.173">@abstractmethod
def visit_list_comprehension(self, o: 'mypy.nodes.ListComprehension') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.174">@abstractmethod
def visit_set_comprehension(self, o: 'mypy.nodes.SetComprehension') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.175">@abstractmethod
def visit_dictionary_comprehension(self, o: 'mypy.nodes.DictionaryComprehension') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.176">@abstractmethod
def visit_generator_expr(self, o: 'mypy.nodes.GeneratorExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.177">@abstractmethod
def visit_slice_expr(self, o: 'mypy.nodes.SliceExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.178">@abstractmethod
def visit_conditional_expr(self, o: 'mypy.nodes.ConditionalExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.179">@abstractmethod
def visit_backquote_expr(self, o: 'mypy.nodes.BackquoteExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.18">@abstractmethod
def visit_overloaded(self, t: Overloaded) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.180">@abstractmethod
def visit_type_var_expr(self, o: 'mypy.nodes.TypeVarExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.181">@abstractmethod
def visit_paramspec_expr(self, o: 'mypy.nodes.ParamSpecExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.182">@abstractmethod
def visit_type_var_tuple_expr(self, o: 'mypy.nodes.TypeVarTupleExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.183">@abstractmethod
def visit_type_alias_expr(self, o: 'mypy.nodes.TypeAliasExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.184">@abstractmethod
def visit_namedtuple_expr(self, o: 'mypy.nodes.NamedTupleExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.185">@abstractmethod
def visit_enum_call_expr(self, o: 'mypy.nodes.EnumCallExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.186">@abstractmethod
def visit_typeddict_expr(self, o: 'mypy.nodes.TypedDictExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.187">@abstractmethod
def visit_newtype_expr(self, o: 'mypy.nodes.NewTypeExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.188">@abstractmethod
def visit__promote_expr(self, o: 'mypy.nodes.PromoteExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.189">@abstractmethod
def visit_await_expr(self, o: 'mypy.nodes.AwaitExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.19">@abstractmethod
def visit_tuple_type(self, t: TupleType) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.190">@abstractmethod
def visit_temp_node(self, o: 'mypy.nodes.TempNode') -&gt; T:
    pass


</t>
<t tx="ekr.20220525082936.191">@trait
@mypyc_attr(allow_interpreted_subclasses=True)
class StatementVisitor(Generic[T]):
    # Definitions

    @others
</t>
<t tx="ekr.20220525082936.192">@abstractmethod
def visit_assignment_stmt(self, o: 'mypy.nodes.AssignmentStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.193">@abstractmethod
def visit_for_stmt(self, o: 'mypy.nodes.ForStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.194">@abstractmethod
def visit_with_stmt(self, o: 'mypy.nodes.WithStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.195">@abstractmethod
def visit_del_stmt(self, o: 'mypy.nodes.DelStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.196">@abstractmethod
def visit_func_def(self, o: 'mypy.nodes.FuncDef') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.197">@abstractmethod
def visit_overloaded_func_def(self, o: 'mypy.nodes.OverloadedFuncDef') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.198">@abstractmethod
def visit_class_def(self, o: 'mypy.nodes.ClassDef') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.199">@abstractmethod
def visit_global_decl(self, o: 'mypy.nodes.GlobalDecl') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.2">def fill_typevars_with_any(typ: TypeInfo) -&gt; Union[Instance, TupleType]:
    """Apply a correct number of Any's as type arguments to a type."""
    inst = Instance(typ, [AnyType(TypeOfAny.special_form)] * len(typ.defn.type_vars))
    if typ.tuple_type is None:
        return inst
    return typ.tuple_type.copy_modified(fallback=inst)


</t>
<t tx="ekr.20220525082936.20">@abstractmethod
def visit_typeddict_type(self, t: TypedDictType) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.200">@abstractmethod
def visit_nonlocal_decl(self, o: 'mypy.nodes.NonlocalDecl') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.201">@abstractmethod
def visit_decorator(self, o: 'mypy.nodes.Decorator') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.202"># Module structure

</t>
<t tx="ekr.20220525082936.203">@abstractmethod
def visit_import(self, o: 'mypy.nodes.Import') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.204">@abstractmethod
def visit_import_from(self, o: 'mypy.nodes.ImportFrom') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.205">@abstractmethod
def visit_import_all(self, o: 'mypy.nodes.ImportAll') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.206"># Statements

</t>
<t tx="ekr.20220525082936.207">@abstractmethod
def visit_block(self, o: 'mypy.nodes.Block') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.208">@abstractmethod
def visit_expression_stmt(self, o: 'mypy.nodes.ExpressionStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.209">@abstractmethod
def visit_operator_assignment_stmt(self, o: 'mypy.nodes.OperatorAssignmentStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.21">@abstractmethod
def visit_literal_type(self, t: LiteralType) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.210">@abstractmethod
def visit_while_stmt(self, o: 'mypy.nodes.WhileStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.211">@abstractmethod
def visit_return_stmt(self, o: 'mypy.nodes.ReturnStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.212">@abstractmethod
def visit_assert_stmt(self, o: 'mypy.nodes.AssertStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.213">@abstractmethod
def visit_if_stmt(self, o: 'mypy.nodes.IfStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.214">@abstractmethod
def visit_break_stmt(self, o: 'mypy.nodes.BreakStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.215">@abstractmethod
def visit_continue_stmt(self, o: 'mypy.nodes.ContinueStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.216">@abstractmethod
def visit_pass_stmt(self, o: 'mypy.nodes.PassStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.217">@abstractmethod
def visit_raise_stmt(self, o: 'mypy.nodes.RaiseStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.218">@abstractmethod
def visit_try_stmt(self, o: 'mypy.nodes.TryStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.219">@abstractmethod
def visit_print_stmt(self, o: 'mypy.nodes.PrintStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.22">@abstractmethod
def visit_union_type(self, t: UnionType) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.220">@abstractmethod
def visit_exec_stmt(self, o: 'mypy.nodes.ExecStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.221">@abstractmethod
def visit_match_stmt(self, o: 'mypy.nodes.MatchStmt') -&gt; T:
    pass


</t>
<t tx="ekr.20220525082936.222">@trait
@mypyc_attr(allow_interpreted_subclasses=True)
class PatternVisitor(Generic[T]):
    @others
</t>
<t tx="ekr.20220525082936.223">@abstractmethod
def visit_as_pattern(self, o: 'mypy.patterns.AsPattern') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.224">@abstractmethod
def visit_or_pattern(self, o: 'mypy.patterns.OrPattern') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.225">@abstractmethod
def visit_value_pattern(self, o: 'mypy.patterns.ValuePattern') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.226">@abstractmethod
def visit_singleton_pattern(self, o: 'mypy.patterns.SingletonPattern') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.227">@abstractmethod
def visit_sequence_pattern(self, o: 'mypy.patterns.SequencePattern') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.228">@abstractmethod
def visit_starred_pattern(self, o: 'mypy.patterns.StarredPattern') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.229">@abstractmethod
def visit_mapping_pattern(self, o: 'mypy.patterns.MappingPattern') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.23">@abstractmethod
def visit_partial_type(self, t: PartialType) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.230">@abstractmethod
def visit_class_pattern(self, o: 'mypy.patterns.ClassPattern') -&gt; T:
    pass


</t>
<t tx="ekr.20220525082936.231">@trait
@mypyc_attr(allow_interpreted_subclasses=True)
class NodeVisitor(Generic[T], ExpressionVisitor[T], StatementVisitor[T], PatternVisitor[T]):
    """Empty base class for parse tree node visitors.

    The T type argument specifies the return type of the visit
    methods. As all methods defined here return None by default,
    subclasses do not always need to override all the methods.

    TODO make the default return value explicit
    """

    # Not in superclasses:

    @others
</t>
<t tx="ekr.20220525082936.232">def visit_mypy_file(self, o: 'mypy.nodes.MypyFile') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.233"># TODO: We have a visit_var method, but no visit_typeinfo or any
# other non-Statement SymbolNode (accepting those will raise a
# runtime error). Maybe this should be resolved in some direction.
def visit_var(self, o: 'mypy.nodes.Var') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.234"># Module structure

</t>
<t tx="ekr.20220525082936.235">def visit_import(self, o: 'mypy.nodes.Import') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.236">def visit_import_from(self, o: 'mypy.nodes.ImportFrom') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.237">def visit_import_all(self, o: 'mypy.nodes.ImportAll') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.238"># Definitions

</t>
<t tx="ekr.20220525082936.239">def visit_func_def(self, o: 'mypy.nodes.FuncDef') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.24">@abstractmethod
def visit_type_type(self, t: TypeType) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.240">def visit_overloaded_func_def(self,
                              o: 'mypy.nodes.OverloadedFuncDef') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.241">def visit_class_def(self, o: 'mypy.nodes.ClassDef') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.242">def visit_global_decl(self, o: 'mypy.nodes.GlobalDecl') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.243">def visit_nonlocal_decl(self, o: 'mypy.nodes.NonlocalDecl') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.244">def visit_decorator(self, o: 'mypy.nodes.Decorator') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.245">def visit_type_alias(self, o: 'mypy.nodes.TypeAlias') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.246">def visit_placeholder_node(self, o: 'mypy.nodes.PlaceholderNode') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.247"># Statements

</t>
<t tx="ekr.20220525082936.248">def visit_block(self, o: 'mypy.nodes.Block') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.249">def visit_expression_stmt(self, o: 'mypy.nodes.ExpressionStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.25">@abstractmethod
def visit_type_alias_type(self, t: TypeAliasType) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.250">def visit_assignment_stmt(self, o: 'mypy.nodes.AssignmentStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.251">def visit_operator_assignment_stmt(self,
                                   o: 'mypy.nodes.OperatorAssignmentStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.252">def visit_while_stmt(self, o: 'mypy.nodes.WhileStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.253">def visit_for_stmt(self, o: 'mypy.nodes.ForStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.254">def visit_return_stmt(self, o: 'mypy.nodes.ReturnStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.255">def visit_assert_stmt(self, o: 'mypy.nodes.AssertStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.256">def visit_del_stmt(self, o: 'mypy.nodes.DelStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.257">def visit_if_stmt(self, o: 'mypy.nodes.IfStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.258">def visit_break_stmt(self, o: 'mypy.nodes.BreakStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.259">def visit_continue_stmt(self, o: 'mypy.nodes.ContinueStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.26">@abstractmethod
def visit_unpack_type(self, t: UnpackType) -&gt; T:
    pass


</t>
<t tx="ekr.20220525082936.260">def visit_pass_stmt(self, o: 'mypy.nodes.PassStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.261">def visit_raise_stmt(self, o: 'mypy.nodes.RaiseStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.262">def visit_try_stmt(self, o: 'mypy.nodes.TryStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.263">def visit_with_stmt(self, o: 'mypy.nodes.WithStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.264">def visit_print_stmt(self, o: 'mypy.nodes.PrintStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.265">def visit_exec_stmt(self, o: 'mypy.nodes.ExecStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.266">def visit_match_stmt(self, o: 'mypy.nodes.MatchStmt') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.267"># Expressions (default no-op implementation)

</t>
<t tx="ekr.20220525082936.268">def visit_int_expr(self, o: 'mypy.nodes.IntExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.269">def visit_str_expr(self, o: 'mypy.nodes.StrExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.27">@trait
@mypyc_attr(allow_interpreted_subclasses=True)
class SyntheticTypeVisitor(TypeVisitor[T]):
    """A TypeVisitor that also knows how to visit synthetic AST constructs.

       Not just real types."""

    @others
</t>
<t tx="ekr.20220525082936.270">def visit_bytes_expr(self, o: 'mypy.nodes.BytesExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.271">def visit_unicode_expr(self, o: 'mypy.nodes.UnicodeExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.272">def visit_float_expr(self, o: 'mypy.nodes.FloatExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.273">def visit_complex_expr(self, o: 'mypy.nodes.ComplexExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.274">def visit_ellipsis(self, o: 'mypy.nodes.EllipsisExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.275">def visit_star_expr(self, o: 'mypy.nodes.StarExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.276">def visit_name_expr(self, o: 'mypy.nodes.NameExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.277">def visit_member_expr(self, o: 'mypy.nodes.MemberExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.278">def visit_yield_from_expr(self, o: 'mypy.nodes.YieldFromExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.279">def visit_yield_expr(self, o: 'mypy.nodes.YieldExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.28">@abstractmethod
def visit_star_type(self, t: StarType) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.280">def visit_call_expr(self, o: 'mypy.nodes.CallExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.281">def visit_op_expr(self, o: 'mypy.nodes.OpExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.282">def visit_comparison_expr(self, o: 'mypy.nodes.ComparisonExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.283">def visit_cast_expr(self, o: 'mypy.nodes.CastExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.284">def visit_assert_type_expr(self, o: 'mypy.nodes.AssertTypeExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.285">def visit_reveal_expr(self, o: 'mypy.nodes.RevealExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.286">def visit_super_expr(self, o: 'mypy.nodes.SuperExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.287">def visit_assignment_expr(self, o: 'mypy.nodes.AssignmentExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.288">def visit_unary_expr(self, o: 'mypy.nodes.UnaryExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.289">def visit_list_expr(self, o: 'mypy.nodes.ListExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.29">@abstractmethod
def visit_type_list(self, t: TypeList) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.290">def visit_dict_expr(self, o: 'mypy.nodes.DictExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.291">def visit_tuple_expr(self, o: 'mypy.nodes.TupleExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.292">def visit_set_expr(self, o: 'mypy.nodes.SetExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.293">def visit_index_expr(self, o: 'mypy.nodes.IndexExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.294">def visit_type_application(self, o: 'mypy.nodes.TypeApplication') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.295">def visit_lambda_expr(self, o: 'mypy.nodes.LambdaExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.296">def visit_list_comprehension(self, o: 'mypy.nodes.ListComprehension') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.297">def visit_set_comprehension(self, o: 'mypy.nodes.SetComprehension') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.298">def visit_dictionary_comprehension(self, o: 'mypy.nodes.DictionaryComprehension') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.299">def visit_generator_expr(self, o: 'mypy.nodes.GeneratorExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.3">def has_no_typevars(typ: Type) -&gt; bool:
    # We test if a type contains type variables by erasing all type variables
    # and comparing the result to the original type. We use comparison by equality that
    # in turn uses `__eq__` defined for types. Note: we can't use `is_same_type` because
    # it is not safe with unresolved forward references, while this function may be called
    # before forward references resolution patch pass. Note also that it is not safe to use
    # `is` comparison because `erase_typevars` doesn't preserve type identity.
    return typ == erase_typevars(typ)
</t>
<t tx="ekr.20220525082936.30">@abstractmethod
def visit_callable_argument(self, t: CallableArgument) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.300">def visit_slice_expr(self, o: 'mypy.nodes.SliceExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.301">def visit_conditional_expr(self, o: 'mypy.nodes.ConditionalExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.302">def visit_backquote_expr(self, o: 'mypy.nodes.BackquoteExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.303">def visit_type_var_expr(self, o: 'mypy.nodes.TypeVarExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.304">def visit_paramspec_expr(self, o: 'mypy.nodes.ParamSpecExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.305">def visit_type_var_tuple_expr(self, o: 'mypy.nodes.TypeVarTupleExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.306">def visit_type_alias_expr(self, o: 'mypy.nodes.TypeAliasExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.307">def visit_namedtuple_expr(self, o: 'mypy.nodes.NamedTupleExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.308">def visit_enum_call_expr(self, o: 'mypy.nodes.EnumCallExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.309">def visit_typeddict_expr(self, o: 'mypy.nodes.TypedDictExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.31">@abstractmethod
def visit_ellipsis_type(self, t: EllipsisType) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.310">def visit_newtype_expr(self, o: 'mypy.nodes.NewTypeExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.311">def visit__promote_expr(self, o: 'mypy.nodes.PromoteExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.312">def visit_await_expr(self, o: 'mypy.nodes.AwaitExpr') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.313">def visit_temp_node(self, o: 'mypy.nodes.TempNode') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.314"># Patterns

</t>
<t tx="ekr.20220525082936.315">def visit_as_pattern(self, o: 'mypy.patterns.AsPattern') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.316">def visit_or_pattern(self, o: 'mypy.patterns.OrPattern') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.317">def visit_value_pattern(self, o: 'mypy.patterns.ValuePattern') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.318">def visit_singleton_pattern(self, o: 'mypy.patterns.SingletonPattern') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.319">def visit_sequence_pattern(self, o: 'mypy.patterns.SequencePattern') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.32">@abstractmethod
def visit_raw_expression_type(self, t: RawExpressionType) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.320">def visit_starred_pattern(self, o: 'mypy.patterns.StarredPattern') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.321">def visit_mapping_pattern(self, o: 'mypy.patterns.MappingPattern') -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.322">def visit_class_pattern(self, o: 'mypy.patterns.ClassPattern') -&gt; T:
    pass
</t>
<t tx="ekr.20220525082936.323">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
# This page intentionally left blank
</t>
<t tx="ekr.20220525082936.324">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Mypy type checker command line tool."""
import os
import sys
import traceback

from mypy.main import main, process_options
from mypy.util import FancyFormatter


@others
if __name__ == '__main__':
    console_entry()
</t>
<t tx="ekr.20220525082936.325">def console_entry() -&gt; None:
    try:
        main(None, sys.stdout, sys.stderr)
        sys.stdout.flush()
        sys.stderr.flush()
    except BrokenPipeError:
        # Python flushes standard streams on exit; redirect remaining output
        # to devnull to avoid another BrokenPipeError at shutdown
        devnull = os.open(os.devnull, os.O_WRONLY)
        os.dup2(devnull, sys.stdout.fileno())
        sys.exit(2)
    except KeyboardInterrupt:
        _, options = process_options(args=sys.argv[1:])
        if options.show_traceback:
            sys.stdout.write(traceback.format_exc())
        formatter = FancyFormatter(sys.stdout, sys.stderr, False)
        msg = "Interrupted\n"
        sys.stdout.write(formatter.style(msg, color="red", bold=True))
        sys.stdout.flush()
        sys.stderr.flush()
        sys.exit(2)


</t>
<t tx="ekr.20220525082936.327"></t>
<t tx="ekr.20220525082936.328">@path C:/Repos/mypy/mypy/dmypy/
@language python
@tabwidth -4
"""Client for mypy daemon mode.

This manages a daemon process which keeps useful state in memory
rather than having to read it back from disk on each run.
"""

import argparse
import base64
import json
import os
import pickle
import sys
import time
import traceback

from typing import Any, Callable, Dict, Mapping, Optional, Tuple, List
from typing_extensions import NoReturn

from mypy.dmypy_util import DEFAULT_STATUS_FILE, receive
from mypy.ipc import IPCClient, IPCException
from mypy.dmypy_os import alive, kill
from mypy.util import check_python_version, get_terminal_width

from mypy.version import __version__

# Argument parser.  Subparsers are tied to action functions by the
# @action(subparse) decorator.


@others
</t>
<t tx="ekr.20220525082936.329">class AugmentedHelpFormatter(argparse.RawDescriptionHelpFormatter):
    def __init__(self, prog: str) -&gt; None:
        super().__init__(prog=prog, max_help_position=30)


</t>
<t tx="ekr.20220525082936.33">@abstractmethod
def visit_placeholder_type(self, t: PlaceholderType) -&gt; T:
    pass


</t>
<t tx="ekr.20220525082936.330">parser = argparse.ArgumentParser(prog='dmypy',
                                 description="Client for mypy daemon mode",
                                 fromfile_prefix_chars='@')
parser.set_defaults(action=None)
parser.add_argument('--status-file', default=DEFAULT_STATUS_FILE,
                    help='status file to retrieve daemon details')
parser.add_argument('-V', '--version', action='version',
                    version='%(prog)s ' + __version__,
                    help="Show program's version number and exit")
subparsers = parser.add_subparsers()

start_parser = p = subparsers.add_parser('start', help="Start daemon")
p.add_argument('--log-file', metavar='FILE', type=str,
               help="Direct daemon stdout/stderr to FILE")
p.add_argument('--timeout', metavar='TIMEOUT', type=int,
               help="Server shutdown timeout (in seconds)")
p.add_argument('flags', metavar='FLAG', nargs='*', type=str,
               help="Regular mypy flags (precede with --)")

restart_parser = p = subparsers.add_parser('restart',
    help="Restart daemon (stop or kill followed by start)")
p.add_argument('--log-file', metavar='FILE', type=str,
               help="Direct daemon stdout/stderr to FILE")
p.add_argument('--timeout', metavar='TIMEOUT', type=int,
               help="Server shutdown timeout (in seconds)")
p.add_argument('flags', metavar='FLAG', nargs='*', type=str,
               help="Regular mypy flags (precede with --)")

status_parser = p = subparsers.add_parser('status', help="Show daemon status")
p.add_argument('-v', '--verbose', action='store_true', help="Print detailed status")
p.add_argument('--fswatcher-dump-file', help="Collect information about the current file state")

stop_parser = p = subparsers.add_parser('stop', help="Stop daemon (asks it politely to go away)")

kill_parser = p = subparsers.add_parser('kill', help="Kill daemon (kills the process)")

check_parser = p = subparsers.add_parser('check', formatter_class=AugmentedHelpFormatter,
                                         help="Check some files (requires daemon)")
p.add_argument('-v', '--verbose', action='store_true', help="Print detailed status")
p.add_argument('-q', '--quiet', action='store_true', help=argparse.SUPPRESS)  # Deprecated
p.add_argument('--junit-xml', help="Write junit.xml to the given file")
p.add_argument('--perf-stats-file', help='write performance information to the given file')
p.add_argument('files', metavar='FILE', nargs='+', help="File (or directory) to check")

run_parser = p = subparsers.add_parser('run', formatter_class=AugmentedHelpFormatter,
                                       help="Check some files, [re]starting daemon if necessary")
p.add_argument('-v', '--verbose', action='store_true', help="Print detailed status")
p.add_argument('--junit-xml', help="Write junit.xml to the given file")
p.add_argument('--perf-stats-file', help='write performance information to the given file')
p.add_argument('--timeout', metavar='TIMEOUT', type=int,
               help="Server shutdown timeout (in seconds)")
p.add_argument('--log-file', metavar='FILE', type=str,
               help="Direct daemon stdout/stderr to FILE")
p.add_argument('flags', metavar='ARG', nargs='*', type=str,
               help="Regular mypy flags and files (precede with --)")

recheck_parser = p = subparsers.add_parser('recheck', formatter_class=AugmentedHelpFormatter,
    help="Re-check the previous list of files, with optional modifications (requires daemon)")
p.add_argument('-v', '--verbose', action='store_true', help="Print detailed status")
p.add_argument('-q', '--quiet', action='store_true', help=argparse.SUPPRESS)  # Deprecated
p.add_argument('--junit-xml', help="Write junit.xml to the given file")
p.add_argument('--perf-stats-file', help='write performance information to the given file')
p.add_argument('--update', metavar='FILE', nargs='*',
               help="Files in the run to add or check again (default: all from previous run)")
p.add_argument('--remove', metavar='FILE', nargs='*',
               help="Files to remove from the run")

suggest_parser = p = subparsers.add_parser('suggest',
    help="Suggest a signature or show call sites for a specific function")
p.add_argument('function', metavar='FUNCTION', type=str,
               help="Function specified as '[package.]module.[class.]function'")
p.add_argument('--json', action='store_true',
               help="Produce json that pyannotate can use to apply a suggestion")
p.add_argument('--no-errors', action='store_true',
               help="Only produce suggestions that cause no errors")
p.add_argument('--no-any', action='store_true',
               help="Only produce suggestions that don't contain Any")
p.add_argument('--flex-any', type=float,
               help="Allow anys in types if they go above a certain score (scores are from 0-1)")
p.add_argument('--try-text', action='store_true',
               help="Try using unicode wherever str is inferred")
p.add_argument('--callsites', action='store_true',
               help="Find callsites instead of suggesting a type")
p.add_argument('--use-fixme', metavar='NAME', type=str,
               help="A dummy name to use instead of Any for types that can't be inferred")
p.add_argument('--max-guesses', type=int,
               help="Set the maximum number of types to try for a function (default 64)")

hang_parser = p = subparsers.add_parser('hang', help="Hang for 100 seconds")

daemon_parser = p = subparsers.add_parser('daemon', help="Run daemon in foreground")
p.add_argument('--timeout', metavar='TIMEOUT', type=int,
               help="Server shutdown timeout (in seconds)")
p.add_argument('flags', metavar='FLAG', nargs='*', type=str,
               help="Regular mypy flags (precede with --)")
p.add_argument('--options-data', help=argparse.SUPPRESS)
help_parser = p = subparsers.add_parser('help')

del p


</t>
<t tx="ekr.20220525082936.331">class BadStatus(Exception):
    """Exception raised when there is something wrong with the status file.

    For example:
    - No status file found
    - Status file malformed
    - Process whose pid is in the status file does not exist
    """
    pass


</t>
<t tx="ekr.20220525082936.332">def main(argv: List[str]) -&gt; None:
    """The code is top-down."""
    check_python_version('dmypy')
    args = parser.parse_args(argv)
    if not args.action:
        parser.print_usage()
    else:
        try:
            args.action(args)
        except BadStatus as err:
            fail(err.args[0])
        except Exception:
            # We do this explicitly to avoid exceptions percolating up
            # through mypy.api invocations
            traceback.print_exc()
            sys.exit(2)


</t>
<t tx="ekr.20220525082936.333">def fail(msg: str) -&gt; NoReturn:
    print(msg, file=sys.stderr)
    sys.exit(2)


</t>
<t tx="ekr.20220525082936.334">ActionFunction = Callable[[argparse.Namespace], None]


</t>
<t tx="ekr.20220525082936.335">def action(subparser: argparse.ArgumentParser) -&gt; Callable[[ActionFunction], ActionFunction]:
    """Decorator to tie an action function to a subparser."""
    def register(func: ActionFunction) -&gt; ActionFunction:
        subparser.set_defaults(action=func)
        return func
    return register


</t>
<t tx="ekr.20220525082936.336"># Action functions (run in client from command line).

</t>
<t tx="ekr.20220525082936.337">@action(start_parser)
def do_start(args: argparse.Namespace) -&gt; None:
    """Start daemon (it must not already be running).

    This is where mypy flags are set from the command line.

    Setting flags is a bit awkward; you have to use e.g.:

      dmypy start -- --strict

    since we don't want to duplicate mypy's huge list of flags.
    """
    try:
        get_status(args.status_file)
    except BadStatus:
        # Bad or missing status file or dead process; good to start.
        pass
    else:
        fail("Daemon is still alive")
    start_server(args)


</t>
<t tx="ekr.20220525082936.338">@action(restart_parser)
def do_restart(args: argparse.Namespace) -&gt; None:
    """Restart daemon (it may or may not be running; but not hanging).

    We first try to stop it politely if it's running.  This also sets
    mypy flags from the command line (see do_start()).
    """
    restart_server(args)


</t>
<t tx="ekr.20220525082936.339">def restart_server(args: argparse.Namespace, allow_sources: bool = False) -&gt; None:
    """Restart daemon (it may or may not be running; but not hanging)."""
    try:
        do_stop(args)
    except BadStatus:
        # Bad or missing status file or dead process; good to start.
        pass
    start_server(args, allow_sources)


</t>
<t tx="ekr.20220525082936.34">@mypyc_attr(allow_interpreted_subclasses=True)
class TypeTranslator(TypeVisitor[Type]):
    """Identity type transformation.

    Subclass this and override some methods to implement a non-trivial
    transformation.
    """

    @others
</t>
<t tx="ekr.20220525082936.340">def start_server(args: argparse.Namespace, allow_sources: bool = False) -&gt; None:
    """Start the server from command arguments and wait for it."""
    # Lazy import so this import doesn't slow down other commands.
    from mypy.dmypy_server import daemonize, process_start_options
    start_options = process_start_options(args.flags, allow_sources)
    if daemonize(start_options, args.status_file, timeout=args.timeout, log_file=args.log_file):
        sys.exit(2)
    wait_for_server(args.status_file)


</t>
<t tx="ekr.20220525082936.341">def wait_for_server(status_file: str, timeout: float = 5.0) -&gt; None:
    """Wait until the server is up.

    Exit if it doesn't happen within the timeout.
    """
    endtime = time.time() + timeout
    while time.time() &lt; endtime:
        try:
            data = read_status(status_file)
        except BadStatus:
            # If the file isn't there yet, retry later.
            time.sleep(0.1)
            continue
        # If the file's content is bogus or the process is dead, fail.
        check_status(data)
        print("Daemon started")
        return
    fail("Timed out waiting for daemon to start")


</t>
<t tx="ekr.20220525082936.342">@action(run_parser)
def do_run(args: argparse.Namespace) -&gt; None:
    """Do a check, starting (or restarting) the daemon as necessary

    Restarts the daemon if the running daemon reports that it is
    required (due to a configuration change, for example).

    Setting flags is a bit awkward; you have to use e.g.:

      dmypy run -- --strict a.py b.py ...

    since we don't want to duplicate mypy's huge list of flags.
    (The -- is only necessary if flags are specified.)
    """
    if not is_running(args.status_file):
        # Bad or missing status file or dead process; good to start.
        start_server(args, allow_sources=True)
    t0 = time.time()
    response = request(args.status_file, 'run', version=__version__, args=args.flags)
    # If the daemon signals that a restart is necessary, do it
    if 'restart' in response:
        print(f"Restarting: {response['restart']}")
        restart_server(args, allow_sources=True)
        response = request(args.status_file, 'run', version=__version__, args=args.flags)

    t1 = time.time()
    response['roundtrip_time'] = t1 - t0
    check_output(response, args.verbose, args.junit_xml, args.perf_stats_file)


</t>
<t tx="ekr.20220525082936.343">@action(status_parser)
def do_status(args: argparse.Namespace) -&gt; None:
    """Print daemon status.

    This verifies that it is responsive to requests.
    """
    status = read_status(args.status_file)
    if args.verbose:
        show_stats(status)
    # Both check_status() and request() may raise BadStatus,
    # which will be handled by main().
    check_status(status)
    response = request(args.status_file, 'status',
                       fswatcher_dump_file=args.fswatcher_dump_file,
                       timeout=5)
    if args.verbose or 'error' in response:
        show_stats(response)
    if 'error' in response:
        fail(f"Daemon is stuck; consider {sys.argv[0]} kill")
    print("Daemon is up and running")


</t>
<t tx="ekr.20220525082936.344">@action(stop_parser)
def do_stop(args: argparse.Namespace) -&gt; None:
    """Stop daemon via a 'stop' request."""
    # May raise BadStatus, which will be handled by main().
    response = request(args.status_file, 'stop', timeout=5)
    if 'error' in response:
        show_stats(response)
        fail(f"Daemon is stuck; consider {sys.argv[0]} kill")
    else:
        print("Daemon stopped")


</t>
<t tx="ekr.20220525082936.345">@action(kill_parser)
def do_kill(args: argparse.Namespace) -&gt; None:
    """Kill daemon process with SIGKILL."""
    pid, _ = get_status(args.status_file)
    try:
        kill(pid)
    except OSError as err:
        fail(str(err))
    else:
        print("Daemon killed")


</t>
<t tx="ekr.20220525082936.346">@action(check_parser)
def do_check(args: argparse.Namespace) -&gt; None:
    """Ask the daemon to check a list of files."""
    t0 = time.time()
    response = request(args.status_file, 'check', files=args.files)
    t1 = time.time()
    response['roundtrip_time'] = t1 - t0
    check_output(response, args.verbose, args.junit_xml, args.perf_stats_file)


</t>
<t tx="ekr.20220525082936.347">@action(recheck_parser)
def do_recheck(args: argparse.Namespace) -&gt; None:
    """Ask the daemon to recheck the previous list of files, with optional modifications.

    If at least one of --remove or --update is given, the server will
    update the list of files to check accordingly and assume that any other files
    are unchanged.  If none of these flags are given, the server will call stat()
    on each file last checked to determine its status.

    Files given in --update ought to exist.  Files given in --remove need not exist;
    if they don't they will be ignored.
    The lists may be empty but oughtn't contain duplicates or overlap.

    NOTE: The list of files is lost when the daemon is restarted.
    """
    t0 = time.time()
    if args.remove is not None or args.update is not None:
        response = request(args.status_file, 'recheck', remove=args.remove, update=args.update)
    else:
        response = request(args.status_file, 'recheck')
    t1 = time.time()
    response['roundtrip_time'] = t1 - t0
    check_output(response, args.verbose, args.junit_xml, args.perf_stats_file)


</t>
<t tx="ekr.20220525082936.348">@action(suggest_parser)
def do_suggest(args: argparse.Namespace) -&gt; None:
    """Ask the daemon for a suggested signature.

    This just prints whatever the daemon reports as output.
    For now it may be closer to a list of call sites.
    """
    response = request(args.status_file, 'suggest', function=args.function,
                       json=args.json, callsites=args.callsites, no_errors=args.no_errors,
                       no_any=args.no_any, flex_any=args.flex_any, try_text=args.try_text,
                       use_fixme=args.use_fixme, max_guesses=args.max_guesses)
    check_output(response, verbose=False, junit_xml=None, perf_stats_file=None)


</t>
<t tx="ekr.20220525082936.349">def check_output(response: Dict[str, Any], verbose: bool,
                 junit_xml: Optional[str],
                 perf_stats_file: Optional[str]) -&gt; None:
    """Print the output from a check or recheck command.

    Call sys.exit() unless the status code is zero.
    """
    if 'error' in response:
        fail(response['error'])
    try:
        out, err, status_code = response['out'], response['err'], response['status']
    except KeyError:
        fail(f"Response: {str(response)}")
    sys.stdout.write(out)
    sys.stdout.flush()
    sys.stderr.write(err)
    if verbose:
        show_stats(response)
    if junit_xml:
        # Lazy import so this import doesn't slow things down when not writing junit
        from mypy.util import write_junit_xml
        messages = (out + err).splitlines()
        write_junit_xml(response['roundtrip_time'], bool(err), messages, junit_xml,
                        response['python_version'], response['platform'])
    if perf_stats_file:
        telemetry = response.get('stats', {})
        with open(perf_stats_file, 'w') as f:
            json.dump(telemetry, f)

    if status_code:
        sys.exit(status_code)


</t>
<t tx="ekr.20220525082936.35">def visit_unbound_type(self, t: UnboundType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082936.350">def show_stats(response: Mapping[str, object]) -&gt; None:
    for key, value in sorted(response.items()):
        if key not in ('out', 'err'):
            print("%-24s: %10s" % (key, "%.3f" % value if isinstance(value, float) else value))
        else:
            value = repr(value)[1:-1]
            if len(value) &gt; 50:
                value = value[:40] + ' ...'
            print("%-24s: %s" % (key, value))


</t>
<t tx="ekr.20220525082936.351">@action(hang_parser)
def do_hang(args: argparse.Namespace) -&gt; None:
    """Hang for 100 seconds, as a debug hack."""
    print(request(args.status_file, 'hang', timeout=1))


</t>
<t tx="ekr.20220525082936.352">@action(daemon_parser)
def do_daemon(args: argparse.Namespace) -&gt; None:
    """Serve requests in the foreground."""
    # Lazy import so this import doesn't slow down other commands.
    from mypy.dmypy_server import Server, process_start_options
    if args.options_data:
        from mypy.options import Options
        options_dict, timeout, log_file = pickle.loads(base64.b64decode(args.options_data))
        options_obj = Options()
        options = options_obj.apply_changes(options_dict)
        if log_file:
            sys.stdout = sys.stderr = open(log_file, 'a', buffering=1)
            fd = sys.stdout.fileno()
            os.dup2(fd, 2)
            os.dup2(fd, 1)
    else:
        options = process_start_options(args.flags, allow_sources=False)
        timeout = args.timeout
    Server(options, args.status_file, timeout=timeout).serve()


</t>
<t tx="ekr.20220525082936.353">@action(help_parser)
def do_help(args: argparse.Namespace) -&gt; None:
    """Print full help (same as dmypy --help)."""
    parser.print_help()


</t>
<t tx="ekr.20220525082936.354"># Client-side infrastructure.


</t>
<t tx="ekr.20220525082936.355">def request(status_file: str, command: str, *, timeout: Optional[int] = None,
            **kwds: object) -&gt; Dict[str, Any]:
    """Send a request to the daemon.

    Return the JSON dict with the response.

    Raise BadStatus if there is something wrong with the status file
    or if the process whose pid is in the status file has died.

    Return {'error': &lt;message&gt;} if an IPC operation or receive()
    raised OSError.  This covers cases such as connection refused or
    closed prematurely as well as invalid JSON received.
    """
    response: Dict[str, str] = {}
    args = dict(kwds)
    args['command'] = command
    # Tell the server whether this request was initiated from a human-facing terminal,
    # so that it can format the type checking output accordingly.
    args['is_tty'] = sys.stdout.isatty() or int(os.getenv('MYPY_FORCE_COLOR', '0')) &gt; 0
    args['terminal_width'] = get_terminal_width()
    bdata = json.dumps(args).encode('utf8')
    _, name = get_status(status_file)
    try:
        with IPCClient(name, timeout) as client:
            client.write(bdata)
            response = receive(client)
    except (OSError, IPCException) as err:
        return {'error': str(err)}
    # TODO: Other errors, e.g. ValueError, UnicodeError
    else:
        return response


</t>
<t tx="ekr.20220525082936.356">def get_status(status_file: str) -&gt; Tuple[int, str]:
    """Read status file and check if the process is alive.

    Return (pid, connection_name) on success.

    Raise BadStatus if something's wrong.
    """
    data = read_status(status_file)
    return check_status(data)


</t>
<t tx="ekr.20220525082936.357">def check_status(data: Dict[str, Any]) -&gt; Tuple[int, str]:
    """Check if the process is alive.

    Return (pid, connection_name) on success.

    Raise BadStatus if something's wrong.
    """
    if 'pid' not in data:
        raise BadStatus("Invalid status file (no pid field)")
    pid = data['pid']
    if not isinstance(pid, int):
        raise BadStatus("pid field is not an int")
    if not alive(pid):
        raise BadStatus("Daemon has died")
    if 'connection_name' not in data:
        raise BadStatus("Invalid status file (no connection_name field)")
    connection_name = data['connection_name']
    if not isinstance(connection_name, str):
        raise BadStatus("connection_name field is not a string")
    return pid, connection_name


</t>
<t tx="ekr.20220525082936.358">def read_status(status_file: str) -&gt; Dict[str, object]:
    """Read status file.

    Raise BadStatus if the status file doesn't exist or contains
    invalid JSON or the JSON is not a dict.
    """
    if not os.path.isfile(status_file):
        raise BadStatus("No status file found")
    with open(status_file) as f:
        try:
            data = json.load(f)
        except Exception as e:
            raise BadStatus("Malformed status file (not JSON)") from e
    if not isinstance(data, dict):
        raise BadStatus("Invalid status file (not a dict)")
    return data


</t>
<t tx="ekr.20220525082936.359">def is_running(status_file: str) -&gt; bool:
    """Check if the server is running cleanly"""
    try:
        get_status(status_file)
    except BadStatus:
        return False
    return True


</t>
<t tx="ekr.20220525082936.36">def visit_any(self, t: AnyType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082936.360"># Run main().
def console_entry() -&gt; None:
    main(sys.argv[1:])
</t>
<t tx="ekr.20220525082936.362">@path C:/Repos/mypy/mypy/dmypy/
@language python
@tabwidth -4
from mypy.dmypy.client import console_entry

if __name__ == '__main__':
    console_entry()
</t>
<t tx="ekr.20220525082936.363"></t>
<t tx="ekr.20220525082936.364">@path C:/Repos/mypy/mypy/plugins/
@language python
@tabwidth -4
"""Plugin for supporting the attrs library (http://www.attrs.org)"""

from mypy.backports import OrderedDict

from typing import Optional, Dict, List, cast, Tuple, Iterable
from typing_extensions import Final

import mypy.plugin  # To avoid circular imports.
from mypy.exprtotype import expr_to_unanalyzed_type, TypeTranslationError
from mypy.nodes import (
    Context, Argument, Var, ARG_OPT, ARG_POS, TypeInfo, AssignmentStmt,
    TupleExpr, ListExpr, NameExpr, CallExpr, RefExpr, FuncDef,
    is_class_var, TempNode, Decorator, MemberExpr, Expression,
    SymbolTableNode, MDEF, JsonDict, OverloadedFuncDef, ARG_NAMED_OPT, ARG_NAMED,
    TypeVarExpr, PlaceholderNode
)
from mypy.plugin import SemanticAnalyzerPluginInterface
from mypy.plugins.common import (
    _get_argument, _get_bool_argument, _get_decorator_bool_argument, add_method,
    deserialize_and_fixup_type, add_attribute_to_class,
)
from mypy.types import (
    TupleType, Type, AnyType, TypeOfAny, CallableType, NoneType, TypeVarType,
    Overloaded, UnionType, FunctionLike, Instance, get_proper_type,
    LiteralType,
)
from mypy.typeops import make_simplified_union, map_type_from_supertype
from mypy.typevars import fill_typevars
from mypy.util import unmangle
from mypy.server.trigger import make_wildcard_trigger

KW_ONLY_PYTHON_2_UNSUPPORTED: Final = "kw_only is not supported in Python 2"

# The names of the different functions that create classes or arguments.
attr_class_makers: Final = {
    'attr.s',
    'attr.attrs',
    'attr.attributes',
}
attr_dataclass_makers: Final = {
    'attr.dataclass',
}
attr_frozen_makers: Final = {"attr.frozen", "attrs.frozen"}
attr_define_makers: Final = {"attr.define", "attr.mutable", "attrs.define", "attrs.mutable"}
attr_attrib_makers: Final = {
    'attr.ib',
    'attr.attrib',
    'attr.attr',
    'attr.field',
    'attrs.field',
}
attr_optional_converters: Final = {'attr.converters.optional', 'attrs.converters.optional'}

SELF_TVAR_NAME: Final = "_AT"
MAGIC_ATTR_NAME: Final = "__attrs_attrs__"
MAGIC_ATTR_CLS_NAME: Final = "_AttrsAttributes"  # The namedtuple subclass name.


@others
</t>
<t tx="ekr.20220525082936.365">class Converter:
    """Holds information about a `converter=` argument"""

    @others
</t>
<t tx="ekr.20220525082936.366">def __init__(self,
             type: Optional[Type] = None,
             is_attr_converters_optional: bool = False,
             is_invalid_converter: bool = False) -&gt; None:
    self.type = type
    self.is_attr_converters_optional = is_attr_converters_optional
    self.is_invalid_converter = is_invalid_converter


</t>
<t tx="ekr.20220525082936.367">class Attribute:
    """The value of an attr.ib() call."""

    @others
</t>
<t tx="ekr.20220525082936.368">def __init__(self, name: str, info: TypeInfo,
             has_default: bool, init: bool, kw_only: bool, converter: Converter,
             context: Context,
             init_type: Optional[Type]) -&gt; None:
    self.name = name
    self.info = info
    self.has_default = has_default
    self.init = init
    self.kw_only = kw_only
    self.converter = converter
    self.context = context
    self.init_type = init_type

</t>
<t tx="ekr.20220525082936.369">def argument(self, ctx: 'mypy.plugin.ClassDefContext') -&gt; Argument:
    """Return this attribute as an argument to __init__."""
    assert self.init

    init_type = self.init_type or self.info[self.name].type

    if self.converter.type and not self.converter.is_invalid_converter:
        # When a converter is set the init_type is overridden by the first argument
        # of the converter method.
        converter_type = self.converter.type
        init_type = None
        converter_type = get_proper_type(converter_type)
        if isinstance(converter_type, CallableType) and converter_type.arg_types:
            init_type = converter_type.arg_types[0]
        elif isinstance(converter_type, Overloaded):
            types: List[Type] = []
            for item in converter_type.items:
                # Walk the overloads looking for methods that can accept one argument.
                num_arg_types = len(item.arg_types)
                if not num_arg_types:
                    continue
                if num_arg_types &gt; 1 and any(kind == ARG_POS for kind in item.arg_kinds[1:]):
                    continue
                types.append(item.arg_types[0])
            # Make a union of all the valid types.
            if types:
                init_type = make_simplified_union(types)

        if self.converter.is_attr_converters_optional and init_type:
            # If the converter was attr.converter.optional(type) then add None to
            # the allowed init_type.
            init_type = UnionType.make_union([init_type, NoneType()])

        if not init_type:
            ctx.api.fail("Cannot determine __init__ type from converter", self.context)
            init_type = AnyType(TypeOfAny.from_error)
    elif self.converter.is_invalid_converter:
        # This means we had a converter but it's not of a type we can infer.
        init_type = AnyType(TypeOfAny.from_error)

    if init_type is None:
        if ctx.api.options.disallow_untyped_defs:
            # This is a compromise.  If you don't have a type here then the
            # __init__ will be untyped. But since the __init__ is added it's
            # pointing at the decorator. So instead we also show the error in the
            # assignment, which is where you would fix the issue.
            node = self.info[self.name].node
            assert node is not None
            ctx.api.msg.need_annotation_for_var(node, self.context)

        # Convert type not set to Any.
        init_type = AnyType(TypeOfAny.unannotated)

    if self.kw_only:
        arg_kind = ARG_NAMED_OPT if self.has_default else ARG_NAMED
    else:
        arg_kind = ARG_OPT if self.has_default else ARG_POS

    # Attrs removes leading underscores when creating the __init__ arguments.
    return Argument(Var(self.name.lstrip("_"), init_type), init_type,
                    None,
                    arg_kind)

</t>
<t tx="ekr.20220525082936.37">def visit_none_type(self, t: NoneType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082936.370">def serialize(self) -&gt; JsonDict:
    """Serialize this object so it can be saved and restored."""
    return {
        'name': self.name,
        'has_default': self.has_default,
        'init': self.init,
        'kw_only': self.kw_only,
        'converter_type': self.converter.type.serialize() if self.converter.type else None,
        'converter_is_attr_converters_optional': self.converter.is_attr_converters_optional,
        'converter_is_invalid_converter': self.converter.is_invalid_converter,
        'context_line': self.context.line,
        'context_column': self.context.column,
        'init_type': self.init_type.serialize() if self.init_type else None,
    }

</t>
<t tx="ekr.20220525082936.371">@classmethod
def deserialize(cls, info: TypeInfo,
                data: JsonDict,
                api: SemanticAnalyzerPluginInterface) -&gt; 'Attribute':
    """Return the Attribute that was serialized."""
    raw_init_type = data['init_type']
    init_type = deserialize_and_fixup_type(raw_init_type, api) if raw_init_type else None

    converter_type = None
    if data['converter_type']:
        converter_type = deserialize_and_fixup_type(data['converter_type'], api)
    return Attribute(data['name'],
        info,
        data['has_default'],
        data['init'],
        data['kw_only'],
        Converter(converter_type, data['converter_is_attr_converters_optional'],
                  data['converter_is_invalid_converter']),
        Context(line=data['context_line'], column=data['context_column']),
        init_type)

</t>
<t tx="ekr.20220525082936.372">def expand_typevar_from_subtype(self, sub_type: TypeInfo) -&gt; None:
    """Expands type vars in the context of a subtype when an attribute is inherited
    from a generic super type."""
    if self.init_type:
        self.init_type = map_type_from_supertype(self.init_type, sub_type, self.info)
    else:
        self.init_type = None


</t>
<t tx="ekr.20220525082936.373">def _determine_eq_order(ctx: 'mypy.plugin.ClassDefContext') -&gt; bool:
    """
    Validate the combination of *cmp*, *eq*, and *order*. Derive the effective
    value of order.
    """
    cmp = _get_decorator_optional_bool_argument(ctx, 'cmp')
    eq = _get_decorator_optional_bool_argument(ctx, 'eq')
    order = _get_decorator_optional_bool_argument(ctx, 'order')

    if cmp is not None and any((eq is not None, order is not None)):
        ctx.api.fail('Don\'t mix "cmp" with "eq" and "order"', ctx.reason)

    # cmp takes precedence due to bw-compatibility.
    if cmp is not None:
        return cmp

    # If left None, equality is on and ordering mirrors equality.
    if eq is None:
        eq = True

    if order is None:
        order = eq

    if eq is False and order is True:
        ctx.api.fail('eq must be True if order is True', ctx.reason)

    return order


</t>
<t tx="ekr.20220525082936.374">def _get_decorator_optional_bool_argument(
    ctx: 'mypy.plugin.ClassDefContext',
    name: str,
    default: Optional[bool] = None,
) -&gt; Optional[bool]:
    """Return the Optional[bool] argument for the decorator.

    This handles both @decorator(...) and @decorator.
    """
    if isinstance(ctx.reason, CallExpr):
        attr_value = _get_argument(ctx.reason, name)
        if attr_value:
            if isinstance(attr_value, NameExpr):
                if attr_value.fullname == 'builtins.True':
                    return True
                if attr_value.fullname == 'builtins.False':
                    return False
                if attr_value.fullname == 'builtins.None':
                    return None
            ctx.api.fail(f'"{name}" argument must be True or False.', ctx.reason)
            return default
        return default
    else:
        return default


</t>
<t tx="ekr.20220525082936.375">def attr_tag_callback(ctx: 'mypy.plugin.ClassDefContext') -&gt; None:
    """Record that we have an attrs class in the main semantic analysis pass.

    The later pass implemented by attr_class_maker_callback will use this
    to detect attrs lasses in base classes.
    """
    # The value is ignored, only the existence matters.
    ctx.cls.info.metadata['attrs_tag'] = {}


</t>
<t tx="ekr.20220525082936.376">def attr_class_maker_callback(ctx: 'mypy.plugin.ClassDefContext',
                              auto_attribs_default: Optional[bool] = False,
                              frozen_default: bool = False) -&gt; bool:
    """Add necessary dunder methods to classes decorated with attr.s.

    attrs is a package that lets you define classes without writing dull boilerplate code.

    At a quick glance, the decorator searches the class body for assignments of `attr.ib`s (or
    annotated variables if auto_attribs=True), then depending on how the decorator is called,
    it will add an __init__ or all the __cmp__ methods.  For frozen=True it will turn the attrs
    into properties.

    See http://www.attrs.org/en/stable/how-does-it-work.html for information on how attrs works.

    If this returns False, some required metadata was not ready yet and we need another
    pass.
    """
    info = ctx.cls.info

    init = _get_decorator_bool_argument(ctx, 'init', True)
    frozen = _get_frozen(ctx, frozen_default)
    order = _determine_eq_order(ctx)
    slots = _get_decorator_bool_argument(ctx, 'slots', False)

    auto_attribs = _get_decorator_optional_bool_argument(ctx, 'auto_attribs', auto_attribs_default)
    kw_only = _get_decorator_bool_argument(ctx, 'kw_only', False)
    match_args = _get_decorator_bool_argument(ctx, 'match_args', True)

    early_fail = False
    if ctx.api.options.python_version[0] &lt; 3:
        if auto_attribs:
            ctx.api.fail("auto_attribs is not supported in Python 2", ctx.reason)
            early_fail = True
        if not info.defn.base_type_exprs:
            # Note: This will not catch subclassing old-style classes.
            ctx.api.fail("attrs only works with new-style classes", info.defn)
            early_fail = True
        if kw_only:
            ctx.api.fail(KW_ONLY_PYTHON_2_UNSUPPORTED, ctx.reason)
            early_fail = True
    if early_fail:
        _add_empty_metadata(info)
        return True

    for super_info in ctx.cls.info.mro[1:-1]:
        if 'attrs_tag' in super_info.metadata and 'attrs' not in super_info.metadata:
            # Super class is not ready yet. Request another pass.
            return False

    attributes = _analyze_class(ctx, auto_attribs, kw_only)

    # Check if attribute types are ready.
    for attr in attributes:
        node = info.get(attr.name)
        if node is None:
            # This name is likely blocked by some semantic analysis error that
            # should have been reported already.
            _add_empty_metadata(info)
            return True

    _add_attrs_magic_attribute(ctx, [(attr.name, info[attr.name].type) for attr in attributes])
    if slots:
        _add_slots(ctx, attributes)
    if match_args and ctx.api.options.python_version[:2] &gt;= (3, 10):
        # `.__match_args__` is only added for python3.10+, but the argument
        # exists for earlier versions as well.
        _add_match_args(ctx, attributes)

    # Save the attributes so that subclasses can reuse them.
    ctx.cls.info.metadata['attrs'] = {
        'attributes': [attr.serialize() for attr in attributes],
        'frozen': frozen,
    }

    adder = MethodAdder(ctx)
    if init:
        _add_init(ctx, attributes, adder)
    if order:
        _add_order(ctx, adder)
    if frozen:
        _make_frozen(ctx, attributes)

    return True


</t>
<t tx="ekr.20220525082936.377">def _get_frozen(ctx: 'mypy.plugin.ClassDefContext', frozen_default: bool) -&gt; bool:
    """Return whether this class is frozen."""
    if _get_decorator_bool_argument(ctx, 'frozen', frozen_default):
        return True
    # Subclasses of frozen classes are frozen so check that.
    for super_info in ctx.cls.info.mro[1:-1]:
        if 'attrs' in super_info.metadata and super_info.metadata['attrs']['frozen']:
            return True
    return False


</t>
<t tx="ekr.20220525082936.378">def _analyze_class(ctx: 'mypy.plugin.ClassDefContext',
                   auto_attribs: Optional[bool],
                   kw_only: bool) -&gt; List[Attribute]:
    """Analyze the class body of an attr maker, its parents, and return the Attributes found.

    auto_attribs=True means we'll generate attributes from type annotations also.
    auto_attribs=None means we'll detect which mode to use.
    kw_only=True means that all attributes created here will be keyword only args in __init__.
    """
    own_attrs: OrderedDict[str, Attribute] = OrderedDict()
    if auto_attribs is None:
        auto_attribs = _detect_auto_attribs(ctx)

    # Walk the body looking for assignments and decorators.
    for stmt in ctx.cls.defs.body:
        if isinstance(stmt, AssignmentStmt):
            for attr in _attributes_from_assignment(ctx, stmt, auto_attribs, kw_only):
                # When attrs are defined twice in the same body we want to use the 2nd definition
                # in the 2nd location. So remove it from the OrderedDict.
                # Unless it's auto_attribs in which case we want the 2nd definition in the
                # 1st location.
                if not auto_attribs and attr.name in own_attrs:
                    del own_attrs[attr.name]
                own_attrs[attr.name] = attr
        elif isinstance(stmt, Decorator):
            _cleanup_decorator(stmt, own_attrs)

    for attribute in own_attrs.values():
        # Even though these look like class level assignments we want them to look like
        # instance level assignments.
        if attribute.name in ctx.cls.info.names:
            node = ctx.cls.info.names[attribute.name].node
            if isinstance(node, PlaceholderNode):
                # This node is not ready yet.
                continue
            assert isinstance(node, Var)
            node.is_initialized_in_class = False

    # Traverse the MRO and collect attributes from the parents.
    taken_attr_names = set(own_attrs)
    super_attrs = []
    for super_info in ctx.cls.info.mro[1:-1]:
        if 'attrs' in super_info.metadata:
            # Each class depends on the set of attributes in its attrs ancestors.
            ctx.api.add_plugin_dependency(make_wildcard_trigger(super_info.fullname))

            for data in super_info.metadata['attrs']['attributes']:
                # Only add an attribute if it hasn't been defined before.  This
                # allows for overwriting attribute definitions by subclassing.
                if data['name'] not in taken_attr_names:
                    a = Attribute.deserialize(super_info, data, ctx.api)
                    a.expand_typevar_from_subtype(ctx.cls.info)
                    super_attrs.append(a)
                    taken_attr_names.add(a.name)
    attributes = super_attrs + list(own_attrs.values())

    # Check the init args for correct default-ness.  Note: This has to be done after all the
    # attributes for all classes have been read, because subclasses can override parents.
    last_default = False

    for i, attribute in enumerate(attributes):
        if not attribute.init:
            continue

        if attribute.kw_only:
            # Keyword-only attributes don't care whether they are default or not.
            continue

        # If the issue comes from merging different classes, report it
        # at the class definition point.
        context = attribute.context if i &gt;= len(super_attrs) else ctx.cls

        if not attribute.has_default and last_default:
            ctx.api.fail(
                "Non-default attributes not allowed after default attributes.",
                context)
        last_default |= attribute.has_default

    return attributes


</t>
<t tx="ekr.20220525082936.379">def _add_empty_metadata(info: TypeInfo) -&gt; None:
    """Add empty metadata to mark that we've finished processing this class."""
    info.metadata['attrs'] = {
        'attributes': [],
        'frozen': False,
    }


</t>
<t tx="ekr.20220525082936.38">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082936.380">def _detect_auto_attribs(ctx: 'mypy.plugin.ClassDefContext') -&gt; bool:
    """Return whether auto_attribs should be enabled or disabled.

    It's disabled if there are any unannotated attribs()
    """
    for stmt in ctx.cls.defs.body:
        if isinstance(stmt, AssignmentStmt):
            for lvalue in stmt.lvalues:
                lvalues, rvalues = _parse_assignments(lvalue, stmt)

                if len(lvalues) != len(rvalues):
                    # This means we have some assignment that isn't 1 to 1.
                    # It can't be an attrib.
                    continue

                for lhs, rvalue in zip(lvalues, rvalues):
                    # Check if the right hand side is a call to an attribute maker.
                    if (isinstance(rvalue, CallExpr)
                            and isinstance(rvalue.callee, RefExpr)
                            and rvalue.callee.fullname in attr_attrib_makers
                            and not stmt.new_syntax):
                        # This means we have an attrib without an annotation and so
                        # we can't do auto_attribs=True
                        return False
    return True


</t>
<t tx="ekr.20220525082936.381">def _attributes_from_assignment(ctx: 'mypy.plugin.ClassDefContext',
                                stmt: AssignmentStmt, auto_attribs: bool,
                                kw_only: bool) -&gt; Iterable[Attribute]:
    """Return Attribute objects that are created by this assignment.

    The assignments can look like this:
        x = attr.ib()
        x = y = attr.ib()
        x, y = attr.ib(), attr.ib()
    or if auto_attribs is enabled also like this:
        x: type
        x: type = default_value
    """
    for lvalue in stmt.lvalues:
        lvalues, rvalues = _parse_assignments(lvalue, stmt)

        if len(lvalues) != len(rvalues):
            # This means we have some assignment that isn't 1 to 1.
            # It can't be an attrib.
            continue

        for lhs, rvalue in zip(lvalues, rvalues):
            # Check if the right hand side is a call to an attribute maker.
            if (isinstance(rvalue, CallExpr)
                    and isinstance(rvalue.callee, RefExpr)
                    and rvalue.callee.fullname in attr_attrib_makers):
                attr = _attribute_from_attrib_maker(ctx, auto_attribs, kw_only, lhs, rvalue, stmt)
                if attr:
                    yield attr
            elif auto_attribs and stmt.type and stmt.new_syntax and not is_class_var(lhs):
                yield _attribute_from_auto_attrib(ctx, kw_only, lhs, rvalue, stmt)


</t>
<t tx="ekr.20220525082936.382">def _cleanup_decorator(stmt: Decorator, attr_map: Dict[str, Attribute]) -&gt; None:
    """Handle decorators in class bodies.

    `x.default` will set a default value on x
    `x.validator` and `x.default` will get removed to avoid throwing a type error.
    """
    remove_me = []
    for func_decorator in stmt.decorators:
        if (isinstance(func_decorator, MemberExpr)
                and isinstance(func_decorator.expr, NameExpr)
                and func_decorator.expr.name in attr_map):

            if func_decorator.name == 'default':
                attr_map[func_decorator.expr.name].has_default = True

            if func_decorator.name in ('default', 'validator'):
                # These are decorators on the attrib object that only exist during
                # class creation time.  In order to not trigger a type error later we
                # just remove them.  This might leave us with a Decorator with no
                # decorators (Emperor's new clothes?)
                # TODO: It would be nice to type-check these rather than remove them.
                #       default should be Callable[[], T]
                #       validator should be Callable[[Any, 'Attribute', T], Any]
                #       where T is the type of the attribute.
                remove_me.append(func_decorator)
    for dec in remove_me:
        stmt.decorators.remove(dec)


</t>
<t tx="ekr.20220525082936.383">def _attribute_from_auto_attrib(ctx: 'mypy.plugin.ClassDefContext',
                                kw_only: bool,
                                lhs: NameExpr,
                                rvalue: Expression,
                                stmt: AssignmentStmt) -&gt; Attribute:
    """Return an Attribute for a new type assignment."""
    name = unmangle(lhs.name)
    # `x: int` (without equal sign) assigns rvalue to TempNode(AnyType())
    has_rhs = not isinstance(rvalue, TempNode)
    sym = ctx.cls.info.names.get(name)
    init_type = sym.type if sym else None
    return Attribute(name, ctx.cls.info, has_rhs, True, kw_only, Converter(), stmt, init_type)


</t>
<t tx="ekr.20220525082936.384">def _attribute_from_attrib_maker(ctx: 'mypy.plugin.ClassDefContext',
                                 auto_attribs: bool,
                                 kw_only: bool,
                                 lhs: NameExpr,
                                 rvalue: CallExpr,
                                 stmt: AssignmentStmt) -&gt; Optional[Attribute]:
    """Return an Attribute from the assignment or None if you can't make one."""
    if auto_attribs and not stmt.new_syntax:
        # auto_attribs requires an annotation on *every* attr.ib.
        assert lhs.node is not None
        ctx.api.msg.need_annotation_for_var(lhs.node, stmt)
        return None

    if len(stmt.lvalues) &gt; 1:
        ctx.api.fail("Too many names for one attribute", stmt)
        return None

    # This is the type that belongs in the __init__ method for this attrib.
    init_type = stmt.type

    # Read all the arguments from the call.
    init = _get_bool_argument(ctx, rvalue, 'init', True)
    # Note: If the class decorator says kw_only=True the attribute is ignored.
    # See https://github.com/python-attrs/attrs/issues/481 for explanation.
    kw_only |= _get_bool_argument(ctx, rvalue, 'kw_only', False)
    if kw_only and ctx.api.options.python_version[0] &lt; 3:
        ctx.api.fail(KW_ONLY_PYTHON_2_UNSUPPORTED, stmt)
        return None

    # TODO: Check for attr.NOTHING
    attr_has_default = bool(_get_argument(rvalue, 'default'))
    attr_has_factory = bool(_get_argument(rvalue, 'factory'))

    if attr_has_default and attr_has_factory:
        ctx.api.fail('Can\'t pass both "default" and "factory".', rvalue)
    elif attr_has_factory:
        attr_has_default = True

    # If the type isn't set through annotation but is passed through `type=` use that.
    type_arg = _get_argument(rvalue, 'type')
    if type_arg and not init_type:
        try:
            un_type = expr_to_unanalyzed_type(type_arg, ctx.api.options, ctx.api.is_stub_file)
        except TypeTranslationError:
            ctx.api.fail('Invalid argument to type', type_arg)
        else:
            init_type = ctx.api.anal_type(un_type)
            if init_type and isinstance(lhs.node, Var) and not lhs.node.type:
                # If there is no annotation, add one.
                lhs.node.type = init_type
                lhs.is_inferred_def = False

    # Note: convert is deprecated but works the same as converter.
    converter = _get_argument(rvalue, 'converter')
    convert = _get_argument(rvalue, 'convert')
    if convert and converter:
        ctx.api.fail('Can\'t pass both "convert" and "converter".', rvalue)
    elif convert:
        ctx.api.fail("convert is deprecated, use converter", rvalue)
        converter = convert
    converter_info = _parse_converter(ctx, converter)

    name = unmangle(lhs.name)
    return Attribute(name, ctx.cls.info, attr_has_default, init,
                     kw_only, converter_info, stmt, init_type)


</t>
<t tx="ekr.20220525082936.385">def _parse_converter(ctx: 'mypy.plugin.ClassDefContext',
                     converter: Optional[Expression]) -&gt; Converter:
    """Return the Converter object from an Expression."""
    # TODO: Support complex converters, e.g. lambdas, calls, etc.
    if converter:
        if isinstance(converter, RefExpr) and converter.node:
            if (isinstance(converter.node, FuncDef)
                    and converter.node.type
                    and isinstance(converter.node.type, FunctionLike)):
                return Converter(converter.node.type)
            elif (isinstance(converter.node, OverloadedFuncDef)
                    and is_valid_overloaded_converter(converter.node)):
                return Converter(converter.node.type)
            elif isinstance(converter.node, TypeInfo):
                from mypy.checkmember import type_object_type  # To avoid import cycle.
                return Converter(type_object_type(converter.node, ctx.api.named_type))

        if (isinstance(converter, CallExpr)
                and isinstance(converter.callee, RefExpr)
                and converter.callee.fullname in attr_optional_converters
                and converter.args
                and converter.args[0]):
            # Special handling for attr.converters.optional(type)
            # We extract the type and add make the init_args Optional in Attribute.argument
            argument = _parse_converter(ctx, converter.args[0])
            argument.is_attr_converters_optional = True
            return argument

        # Signal that we have an unsupported converter.
        ctx.api.fail(
            "Unsupported converter, only named functions and types are currently supported",
            converter
        )
        return Converter(None, is_invalid_converter=True)
    return Converter(None)


</t>
<t tx="ekr.20220525082936.386">def is_valid_overloaded_converter(defn: OverloadedFuncDef) -&gt; bool:
    return all((not isinstance(item, Decorator) or isinstance(item.func.type, FunctionLike))
               for item in defn.items)


</t>
<t tx="ekr.20220525082936.387">def _parse_assignments(
        lvalue: Expression,
        stmt: AssignmentStmt) -&gt; Tuple[List[NameExpr], List[Expression]]:
    """Convert a possibly complex assignment expression into lists of lvalues and rvalues."""
    lvalues: List[NameExpr] = []
    rvalues: List[Expression] = []
    if isinstance(lvalue, (TupleExpr, ListExpr)):
        if all(isinstance(item, NameExpr) for item in lvalue.items):
            lvalues = cast(List[NameExpr], lvalue.items)
        if isinstance(stmt.rvalue, (TupleExpr, ListExpr)):
            rvalues = stmt.rvalue.items
    elif isinstance(lvalue, NameExpr):
        lvalues = [lvalue]
        rvalues = [stmt.rvalue]
    return lvalues, rvalues


</t>
<t tx="ekr.20220525082936.388">def _add_order(ctx: 'mypy.plugin.ClassDefContext', adder: 'MethodAdder') -&gt; None:
    """Generate all the ordering methods for this class."""
    bool_type = ctx.api.named_type('builtins.bool')
    object_type = ctx.api.named_type('builtins.object')
    # Make the types be:
    #    AT = TypeVar('AT')
    #    def __lt__(self: AT, other: AT) -&gt; bool
    # This way comparisons with subclasses will work correctly.
    tvd = TypeVarType(SELF_TVAR_NAME, ctx.cls.info.fullname + '.' + SELF_TVAR_NAME,
                     -1, [], object_type)
    self_tvar_expr = TypeVarExpr(SELF_TVAR_NAME, ctx.cls.info.fullname + '.' + SELF_TVAR_NAME,
                                 [], object_type)
    ctx.cls.info.names[SELF_TVAR_NAME] = SymbolTableNode(MDEF, self_tvar_expr)

    args = [Argument(Var('other', tvd), tvd, None, ARG_POS)]
    for method in ['__lt__', '__le__', '__gt__', '__ge__']:
        adder.add_method(method, args, bool_type, self_type=tvd, tvd=tvd)


</t>
<t tx="ekr.20220525082936.389">def _make_frozen(ctx: 'mypy.plugin.ClassDefContext', attributes: List[Attribute]) -&gt; None:
    """Turn all the attributes into properties to simulate frozen classes."""
    for attribute in attributes:
        if attribute.name in ctx.cls.info.names:
            # This variable belongs to this class so we can modify it.
            node = ctx.cls.info.names[attribute.name].node
            assert isinstance(node, Var)
            node.is_property = True
        else:
            # This variable belongs to a super class so create new Var so we
            # can modify it.
            var = Var(attribute.name, ctx.cls.info[attribute.name].type)
            var.info = ctx.cls.info
            var._fullname = f'{ctx.cls.info.fullname}.{var.name}'
            ctx.cls.info.names[var.name] = SymbolTableNode(MDEF, var)
            var.is_property = True


</t>
<t tx="ekr.20220525082936.39">def visit_erased_type(self, t: ErasedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082936.390">def _add_init(ctx: 'mypy.plugin.ClassDefContext', attributes: List[Attribute],
              adder: 'MethodAdder') -&gt; None:
    """Generate an __init__ method for the attributes and add it to the class."""
    # Convert attributes to arguments with kw_only arguments at the  end of
    # the argument list
    pos_args = []
    kw_only_args = []
    for attribute in attributes:
        if not attribute.init:
            continue
        if attribute.kw_only:
            kw_only_args.append(attribute.argument(ctx))
        else:
            pos_args.append(attribute.argument(ctx))
    args = pos_args + kw_only_args
    if all(
        # We use getattr rather than instance checks because the variable.type
        # might be wrapped into a Union or some other type, but even non-Any
        # types reliably track the fact that the argument was not annotated.
        getattr(arg.variable.type, "type_of_any", None) == TypeOfAny.unannotated
        for arg in args
    ):
        # This workaround makes --disallow-incomplete-defs usable with attrs,
        # but is definitely suboptimal as a long-term solution.
        # See https://github.com/python/mypy/issues/5954 for discussion.
        for a in args:
            a.variable.type = AnyType(TypeOfAny.implementation_artifact)
            a.type_annotation = AnyType(TypeOfAny.implementation_artifact)
    adder.add_method('__init__', args, NoneType())


</t>
<t tx="ekr.20220525082936.391">def _add_attrs_magic_attribute(ctx: 'mypy.plugin.ClassDefContext',
                               attrs: 'List[Tuple[str, Optional[Type]]]') -&gt; None:
    any_type = AnyType(TypeOfAny.explicit)
    attributes_types: 'List[Type]' = [
        ctx.api.named_type_or_none('attr.Attribute', [attr_type or any_type]) or any_type
        for _, attr_type in attrs
    ]
    fallback_type = ctx.api.named_type('builtins.tuple', [
        ctx.api.named_type_or_none('attr.Attribute', [any_type]) or any_type,
    ])

    ti = ctx.api.basic_new_typeinfo(MAGIC_ATTR_CLS_NAME, fallback_type, 0)
    ti.is_named_tuple = True
    for (name, _), attr_type in zip(attrs, attributes_types):
        var = Var(name, attr_type)
        var.is_property = True
        proper_type = get_proper_type(attr_type)
        if isinstance(proper_type, Instance):
            var.info = proper_type.type
        ti.names[name] = SymbolTableNode(MDEF, var, plugin_generated=True)
    attributes_type = Instance(ti, [])

    # TODO: refactor using `add_attribute_to_class`
    var = Var(name=MAGIC_ATTR_NAME, type=TupleType(attributes_types, fallback=attributes_type))
    var.info = ctx.cls.info
    var.is_classvar = True
    var._fullname = f"{ctx.cls.fullname}.{MAGIC_ATTR_CLS_NAME}"
    var.allow_incompatible_override = True
    ctx.cls.info.names[MAGIC_ATTR_NAME] = SymbolTableNode(
        kind=MDEF,
        node=var,
        plugin_generated=True,
        no_serialize=True,
    )


</t>
<t tx="ekr.20220525082936.392">def _add_slots(ctx: 'mypy.plugin.ClassDefContext',
               attributes: List[Attribute]) -&gt; None:
    # Unlike `@dataclasses.dataclass`, `__slots__` is rewritten here.
    ctx.cls.info.slots = {attr.name for attr in attributes}


</t>
<t tx="ekr.20220525082936.393">def _add_match_args(ctx: 'mypy.plugin.ClassDefContext',
                    attributes: List[Attribute]) -&gt; None:
    if ('__match_args__' not in ctx.cls.info.names
            or ctx.cls.info.names['__match_args__'].plugin_generated):
        str_type = ctx.api.named_type('builtins.str')
        match_args = TupleType(
            [
                str_type.copy_modified(
                    last_known_value=LiteralType(attr.name, fallback=str_type),
                )
                for attr in attributes
                if not attr.kw_only and attr.init
            ],
            fallback=ctx.api.named_type('builtins.tuple'),
        )
        add_attribute_to_class(
            api=ctx.api,
            cls=ctx.cls,
            name='__match_args__',
            typ=match_args,
        )


</t>
<t tx="ekr.20220525082936.394">class MethodAdder:
    """Helper to add methods to a TypeInfo.

    ctx: The ClassDefCtx we are using on which we will add methods.
    """

    # TODO: Combine this with the code build_namedtuple_typeinfo to support both.

    @others
</t>
<t tx="ekr.20220525082936.395">def __init__(self, ctx: 'mypy.plugin.ClassDefContext') -&gt; None:
    self.ctx = ctx
    self.self_type = fill_typevars(ctx.cls.info)

</t>
<t tx="ekr.20220525082936.396">def add_method(self,
               method_name: str, args: List[Argument], ret_type: Type,
               self_type: Optional[Type] = None,
               tvd: Optional[TypeVarType] = None) -&gt; None:
    """Add a method: def &lt;method_name&gt;(self, &lt;args&gt;) -&gt; &lt;ret_type&gt;): ... to info.

    self_type: The type to use for the self argument or None to use the inferred self type.
    tvd: If the method is generic these should be the type variables.
    """
    self_type = self_type if self_type is not None else self.self_type
    add_method(self.ctx, method_name, args, ret_type, self_type, tvd)
</t>
<t tx="ekr.20220525082936.397">@path C:/Repos/mypy/mypy/plugins/
@language python
@tabwidth -4
from typing import List, Optional, Union

from mypy.nodes import (
    ARG_POS, MDEF, Argument, Block, CallExpr, ClassDef, Expression, SYMBOL_FUNCBASE_TYPES,
    FuncDef, PassStmt, RefExpr, SymbolTableNode, Var, JsonDict,
)
from mypy.plugin import CheckerPluginInterface, ClassDefContext, SemanticAnalyzerPluginInterface
from mypy.semanal import set_callable_name, ALLOW_INCOMPATIBLE_OVERRIDE
from mypy.types import (
    CallableType, Overloaded, Type, TypeVarType, deserialize_type, get_proper_type,
)
from mypy.typevars import fill_typevars
from mypy.util import get_unique_redefinition_name
from mypy.typeops import try_getting_str_literals  # noqa: F401  # Part of public API
from mypy.fixup import TypeFixer


@others
</t>
<t tx="ekr.20220525082936.398">def _get_decorator_bool_argument(
        ctx: ClassDefContext,
        name: str,
        default: bool,
) -&gt; bool:
    """Return the bool argument for the decorator.

    This handles both @decorator(...) and @decorator.
    """
    if isinstance(ctx.reason, CallExpr):
        return _get_bool_argument(ctx, ctx.reason, name, default)
    else:
        return default


</t>
<t tx="ekr.20220525082936.399">def _get_bool_argument(ctx: ClassDefContext, expr: CallExpr,
                       name: str, default: bool) -&gt; bool:
    """Return the boolean value for an argument to a call or the
    default if it's not found.
    """
    attr_value = _get_argument(expr, name)
    if attr_value:
        ret = ctx.api.parse_bool(attr_value)
        if ret is None:
            ctx.api.fail(f'"{name}" argument must be True or False.', expr)
            return default
        return ret
    return default


</t>
<t tx="ekr.20220525082936.4">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Type visitor classes.

This module defines the type visitors that are intended to be
subclassed by other code.  They have been separated out into their own
module to ease converting mypy to run under mypyc, since currently
mypyc-extension classes can extend interpreted classes but not the
other way around. Separating them out, then, allows us to compile
types before we can compile everything that uses a TypeVisitor.

The visitors are all re-exported from mypy.types and that is how
other modules refer to them.
"""

from abc import abstractmethod
from mypy.backports import OrderedDict
from typing import Generic, TypeVar, cast, Any, List, Callable, Iterable, Optional, Set, Sequence
from mypy_extensions import trait, mypyc_attr

T = TypeVar('T')

from mypy.types import (
    Type, AnyType, CallableType, Overloaded, TupleType, TypedDictType, LiteralType,
    Parameters, RawExpressionType, Instance, NoneType, TypeType,
    UnionType, TypeVarType, PartialType, DeletedType, UninhabitedType, TypeVarLikeType,
    UnboundType, ErasedType, StarType, EllipsisType, TypeList, CallableArgument,
    PlaceholderType, TypeAliasType, ParamSpecType, UnpackType, TypeVarTupleType,
    get_proper_type
)


@others
</t>
<t tx="ekr.20220525082936.40">def visit_deleted_type(self, t: DeletedType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082936.400">def _get_argument(call: CallExpr, name: str) -&gt; Optional[Expression]:
    """Return the expression for the specific argument."""
    # To do this we use the CallableType of the callee to find the FormalArgument,
    # then walk the actual CallExpr looking for the appropriate argument.
    #
    # Note: I'm not hard-coding the index so that in the future we can support other
    # attrib and class makers.
    if not isinstance(call.callee, RefExpr):
        return None

    callee_type = None
    callee_node = call.callee.node
    if (isinstance(callee_node, (Var, SYMBOL_FUNCBASE_TYPES))
            and callee_node.type):
        callee_node_type = get_proper_type(callee_node.type)
        if isinstance(callee_node_type, Overloaded):
            # We take the last overload.
            callee_type = callee_node_type.items[-1]
        elif isinstance(callee_node_type, CallableType):
            callee_type = callee_node_type

    if not callee_type:
        return None

    argument = callee_type.argument_by_name(name)
    if not argument:
        return None
    assert argument.name

    for i, (attr_name, attr_value) in enumerate(zip(call.arg_names, call.args)):
        if argument.pos is not None and not attr_name and i == argument.pos:
            return attr_value
        if attr_name == argument.name:
            return attr_value
    return None


</t>
<t tx="ekr.20220525082936.401">def add_method(
        ctx: ClassDefContext,
        name: str,
        args: List[Argument],
        return_type: Type,
        self_type: Optional[Type] = None,
        tvar_def: Optional[TypeVarType] = None,
) -&gt; None:
    """
    Adds a new method to a class.
    Deprecated, use add_method_to_class() instead.
    """
    add_method_to_class(ctx.api, ctx.cls,
                        name=name,
                        args=args,
                        return_type=return_type,
                        self_type=self_type,
                        tvar_def=tvar_def)


</t>
<t tx="ekr.20220525082936.402">def add_method_to_class(
        api: Union[SemanticAnalyzerPluginInterface, CheckerPluginInterface],
        cls: ClassDef,
        name: str,
        args: List[Argument],
        return_type: Type,
        self_type: Optional[Type] = None,
        tvar_def: Optional[TypeVarType] = None,
) -&gt; None:
    """Adds a new method to a class definition."""
    info = cls.info

    # First remove any previously generated methods with the same name
    # to avoid clashes and problems in the semantic analyzer.
    if name in info.names:
        sym = info.names[name]
        if sym.plugin_generated and isinstance(sym.node, FuncDef):
            cls.defs.body.remove(sym.node)

    self_type = self_type or fill_typevars(info)
    if isinstance(api, SemanticAnalyzerPluginInterface):
        function_type = api.named_type('builtins.function')
    else:
        function_type = api.named_generic_type('builtins.function', [])

    args = [Argument(Var('self'), self_type, None, ARG_POS)] + args
    arg_types, arg_names, arg_kinds = [], [], []
    for arg in args:
        assert arg.type_annotation, 'All arguments must be fully typed.'
        arg_types.append(arg.type_annotation)
        arg_names.append(arg.variable.name)
        arg_kinds.append(arg.kind)

    signature = CallableType(arg_types, arg_kinds, arg_names, return_type, function_type)
    if tvar_def:
        signature.variables = [tvar_def]

    func = FuncDef(name, args, Block([PassStmt()]))
    func.info = info
    func.type = set_callable_name(signature, func)
    func._fullname = info.fullname + '.' + name
    func.line = info.line

    # NOTE: we would like the plugin generated node to dominate, but we still
    # need to keep any existing definitions so they get semantically analyzed.
    if name in info.names:
        # Get a nice unique name instead.
        r_name = get_unique_redefinition_name(name, info.names)
        info.names[r_name] = info.names[name]

    info.names[name] = SymbolTableNode(MDEF, func, plugin_generated=True)
    info.defn.defs.body.append(func)


</t>
<t tx="ekr.20220525082936.403">def add_attribute_to_class(
        api: SemanticAnalyzerPluginInterface,
        cls: ClassDef,
        name: str,
        typ: Type,
        final: bool = False,
        no_serialize: bool = False,
        override_allow_incompatible: bool = False,
) -&gt; None:
    """
    Adds a new attribute to a class definition.
    This currently only generates the symbol table entry and no corresponding AssignmentStatement
    """
    info = cls.info

    # NOTE: we would like the plugin generated node to dominate, but we still
    # need to keep any existing definitions so they get semantically analyzed.
    if name in info.names:
        # Get a nice unique name instead.
        r_name = get_unique_redefinition_name(name, info.names)
        info.names[r_name] = info.names[name]

    node = Var(name, typ)
    node.info = info
    node.is_final = final
    if name in ALLOW_INCOMPATIBLE_OVERRIDE:
        node.allow_incompatible_override = True
    else:
        node.allow_incompatible_override = override_allow_incompatible
    node._fullname = info.fullname + '.' + name
    info.names[name] = SymbolTableNode(
        MDEF,
        node,
        plugin_generated=True,
        no_serialize=no_serialize,
    )


</t>
<t tx="ekr.20220525082936.404">def deserialize_and_fixup_type(
    data: Union[str, JsonDict], api: SemanticAnalyzerPluginInterface
) -&gt; Type:
    typ = deserialize_type(data)
    typ.accept(TypeFixer(api.modules, allow_missing=False))
    return typ
</t>
<t tx="ekr.20220525082936.405">@path C:/Repos/mypy/mypy/plugins/
@language python
@tabwidth -4
"""Plugin to provide accurate types for some parts of the ctypes module."""

from typing import List, Optional

# Fully qualified instead of "from mypy.plugin import ..." to avoid circular import problems.
import mypy.plugin
from mypy import nodes
from mypy.maptype import map_instance_to_supertype
from mypy.messages import format_type
from mypy.subtypes import is_subtype
from mypy.types import (
    AnyType, CallableType, Instance, NoneType, Type, TypeOfAny, UnionType,
    union_items, ProperType, get_proper_type
)
from mypy.typeops import make_simplified_union


@others
</t>
<t tx="ekr.20220525082936.406">def _get_bytes_type(api: 'mypy.plugin.CheckerPluginInterface') -&gt; Instance:
    """Return the type corresponding to bytes on the current Python version.

    This is bytes in Python 3, and str in Python 2.
    """
    return api.named_generic_type(
        'builtins.bytes' if api.options.python_version &gt;= (3,) else 'builtins.str', [])


</t>
<t tx="ekr.20220525082936.407">def _get_text_type(api: 'mypy.plugin.CheckerPluginInterface') -&gt; Instance:
    """Return the type corresponding to Text on the current Python version.

    This is str in Python 3, and unicode in Python 2.
    """
    return api.named_generic_type(
        'builtins.str' if api.options.python_version &gt;= (3,) else 'builtins.unicode', [])


</t>
<t tx="ekr.20220525082936.408">def _find_simplecdata_base_arg(tp: Instance, api: 'mypy.plugin.CheckerPluginInterface'
                               ) -&gt; Optional[ProperType]:
    """Try to find a parametrized _SimpleCData in tp's bases and return its single type argument.

    None is returned if _SimpleCData appears nowhere in tp's (direct or indirect) bases.
    """
    if tp.type.has_base('ctypes._SimpleCData'):
        simplecdata_base = map_instance_to_supertype(tp,
            api.named_generic_type('ctypes._SimpleCData', [AnyType(TypeOfAny.special_form)]).type)
        assert len(simplecdata_base.args) == 1, '_SimpleCData takes exactly one type argument'
        return get_proper_type(simplecdata_base.args[0])
    return None


</t>
<t tx="ekr.20220525082936.409">def _autoconvertible_to_cdata(tp: Type, api: 'mypy.plugin.CheckerPluginInterface') -&gt; Type:
    """Get a type that is compatible with all types that can be implicitly converted to the given
    CData type.

    Examples:
    * c_int -&gt; Union[c_int, int]
    * c_char_p -&gt; Union[c_char_p, bytes, int, NoneType]
    * MyStructure -&gt; MyStructure
    """
    allowed_types = []
    # If tp is a union, we allow all types that are convertible to at least one of the union
    # items. This is not quite correct - strictly speaking, only types convertible to *all* of the
    # union items should be allowed. This may be worth changing in the future, but the more
    # correct algorithm could be too strict to be useful.
    for t in union_items(tp):
        # Every type can be converted from itself (obviously).
        allowed_types.append(t)
        if isinstance(t, Instance):
            unboxed = _find_simplecdata_base_arg(t, api)
            if unboxed is not None:
                # If _SimpleCData appears in tp's (direct or indirect) bases, its type argument
                # specifies the type's "unboxed" version, which can always be converted back to
                # the original "boxed" type.
                allowed_types.append(unboxed)

                if t.type.has_base('ctypes._PointerLike'):
                    # Pointer-like _SimpleCData subclasses can also be converted from
                    # an int or None.
                    allowed_types.append(api.named_generic_type('builtins.int', []))
                    allowed_types.append(NoneType())

    return make_simplified_union(allowed_types)


</t>
<t tx="ekr.20220525082936.41">def visit_instance(self, t: Instance) -&gt; Type:
    last_known_value: Optional[LiteralType] = None
    if t.last_known_value is not None:
        raw_last_known_value = t.last_known_value.accept(self)
        assert isinstance(raw_last_known_value, LiteralType)  # type: ignore
        last_known_value = raw_last_known_value
    return Instance(
        typ=t.type,
        args=self.translate_types(t.args),
        line=t.line,
        column=t.column,
        last_known_value=last_known_value,
    )

</t>
<t tx="ekr.20220525082936.410">def _autounboxed_cdata(tp: Type) -&gt; ProperType:
    """Get the auto-unboxed version of a CData type, if applicable.

    For *direct* _SimpleCData subclasses, the only type argument of _SimpleCData in the bases list
    is returned.
    For all other CData types, including indirect _SimpleCData subclasses, tp is returned as-is.
    """
    tp = get_proper_type(tp)

    if isinstance(tp, UnionType):
        return make_simplified_union([_autounboxed_cdata(t) for t in tp.items])
    elif isinstance(tp, Instance):
        for base in tp.type.bases:
            if base.type.fullname == 'ctypes._SimpleCData':
                # If tp has _SimpleCData as a direct base class,
                # the auto-unboxed type is the single type argument of the _SimpleCData type.
                assert len(base.args) == 1
                return get_proper_type(base.args[0])
    # If tp is not a concrete type, or if there is no _SimpleCData in the bases,
    # the type is not auto-unboxed.
    return tp


</t>
<t tx="ekr.20220525082936.411">def _get_array_element_type(tp: Type) -&gt; Optional[ProperType]:
    """Get the element type of the Array type tp, or None if not specified."""
    tp = get_proper_type(tp)
    if isinstance(tp, Instance):
        assert tp.type.fullname == 'ctypes.Array'
        if len(tp.args) == 1:
            return get_proper_type(tp.args[0])
    return None


</t>
<t tx="ekr.20220525082936.412">def array_constructor_callback(ctx: 'mypy.plugin.FunctionContext') -&gt; Type:
    """Callback to provide an accurate signature for the ctypes.Array constructor."""
    # Extract the element type from the constructor's return type, i. e. the type of the array
    # being constructed.
    et = _get_array_element_type(ctx.default_return_type)
    if et is not None:
        allowed = _autoconvertible_to_cdata(et, ctx.api)
        assert len(ctx.arg_types) == 1, \
            "The stub of the ctypes.Array constructor should have a single vararg parameter"
        for arg_num, (arg_kind, arg_type) in enumerate(zip(ctx.arg_kinds[0], ctx.arg_types[0]), 1):
            if arg_kind == nodes.ARG_POS and not is_subtype(arg_type, allowed):
                ctx.api.msg.fail(
                    'Array constructor argument {} of type {}'
                    ' is not convertible to the array element type {}'
                    .format(arg_num, format_type(arg_type), format_type(et)), ctx.context)
            elif arg_kind == nodes.ARG_STAR:
                ty = ctx.api.named_generic_type("typing.Iterable", [allowed])
                if not is_subtype(arg_type, ty):
                    it = ctx.api.named_generic_type("typing.Iterable", [et])
                    ctx.api.msg.fail(
                        'Array constructor argument {} of type {}'
                        ' is not convertible to the array element type {}'
                        .format(arg_num, format_type(arg_type), format_type(it)), ctx.context)

    return ctx.default_return_type


</t>
<t tx="ekr.20220525082936.413">def array_getitem_callback(ctx: 'mypy.plugin.MethodContext') -&gt; Type:
    """Callback to provide an accurate return type for ctypes.Array.__getitem__."""
    et = _get_array_element_type(ctx.type)
    if et is not None:
        unboxed = _autounboxed_cdata(et)
        assert len(ctx.arg_types) == 1, \
            'The stub of ctypes.Array.__getitem__ should have exactly one parameter'
        assert len(ctx.arg_types[0]) == 1, \
            "ctypes.Array.__getitem__'s parameter should not be variadic"
        index_type = get_proper_type(ctx.arg_types[0][0])
        if isinstance(index_type, Instance):
            if index_type.type.has_base('builtins.int'):
                return unboxed
            elif index_type.type.has_base('builtins.slice'):
                return ctx.api.named_generic_type('builtins.list', [unboxed])
    return ctx.default_return_type


</t>
<t tx="ekr.20220525082936.414">def array_setitem_callback(ctx: 'mypy.plugin.MethodSigContext') -&gt; CallableType:
    """Callback to provide an accurate signature for ctypes.Array.__setitem__."""
    et = _get_array_element_type(ctx.type)
    if et is not None:
        allowed = _autoconvertible_to_cdata(et, ctx.api)
        assert len(ctx.default_signature.arg_types) == 2
        index_type = get_proper_type(ctx.default_signature.arg_types[0])
        if isinstance(index_type, Instance):
            arg_type = None
            if index_type.type.has_base('builtins.int'):
                arg_type = allowed
            elif index_type.type.has_base('builtins.slice'):
                arg_type = ctx.api.named_generic_type('builtins.list', [allowed])
            if arg_type is not None:
                # Note: arg_type can only be None if index_type is invalid, in which case we use
                # the default signature and let mypy report an error about it.
                return ctx.default_signature.copy_modified(
                    arg_types=ctx.default_signature.arg_types[:1] + [arg_type],
                )
    return ctx.default_signature


</t>
<t tx="ekr.20220525082936.415">def array_iter_callback(ctx: 'mypy.plugin.MethodContext') -&gt; Type:
    """Callback to provide an accurate return type for ctypes.Array.__iter__."""
    et = _get_array_element_type(ctx.type)
    if et is not None:
        unboxed = _autounboxed_cdata(et)
        return ctx.api.named_generic_type('typing.Iterator', [unboxed])
    return ctx.default_return_type


</t>
<t tx="ekr.20220525082936.416">def array_value_callback(ctx: 'mypy.plugin.AttributeContext') -&gt; Type:
    """Callback to provide an accurate type for ctypes.Array.value."""
    et = _get_array_element_type(ctx.type)
    if et is not None:
        types: List[Type] = []
        for tp in union_items(et):
            if isinstance(tp, AnyType):
                types.append(AnyType(TypeOfAny.from_another_any, source_any=tp))
            elif isinstance(tp, Instance) and tp.type.fullname == 'ctypes.c_char':
                types.append(_get_bytes_type(ctx.api))
            elif isinstance(tp, Instance) and tp.type.fullname == 'ctypes.c_wchar':
                types.append(_get_text_type(ctx.api))
            else:
                ctx.api.msg.fail(
                    'Array attribute "value" is only available'
                    ' with element type "c_char" or "c_wchar", not {}'
                    .format(format_type(et)), ctx.context)
        return make_simplified_union(types)
    return ctx.default_attr_type


</t>
<t tx="ekr.20220525082936.417">def array_raw_callback(ctx: 'mypy.plugin.AttributeContext') -&gt; Type:
    """Callback to provide an accurate type for ctypes.Array.raw."""
    et = _get_array_element_type(ctx.type)
    if et is not None:
        types: List[Type] = []
        for tp in union_items(et):
            if (isinstance(tp, AnyType)
                    or isinstance(tp, Instance) and tp.type.fullname == 'ctypes.c_char'):
                types.append(_get_bytes_type(ctx.api))
            else:
                ctx.api.msg.fail(
                    'Array attribute "raw" is only available'
                    ' with element type "c_char", not {}'
                    .format(format_type(et)), ctx.context)
        return make_simplified_union(types)
    return ctx.default_attr_type
</t>
<t tx="ekr.20220525082936.418">@path C:/Repos/mypy/mypy/plugins/
@language python
@tabwidth -4
"""Plugin that provides support for dataclasses."""

from typing import Dict, List, Set, Tuple, Optional
from typing_extensions import Final

from mypy.nodes import (
    ARG_OPT, ARG_NAMED, ARG_NAMED_OPT, ARG_POS, ARG_STAR, ARG_STAR2, MDEF,
    Argument, AssignmentStmt, CallExpr,  TypeAlias,  Context, Expression, JsonDict,
    NameExpr, RefExpr, SymbolTableNode, TempNode, TypeInfo, Var, TypeVarExpr,
    PlaceholderNode
)
from mypy.plugin import ClassDefContext, SemanticAnalyzerPluginInterface
from mypy.plugins.common import (
    add_method, _get_decorator_bool_argument, deserialize_and_fixup_type, add_attribute_to_class,
)
from mypy.typeops import map_type_from_supertype
from mypy.types import (
    Type, Instance, NoneType, TypeVarType, CallableType, TupleType, LiteralType,
    get_proper_type, AnyType, TypeOfAny,
)
from mypy.server.trigger import make_wildcard_trigger
from mypy.state import state

# The set of decorators that generate dataclasses.
dataclass_makers: Final = {
    'dataclass',
    'dataclasses.dataclass',
}
# The set of functions that generate dataclass fields.
field_makers: Final = {
    'dataclasses.field',
}


SELF_TVAR_NAME: Final = "_DT"


@others
</t>
<t tx="ekr.20220525082936.419">class DataclassAttribute:
    @others
</t>
<t tx="ekr.20220525082936.42">def visit_type_var(self, t: TypeVarType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082936.420">def __init__(
        self,
        name: str,
        is_in_init: bool,
        is_init_var: bool,
        has_default: bool,
        line: int,
        column: int,
        type: Optional[Type],
        info: TypeInfo,
        kw_only: bool,
) -&gt; None:
    self.name = name
    self.is_in_init = is_in_init
    self.is_init_var = is_init_var
    self.has_default = has_default
    self.line = line
    self.column = column
    self.type = type
    self.info = info
    self.kw_only = kw_only

</t>
<t tx="ekr.20220525082936.421">def to_argument(self) -&gt; Argument:
    arg_kind = ARG_POS
    if self.kw_only and self.has_default:
        arg_kind = ARG_NAMED_OPT
    elif self.kw_only and not self.has_default:
        arg_kind = ARG_NAMED
    elif not self.kw_only and self.has_default:
        arg_kind = ARG_OPT
    return Argument(
        variable=self.to_var(),
        type_annotation=self.type,
        initializer=None,
        kind=arg_kind,
    )

</t>
<t tx="ekr.20220525082936.422">def to_var(self) -&gt; Var:
    return Var(self.name, self.type)

</t>
<t tx="ekr.20220525082936.423">def serialize(self) -&gt; JsonDict:
    assert self.type
    return {
        'name': self.name,
        'is_in_init': self.is_in_init,
        'is_init_var': self.is_init_var,
        'has_default': self.has_default,
        'line': self.line,
        'column': self.column,
        'type': self.type.serialize(),
        'kw_only': self.kw_only,
    }

</t>
<t tx="ekr.20220525082936.424">@classmethod
def deserialize(
    cls, info: TypeInfo, data: JsonDict, api: SemanticAnalyzerPluginInterface
) -&gt; 'DataclassAttribute':
    data = data.copy()
    if data.get('kw_only') is None:
        data['kw_only'] = False
    typ = deserialize_and_fixup_type(data.pop('type'), api)
    return cls(type=typ, info=info, **data)

</t>
<t tx="ekr.20220525082936.425">def expand_typevar_from_subtype(self, sub_type: TypeInfo) -&gt; None:
    """Expands type vars in the context of a subtype when an attribute is inherited
    from a generic super type."""
    if self.type is not None:
        self.type = map_type_from_supertype(self.type, sub_type, self.info)


</t>
<t tx="ekr.20220525082936.426">class DataclassTransformer:
    """Implement the behavior of @dataclass.

    Note that this may be executed multiple times on the same class, so
    everything here must be idempotent.

    This runs after the main semantic analysis pass, so you can assume that
    there are no placeholders.
    """

    @others
</t>
<t tx="ekr.20220525082936.427">def __init__(self, ctx: ClassDefContext) -&gt; None:
    self._ctx = ctx

</t>
<t tx="ekr.20220525082936.428">def transform(self) -&gt; bool:
    """Apply all the necessary transformations to the underlying
    dataclass so as to ensure it is fully type checked according
    to the rules in PEP 557.
    """
    ctx = self._ctx
    info = self._ctx.cls.info
    attributes = self.collect_attributes()
    if attributes is None:
        # Some definitions are not ready. We need another pass.
        return False
    for attr in attributes:
        if attr.type is None:
            return False
    decorator_arguments = {
        'init': _get_decorator_bool_argument(self._ctx, 'init', True),
        'eq': _get_decorator_bool_argument(self._ctx, 'eq', True),
        'order': _get_decorator_bool_argument(self._ctx, 'order', False),
        'frozen': _get_decorator_bool_argument(self._ctx, 'frozen', False),
        'slots': _get_decorator_bool_argument(self._ctx, 'slots', False),
        'match_args': _get_decorator_bool_argument(self._ctx, 'match_args', True),
    }
    py_version = self._ctx.api.options.python_version

    # If there are no attributes, it may be that the semantic analyzer has not
    # processed them yet. In order to work around this, we can simply skip generating
    # __init__ if there are no attributes, because if the user truly did not define any,
    # then the object default __init__ with an empty signature will be present anyway.
    if (decorator_arguments['init'] and
            ('__init__' not in info.names or info.names['__init__'].plugin_generated) and
            attributes):

        args = [attr.to_argument() for attr in attributes if attr.is_in_init
                and not self._is_kw_only_type(attr.type)]

        if info.fallback_to_any:
            # Make positional args optional since we don't know their order.
            # This will at least allow us to typecheck them if they are called
            # as kwargs
            for arg in args:
                if arg.kind == ARG_POS:
                    arg.kind = ARG_OPT

            nameless_var = Var('')
            args = [Argument(nameless_var, AnyType(TypeOfAny.explicit), None, ARG_STAR),
                    *args,
                    Argument(nameless_var, AnyType(TypeOfAny.explicit), None, ARG_STAR2),
                    ]

        add_method(
            ctx,
            '__init__',
            args=args,
            return_type=NoneType(),
        )

    if (decorator_arguments['eq'] and info.get('__eq__') is None or
            decorator_arguments['order']):
        # Type variable for self types in generated methods.
        obj_type = ctx.api.named_type('builtins.object')
        self_tvar_expr = TypeVarExpr(SELF_TVAR_NAME, info.fullname + '.' + SELF_TVAR_NAME,
                                     [], obj_type)
        info.names[SELF_TVAR_NAME] = SymbolTableNode(MDEF, self_tvar_expr)

    # Add &lt;, &gt;, &lt;=, &gt;=, but only if the class has an eq method.
    if decorator_arguments['order']:
        if not decorator_arguments['eq']:
            ctx.api.fail('eq must be True if order is True', ctx.cls)

        for method_name in ['__lt__', '__gt__', '__le__', '__ge__']:
            # Like for __eq__ and __ne__, we want "other" to match
            # the self type.
            obj_type = ctx.api.named_type('builtins.object')
            order_tvar_def = TypeVarType(SELF_TVAR_NAME, info.fullname + '.' + SELF_TVAR_NAME,
                                        -1, [], obj_type)
            order_return_type = ctx.api.named_type('builtins.bool')
            order_args = [
                Argument(Var('other', order_tvar_def), order_tvar_def, None, ARG_POS)
            ]

            existing_method = info.get(method_name)
            if existing_method is not None and not existing_method.plugin_generated:
                assert existing_method.node
                ctx.api.fail(
                    f'You may not have a custom {method_name} method when order=True',
                    existing_method.node,
                )

            add_method(
                ctx,
                method_name,
                args=order_args,
                return_type=order_return_type,
                self_type=order_tvar_def,
                tvar_def=order_tvar_def,
            )

    if decorator_arguments['frozen']:
        self._propertize_callables(attributes, settable=False)
        self._freeze(attributes)
    else:
        self._propertize_callables(attributes)

    if decorator_arguments['slots']:
        self.add_slots(info, attributes, correct_version=py_version &gt;= (3, 10))

    self.reset_init_only_vars(info, attributes)

    if (decorator_arguments['match_args'] and
            ('__match_args__' not in info.names or
             info.names['__match_args__'].plugin_generated) and
            attributes):
        str_type = ctx.api.named_type("builtins.str")
        literals: List[Type] = [LiteralType(attr.name, str_type)
                    for attr in attributes if attr.is_in_init]
        match_args_type = TupleType(literals, ctx.api.named_type("builtins.tuple"))
        add_attribute_to_class(ctx.api, ctx.cls, "__match_args__", match_args_type)

    self._add_dataclass_fields_magic_attribute()

    info.metadata['dataclass'] = {
        'attributes': [attr.serialize() for attr in attributes],
        'frozen': decorator_arguments['frozen'],
    }

    return True

</t>
<t tx="ekr.20220525082936.429">def add_slots(self,
              info: TypeInfo,
              attributes: List[DataclassAttribute],
              *,
              correct_version: bool) -&gt; None:
    if not correct_version:
        # This means that version is lower than `3.10`,
        # it is just a non-existent argument for `dataclass` function.
        self._ctx.api.fail(
            'Keyword argument "slots" for "dataclass" '
            'is only valid in Python 3.10 and higher',
            self._ctx.reason,
        )
        return

    generated_slots = {attr.name for attr in attributes}
    if ((info.slots is not None and info.slots != generated_slots)
            or info.names.get('__slots__')):
        # This means we have a slots conflict.
        # Class explicitly specifies a different `__slots__` field.
        # And `@dataclass(slots=True)` is used.
        # In runtime this raises a type error.
        self._ctx.api.fail(
            '"{}" both defines "__slots__" and is used with "slots=True"'.format(
                self._ctx.cls.name,
            ),
            self._ctx.cls,
        )
        return

    info.slots = generated_slots

</t>
<t tx="ekr.20220525082936.43">def visit_param_spec(self, t: ParamSpecType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082936.430">def reset_init_only_vars(self, info: TypeInfo, attributes: List[DataclassAttribute]) -&gt; None:
    """Remove init-only vars from the class and reset init var declarations."""
    for attr in attributes:
        if attr.is_init_var:
            if attr.name in info.names:
                del info.names[attr.name]
            else:
                # Nodes of superclass InitVars not used in __init__ cannot be reached.
                assert attr.is_init_var
            for stmt in info.defn.defs.body:
                if isinstance(stmt, AssignmentStmt) and stmt.unanalyzed_type:
                    lvalue = stmt.lvalues[0]
                    if isinstance(lvalue, NameExpr) and lvalue.name == attr.name:
                        # Reset node so that another semantic analysis pass will
                        # recreate a symbol node for this attribute.
                        lvalue.node = None

</t>
<t tx="ekr.20220525082936.431">def collect_attributes(self) -&gt; Optional[List[DataclassAttribute]]:
    """Collect all attributes declared in the dataclass and its parents.

    All assignments of the form

      a: SomeType
      b: SomeOtherType = ...

    are collected.

    Return None if some dataclass base class hasn't been processed
    yet and thus we'll need to ask for another pass.
    """
    # First, collect attributes belonging to the current class.
    ctx = self._ctx
    cls = self._ctx.cls
    attrs: List[DataclassAttribute] = []
    known_attrs: Set[str] = set()
    kw_only = _get_decorator_bool_argument(ctx, 'kw_only', False)
    for stmt in cls.defs.body:
        # Any assignment that doesn't use the new type declaration
        # syntax can be ignored out of hand.
        if not (isinstance(stmt, AssignmentStmt) and stmt.new_syntax):
            continue

        # a: int, b: str = 1, 'foo' is not supported syntax so we
        # don't have to worry about it.
        lhs = stmt.lvalues[0]
        if not isinstance(lhs, NameExpr):
            continue

        sym = cls.info.names.get(lhs.name)
        if sym is None:
            # There was probably a semantic analysis error.
            continue

        node = sym.node
        assert not isinstance(node, PlaceholderNode)

        if isinstance(node, TypeAlias):
            ctx.api.fail(
                (
                    'Type aliases inside dataclass definitions '
                    'are not supported at runtime'
                ),
                node
            )
            # Skip processing this node. This doesn't match the runtime behaviour,
            # but the only alternative would be to modify the SymbolTable,
            # and it's a little hairy to do that in a plugin.
            continue

        assert isinstance(node, Var)

        # x: ClassVar[int] is ignored by dataclasses.
        if node.is_classvar:
            continue

        # x: InitVar[int] is turned into x: int and is removed from the class.
        is_init_var = False
        node_type = get_proper_type(node.type)
        if (isinstance(node_type, Instance) and
                node_type.type.fullname == 'dataclasses.InitVar'):
            is_init_var = True
            node.type = node_type.args[0]

        if self._is_kw_only_type(node_type):
            kw_only = True

        has_field_call, field_args = _collect_field_args(stmt.rvalue, ctx)

        is_in_init_param = field_args.get('init')
        if is_in_init_param is None:
            is_in_init = True
        else:
            is_in_init = bool(ctx.api.parse_bool(is_in_init_param))

        has_default = False
        # Ensure that something like x: int = field() is rejected
        # after an attribute with a default.
        if has_field_call:
            has_default = 'default' in field_args or 'default_factory' in field_args

        # All other assignments are already type checked.
        elif not isinstance(stmt.rvalue, TempNode):
            has_default = True

        if not has_default:
            # Make all non-default attributes implicit because they are de-facto set
            # on self in the generated __init__(), not in the class body.
            sym.implicit = True

        is_kw_only = kw_only
        # Use the kw_only field arg if it is provided. Otherwise use the
        # kw_only value from the decorator parameter.
        field_kw_only_param = field_args.get('kw_only')
        if field_kw_only_param is not None:
            is_kw_only = bool(ctx.api.parse_bool(field_kw_only_param))

        known_attrs.add(lhs.name)
        attrs.append(DataclassAttribute(
            name=lhs.name,
            is_in_init=is_in_init,
            is_init_var=is_init_var,
            has_default=has_default,
            line=stmt.line,
            column=stmt.column,
            type=sym.type,
            info=cls.info,
            kw_only=is_kw_only,
        ))

    # Next, collect attributes belonging to any class in the MRO
    # as long as those attributes weren't already collected.  This
    # makes it possible to overwrite attributes in subclasses.
    # copy() because we potentially modify all_attrs below and if this code requires debugging
    # we'll have unmodified attrs laying around.
    all_attrs = attrs.copy()
    for info in cls.info.mro[1:-1]:
        if 'dataclass_tag' in info.metadata and 'dataclass' not in info.metadata:
            # We haven't processed the base class yet. Need another pass.
            return None
        if 'dataclass' not in info.metadata:
            continue

        super_attrs = []
        # Each class depends on the set of attributes in its dataclass ancestors.
        ctx.api.add_plugin_dependency(make_wildcard_trigger(info.fullname))

        for data in info.metadata["dataclass"]["attributes"]:
            name: str = data["name"]
            if name not in known_attrs:
                attr = DataclassAttribute.deserialize(info, data, ctx.api)
                # TODO: We shouldn't be performing type operations during the main
                #       semantic analysis pass, since some TypeInfo attributes might
                #       still be in flux. This should be performed in a later phase.
                with state.strict_optional_set(ctx.api.options.strict_optional):
                    attr.expand_typevar_from_subtype(ctx.cls.info)
                known_attrs.add(name)
                super_attrs.append(attr)
            elif all_attrs:
                # How early in the attribute list an attribute appears is determined by the
                # reverse MRO, not simply MRO.
                # See https://docs.python.org/3/library/dataclasses.html#inheritance for
                # details.
                for attr in all_attrs:
                    if attr.name == name:
                        all_attrs.remove(attr)
                        super_attrs.append(attr)
                        break
        all_attrs = super_attrs + all_attrs
        all_attrs.sort(key=lambda a: a.kw_only)

    # Ensure that arguments without a default don't follow
    # arguments that have a default.
    found_default = False
    # Ensure that the KW_ONLY sentinel is only provided once
    found_kw_sentinel = False
    for attr in all_attrs:
        # If we find any attribute that is_in_init, not kw_only, and that
        # doesn't have a default after one that does have one,
        # then that's an error.
        if found_default and attr.is_in_init and not attr.has_default and not attr.kw_only:
            # If the issue comes from merging different classes, report it
            # at the class definition point.
            context = (Context(line=attr.line, column=attr.column) if attr in attrs
                       else ctx.cls)
            ctx.api.fail(
                'Attributes without a default cannot follow attributes with one',
                context,
            )

        found_default = found_default or (attr.has_default and attr.is_in_init)
        if found_kw_sentinel and self._is_kw_only_type(attr.type):
            context = (Context(line=attr.line, column=attr.column) if attr in attrs
                       else ctx.cls)
            ctx.api.fail(
                'There may not be more than one field with the KW_ONLY type',
                context,
            )
        found_kw_sentinel = found_kw_sentinel or self._is_kw_only_type(attr.type)

    return all_attrs

</t>
<t tx="ekr.20220525082936.432">def _freeze(self, attributes: List[DataclassAttribute]) -&gt; None:
    """Converts all attributes to @property methods in order to
    emulate frozen classes.
    """
    info = self._ctx.cls.info
    for attr in attributes:
        sym_node = info.names.get(attr.name)
        if sym_node is not None:
            var = sym_node.node
            assert isinstance(var, Var)
            var.is_property = True
        else:
            var = attr.to_var()
            var.info = info
            var.is_property = True
            var._fullname = info.fullname + '.' + var.name
            info.names[var.name] = SymbolTableNode(MDEF, var)

</t>
<t tx="ekr.20220525082936.433">def _propertize_callables(self,
                          attributes: List[DataclassAttribute],
                          settable: bool = True) -&gt; None:
    """Converts all attributes with callable types to @property methods.

    This avoids the typechecker getting confused and thinking that
    `my_dataclass_instance.callable_attr(foo)` is going to receive a
    `self` argument (it is not).

    """
    info = self._ctx.cls.info
    for attr in attributes:
        if isinstance(get_proper_type(attr.type), CallableType):
            var = attr.to_var()
            var.info = info
            var.is_property = True
            var.is_settable_property = settable
            var._fullname = info.fullname + '.' + var.name
            info.names[var.name] = SymbolTableNode(MDEF, var)

</t>
<t tx="ekr.20220525082936.434">def _is_kw_only_type(self, node: Optional[Type]) -&gt; bool:
    """Checks if the type of the node is the KW_ONLY sentinel value."""
    if node is None:
        return False
    node_type = get_proper_type(node)
    if not isinstance(node_type, Instance):
        return False
    return node_type.type.fullname == 'dataclasses.KW_ONLY'

</t>
<t tx="ekr.20220525082936.435">def _add_dataclass_fields_magic_attribute(self) -&gt; None:
    attr_name = '__dataclass_fields__'
    any_type = AnyType(TypeOfAny.explicit)
    field_type = self._ctx.api.named_type_or_none('dataclasses.Field', [any_type]) or any_type
    attr_type = self._ctx.api.named_type('builtins.dict', [
        self._ctx.api.named_type('builtins.str'),
        field_type,
    ])
    var = Var(name=attr_name, type=attr_type)
    var.info = self._ctx.cls.info
    var._fullname = self._ctx.cls.info.fullname + '.' + attr_name
    self._ctx.cls.info.names[attr_name] = SymbolTableNode(
        kind=MDEF,
        node=var,
        plugin_generated=True,
    )


</t>
<t tx="ekr.20220525082936.436">def dataclass_tag_callback(ctx: ClassDefContext) -&gt; None:
    """Record that we have a dataclass in the main semantic analysis pass.

    The later pass implemented by DataclassTransformer will use this
    to detect dataclasses in base classes.
    """
    # The value is ignored, only the existence matters.
    ctx.cls.info.metadata['dataclass_tag'] = {}


</t>
<t tx="ekr.20220525082936.437">def dataclass_class_maker_callback(ctx: ClassDefContext) -&gt; bool:
    """Hooks into the class typechecking process to add support for dataclasses.
    """
    transformer = DataclassTransformer(ctx)
    return transformer.transform()


</t>
<t tx="ekr.20220525082936.438">def _collect_field_args(expr: Expression,
                        ctx: ClassDefContext) -&gt; Tuple[bool, Dict[str, Expression]]:
    """Returns a tuple where the first value represents whether or not
    the expression is a call to dataclass.field and the second is a
    dictionary of the keyword arguments that field() was called with.
    """
    if (
            isinstance(expr, CallExpr) and
            isinstance(expr.callee, RefExpr) and
            expr.callee.fullname in field_makers
    ):
        # field() only takes keyword arguments.
        args = {}
        for name, arg, kind in zip(expr.arg_names, expr.args, expr.arg_kinds):
            if not kind.is_named():
                if kind.is_named(star=True):
                    # This means that `field` is used with `**` unpacking,
                    # the best we can do for now is not to fail.
                    # TODO: we can infer what's inside `**` and try to collect it.
                    message = 'Unpacking **kwargs in "field()" is not supported'
                else:
                    message = '"field()" does not accept positional arguments'
                ctx.api.fail(message, expr)
                return True, {}
            assert name is not None
            args[name] = arg
        return True, args
    return False, {}
</t>
<t tx="ekr.20220525082936.439">@path C:/Repos/mypy/mypy/plugins/
@language python
@tabwidth -4
from functools import partial
from typing import Callable, Optional, List

from mypy import message_registry
from mypy.nodes import StrExpr, IntExpr, DictExpr, UnaryExpr
from mypy.plugin import (
    Plugin, FunctionContext, MethodContext, MethodSigContext, AttributeContext, ClassDefContext
)
from mypy.plugins.common import try_getting_str_literals
from mypy.types import (
    FunctionLike, Type, Instance, AnyType, TypeOfAny, CallableType, NoneType, TypedDictType,
    TypeVarType, TPDICT_FB_NAMES, get_proper_type, LiteralType, TupleType
)
from mypy.subtypes import is_subtype
from mypy.typeops import make_simplified_union
from mypy.checkexpr import is_literal_type_like


@others
</t>
<t tx="ekr.20220525082936.44">def visit_parameters(self, t: Parameters) -&gt; Type:
    return t.copy_modified(arg_types=self.translate_types(t.arg_types))

</t>
<t tx="ekr.20220525082936.440">class DefaultPlugin(Plugin):
    """Type checker plugin that is enabled by default."""

    @others
</t>
<t tx="ekr.20220525082936.441">def get_function_hook(self, fullname: str
                      ) -&gt; Optional[Callable[[FunctionContext], Type]]:
    from mypy.plugins import ctypes, singledispatch

    if fullname in ('contextlib.contextmanager', 'contextlib.asynccontextmanager'):
        return contextmanager_callback
    elif fullname == 'ctypes.Array':
        return ctypes.array_constructor_callback
    elif fullname == 'functools.singledispatch':
        return singledispatch.create_singledispatch_function_callback
    return None

</t>
<t tx="ekr.20220525082936.442">def get_method_signature_hook(self, fullname: str
                              ) -&gt; Optional[Callable[[MethodSigContext], FunctionLike]]:
    from mypy.plugins import ctypes, singledispatch

    if fullname == 'typing.Mapping.get':
        return typed_dict_get_signature_callback
    elif fullname in {n + '.setdefault' for n in TPDICT_FB_NAMES}:
        return typed_dict_setdefault_signature_callback
    elif fullname in {n + '.pop' for n in TPDICT_FB_NAMES}:
        return typed_dict_pop_signature_callback
    elif fullname in {n + '.update' for n in TPDICT_FB_NAMES}:
        return typed_dict_update_signature_callback
    elif fullname == 'ctypes.Array.__setitem__':
        return ctypes.array_setitem_callback
    elif fullname == singledispatch.SINGLEDISPATCH_CALLABLE_CALL_METHOD:
        return singledispatch.call_singledispatch_function_callback
    return None

</t>
<t tx="ekr.20220525082936.443">def get_method_hook(self, fullname: str
                    ) -&gt; Optional[Callable[[MethodContext], Type]]:
    from mypy.plugins import ctypes, singledispatch

    if fullname == 'typing.Mapping.get':
        return typed_dict_get_callback
    elif fullname == 'builtins.int.__pow__':
        return int_pow_callback
    elif fullname == 'builtins.int.__neg__':
        return int_neg_callback
    elif fullname in ('builtins.tuple.__mul__', 'builtins.tuple.__rmul__'):
        return tuple_mul_callback
    elif fullname in {n + '.setdefault' for n in TPDICT_FB_NAMES}:
        return typed_dict_setdefault_callback
    elif fullname in {n + '.pop' for n in TPDICT_FB_NAMES}:
        return typed_dict_pop_callback
    elif fullname in {n + '.__delitem__' for n in TPDICT_FB_NAMES}:
        return typed_dict_delitem_callback
    elif fullname == 'ctypes.Array.__getitem__':
        return ctypes.array_getitem_callback
    elif fullname == 'ctypes.Array.__iter__':
        return ctypes.array_iter_callback
    elif fullname == singledispatch.SINGLEDISPATCH_REGISTER_METHOD:
        return singledispatch.singledispatch_register_callback
    elif fullname == singledispatch.REGISTER_CALLABLE_CALL_METHOD:
        return singledispatch.call_singledispatch_function_after_register_argument
    return None

</t>
<t tx="ekr.20220525082936.444">def get_attribute_hook(self, fullname: str
                       ) -&gt; Optional[Callable[[AttributeContext], Type]]:
    from mypy.plugins import ctypes
    from mypy.plugins import enums

    if fullname == 'ctypes.Array.value':
        return ctypes.array_value_callback
    elif fullname == 'ctypes.Array.raw':
        return ctypes.array_raw_callback
    elif fullname in enums.ENUM_NAME_ACCESS:
        return enums.enum_name_callback
    elif fullname in enums.ENUM_VALUE_ACCESS:
        return enums.enum_value_callback
    return None

</t>
<t tx="ekr.20220525082936.445">def get_class_decorator_hook(self, fullname: str
                             ) -&gt; Optional[Callable[[ClassDefContext], None]]:
    from mypy.plugins import dataclasses
    from mypy.plugins import attrs

    # These dataclass and attrs hooks run in the main semantic analysis pass
    # and only tag known dataclasses/attrs classes, so that the second
    # hooks (in get_class_decorator_hook_2) can detect dataclasses/attrs classes
    # in the MRO.
    if fullname in dataclasses.dataclass_makers:
        return dataclasses.dataclass_tag_callback
    if (fullname in attrs.attr_class_makers
            or fullname in attrs.attr_dataclass_makers
            or fullname in attrs.attr_frozen_makers
            or fullname in attrs.attr_define_makers):
        return attrs.attr_tag_callback

    return None

</t>
<t tx="ekr.20220525082936.446">def get_class_decorator_hook_2(self, fullname: str
                               ) -&gt; Optional[Callable[[ClassDefContext], bool]]:
    from mypy.plugins import dataclasses
    from mypy.plugins import functools
    from mypy.plugins import attrs

    if fullname in dataclasses.dataclass_makers:
        return dataclasses.dataclass_class_maker_callback
    elif fullname in functools.functools_total_ordering_makers:
        return functools.functools_total_ordering_maker_callback
    elif fullname in attrs.attr_class_makers:
        return attrs.attr_class_maker_callback
    elif fullname in attrs.attr_dataclass_makers:
        return partial(
            attrs.attr_class_maker_callback,
            auto_attribs_default=True,
        )
    elif fullname in attrs.attr_frozen_makers:
        return partial(
            attrs.attr_class_maker_callback,
            auto_attribs_default=None,
            frozen_default=True,
        )
    elif fullname in attrs.attr_define_makers:
        return partial(
            attrs.attr_class_maker_callback,
            auto_attribs_default=None,
        )

    return None


</t>
<t tx="ekr.20220525082936.447">def contextmanager_callback(ctx: FunctionContext) -&gt; Type:
    """Infer a better return type for 'contextlib.contextmanager'."""
    # Be defensive, just in case.
    if ctx.arg_types and len(ctx.arg_types[0]) == 1:
        arg_type = get_proper_type(ctx.arg_types[0][0])
        default_return = get_proper_type(ctx.default_return_type)
        if (isinstance(arg_type, CallableType)
                and isinstance(default_return, CallableType)):
            # The stub signature doesn't preserve information about arguments so
            # add them back here.
            return default_return.copy_modified(
                arg_types=arg_type.arg_types,
                arg_kinds=arg_type.arg_kinds,
                arg_names=arg_type.arg_names,
                variables=arg_type.variables,
                is_ellipsis_args=arg_type.is_ellipsis_args)
    return ctx.default_return_type


</t>
<t tx="ekr.20220525082936.448">def typed_dict_get_signature_callback(ctx: MethodSigContext) -&gt; CallableType:
    """Try to infer a better signature type for TypedDict.get.

    This is used to get better type context for the second argument that
    depends on a TypedDict value type.
    """
    signature = ctx.default_signature
    if (isinstance(ctx.type, TypedDictType)
            and len(ctx.args) == 2
            and len(ctx.args[0]) == 1
            and isinstance(ctx.args[0][0], StrExpr)
            and len(signature.arg_types) == 2
            and len(signature.variables) == 1
            and len(ctx.args[1]) == 1):
        key = ctx.args[0][0].value
        value_type = get_proper_type(ctx.type.items.get(key))
        ret_type = signature.ret_type
        if value_type:
            default_arg = ctx.args[1][0]
            if (isinstance(value_type, TypedDictType)
                    and isinstance(default_arg, DictExpr)
                    and len(default_arg.items) == 0):
                # Caller has empty dict {} as default for typed dict.
                value_type = value_type.copy_modified(required_keys=set())
            # Tweak the signature to include the value type as context. It's
            # only needed for type inference since there's a union with a type
            # variable that accepts everything.
            tv = signature.variables[0]
            assert isinstance(tv, TypeVarType)
            return signature.copy_modified(
                arg_types=[signature.arg_types[0],
                           make_simplified_union([value_type, tv])],
                ret_type=ret_type)
    return signature


</t>
<t tx="ekr.20220525082936.449">def typed_dict_get_callback(ctx: MethodContext) -&gt; Type:
    """Infer a precise return type for TypedDict.get with literal first argument."""
    if (isinstance(ctx.type, TypedDictType)
            and len(ctx.arg_types) &gt;= 1
            and len(ctx.arg_types[0]) == 1):
        keys = try_getting_str_literals(ctx.args[0][0], ctx.arg_types[0][0])
        if keys is None:
            return ctx.default_return_type

        output_types: List[Type] = []
        for key in keys:
            value_type = get_proper_type(ctx.type.items.get(key))
            if value_type is None:
                return ctx.default_return_type

            if len(ctx.arg_types) == 1:
                output_types.append(value_type)
            elif (len(ctx.arg_types) == 2 and len(ctx.arg_types[1]) == 1
                  and len(ctx.args[1]) == 1):
                default_arg = ctx.args[1][0]
                if (isinstance(default_arg, DictExpr) and len(default_arg.items) == 0
                        and isinstance(value_type, TypedDictType)):
                    # Special case '{}' as the default for a typed dict type.
                    output_types.append(value_type.copy_modified(required_keys=set()))
                else:
                    output_types.append(value_type)
                    output_types.append(ctx.arg_types[1][0])

        if len(ctx.arg_types) == 1:
            output_types.append(NoneType())

        return make_simplified_union(output_types)
    return ctx.default_return_type


</t>
<t tx="ekr.20220525082936.45">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082936.450">def typed_dict_pop_signature_callback(ctx: MethodSigContext) -&gt; CallableType:
    """Try to infer a better signature type for TypedDict.pop.

    This is used to get better type context for the second argument that
    depends on a TypedDict value type.
    """
    signature = ctx.default_signature
    str_type = ctx.api.named_generic_type('builtins.str', [])
    if (isinstance(ctx.type, TypedDictType)
            and len(ctx.args) == 2
            and len(ctx.args[0]) == 1
            and isinstance(ctx.args[0][0], StrExpr)
            and len(signature.arg_types) == 2
            and len(signature.variables) == 1
            and len(ctx.args[1]) == 1):
        key = ctx.args[0][0].value
        value_type = ctx.type.items.get(key)
        if value_type:
            # Tweak the signature to include the value type as context. It's
            # only needed for type inference since there's a union with a type
            # variable that accepts everything.
            tv = signature.variables[0]
            assert isinstance(tv, TypeVarType)
            typ = make_simplified_union([value_type, tv])
            return signature.copy_modified(
                arg_types=[str_type, typ],
                ret_type=typ)
    return signature.copy_modified(arg_types=[str_type, signature.arg_types[1]])


</t>
<t tx="ekr.20220525082936.451">def typed_dict_pop_callback(ctx: MethodContext) -&gt; Type:
    """Type check and infer a precise return type for TypedDict.pop."""
    if (isinstance(ctx.type, TypedDictType)
            and len(ctx.arg_types) &gt;= 1
            and len(ctx.arg_types[0]) == 1):
        keys = try_getting_str_literals(ctx.args[0][0], ctx.arg_types[0][0])
        if keys is None:
            ctx.api.fail(message_registry.TYPEDDICT_KEY_MUST_BE_STRING_LITERAL, ctx.context)
            return AnyType(TypeOfAny.from_error)

        value_types = []
        for key in keys:
            if key in ctx.type.required_keys:
                ctx.api.msg.typeddict_key_cannot_be_deleted(ctx.type, key, ctx.context)

            value_type = ctx.type.items.get(key)
            if value_type:
                value_types.append(value_type)
            else:
                ctx.api.msg.typeddict_key_not_found(ctx.type, key, ctx.context)
                return AnyType(TypeOfAny.from_error)

        if len(ctx.args[1]) == 0:
            return make_simplified_union(value_types)
        elif (len(ctx.arg_types) == 2 and len(ctx.arg_types[1]) == 1
              and len(ctx.args[1]) == 1):
            return make_simplified_union([*value_types, ctx.arg_types[1][0]])
    return ctx.default_return_type


</t>
<t tx="ekr.20220525082936.452">def typed_dict_setdefault_signature_callback(ctx: MethodSigContext) -&gt; CallableType:
    """Try to infer a better signature type for TypedDict.setdefault.

    This is used to get better type context for the second argument that
    depends on a TypedDict value type.
    """
    signature = ctx.default_signature
    str_type = ctx.api.named_generic_type('builtins.str', [])
    if (isinstance(ctx.type, TypedDictType)
            and len(ctx.args) == 2
            and len(ctx.args[0]) == 1
            and isinstance(ctx.args[0][0], StrExpr)
            and len(signature.arg_types) == 2
            and len(ctx.args[1]) == 1):
        key = ctx.args[0][0].value
        value_type = ctx.type.items.get(key)
        if value_type:
            return signature.copy_modified(arg_types=[str_type, value_type])
    return signature.copy_modified(arg_types=[str_type, signature.arg_types[1]])


</t>
<t tx="ekr.20220525082936.453">def typed_dict_setdefault_callback(ctx: MethodContext) -&gt; Type:
    """Type check TypedDict.setdefault and infer a precise return type."""
    if (isinstance(ctx.type, TypedDictType)
            and len(ctx.arg_types) == 2
            and len(ctx.arg_types[0]) == 1
            and len(ctx.arg_types[1]) == 1):
        keys = try_getting_str_literals(ctx.args[0][0], ctx.arg_types[0][0])
        if keys is None:
            ctx.api.fail(message_registry.TYPEDDICT_KEY_MUST_BE_STRING_LITERAL, ctx.context)
            return AnyType(TypeOfAny.from_error)

        default_type = ctx.arg_types[1][0]

        value_types = []
        for key in keys:
            value_type = ctx.type.items.get(key)

            if value_type is None:
                ctx.api.msg.typeddict_key_not_found(ctx.type, key, ctx.context)
                return AnyType(TypeOfAny.from_error)

            # The signature_callback above can't always infer the right signature
            # (e.g. when the expression is a variable that happens to be a Literal str)
            # so we need to handle the check ourselves here and make sure the provided
            # default can be assigned to all key-value pairs we're updating.
            if not is_subtype(default_type, value_type):
                ctx.api.msg.typeddict_setdefault_arguments_inconsistent(
                    default_type, value_type, ctx.context)
                return AnyType(TypeOfAny.from_error)

            value_types.append(value_type)

        return make_simplified_union(value_types)
    return ctx.default_return_type


</t>
<t tx="ekr.20220525082936.454">def typed_dict_delitem_callback(ctx: MethodContext) -&gt; Type:
    """Type check TypedDict.__delitem__."""
    if (isinstance(ctx.type, TypedDictType)
            and len(ctx.arg_types) == 1
            and len(ctx.arg_types[0]) == 1):
        keys = try_getting_str_literals(ctx.args[0][0], ctx.arg_types[0][0])
        if keys is None:
            ctx.api.fail(message_registry.TYPEDDICT_KEY_MUST_BE_STRING_LITERAL, ctx.context)
            return AnyType(TypeOfAny.from_error)

        for key in keys:
            if key in ctx.type.required_keys:
                ctx.api.msg.typeddict_key_cannot_be_deleted(ctx.type, key, ctx.context)
            elif key not in ctx.type.items:
                ctx.api.msg.typeddict_key_not_found(ctx.type, key, ctx.context)
    return ctx.default_return_type


</t>
<t tx="ekr.20220525082936.455">def typed_dict_update_signature_callback(ctx: MethodSigContext) -&gt; CallableType:
    """Try to infer a better signature type for TypedDict.update."""
    signature = ctx.default_signature
    if (isinstance(ctx.type, TypedDictType)
            and len(signature.arg_types) == 1):
        arg_type = get_proper_type(signature.arg_types[0])
        assert isinstance(arg_type, TypedDictType)
        arg_type = arg_type.as_anonymous()
        arg_type = arg_type.copy_modified(required_keys=set())
        return signature.copy_modified(arg_types=[arg_type])
    return signature


</t>
<t tx="ekr.20220525082936.456">def int_pow_callback(ctx: MethodContext) -&gt; Type:
    """Infer a more precise return type for int.__pow__."""
    # int.__pow__ has an optional modulo argument,
    # so we expect 2 argument positions
    if (len(ctx.arg_types) == 2
            and len(ctx.arg_types[0]) == 1 and len(ctx.arg_types[1]) == 0):
        arg = ctx.args[0][0]
        if isinstance(arg, IntExpr):
            exponent = arg.value
        elif isinstance(arg, UnaryExpr) and arg.op == '-' and isinstance(arg.expr, IntExpr):
            exponent = -arg.expr.value
        else:
            # Right operand not an int literal or a negated literal -- give up.
            return ctx.default_return_type
        if exponent &gt;= 0:
            return ctx.api.named_generic_type('builtins.int', [])
        else:
            return ctx.api.named_generic_type('builtins.float', [])
    return ctx.default_return_type


</t>
<t tx="ekr.20220525082936.457">def int_neg_callback(ctx: MethodContext) -&gt; Type:
    """Infer a more precise return type for int.__neg__.

    This is mainly used to infer the return type as LiteralType
    if the original underlying object is a LiteralType object
    """
    if isinstance(ctx.type, Instance) and ctx.type.last_known_value is not None:
        value = ctx.type.last_known_value.value
        fallback = ctx.type.last_known_value.fallback
        if isinstance(value, int):
            if is_literal_type_like(ctx.api.type_context[-1]):
                return LiteralType(value=-value, fallback=fallback)
            else:
                return ctx.type.copy_modified(last_known_value=LiteralType(
                    value=-value,
                    fallback=ctx.type,
                    line=ctx.type.line,
                    column=ctx.type.column,
                ))
    elif isinstance(ctx.type, LiteralType):
        value = ctx.type.value
        fallback = ctx.type.fallback
        if isinstance(value, int):
            return LiteralType(value=-value, fallback=fallback)
    return ctx.default_return_type


</t>
<t tx="ekr.20220525082936.458">def tuple_mul_callback(ctx: MethodContext) -&gt; Type:
    """Infer a more precise return type for tuple.__mul__ and tuple.__rmul__.

    This is used to return a specific sized tuple if multiplied by Literal int
    """
    if not isinstance(ctx.type, TupleType):
        return ctx.default_return_type

    arg_type = get_proper_type(ctx.arg_types[0][0])
    if isinstance(arg_type, Instance) and arg_type.last_known_value is not None:
        value = arg_type.last_known_value.value
        if isinstance(value, int):
            return ctx.type.copy_modified(items=ctx.type.items * value)
    elif isinstance(ctx.type, LiteralType):
        value = arg_type.value
        if isinstance(value, int):
            return ctx.type.copy_modified(items=ctx.type.items * value)

    return ctx.default_return_type
</t>
<t tx="ekr.20220525082936.459">@path C:/Repos/mypy/mypy/plugins/
@language python
@tabwidth -4
"""
This file contains a variety of plugins for refining how mypy infers types of
expressions involving Enums.

Currently, this file focuses on providing better inference for expressions like
'SomeEnum.FOO.name' and 'SomeEnum.FOO.value'. Note that the type of both expressions
will vary depending on exactly which instance of SomeEnum we're looking at.

Note that this file does *not* contain all special-cased logic related to enums:
we actually bake some of it directly in to the semantic analysis layer (see
semanal_enum.py).
"""
from typing import Iterable, Optional, Sequence, TypeVar, cast
from typing_extensions import Final

import mypy.plugin  # To avoid circular imports.
from mypy.types import Type, Instance, LiteralType, CallableType, ProperType, get_proper_type
from mypy.typeops import make_simplified_union
from mypy.nodes import TypeInfo
from mypy.subtypes import is_equivalent
from mypy.semanal_enum import ENUM_BASES

ENUM_NAME_ACCESS: Final = {f"{prefix}.name" for prefix in ENUM_BASES} | {
    f"{prefix}._name_" for prefix in ENUM_BASES
}
ENUM_VALUE_ACCESS: Final = {f"{prefix}.value" for prefix in ENUM_BASES} | {
    f"{prefix}._value_" for prefix in ENUM_BASES
}


@others
</t>
<t tx="ekr.20220525082936.46">def visit_partial_type(self, t: PartialType) -&gt; Type:
    return t

</t>
<t tx="ekr.20220525082936.460">def enum_name_callback(ctx: 'mypy.plugin.AttributeContext') -&gt; Type:
    """This plugin refines the 'name' attribute in enums to act as if
    they were declared to be final.

    For example, the expression 'MyEnum.FOO.name' normally is inferred
    to be of type 'str'.

    This plugin will instead make the inferred type be a 'str' where the
    last known value is 'Literal["FOO"]'. This means it would be legal to
    use 'MyEnum.FOO.name' in contexts that expect a Literal type, just like
    any other Final variable or attribute.

    This plugin assumes that the provided context is an attribute access
    matching one of the strings found in 'ENUM_NAME_ACCESS'.
    """
    enum_field_name = _extract_underlying_field_name(ctx.type)
    if enum_field_name is None:
        return ctx.default_attr_type
    else:
        str_type = ctx.api.named_generic_type('builtins.str', [])
        literal_type = LiteralType(enum_field_name, fallback=str_type)
        return str_type.copy_modified(last_known_value=literal_type)


</t>
<t tx="ekr.20220525082936.461">_T = TypeVar('_T')


</t>
<t tx="ekr.20220525082936.462">def _first(it: Iterable[_T]) -&gt; Optional[_T]:
    """Return the first value from any iterable.

    Returns ``None`` if the iterable is empty.
    """
    for val in it:
        return val
    return None


</t>
<t tx="ekr.20220525082936.463">def _infer_value_type_with_auto_fallback(
        ctx: 'mypy.plugin.AttributeContext',
        proper_type: Optional[ProperType]) -&gt; Optional[Type]:
    """Figure out the type of an enum value accounting for `auto()`.

    This method is a no-op for a `None` proper_type and also in the case where
    the type is not "enum.auto"
    """
    if proper_type is None:
        return None
    if not (isinstance(proper_type, Instance) and
            proper_type.type.fullname == 'enum.auto'):
        return proper_type
    assert isinstance(ctx.type, Instance), 'An incorrect ctx.type was passed.'
    info = ctx.type.type
    # Find the first _generate_next_value_ on the mro.  We need to know
    # if it is `Enum` because `Enum` types say that the return-value of
    # `_generate_next_value_` is `Any`.  In reality the default `auto()`
    # returns an `int` (presumably the `Any` in typeshed is to make it
    # easier to subclass and change the returned type).
    type_with_gnv = _first(
        ti for ti in info.mro if ti.names.get('_generate_next_value_'))
    if type_with_gnv is None:
        return ctx.default_attr_type

    stnode = type_with_gnv.names['_generate_next_value_']

    # This should be a `CallableType`
    node_type = get_proper_type(stnode.type)
    if isinstance(node_type, CallableType):
        if type_with_gnv.fullname == 'enum.Enum':
            int_type = ctx.api.named_generic_type('builtins.int', [])
            return int_type
        return get_proper_type(node_type.ret_type)
    return ctx.default_attr_type


</t>
<t tx="ekr.20220525082936.464">def _implements_new(info: TypeInfo) -&gt; bool:
    """Check whether __new__ comes from enum.Enum or was implemented in a
    subclass. In the latter case, we must infer Any as long as mypy can't infer
    the type of _value_ from assignments in __new__.
    """
    type_with_new = _first(
        ti
        for ti in info.mro
        if ti.names.get('__new__') and not ti.fullname.startswith('builtins.')
    )
    if type_with_new is None:
        return False
    return type_with_new.fullname not in ('enum.Enum', 'enum.IntEnum', 'enum.StrEnum')


</t>
<t tx="ekr.20220525082936.465">def enum_value_callback(ctx: 'mypy.plugin.AttributeContext') -&gt; Type:
    """This plugin refines the 'value' attribute in enums to refer to
    the original underlying value. For example, suppose we have the
    following:

        class SomeEnum:
            FOO = A()
            BAR = B()

    By default, mypy will infer that 'SomeEnum.FOO.value' and
    'SomeEnum.BAR.value' both are of type 'Any'. This plugin refines
    this inference so that mypy understands the expressions are
    actually of types 'A' and 'B' respectively. This better reflects
    the actual runtime behavior.

    This plugin works simply by looking up the original value assigned
    to the enum. For example, when this plugin sees 'SomeEnum.BAR.value',
    it will look up whatever type 'BAR' had in the SomeEnum TypeInfo and
    use that as the inferred type of the overall expression.

    This plugin assumes that the provided context is an attribute access
    matching one of the strings found in 'ENUM_VALUE_ACCESS'.
    """
    enum_field_name = _extract_underlying_field_name(ctx.type)
    if enum_field_name is None:
        # We do not know the enum field name (perhaps it was passed to a
        # function and we only know that it _is_ a member).  All is not lost
        # however, if we can prove that the all of the enum members have the
        # same value-type, then it doesn't matter which member was passed in.
        # The value-type is still known.
        if isinstance(ctx.type, Instance):
            info = ctx.type.type

            # As long as mypy doesn't understand attribute creation in __new__,
            # there is no way to predict the value type if the enum class has a
            # custom implementation
            if _implements_new(info):
                return ctx.default_attr_type

            stnodes = (info.get(name) for name in info.names)

            # Enums _can_ have methods and instance attributes.
            # Omit methods and attributes created by assigning to self.*
            # for our value inference.
            node_types = (
                get_proper_type(n.type) if n else None
                for n in stnodes
                if n is None or not n.implicit)
            proper_types = list(
                _infer_value_type_with_auto_fallback(ctx, t)
                for t in node_types
                if t is None or not isinstance(t, CallableType))
            underlying_type = _first(proper_types)
            if underlying_type is None:
                return ctx.default_attr_type

            # At first we try to predict future `value` type if all other items
            # have the same type. For example, `int`.
            # If this is the case, we simply return this type.
            # See https://github.com/python/mypy/pull/9443
            all_same_value_type = all(
                proper_type is not None and proper_type == underlying_type
                for proper_type in proper_types)
            if all_same_value_type:
                if underlying_type is not None:
                    return underlying_type

            # But, after we started treating all `Enum` values as `Final`,
            # we start to infer types in
            # `item = 1` as `Literal[1]`, not just `int`.
            # So, for example types in this `Enum` will all be different:
            #
            #  class Ordering(IntEnum):
            #      one = 1
            #      two = 2
            #      three = 3
            #
            # We will infer three `Literal` types here.
            # They are not the same, but they are equivalent.
            # So, we unify them to make sure `.value` prediction still works.
            # Result will be `Literal[1] | Literal[2] | Literal[3]` for this case.
            all_equivalent_types = all(
                proper_type is not None and is_equivalent(proper_type, underlying_type)
                for proper_type in proper_types)
            if all_equivalent_types:
                return make_simplified_union(cast(Sequence[Type], proper_types))
        return ctx.default_attr_type

    assert isinstance(ctx.type, Instance)
    info = ctx.type.type

    # As long as mypy doesn't understand attribute creation in __new__,
    # there is no way to predict the value type if the enum class has a
    # custom implementation
    if _implements_new(info):
        return ctx.default_attr_type

    stnode = info.get(enum_field_name)
    if stnode is None:
        return ctx.default_attr_type

    underlying_type = _infer_value_type_with_auto_fallback(
        ctx, get_proper_type(stnode.type))
    if underlying_type is None:
        return ctx.default_attr_type

    return underlying_type


</t>
<t tx="ekr.20220525082936.466">def _extract_underlying_field_name(typ: Type) -&gt; Optional[str]:
    """If the given type corresponds to some Enum instance, returns the
    original name of that enum. For example, if we receive in the type
    corresponding to 'SomeEnum.FOO', we return the string "SomeEnum.Foo".

    This helper takes advantage of the fact that Enum instances are valid
    to use inside Literal[...] types. An expression like 'SomeEnum.FOO' is
    actually represented by an Instance type with a Literal enum fallback.

    We can examine this Literal fallback to retrieve the string.
    """
    typ = get_proper_type(typ)
    if not isinstance(typ, Instance):
        return None

    if not typ.type.is_enum:
        return None

    underlying_literal = typ.last_known_value
    if underlying_literal is None:
        return None

    # The checks above have verified this LiteralType is representing an enum value,
    # which means the 'value' field is guaranteed to be the name of the enum field
    # as a string.
    assert isinstance(underlying_literal.value, str)
    return underlying_literal.value
</t>
<t tx="ekr.20220525082936.467">@path C:/Repos/mypy/mypy/plugins/
@language python
@tabwidth -4
"""Plugin for supporting the functools standard library module."""
from typing import Dict, NamedTuple, Optional
from typing_extensions import Final

import mypy.plugin
from mypy.nodes import ARG_POS, ARG_STAR2, Argument, FuncItem, Var
from mypy.plugins.common import add_method_to_class
from mypy.types import AnyType, CallableType, get_proper_type, Type, TypeOfAny, UnboundType


functools_total_ordering_makers: Final = {
    'functools.total_ordering',
}

_ORDERING_METHODS: Final = {
    '__lt__',
    '__le__',
    '__gt__',
    '__ge__',
}


@others
</t>
<t tx="ekr.20220525082936.468">class _MethodInfo(NamedTuple):
    is_static: bool
    type: CallableType


</t>
<t tx="ekr.20220525082936.469">def functools_total_ordering_maker_callback(ctx: mypy.plugin.ClassDefContext,
                                            auto_attribs_default: bool = False) -&gt; bool:
    """Add dunder methods to classes decorated with functools.total_ordering."""
    if ctx.api.options.python_version &lt; (3,):
        # This plugin is not supported in Python 2 mode (it's a no-op).
        return True

    comparison_methods = _analyze_class(ctx)
    if not comparison_methods:
        ctx.api.fail(
            'No ordering operation defined when using "functools.total_ordering": &lt; &gt; &lt;= &gt;=',
            ctx.reason)
        return True

    # prefer __lt__ to __le__ to __gt__ to __ge__
    root = max(comparison_methods, key=lambda k: (comparison_methods[k] is None, k))
    root_method = comparison_methods[root]
    if not root_method:
        # None of the defined comparison methods can be analysed
        return True

    other_type = _find_other_type(root_method)
    bool_type = ctx.api.named_type('builtins.bool')
    ret_type: Type = bool_type
    if root_method.type.ret_type != ctx.api.named_type('builtins.bool'):
        proper_ret_type = get_proper_type(root_method.type.ret_type)
        if not (isinstance(proper_ret_type, UnboundType)
                and proper_ret_type.name.split('.')[-1] == 'bool'):
            ret_type = AnyType(TypeOfAny.implementation_artifact)
    for additional_op in _ORDERING_METHODS:
        # Either the method is not implemented
        # or has an unknown signature that we can now extrapolate.
        if not comparison_methods.get(additional_op):
            args = [Argument(Var('other', other_type), other_type, None, ARG_POS)]
            add_method_to_class(ctx.api, ctx.cls, additional_op, args, ret_type)

    return True


</t>
<t tx="ekr.20220525082936.47">def visit_unpack_type(self, t: UnpackType) -&gt; Type:
    return t.type.accept(self)

</t>
<t tx="ekr.20220525082936.470">def _find_other_type(method: _MethodInfo) -&gt; Type:
    """Find the type of the ``other`` argument in a comparison method."""
    first_arg_pos = 0 if method.is_static else 1
    cur_pos_arg = 0
    other_arg = None
    for arg_kind, arg_type in zip(method.type.arg_kinds, method.type.arg_types):
        if arg_kind.is_positional():
            if cur_pos_arg == first_arg_pos:
                other_arg = arg_type
                break

            cur_pos_arg += 1
        elif arg_kind != ARG_STAR2:
            other_arg = arg_type
            break

    if other_arg is None:
        return AnyType(TypeOfAny.implementation_artifact)

    return other_arg


</t>
<t tx="ekr.20220525082936.471">def _analyze_class(ctx: mypy.plugin.ClassDefContext) -&gt; Dict[str, Optional[_MethodInfo]]:
    """Analyze the class body, its parents, and return the comparison methods found."""
    # Traverse the MRO and collect ordering methods.
    comparison_methods: Dict[str, Optional[_MethodInfo]] = {}
    # Skip object because total_ordering does not use methods from object
    for cls in ctx.cls.info.mro[:-1]:
        for name in _ORDERING_METHODS:
            if name in cls.names and name not in comparison_methods:
                node = cls.names[name].node
                if isinstance(node, FuncItem) and isinstance(node.type, CallableType):
                    comparison_methods[name] = _MethodInfo(node.is_static, node.type)
                    continue

                if isinstance(node, Var):
                    proper_type = get_proper_type(node.type)
                    if isinstance(proper_type, CallableType):
                        comparison_methods[name] = _MethodInfo(node.is_staticmethod, proper_type)
                        continue

                comparison_methods[name] = None

    return comparison_methods
</t>
<t tx="ekr.20220525082936.472">@path C:/Repos/mypy/mypy/plugins/
@language python
@tabwidth -4
from mypy.messages import format_type
from mypy.plugins.common import add_method_to_class
from mypy.nodes import (
    ARG_POS, Argument, Block, ClassDef, SymbolTable, TypeInfo, Var, Context
)
from mypy.subtypes import is_subtype
from mypy.types import (
    AnyType, CallableType, Instance, NoneType, Overloaded, Type, TypeOfAny, get_proper_type,
    FunctionLike
)
from mypy.plugin import CheckerPluginInterface, FunctionContext, MethodContext, MethodSigContext
from typing import List, NamedTuple, Optional, Sequence, TypeVar, Union
from typing_extensions import Final


@others
</t>
<t tx="ekr.20220525082936.473">class SingledispatchTypeVars(NamedTuple):
    return_type: Type
    fallback: CallableType


</t>
<t tx="ekr.20220525082936.474">class RegisterCallableInfo(NamedTuple):
    register_type: Type
    singledispatch_obj: Instance


</t>
<t tx="ekr.20220525082936.475">SINGLEDISPATCH_TYPE: Final = 'functools._SingleDispatchCallable'

SINGLEDISPATCH_REGISTER_METHOD: Final = f'{SINGLEDISPATCH_TYPE}.register'

SINGLEDISPATCH_CALLABLE_CALL_METHOD: Final = f'{SINGLEDISPATCH_TYPE}.__call__'


</t>
<t tx="ekr.20220525082936.476">def get_singledispatch_info(typ: Instance) -&gt; Optional[SingledispatchTypeVars]:
    if len(typ.args) == 2:
        return SingledispatchTypeVars(*typ.args)  # type: ignore
    return None


</t>
<t tx="ekr.20220525082936.477">T = TypeVar('T')


</t>
<t tx="ekr.20220525082936.478">def get_first_arg(args: List[List[T]]) -&gt; Optional[T]:
    """Get the element that corresponds to the first argument passed to the function"""
    if args and args[0]:
        return args[0][0]
    return None


</t>
<t tx="ekr.20220525082936.479">REGISTER_RETURN_CLASS: Final = '_SingleDispatchRegisterCallable'

REGISTER_CALLABLE_CALL_METHOD: Final = f'functools.{REGISTER_RETURN_CLASS}.__call__'


</t>
<t tx="ekr.20220525082936.48">def visit_callable_type(self, t: CallableType) -&gt; Type:
    return t.copy_modified(arg_types=self.translate_types(t.arg_types),
                           ret_type=t.ret_type.accept(self),
                           variables=self.translate_variables(t.variables))

</t>
<t tx="ekr.20220525082936.480">def make_fake_register_class_instance(api: CheckerPluginInterface, type_args: Sequence[Type]
                                      ) -&gt; Instance:
    defn = ClassDef(REGISTER_RETURN_CLASS, Block([]))
    defn.fullname = f'functools.{REGISTER_RETURN_CLASS}'
    info = TypeInfo(SymbolTable(), defn, "functools")
    obj_type = api.named_generic_type('builtins.object', []).type
    info.bases = [Instance(obj_type, [])]
    info.mro = [info, obj_type]
    defn.info = info

    func_arg = Argument(Var('name'), AnyType(TypeOfAny.implementation_artifact), None, ARG_POS)
    add_method_to_class(api, defn, '__call__', [func_arg], NoneType())

    return Instance(info, type_args)


</t>
<t tx="ekr.20220525082936.481">PluginContext = Union[FunctionContext, MethodContext]


</t>
<t tx="ekr.20220525082936.482">def fail(ctx: PluginContext, msg: str, context: Optional[Context]) -&gt; None:
    """Emit an error message.

    This tries to emit an error message at the location specified by `context`, falling back to the
    location specified by `ctx.context`. This is helpful when the only context information about
    where you want to put the error message may be None (like it is for `CallableType.definition`)
    and falling back to the location of the calling function is fine."""
    # TODO: figure out if there is some more reliable way of getting context information, so this
    # function isn't necessary
    if context is not None:
        err_context = context
    else:
        err_context = ctx.context
    ctx.api.fail(msg, err_context)


</t>
<t tx="ekr.20220525082936.483">def create_singledispatch_function_callback(ctx: FunctionContext) -&gt; Type:
    """Called for functools.singledispatch"""
    func_type = get_proper_type(get_first_arg(ctx.arg_types))
    if isinstance(func_type, CallableType):

        if len(func_type.arg_kinds) &lt; 1:
            fail(
                ctx,
                'Singledispatch function requires at least one argument',
                func_type.definition,
            )
            return ctx.default_return_type

        elif not func_type.arg_kinds[0].is_positional(star=True):
            fail(
                ctx,
                'First argument to singledispatch function must be a positional argument',
                func_type.definition,
            )
            return ctx.default_return_type

        # singledispatch returns an instance of functools._SingleDispatchCallable according to
        # typeshed
        singledispatch_obj = get_proper_type(ctx.default_return_type)
        assert isinstance(singledispatch_obj, Instance)
        singledispatch_obj.args += (func_type,)

    return ctx.default_return_type


</t>
<t tx="ekr.20220525082936.484">def singledispatch_register_callback(ctx: MethodContext) -&gt; Type:
    """Called for functools._SingleDispatchCallable.register"""
    assert isinstance(ctx.type, Instance)
    # TODO: check that there's only one argument
    first_arg_type = get_proper_type(get_first_arg(ctx.arg_types))
    if isinstance(first_arg_type, (CallableType, Overloaded)) and first_arg_type.is_type_obj():
        # HACK: We received a class as an argument to register. We need to be able
        # to access the function that register is being applied to, and the typeshed definition
        # of register has it return a generic Callable, so we create a new
        # SingleDispatchRegisterCallable class, define a __call__ method, and then add a
        # plugin hook for that.

        # is_subtype doesn't work when the right type is Overloaded, so we need the
        # actual type
        register_type = first_arg_type.items[0].ret_type
        type_args = RegisterCallableInfo(register_type, ctx.type)
        register_callable = make_fake_register_class_instance(
            ctx.api,
            type_args
        )
        return register_callable
    elif isinstance(first_arg_type, CallableType):
        # TODO: do more checking for registered functions
        register_function(ctx, ctx.type, first_arg_type)
        # The typeshed stubs for register say that the function returned is Callable[..., T], even
        # though the function returned is the same as the one passed in. We return the type of the
        # function so that mypy can properly type check cases where the registered function is used
        # directly (instead of through singledispatch)
        return first_arg_type

    # fallback in case we don't recognize the arguments
    return ctx.default_return_type


</t>
<t tx="ekr.20220525082936.485">def register_function(ctx: PluginContext, singledispatch_obj: Instance, func: Type,
                      register_arg: Optional[Type] = None) -&gt; None:
    """Register a function"""

    func = get_proper_type(func)
    if not isinstance(func, CallableType):
        return
    metadata = get_singledispatch_info(singledispatch_obj)
    if metadata is None:
        # if we never added the fallback to the type variables, we already reported an error, so
        # just don't do anything here
        return
    dispatch_type = get_dispatch_type(func, register_arg)
    if dispatch_type is None:
        # TODO: report an error here that singledispatch requires at least one argument
        # (might want to do the error reporting in get_dispatch_type)
        return
    fallback = metadata.fallback

    fallback_dispatch_type = fallback.arg_types[0]
    if not is_subtype(dispatch_type, fallback_dispatch_type):

        fail(ctx, 'Dispatch type {} must be subtype of fallback function first argument {}'.format(
                format_type(dispatch_type), format_type(fallback_dispatch_type)
            ), func.definition)
        return
    return


</t>
<t tx="ekr.20220525082936.486">def get_dispatch_type(func: CallableType, register_arg: Optional[Type]) -&gt; Optional[Type]:
    if register_arg is not None:
        return register_arg
    if func.arg_types:
        return func.arg_types[0]
    return None


</t>
<t tx="ekr.20220525082936.487">def call_singledispatch_function_after_register_argument(ctx: MethodContext) -&gt; Type:
    """Called on the function after passing a type to register"""
    register_callable = ctx.type
    if isinstance(register_callable, Instance):
        type_args = RegisterCallableInfo(*register_callable.args)  # type: ignore
        func = get_first_arg(ctx.arg_types)
        if func is not None:
            register_function(ctx, type_args.singledispatch_obj, func, type_args.register_type)
            # see call to register_function in the callback for register
            return func
    return ctx.default_return_type


</t>
<t tx="ekr.20220525082936.488">def call_singledispatch_function_callback(ctx: MethodSigContext) -&gt; FunctionLike:
    """Called for functools._SingleDispatchCallable.__call__"""
    if not isinstance(ctx.type, Instance):
        return ctx.default_signature
    metadata = get_singledispatch_info(ctx.type)
    if metadata is None:
        return ctx.default_signature
    return metadata.fallback
</t>
<t tx="ekr.20220525082936.49">def visit_tuple_type(self, t: TupleType) -&gt; Type:
    return TupleType(self.translate_types(t.items),
                     # TODO: This appears to be unsafe.
                     cast(Any, t.partial_fallback.accept(self)),
                     t.line, t.column)

</t>
<t tx="ekr.20220525082936.491"></t>
<t tx="ekr.20220525082936.492">@path C:/Repos/mypy/mypy/server/
@language python
@tabwidth -4
"""Utilities for comparing two versions of a module symbol table.

The goal is to find which AST nodes have externally visible changes, so
that we can fire triggers and re-process other parts of the program
that are stale because of the changes.

Only look at detail at definitions at the current module -- don't
recurse into other modules.

A summary of the module contents:

* snapshot_symbol_table(...) creates an opaque snapshot description of a
  module/class symbol table (recursing into nested class symbol tables).

* compare_symbol_table_snapshots(...) compares two snapshots for the same
  module id and returns fully qualified names of differences (which act as
  triggers).

To compare two versions of a module symbol table, take snapshots of both
versions and compare the snapshots. The use of snapshots makes it easy to
compare two versions of the *same* symbol table that is being mutated.

Summary of how this works for certain kinds of differences:

* If a symbol table node is deleted or added (only present in old/new version
  of the symbol table), it is considered different, of course.

* If a symbol table node refers to a different sort of thing in the new version,
  it is considered different (for example, if a class is replaced with a
  function).

* If the signature of a function has changed, it is considered different.

* If the type of a variable changes, it is considered different.

* If the MRO of a class changes, or a non-generic class is turned into a
  generic class, the class is considered different (there are other such "big"
  differences that cause a class to be considered changed). However, just changes
  to attributes or methods don't generally constitute a difference at the
  class level -- these are handled at attribute level (say, 'mod.Cls.method'
  is different rather than 'mod.Cls' being different).

* If an imported name targets a different name (say, 'from x import y' is
  replaced with 'from z import y'), the name in the module is considered
  different. If the target of an import continues to have the same name,
  but it's specifics change, this doesn't mean that the imported name is
  treated as changed. Say, there is 'from x import y' in 'm', and the
  type of 'x.y' has changed. This doesn't mean that that 'm.y' is considered
  changed. Instead, processing the difference in 'm' will be handled through
  fine-grained dependencies.
"""

from typing import Set, Dict, Tuple, Optional, Sequence, Union

from mypy.nodes import (
    SymbolTable, TypeInfo, Var, SymbolNode, Decorator, TypeVarExpr, TypeAlias,
    FuncBase, OverloadedFuncDef, FuncItem, MypyFile, ParamSpecExpr, UNBOUND_IMPORTED
)
from mypy.types import (
    Type, TypeVisitor, UnboundType, AnyType, NoneType, UninhabitedType,
    ErasedType, DeletedType, Instance, TypeVarType, CallableType, TupleType, TypedDictType,
    UnionType, Overloaded, PartialType, TypeType, LiteralType, TypeAliasType, ParamSpecType,
    Parameters, UnpackType, TypeVarTupleType,
)
from mypy.util import get_prefix


# Snapshot representation of a symbol table node or type. The representation is
# opaque -- the only supported operations are comparing for equality and
# hashing (latter for type snapshots only). Snapshots can contain primitive
# objects, nested tuples, lists and dictionaries and primitive objects (type
# snapshots are immutable).
#
# For example, the snapshot of the 'int' type is ('Instance', 'builtins.int', ()).
SnapshotItem = Tuple[object, ...]


@others
</t>
<t tx="ekr.20220525082936.493">def compare_symbol_table_snapshots(
        name_prefix: str,
        snapshot1: Dict[str, SnapshotItem],
        snapshot2: Dict[str, SnapshotItem]) -&gt; Set[str]:
    """Return names that are different in two snapshots of a symbol table.

    Only shallow (intra-module) differences are considered. References to things defined
    outside the module are compared based on the name of the target only.

    Recurse into class symbol tables (if the class is defined in the target module).

    Return a set of fully-qualified names (e.g., 'mod.func' or 'mod.Class.method').
    """
    # Find names only defined only in one version.
    names1 = {f'{name_prefix}.{name}' for name in snapshot1}
    names2 = {f'{name_prefix}.{name}' for name in snapshot2}
    triggers = names1 ^ names2

    # Look for names defined in both versions that are different.
    for name in set(snapshot1.keys()) &amp; set(snapshot2.keys()):
        item1 = snapshot1[name]
        item2 = snapshot2[name]
        kind1 = item1[0]
        kind2 = item2[0]
        item_name = f'{name_prefix}.{name}'
        if kind1 != kind2:
            # Different kind of node in two snapshots -&gt; trivially different.
            triggers.add(item_name)
        elif kind1 == 'TypeInfo':
            if item1[:-1] != item2[:-1]:
                # Record major difference (outside class symbol tables).
                triggers.add(item_name)
            # Look for differences in nested class symbol table entries.
            assert isinstance(item1[-1], dict)
            assert isinstance(item2[-1], dict)
            triggers |= compare_symbol_table_snapshots(item_name, item1[-1], item2[-1])
        else:
            # Shallow node (no interesting internal structure). Just use equality.
            if snapshot1[name] != snapshot2[name]:
                triggers.add(item_name)

    return triggers


</t>
<t tx="ekr.20220525082936.494">def snapshot_symbol_table(name_prefix: str, table: SymbolTable) -&gt; Dict[str, SnapshotItem]:
    """Create a snapshot description that represents the state of a symbol table.

    The snapshot has a representation based on nested tuples and dicts
    that makes it easy and fast to find differences.

    Only "shallow" state is included in the snapshot -- references to
    things defined in other modules are represented just by the names of
    the targets.
    """
    result: Dict[str, SnapshotItem] = {}
    for name, symbol in table.items():
        node = symbol.node
        # TODO: cross_ref?
        fullname = node.fullname if node else None
        common = (fullname, symbol.kind, symbol.module_public)
        if isinstance(node, MypyFile):
            # This is a cross-reference to another module.
            # If the reference is busted because the other module is missing,
            # the node will be a "stale_info" TypeInfo produced by fixup,
            # but that doesn't really matter to us here.
            result[name] = ('Moduleref', common)
        elif isinstance(node, TypeVarExpr):
            result[name] = ('TypeVar',
                            node.variance,
                            [snapshot_type(value) for value in node.values],
                            snapshot_type(node.upper_bound))
        elif isinstance(node, TypeAlias):
            result[name] = ('TypeAlias',
                            node.alias_tvars,
                            node.normalized,
                            node.no_args,
                            snapshot_optional_type(node.target))
        elif isinstance(node, ParamSpecExpr):
            result[name] = ('ParamSpec',
                            node.variance,
                            snapshot_type(node.upper_bound))
        else:
            assert symbol.kind != UNBOUND_IMPORTED
            if node and get_prefix(node.fullname) != name_prefix:
                # This is a cross-reference to a node defined in another module.
                result[name] = ('CrossRef', common)
            else:
                result[name] = snapshot_definition(node, common)
    return result


</t>
<t tx="ekr.20220525082936.495">def snapshot_definition(node: Optional[SymbolNode],
                        common: Tuple[object, ...]) -&gt; Tuple[object, ...]:
    """Create a snapshot description of a symbol table node.

    The representation is nested tuples and dicts. Only externally
    visible attributes are included.
    """
    if isinstance(node, FuncBase):
        # TODO: info
        if node.type:
            signature = snapshot_type(node.type)
        else:
            signature = snapshot_untyped_signature(node)
        return ('Func', common,
                node.is_property, node.is_final,
                node.is_class, node.is_static,
                signature)
    elif isinstance(node, Var):
        return ('Var', common,
                snapshot_optional_type(node.type),
                node.is_final)
    elif isinstance(node, Decorator):
        # Note that decorated methods are represented by Decorator instances in
        # a symbol table since we need to preserve information about the
        # decorated function (whether it's a class function, for
        # example). Top-level decorated functions, however, are represented by
        # the corresponding Var node, since that happens to provide enough
        # context.
        return ('Decorator',
                node.is_overload,
                snapshot_optional_type(node.var.type),
                snapshot_definition(node.func, common))
    elif isinstance(node, TypeInfo):
        attrs = (node.is_abstract,
                 node.is_enum,
                 node.is_protocol,
                 node.fallback_to_any,
                 node.is_named_tuple,
                 node.is_newtype,
                 # We need this to e.g. trigger metaclass calculation in subclasses.
                 snapshot_optional_type(node.metaclass_type),
                 snapshot_optional_type(node.tuple_type),
                 snapshot_optional_type(node.typeddict_type),
                 [base.fullname for base in node.mro],
                 # Note that the structure of type variables is a part of the external interface,
                 # since creating instances might fail, for example:
                 #     T = TypeVar('T', bound=int)
                 #     class C(Generic[T]):
                 #         ...
                 #     x: C[str] &lt;- this is invalid, and needs to be re-checked if `T` changes.
                 # An alternative would be to create both deps: &lt;...&gt; -&gt; C, and &lt;...&gt; -&gt; &lt;C&gt;,
                 # but this currently seems a bit ad hoc.
                 tuple(snapshot_type(tdef) for tdef in node.defn.type_vars),
                 [snapshot_type(base) for base in node.bases],
                 snapshot_optional_type(node._promote))
        prefix = node.fullname
        symbol_table = snapshot_symbol_table(prefix, node.names)
        # Special dependency for abstract attribute handling.
        symbol_table['(abstract)'] = ('Abstract', tuple(sorted(node.abstract_attributes)))
        return ('TypeInfo', common, attrs, symbol_table)
    else:
        # Other node types are handled elsewhere.
        assert False, type(node)


</t>
<t tx="ekr.20220525082936.496">def snapshot_type(typ: Type) -&gt; SnapshotItem:
    """Create a snapshot representation of a type using nested tuples."""
    return typ.accept(SnapshotTypeVisitor())


</t>
<t tx="ekr.20220525082936.497">def snapshot_optional_type(typ: Optional[Type]) -&gt; Optional[SnapshotItem]:
    if typ:
        return snapshot_type(typ)
    else:
        return None


</t>
<t tx="ekr.20220525082936.498">def snapshot_types(types: Sequence[Type]) -&gt; SnapshotItem:
    return tuple(snapshot_type(item) for item in types)


</t>
<t tx="ekr.20220525082936.499">def snapshot_simple_type(typ: Type) -&gt; SnapshotItem:
    return (type(typ).__name__,)


</t>
<t tx="ekr.20220525082936.5">@trait
@mypyc_attr(allow_interpreted_subclasses=True)
class TypeVisitor(Generic[T]):
    """Visitor class for types (Type subclasses).

    The parameter T is the return type of the visit methods.
    """

    @others
</t>
<t tx="ekr.20220525082936.50">def visit_typeddict_type(self, t: TypedDictType) -&gt; Type:
    items = OrderedDict([
        (item_name, item_type.accept(self))
        for (item_name, item_type) in t.items.items()
    ])
    return TypedDictType(items,
                         t.required_keys,
                         # TODO: This appears to be unsafe.
                         cast(Any, t.fallback.accept(self)),
                         t.line, t.column)

</t>
<t tx="ekr.20220525082936.500">def encode_optional_str(s: Optional[str]) -&gt; str:
    if s is None:
        return '&lt;None&gt;'
    else:
        return s


</t>
<t tx="ekr.20220525082936.501">class SnapshotTypeVisitor(TypeVisitor[SnapshotItem]):
    """Creates a read-only, self-contained snapshot of a type object.

    Properties of a snapshot:

    - Contains (nested) tuples and other immutable primitive objects only.
    - References to AST nodes are replaced with full names of targets.
    - Has no references to mutable or non-primitive objects.
    - Two snapshots represent the same object if and only if they are
      equal.
    - Results must be sortable. It's important that tuples have
      consistent types and can't arbitrarily mix str and None values,
      for example, since they can't be compared.
    """

    @others
</t>
<t tx="ekr.20220525082936.502">def visit_unbound_type(self, typ: UnboundType) -&gt; SnapshotItem:
    return ('UnboundType',
            typ.name,
            typ.optional,
            typ.empty_tuple_index,
            snapshot_types(typ.args))

</t>
<t tx="ekr.20220525082936.503">def visit_any(self, typ: AnyType) -&gt; SnapshotItem:
    return snapshot_simple_type(typ)

</t>
<t tx="ekr.20220525082936.504">def visit_none_type(self, typ: NoneType) -&gt; SnapshotItem:
    return snapshot_simple_type(typ)

</t>
<t tx="ekr.20220525082936.505">def visit_uninhabited_type(self, typ: UninhabitedType) -&gt; SnapshotItem:
    return snapshot_simple_type(typ)

</t>
<t tx="ekr.20220525082936.506">def visit_erased_type(self, typ: ErasedType) -&gt; SnapshotItem:
    return snapshot_simple_type(typ)

</t>
<t tx="ekr.20220525082936.507">def visit_deleted_type(self, typ: DeletedType) -&gt; SnapshotItem:
    return snapshot_simple_type(typ)

</t>
<t tx="ekr.20220525082936.508">def visit_instance(self, typ: Instance) -&gt; SnapshotItem:
    return ('Instance',
            encode_optional_str(typ.type.fullname),
            snapshot_types(typ.args),
            ('None',) if typ.last_known_value is None else snapshot_type(typ.last_known_value))

</t>
<t tx="ekr.20220525082936.509">def visit_type_var(self, typ: TypeVarType) -&gt; SnapshotItem:
    return ('TypeVar',
            typ.name,
            typ.fullname,
            typ.id.raw_id,
            typ.id.meta_level,
            snapshot_types(typ.values),
            snapshot_type(typ.upper_bound),
            typ.variance)

</t>
<t tx="ekr.20220525082936.51">def visit_literal_type(self, t: LiteralType) -&gt; Type:
    fallback = t.fallback.accept(self)
    assert isinstance(fallback, Instance)  # type: ignore
    return LiteralType(
        value=t.value,
        fallback=fallback,
        line=t.line,
        column=t.column,
    )

</t>
<t tx="ekr.20220525082936.510">def visit_param_spec(self, typ: ParamSpecType) -&gt; SnapshotItem:
    return ('ParamSpec',
            typ.id.raw_id,
            typ.id.meta_level,
            typ.flavor,
            snapshot_type(typ.upper_bound))

</t>
<t tx="ekr.20220525082936.511">def visit_type_var_tuple(self, typ: TypeVarTupleType) -&gt; SnapshotItem:
    return ('TypeVarTupleType',
            typ.id.raw_id,
            typ.id.meta_level,
            snapshot_type(typ.upper_bound))

</t>
<t tx="ekr.20220525082936.512">def visit_unpack_type(self, typ: UnpackType) -&gt; SnapshotItem:
    return ('UnpackType', snapshot_type(typ.type))

</t>
<t tx="ekr.20220525082936.513">def visit_parameters(self, typ: Parameters) -&gt; SnapshotItem:
    return ('Parameters',
            snapshot_types(typ.arg_types),
            tuple(encode_optional_str(name) for name in typ.arg_names),
            tuple(typ.arg_kinds))

</t>
<t tx="ekr.20220525082936.514">def visit_callable_type(self, typ: CallableType) -&gt; SnapshotItem:
    # FIX generics
    return ('CallableType',
            snapshot_types(typ.arg_types),
            snapshot_type(typ.ret_type),
            tuple(encode_optional_str(name) for name in typ.arg_names),
            tuple(typ.arg_kinds),
            typ.is_type_obj(),
            typ.is_ellipsis_args)

</t>
<t tx="ekr.20220525082936.515">def visit_tuple_type(self, typ: TupleType) -&gt; SnapshotItem:
    return ('TupleType', snapshot_types(typ.items))

</t>
<t tx="ekr.20220525082936.516">def visit_typeddict_type(self, typ: TypedDictType) -&gt; SnapshotItem:
    items = tuple((key, snapshot_type(item_type))
                  for key, item_type in typ.items.items())
    required = tuple(sorted(typ.required_keys))
    return ('TypedDictType', items, required)

</t>
<t tx="ekr.20220525082936.517">def visit_literal_type(self, typ: LiteralType) -&gt; SnapshotItem:
    return ('LiteralType', snapshot_type(typ.fallback), typ.value)

</t>
<t tx="ekr.20220525082936.518">def visit_union_type(self, typ: UnionType) -&gt; SnapshotItem:
    # Sort and remove duplicates so that we can use equality to test for
    # equivalent union type snapshots.
    items = {snapshot_type(item) for item in typ.items}
    normalized = tuple(sorted(items))
    return ('UnionType', normalized)

</t>
<t tx="ekr.20220525082936.519">def visit_overloaded(self, typ: Overloaded) -&gt; SnapshotItem:
    return ('Overloaded', snapshot_types(typ.items))

</t>
<t tx="ekr.20220525082936.52">def visit_union_type(self, t: UnionType) -&gt; Type:
    return UnionType(self.translate_types(t.items), t.line, t.column)

</t>
<t tx="ekr.20220525082936.520">def visit_partial_type(self, typ: PartialType) -&gt; SnapshotItem:
    # A partial type is not fully defined, so the result is indeterminate. We shouldn't
    # get here.
    raise RuntimeError

</t>
<t tx="ekr.20220525082936.521">def visit_type_type(self, typ: TypeType) -&gt; SnapshotItem:
    return ('TypeType', snapshot_type(typ.item))

</t>
<t tx="ekr.20220525082936.522">def visit_type_alias_type(self, typ: TypeAliasType) -&gt; SnapshotItem:
    assert typ.alias is not None
    return ('TypeAliasType', typ.alias.fullname, snapshot_types(typ.args))


</t>
<t tx="ekr.20220525082936.523">def snapshot_untyped_signature(func: Union[OverloadedFuncDef, FuncItem]) -&gt; Tuple[object, ...]:
    """Create a snapshot of the signature of a function that has no explicit signature.

    If the arguments to a function without signature change, it must be
    considered as different. We have this special casing since we don't store
    the implicit signature anywhere, and we'd rather not construct new
    Callable objects in this module (the idea is to only read properties of
    the AST here).
    """
    if isinstance(func, FuncItem):
        return (tuple(func.arg_names), tuple(func.arg_kinds))
    else:
        result = []
        for item in func.items:
            if isinstance(item, Decorator):
                if item.var.type:
                    result.append(snapshot_type(item.var.type))
                else:
                    result.append(('DecoratorWithoutType',))
            else:
                result.append(snapshot_untyped_signature(item))
        return tuple(result)
</t>
<t tx="ekr.20220525082936.524">@path C:/Repos/mypy/mypy/server/
@language python
@tabwidth -4
"""Merge a new version of a module AST and symbol table to older versions of those.

When the source code of a module has a change in fine-grained incremental mode,
we build a new AST from the updated source. However, other parts of the program
may have direct references to parts of the old AST (namely, those nodes exposed
in the module symbol table). The merge operation changes the identities of new
AST nodes that have a correspondence in the old AST to the old ones so that
existing cross-references in other modules will continue to point to the correct
nodes. Also internal cross-references within the new AST are replaced. AST nodes
that aren't externally visible will get new, distinct object identities. This
applies to most expression and statement nodes, for example.

We perform this merge operation so that we don't have to update all
external references (which would be slow and fragile) or always perform
translation when looking up references (which would be hard to retrofit).

The AST merge operation is performed after semantic analysis. Semantic
analysis has to deal with potentially multiple aliases to certain AST
nodes (in particular, MypyFile nodes). Type checking assumes that we
don't have multiple variants of a single AST node visible to the type
checker.

Discussion of some notable special cases:

* If a node is replaced with a different kind of node (say, a function is
  replaced with a class), we don't perform the merge. Fine-grained dependencies
  will be used to rebind all references to the node.

* If a function is replaced with another function with an identical signature,
  call sites continue to point to the same object (by identity) and don't need
  to be reprocessed. Similarly, if a class is replaced with a class that is
  sufficiently similar (MRO preserved, etc.), class references don't need any
  processing. A typical incremental update to a file only changes a few
  externally visible things in a module, and this means that often only few
  external references need any processing, even if the modified module is large.

* A no-op update of a module should not require any processing outside the
  module, since all relevant object identities are preserved.

* The AST diff operation (mypy.server.astdiff) and the top-level fine-grained
  incremental logic (mypy.server.update) handle the cases where the new AST has
  differences from the old one that may need to be propagated to elsewhere in the
  program.

See the main entry point merge_asts for more details.
"""

from typing import Dict, List, cast, TypeVar, Optional

from mypy.nodes import (
    MypyFile, SymbolTable, Block, AssignmentStmt, NameExpr, MemberExpr, RefExpr, TypeInfo,
    FuncDef, ClassDef, NamedTupleExpr, SymbolNode, Var, Statement, SuperExpr, NewTypeExpr,
    OverloadedFuncDef, LambdaExpr, TypedDictExpr, EnumCallExpr, FuncBase, TypeAliasExpr, CallExpr,
    CastExpr, TypeAlias, AssertTypeExpr,
    MDEF
)
from mypy.traverser import TraverserVisitor
from mypy.types import (
    Type, SyntheticTypeVisitor, Instance, AnyType, NoneType, CallableType, ErasedType, DeletedType,
    TupleType, TypeType, TypedDictType, UnboundType, UninhabitedType, UnionType,
    Overloaded, TypeVarType, TypeList, CallableArgument, EllipsisType, StarType, LiteralType,
    RawExpressionType, PartialType, PlaceholderType, TypeAliasType, ParamSpecType, Parameters,
    UnpackType, TypeVarTupleType,
)
from mypy.util import get_prefix, replace_object_state
from mypy.typestate import TypeState


@others
</t>
<t tx="ekr.20220525082936.525">def merge_asts(old: MypyFile, old_symbols: SymbolTable,
               new: MypyFile, new_symbols: SymbolTable) -&gt; None:
    """Merge a new version of a module AST to a previous version.

    The main idea is to preserve the identities of externally visible
    nodes in the old AST (that have a corresponding node in the new AST).
    All old node state (outside identity) will come from the new AST.

    When this returns, 'old' will refer to the merged AST, but 'new_symbols'
    will be the new symbol table. 'new' and 'old_symbols' will no longer be
    valid.
    """
    assert new.fullname == old.fullname
    # Find the mapping from new to old node identities for all nodes
    # whose identities should be preserved.
    replacement_map = replacement_map_from_symbol_table(
        old_symbols, new_symbols, prefix=old.fullname)
    # Also replace references to the new MypyFile node.
    replacement_map[new] = old
    # Perform replacements to everywhere within the new AST (not including symbol
    # tables).
    node = replace_nodes_in_ast(new, replacement_map)
    assert node is old
    # Also replace AST node references in the *new* symbol table (we'll
    # continue to use the new symbol table since it has all the new definitions
    # that have no correspondence in the old AST).
    replace_nodes_in_symbol_table(new_symbols, replacement_map)


</t>
<t tx="ekr.20220525082936.526">def replacement_map_from_symbol_table(
        old: SymbolTable, new: SymbolTable, prefix: str) -&gt; Dict[SymbolNode, SymbolNode]:
    """Create a new-to-old object identity map by comparing two symbol table revisions.

    Both symbol tables must refer to revisions of the same module id. The symbol tables
    are compared recursively (recursing into nested class symbol tables), but only within
    the given module prefix. Don't recurse into other modules accessible through the symbol
    table.
    """
    replacements: Dict[SymbolNode, SymbolNode] = {}
    for name, node in old.items():
        if (name in new and (node.kind == MDEF
                             or node.node and get_prefix(node.node.fullname) == prefix)):
            new_node = new[name]
            if (type(new_node.node) == type(node.node)  # noqa
                    and new_node.node and node.node and
                    new_node.node.fullname == node.node.fullname and
                    new_node.kind == node.kind):
                replacements[new_node.node] = node.node
                if isinstance(node.node, TypeInfo) and isinstance(new_node.node, TypeInfo):
                    type_repl = replacement_map_from_symbol_table(
                        node.node.names,
                        new_node.node.names,
                        prefix)
                    replacements.update(type_repl)
    return replacements


</t>
<t tx="ekr.20220525082936.527">def replace_nodes_in_ast(node: SymbolNode,
                         replacements: Dict[SymbolNode, SymbolNode]) -&gt; SymbolNode:
    """Replace all references to replacement map keys within an AST node, recursively.

    Also replace the *identity* of any nodes that have replacements. Return the
    *replaced* version of the argument node (which may have a different identity, if
    it's included in the replacement map).
    """
    visitor = NodeReplaceVisitor(replacements)
    node.accept(visitor)
    return replacements.get(node, node)


</t>
<t tx="ekr.20220525082936.528">SN = TypeVar('SN', bound=SymbolNode)


</t>
<t tx="ekr.20220525082936.529">class NodeReplaceVisitor(TraverserVisitor):
    """Transform some nodes to new identities in an AST.

    Only nodes that live in the symbol table may be
    replaced, which simplifies the implementation some. Also
    replace all references to the old identities.
    """

    @others
</t>
<t tx="ekr.20220525082936.53">def translate_types(self, types: Iterable[Type]) -&gt; List[Type]:
    return [t.accept(self) for t in types]

</t>
<t tx="ekr.20220525082936.530">def __init__(self, replacements: Dict[SymbolNode, SymbolNode]) -&gt; None:
    self.replacements = replacements

</t>
<t tx="ekr.20220525082936.531">def visit_mypy_file(self, node: MypyFile) -&gt; None:
    node = self.fixup(node)
    node.defs = self.replace_statements(node.defs)
    super().visit_mypy_file(node)

</t>
<t tx="ekr.20220525082936.532">def visit_block(self, node: Block) -&gt; None:
    super().visit_block(node)
    node.body = self.replace_statements(node.body)

</t>
<t tx="ekr.20220525082936.533">def visit_func_def(self, node: FuncDef) -&gt; None:
    node = self.fixup(node)
    self.process_base_func(node)
    super().visit_func_def(node)

</t>
<t tx="ekr.20220525082936.534">def visit_overloaded_func_def(self, node: OverloadedFuncDef) -&gt; None:
    self.process_base_func(node)
    super().visit_overloaded_func_def(node)

</t>
<t tx="ekr.20220525082936.535">def visit_class_def(self, node: ClassDef) -&gt; None:
    # TODO additional things?
    node.info = self.fixup_and_reset_typeinfo(node.info)
    node.defs.body = self.replace_statements(node.defs.body)
    info = node.info
    for tv in node.type_vars:
        if isinstance(tv, TypeVarType):
            self.process_type_var_def(tv)
    if info:
        if info.is_named_tuple:
            self.process_synthetic_type_info(info)
        else:
            self.process_type_info(info)
    super().visit_class_def(node)

</t>
<t tx="ekr.20220525082936.536">def process_base_func(self, node: FuncBase) -&gt; None:
    self.fixup_type(node.type)
    node.info = self.fixup(node.info)
    if node.unanalyzed_type:
        # Unanalyzed types can have AST node references
        self.fixup_type(node.unanalyzed_type)

</t>
<t tx="ekr.20220525082936.537">def process_type_var_def(self, tv: TypeVarType) -&gt; None:
    for value in tv.values:
        self.fixup_type(value)
    self.fixup_type(tv.upper_bound)

</t>
<t tx="ekr.20220525082936.538">def visit_assignment_stmt(self, node: AssignmentStmt) -&gt; None:
    self.fixup_type(node.type)
    super().visit_assignment_stmt(node)

</t>
<t tx="ekr.20220525082936.539"># Expressions

</t>
<t tx="ekr.20220525082936.54">def translate_variables(self,
                        variables: Sequence[TypeVarLikeType]) -&gt; Sequence[TypeVarLikeType]:
    return variables

</t>
<t tx="ekr.20220525082936.540">def visit_name_expr(self, node: NameExpr) -&gt; None:
    self.visit_ref_expr(node)

</t>
<t tx="ekr.20220525082936.541">def visit_member_expr(self, node: MemberExpr) -&gt; None:
    if node.def_var:
        node.def_var = self.fixup(node.def_var)
    self.visit_ref_expr(node)
    super().visit_member_expr(node)

</t>
<t tx="ekr.20220525082936.542">def visit_ref_expr(self, node: RefExpr) -&gt; None:
    if node.node is not None:
        node.node = self.fixup(node.node)
        if isinstance(node.node, Var):
            # The Var node may be an orphan and won't otherwise be processed.
            node.node.accept(self)

</t>
<t tx="ekr.20220525082936.543">def visit_namedtuple_expr(self, node: NamedTupleExpr) -&gt; None:
    super().visit_namedtuple_expr(node)
    node.info = self.fixup_and_reset_typeinfo(node.info)
    self.process_synthetic_type_info(node.info)

</t>
<t tx="ekr.20220525082936.544">def visit_cast_expr(self, node: CastExpr) -&gt; None:
    super().visit_cast_expr(node)
    self.fixup_type(node.type)

</t>
<t tx="ekr.20220525082936.545">def visit_assert_type_expr(self, node: AssertTypeExpr) -&gt; None:
    super().visit_assert_type_expr(node)
    self.fixup_type(node.type)

</t>
<t tx="ekr.20220525082936.546">def visit_super_expr(self, node: SuperExpr) -&gt; None:
    super().visit_super_expr(node)
    if node.info is not None:
        node.info = self.fixup(node.info)

</t>
<t tx="ekr.20220525082936.547">def visit_call_expr(self, node: CallExpr) -&gt; None:
    super().visit_call_expr(node)
    if isinstance(node.analyzed, SymbolNode):
        node.analyzed = self.fixup(node.analyzed)

</t>
<t tx="ekr.20220525082936.548">def visit_newtype_expr(self, node: NewTypeExpr) -&gt; None:
    if node.info:
        node.info = self.fixup_and_reset_typeinfo(node.info)
        self.process_synthetic_type_info(node.info)
    self.fixup_type(node.old_type)
    super().visit_newtype_expr(node)

</t>
<t tx="ekr.20220525082936.549">def visit_lambda_expr(self, node: LambdaExpr) -&gt; None:
    node.info = self.fixup(node.info)
    super().visit_lambda_expr(node)

</t>
<t tx="ekr.20220525082936.55">def visit_overloaded(self, t: Overloaded) -&gt; Type:
    items: List[CallableType] = []
    for item in t.items:
        new = item.accept(self)
        assert isinstance(new, CallableType)  # type: ignore
        items.append(new)
    return Overloaded(items=items)

</t>
<t tx="ekr.20220525082936.550">def visit_typeddict_expr(self, node: TypedDictExpr) -&gt; None:
    super().visit_typeddict_expr(node)
    node.info = self.fixup_and_reset_typeinfo(node.info)
    self.process_synthetic_type_info(node.info)

</t>
<t tx="ekr.20220525082936.551">def visit_enum_call_expr(self, node: EnumCallExpr) -&gt; None:
    node.info = self.fixup_and_reset_typeinfo(node.info)
    self.process_synthetic_type_info(node.info)
    super().visit_enum_call_expr(node)

</t>
<t tx="ekr.20220525082936.552">def visit_type_alias_expr(self, node: TypeAliasExpr) -&gt; None:
    self.fixup_type(node.type)
    super().visit_type_alias_expr(node)

</t>
<t tx="ekr.20220525082936.553"># Others

</t>
<t tx="ekr.20220525082936.554">def visit_var(self, node: Var) -&gt; None:
    node.info = self.fixup(node.info)
    self.fixup_type(node.type)
    super().visit_var(node)

</t>
<t tx="ekr.20220525082936.555">def visit_type_alias(self, node: TypeAlias) -&gt; None:
    self.fixup_type(node.target)
    super().visit_type_alias(node)

</t>
<t tx="ekr.20220525082936.556"># Helpers

</t>
<t tx="ekr.20220525082936.557">def fixup(self, node: SN) -&gt; SN:
    if node in self.replacements:
        new = self.replacements[node]
        replace_object_state(new, node)
        return cast(SN, new)
    return node

</t>
<t tx="ekr.20220525082936.558">def fixup_and_reset_typeinfo(self, node: TypeInfo) -&gt; TypeInfo:
    """Fix-up type info and reset subtype caches.

    This needs to be called at least once per each merged TypeInfo, as otherwise we
    may leak stale caches.
    """
    if node in self.replacements:
        # The subclass relationships may change, so reset all caches relevant to the
        # old MRO.
        new = cast(TypeInfo, self.replacements[node])
        TypeState.reset_all_subtype_caches_for(new)
    return self.fixup(node)

</t>
<t tx="ekr.20220525082936.559">def fixup_type(self, typ: Optional[Type]) -&gt; None:
    if typ is not None:
        typ.accept(TypeReplaceVisitor(self.replacements))

</t>
<t tx="ekr.20220525082936.56">def visit_type_type(self, t: TypeType) -&gt; Type:
    return TypeType.make_normalized(t.item.accept(self), line=t.line, column=t.column)

</t>
<t tx="ekr.20220525082936.560">def process_type_info(self, info: Optional[TypeInfo]) -&gt; None:
    if info is None:
        return
    self.fixup_type(info.declared_metaclass)
    self.fixup_type(info.metaclass_type)
    self.fixup_type(info._promote)
    self.fixup_type(info.tuple_type)
    self.fixup_type(info.typeddict_type)
    info.defn.info = self.fixup(info)
    replace_nodes_in_symbol_table(info.names, self.replacements)
    for i, item in enumerate(info.mro):
        info.mro[i] = self.fixup(info.mro[i])
    for i, base in enumerate(info.bases):
        self.fixup_type(info.bases[i])

</t>
<t tx="ekr.20220525082936.561">def process_synthetic_type_info(self, info: TypeInfo) -&gt; None:
    # Synthetic types (types not created using a class statement) don't
    # have bodies in the AST so we need to iterate over their symbol
    # tables separately, unlike normal classes.
    self.process_type_info(info)
    for name, node in info.names.items():
        if node.node:
            node.node.accept(self)

</t>
<t tx="ekr.20220525082936.562">def replace_statements(self, nodes: List[Statement]) -&gt; List[Statement]:
    result = []
    for node in nodes:
        if isinstance(node, SymbolNode):
            node = self.fixup(node)
        result.append(node)
    return result


</t>
<t tx="ekr.20220525082936.563">class TypeReplaceVisitor(SyntheticTypeVisitor[None]):
    """Similar to NodeReplaceVisitor, but for type objects.

    Note: this visitor may sometimes visit unanalyzed types
    such as 'UnboundType' and 'RawExpressionType' For example, see
    NodeReplaceVisitor.process_base_func.
    """

    @others
</t>
<t tx="ekr.20220525082936.564">def __init__(self, replacements: Dict[SymbolNode, SymbolNode]) -&gt; None:
    self.replacements = replacements

</t>
<t tx="ekr.20220525082936.565">def visit_instance(self, typ: Instance) -&gt; None:
    typ.type = self.fixup(typ.type)
    for arg in typ.args:
        arg.accept(self)
    if typ.last_known_value:
        typ.last_known_value.accept(self)

</t>
<t tx="ekr.20220525082936.566">def visit_type_alias_type(self, typ: TypeAliasType) -&gt; None:
    assert typ.alias is not None
    typ.alias = self.fixup(typ.alias)
    for arg in typ.args:
        arg.accept(self)

</t>
<t tx="ekr.20220525082936.567">def visit_any(self, typ: AnyType) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082936.568">def visit_none_type(self, typ: NoneType) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082936.569">def visit_callable_type(self, typ: CallableType) -&gt; None:
    for arg in typ.arg_types:
        arg.accept(self)
    typ.ret_type.accept(self)
    if typ.definition:
        # No need to fixup since this is just a cross-reference.
        typ.definition = self.replacements.get(typ.definition, typ.definition)
    # Fallback can be None for callable types that haven't been semantically analyzed.
    if typ.fallback is not None:
        typ.fallback.accept(self)
    for tv in typ.variables:
        if isinstance(tv, TypeVarType):
            tv.upper_bound.accept(self)
            for value in tv.values:
                value.accept(self)

</t>
<t tx="ekr.20220525082936.57">@abstractmethod
def visit_type_alias_type(self, t: TypeAliasType) -&gt; Type:
    # This method doesn't have a default implementation for type translators,
    # because type aliases are special: some information is contained in the
    # TypeAlias node, and we normally don't generate new nodes. Every subclass
    # must implement this depending on its semantics.
    pass


</t>
<t tx="ekr.20220525082936.570">def visit_overloaded(self, t: Overloaded) -&gt; None:
    for item in t.items:
        item.accept(self)
    # Fallback can be None for overloaded types that haven't been semantically analyzed.
    if t.fallback is not None:
        t.fallback.accept(self)

</t>
<t tx="ekr.20220525082936.571">def visit_erased_type(self, t: ErasedType) -&gt; None:
    # This type should exist only temporarily during type inference
    raise RuntimeError

</t>
<t tx="ekr.20220525082936.572">def visit_deleted_type(self, typ: DeletedType) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082936.573">def visit_partial_type(self, typ: PartialType) -&gt; None:
    raise RuntimeError

</t>
<t tx="ekr.20220525082936.574">def visit_tuple_type(self, typ: TupleType) -&gt; None:
    for item in typ.items:
        item.accept(self)
    # Fallback can be None for implicit tuple types that haven't been semantically analyzed.
    if typ.partial_fallback is not None:
        typ.partial_fallback.accept(self)

</t>
<t tx="ekr.20220525082936.575">def visit_type_type(self, typ: TypeType) -&gt; None:
    typ.item.accept(self)

</t>
<t tx="ekr.20220525082936.576">def visit_type_var(self, typ: TypeVarType) -&gt; None:
    typ.upper_bound.accept(self)
    for value in typ.values:
        value.accept(self)

</t>
<t tx="ekr.20220525082936.577">def visit_param_spec(self, typ: ParamSpecType) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082936.578">def visit_type_var_tuple(self, typ: TypeVarTupleType) -&gt; None:
    typ.upper_bound.accept(self)

</t>
<t tx="ekr.20220525082936.579">def visit_unpack_type(self, typ: UnpackType) -&gt; None:
    typ.type.accept(self)

</t>
<t tx="ekr.20220525082936.58">@mypyc_attr(allow_interpreted_subclasses=True)
class TypeQuery(SyntheticTypeVisitor[T]):
    """Visitor for performing queries of types.

    strategy is used to combine results for a series of types,
    common use cases involve a boolean query using `any` or `all`.

    Note: this visitor keeps an internal state (tracks type aliases to avoid
    recursion), so it should *never* be re-used for querying different types,
    create a new visitor instance instead.

    # TODO: check that we don't have existing violations of this rule.
    """

    @others
</t>
<t tx="ekr.20220525082936.580">def visit_parameters(self, typ: Parameters) -&gt; None:
    for arg in typ.arg_types:
        arg.accept(self)

</t>
<t tx="ekr.20220525082936.581">def visit_typeddict_type(self, typ: TypedDictType) -&gt; None:
    for value_type in typ.items.values():
        value_type.accept(self)
    typ.fallback.accept(self)

</t>
<t tx="ekr.20220525082936.582">def visit_raw_expression_type(self, t: RawExpressionType) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082936.583">def visit_literal_type(self, typ: LiteralType) -&gt; None:
    typ.fallback.accept(self)

</t>
<t tx="ekr.20220525082936.584">def visit_unbound_type(self, typ: UnboundType) -&gt; None:
    for arg in typ.args:
        arg.accept(self)

</t>
<t tx="ekr.20220525082936.585">def visit_type_list(self, typ: TypeList) -&gt; None:
    for item in typ.items:
        item.accept(self)

</t>
<t tx="ekr.20220525082936.586">def visit_callable_argument(self, typ: CallableArgument) -&gt; None:
    typ.typ.accept(self)

</t>
<t tx="ekr.20220525082936.587">def visit_ellipsis_type(self, typ: EllipsisType) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082936.588">def visit_star_type(self, typ: StarType) -&gt; None:
    typ.type.accept(self)

</t>
<t tx="ekr.20220525082936.589">def visit_uninhabited_type(self, typ: UninhabitedType) -&gt; None:
    pass

</t>
<t tx="ekr.20220525082936.59">def __init__(self, strategy: Callable[[Iterable[T]], T]) -&gt; None:
    self.strategy = strategy
    # Keep track of the type aliases already visited. This is needed to avoid
    # infinite recursion on types like A = Union[int, List[A]].
    self.seen_aliases: Set[TypeAliasType] = set()

</t>
<t tx="ekr.20220525082936.590">def visit_union_type(self, typ: UnionType) -&gt; None:
    for item in typ.items:
        item.accept(self)

</t>
<t tx="ekr.20220525082936.591">def visit_placeholder_type(self, t: PlaceholderType) -&gt; None:
    for item in t.args:
        item.accept(self)

</t>
<t tx="ekr.20220525082936.592"># Helpers

</t>
<t tx="ekr.20220525082936.593">def fixup(self, node: SN) -&gt; SN:
    if node in self.replacements:
        new = self.replacements[node]
        return cast(SN, new)
    return node


</t>
<t tx="ekr.20220525082936.594">def replace_nodes_in_symbol_table(symbols: SymbolTable,
                                  replacements: Dict[SymbolNode, SymbolNode]) -&gt; None:
    for name, node in symbols.items():
        if node.node:
            if node.node in replacements:
                new = replacements[node.node]
                old = node.node
                replace_object_state(new, old)
                node.node = new
            if isinstance(node.node, (Var, TypeAlias)):
                # Handle them here just in case these aren't exposed through the AST.
                node.node.accept(NodeReplaceVisitor(replacements))
</t>
<t tx="ekr.20220525082936.595">@path C:/Repos/mypy/mypy/server/
@language python
@tabwidth -4
"""Strip/reset AST in-place to match state after semantic analyzer pre-analysis.

Fine-grained incremental mode reruns semantic analysis main pass
and type checking for *existing* AST nodes (targets) when changes are
propagated using fine-grained dependencies.  AST nodes attributes are
sometimes changed during semantic analysis main pass, and running
semantic analysis again on those nodes would produce incorrect
results, since this pass isn't idempotent. This pass resets AST
nodes to reflect the state after semantic pre-analysis, so that we
can rerun semantic analysis.
(The above is in contrast to behavior with modules that have source code
changes, for which we re-parse the entire module and reconstruct a fresh
AST. No stripping is required in this case. Both modes of operation should
have the same outcome.)
Notes:
* This is currently pretty fragile, as we must carefully undo whatever
  changes can be made in semantic analysis main pass, including changes
  to symbol tables.
* We reuse existing AST nodes because it makes it relatively straightforward
  to reprocess only a single target within a module efficiently. If there
  was a way to parse a single target within a file, in time proportional to
  the size of the target, we'd rather create fresh AST nodes than strip them.
  (This is possible only in Python 3.8+)
* Currently we don't actually reset all changes, but only those known to affect
  non-idempotent semantic analysis behavior.
  TODO: It would be more principled and less fragile to reset everything
      changed in semantic analysis main pass and later.
* Reprocessing may recreate AST nodes (such as Var nodes, and TypeInfo nodes
  created with assignment statements) that will get different identities from
  the original AST. Thus running an AST merge is necessary after stripping,
  even though some identities are preserved.
"""

import contextlib
from typing import Union, Iterator, Optional, Dict, Tuple

from mypy.backports import nullcontext
from mypy.nodes import (
    FuncDef, NameExpr, MemberExpr, RefExpr, MypyFile, ClassDef, AssignmentStmt,
    ImportFrom, CallExpr, Decorator, OverloadedFuncDef, Node, TupleExpr, ListExpr,
    SuperExpr, IndexExpr, ImportAll, ForStmt, Block, CLASSDEF_NO_INFO, TypeInfo,
    StarExpr, Var, SymbolTableNode
)
from mypy.traverser import TraverserVisitor
from mypy.types import CallableType
from mypy.typestate import TypeState


SavedAttributes = Dict[Tuple[ClassDef, str], SymbolTableNode]


@others
</t>
<t tx="ekr.20220525082936.596">def strip_target(node: Union[MypyFile, FuncDef, OverloadedFuncDef],
                 saved_attrs: SavedAttributes) -&gt; None:
    """Reset a fine-grained incremental target to state before semantic analysis.

    All TypeInfos are killed. Therefore we need to preserve the variables
    defined as attributes on self. This is done by patches (callbacks)
    returned from this function that re-add these variables when called.

    Args:
        node: node to strip
        saved_attrs: collect attributes here that may need to be re-added to
            classes afterwards if stripping a class body (this dict is mutated)
    """
    visitor = NodeStripVisitor(saved_attrs)
    if isinstance(node, MypyFile):
        visitor.strip_file_top_level(node)
    else:
        node.accept(visitor)


</t>
<t tx="ekr.20220525082936.597">class NodeStripVisitor(TraverserVisitor):
    @others
</t>
<t tx="ekr.20220525082936.598">def __init__(self, saved_class_attrs: SavedAttributes) -&gt; None:
    # The current active class.
    self.type: Optional[TypeInfo] = None
    # This is True at class scope, but not in methods.
    self.is_class_body = False
    # By default, process function definitions. If False, don't -- this is used for
    # processing module top levels.
    self.recurse_into_functions = True
    # These attributes were removed from top-level classes during strip and
    # will be added afterwards (if no existing definition is found). These
    # must be added back before semantically analyzing any methods.
    self.saved_class_attrs = saved_class_attrs

</t>
<t tx="ekr.20220525082936.599">def strip_file_top_level(self, file_node: MypyFile) -&gt; None:
    """Strip a module top-level (don't recursive into functions)."""
    self.recurse_into_functions = False
    file_node.plugin_deps.clear()
    file_node.accept(self)
    for name in file_node.names.copy():
        # TODO: this is a hot fix, we should delete all names,
        # see https://github.com/python/mypy/issues/6422.
        if '@' not in name:
            del file_node.names[name]

</t>
<t tx="ekr.20220525082936.6">@abstractmethod
def visit_unbound_type(self, t: UnboundType) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.60">def visit_unbound_type(self, t: UnboundType) -&gt; T:
    return self.query_types(t.args)

</t>
<t tx="ekr.20220525082936.600">def visit_block(self, b: Block) -&gt; None:
    if b.is_unreachable:
        return
    super().visit_block(b)

</t>
<t tx="ekr.20220525082936.601">def visit_class_def(self, node: ClassDef) -&gt; None:
    """Strip class body and type info, but don't strip methods."""
    # We need to save the implicitly defined instance variables,
    # i.e. those defined as attributes on self. Otherwise, they would
    # be lost if we only reprocess top-levels (this kills TypeInfos)
    # but not the methods that defined those variables.
    if not self.recurse_into_functions:
        self.save_implicit_attributes(node)
    # We need to delete any entries that were generated by plugins,
    # since they will get regenerated.
    to_delete = {v.node for v in node.info.names.values() if v.plugin_generated}
    node.type_vars = []
    node.base_type_exprs.extend(node.removed_base_type_exprs)
    node.removed_base_type_exprs = []
    node.defs.body = [s for s in node.defs.body
                      if s not in to_delete]  # type: ignore[comparison-overlap]
    with self.enter_class(node.info):
        super().visit_class_def(node)
    TypeState.reset_subtype_caches_for(node.info)
    # Kill the TypeInfo, since there is none before semantic analysis.
    node.info = CLASSDEF_NO_INFO

</t>
<t tx="ekr.20220525082936.602">def save_implicit_attributes(self, node: ClassDef) -&gt; None:
    """Produce callbacks that re-add attributes defined on self."""
    for name, sym in node.info.names.items():
        if isinstance(sym.node, Var) and sym.implicit:
            self.saved_class_attrs[node, name] = sym

</t>
<t tx="ekr.20220525082936.603">def visit_func_def(self, node: FuncDef) -&gt; None:
    if not self.recurse_into_functions:
        return
    node.expanded = []
    node.type = node.unanalyzed_type
    if node.type:
        # Type variable binder binds type variables before the type is analyzed,
        # this causes unanalyzed_type to be modified in place. We needed to revert this
        # in order to get the state exactly as it was before semantic analysis.
        # See also #4814.
        assert isinstance(node.type, CallableType)
        node.type.variables = []
    with self.enter_method(node.info) if node.info else nullcontext():
        super().visit_func_def(node)

</t>
<t tx="ekr.20220525082936.604">def visit_decorator(self, node: Decorator) -&gt; None:
    node.var.type = None
    for expr in node.decorators:
        expr.accept(self)
    if self.recurse_into_functions:
        node.func.accept(self)
    else:
        # Only touch the final status if we re-process
        # the top level, since decorators are processed there.
        node.var.is_final = False
        node.func.is_final = False

</t>
<t tx="ekr.20220525082936.605">def visit_overloaded_func_def(self, node: OverloadedFuncDef) -&gt; None:
    if not self.recurse_into_functions:
        return
    # Revert change made during semantic analysis main pass.
    node.items = node.unanalyzed_items.copy()
    node.impl = None
    node.is_final = False
    super().visit_overloaded_func_def(node)

</t>
<t tx="ekr.20220525082936.606">def visit_assignment_stmt(self, node: AssignmentStmt) -&gt; None:
    node.type = node.unanalyzed_type
    node.is_final_def = False
    node.is_alias_def = False
    if self.type and not self.is_class_body:
        for lvalue in node.lvalues:
            # Revert assignments made via self attributes.
            self.process_lvalue_in_method(lvalue)
    super().visit_assignment_stmt(node)

</t>
<t tx="ekr.20220525082936.607">def visit_import_from(self, node: ImportFrom) -&gt; None:
    node.assignments = []

</t>
<t tx="ekr.20220525082936.608">def visit_import_all(self, node: ImportAll) -&gt; None:
    node.assignments = []

</t>
<t tx="ekr.20220525082936.609">def visit_for_stmt(self, node: ForStmt) -&gt; None:
    node.index_type = node.unanalyzed_index_type
    node.inferred_item_type = None
    node.inferred_iterator_type = None
    super().visit_for_stmt(node)

</t>
<t tx="ekr.20220525082936.61">def visit_type_list(self, t: TypeList) -&gt; T:
    return self.query_types(t.items)

</t>
<t tx="ekr.20220525082936.610">def visit_name_expr(self, node: NameExpr) -&gt; None:
    self.strip_ref_expr(node)

</t>
<t tx="ekr.20220525082936.611">def visit_member_expr(self, node: MemberExpr) -&gt; None:
    self.strip_ref_expr(node)
    super().visit_member_expr(node)

</t>
<t tx="ekr.20220525082936.612">def visit_index_expr(self, node: IndexExpr) -&gt; None:
    node.analyzed = None  # May have been an alias or type application.
    super().visit_index_expr(node)

</t>
<t tx="ekr.20220525082936.613">def strip_ref_expr(self, node: RefExpr) -&gt; None:
    node.kind = None
    node.node = None
    node.fullname = None
    node.is_new_def = False
    node.is_inferred_def = False

</t>
<t tx="ekr.20220525082936.614">def visit_call_expr(self, node: CallExpr) -&gt; None:
    node.analyzed = None
    super().visit_call_expr(node)

</t>
<t tx="ekr.20220525082936.615">def visit_super_expr(self, node: SuperExpr) -&gt; None:
    node.info = None
    super().visit_super_expr(node)

</t>
<t tx="ekr.20220525082936.616">def process_lvalue_in_method(self, lvalue: Node) -&gt; None:
    if isinstance(lvalue, MemberExpr):
        if lvalue.is_new_def:
            # Remove defined attribute from the class symbol table. If is_new_def is
            # true for a MemberExpr, we know that it must be an assignment through
            # self, since only those can define new attributes.
            assert self.type is not None
            if lvalue.name in self.type.names:
                del self.type.names[lvalue.name]
            key = (self.type.defn, lvalue.name)
            if key in self.saved_class_attrs:
                del self.saved_class_attrs[key]
    elif isinstance(lvalue, (TupleExpr, ListExpr)):
        for item in lvalue.items:
            self.process_lvalue_in_method(item)
    elif isinstance(lvalue, StarExpr):
        self.process_lvalue_in_method(lvalue.expr)

</t>
<t tx="ekr.20220525082936.617">@contextlib.contextmanager
def enter_class(self, info: TypeInfo) -&gt; Iterator[None]:
    old_type = self.type
    old_is_class_body = self.is_class_body
    self.type = info
    self.is_class_body = True
    yield
    self.type = old_type
    self.is_class_body = old_is_class_body

</t>
<t tx="ekr.20220525082936.618">@contextlib.contextmanager
def enter_method(self, info: TypeInfo) -&gt; Iterator[None]:
    old_type = self.type
    old_is_class_body = self.is_class_body
    self.type = info
    self.is_class_body = False
    yield
    self.type = old_type
    self.is_class_body = old_is_class_body
</t>
<t tx="ekr.20220525082936.619">@path C:/Repos/mypy/mypy/server/
@language python
@tabwidth -4
"""Generate fine-grained dependencies for AST nodes, for use in the daemon mode.

Dependencies are stored in a map from *triggers* to *sets of affected locations*.

A trigger is a string that represents a program property that has changed, such
as the signature of a specific function. Triggers are written as '&lt;...&gt;' (angle
brackets). When a program property changes, we determine the relevant trigger(s)
and all affected locations. The latter are stale and will have to be reprocessed.

An affected location is a string than can refer to a *target* (a non-nested
function or method, or a module top level), a class, or a trigger (for
recursively triggering other triggers).

Here's an example representation of a simple dependency map (in format
"&lt;trigger&gt; -&gt; locations"):

  &lt;m.A.g&gt; -&gt; m.f
  &lt;m.A&gt; -&gt; &lt;m.f&gt;, m.A, m.f

Assuming 'A' is a class, this means that

1) if a property of 'm.A.g', such as the signature, is changed, we need
   to process target (function) 'm.f'

2) if the MRO or other significant property of class 'm.A' changes, we
   need to process target 'm.f', the entire class 'm.A', and locations
   triggered by trigger '&lt;m.f&gt;' (this explanation is a bit simplified;
   see below for more details).

The triggers to fire are determined using mypy.server.astdiff.

Examples of triggers:

* '&lt;mod.x&gt;' represents a module attribute/function/class. If any externally
  visible property of 'x' changes, this gets fired. For changes within
  classes, only "big" changes cause the class to be triggered (such as a
  change in MRO). Smaller changes, such as changes to some attributes, don't
  trigger the entire class.
* '&lt;mod.Cls.x&gt;' represents the type and kind of attribute/method 'x' of
  class 'mod.Cls'. This can also refer to an attribute inherited from a
  base class (relevant if it's accessed through a value of type 'Cls'
  instead of the base class type).
* '&lt;package.mod&gt;' represents the existence of module 'package.mod'. This
  gets triggered if 'package.mod' is created or deleted, or if it gets
  changed into something other than a module.

Examples of locations:

* 'mod' is the top level of module 'mod' (doesn't include any function bodies,
  but includes class bodies not nested within a function).
* 'mod.f' is function 'f' in module 'mod' (module-level variables aren't separate
  locations but are included in the module top level). Functions also include
  any nested functions and classes -- such nested definitions aren't separate
  locations, for simplicity of implementation.
* 'mod.Cls.f' is method 'f' of 'mod.Cls'. Non-method attributes aren't locations.
* 'mod.Cls' represents each method in class 'mod.Cls' + the top-level of the
  module 'mod'. (To simplify the implementation, there is no location that only
  includes the body of a class without the entire surrounding module top level.)
* Trigger '&lt;...&gt;' as a location is an indirect way of referring to to all
  locations triggered by the trigger. These indirect locations keep the
  dependency map smaller and easier to manage.

Triggers can be triggered by program changes such as these:

* Addition or deletion of an attribute (or module).
* Change of the kind of thing a name represents (such as a change from a function
  to a class).
* Change of the static type of a name.

Changes in the body of a function that aren't reflected in the signature don't
cause the function to be triggered. More generally, we trigger only on changes
that may affect type checking results outside the module that contains the
change.

We don't generate dependencies from builtins and certain other stdlib modules,
since these change very rarely, and they would just increase the size of the
dependency map significantly without significant benefit.

Test cases for this module live in 'test-data/unit/deps*.test'.
"""

from typing import Dict, List, Set, Optional, Tuple
from typing_extensions import DefaultDict

from mypy.checkmember import bind_self
from mypy.nodes import (
    Node, Expression, MypyFile, FuncDef, ClassDef, AssignmentStmt, NameExpr, MemberExpr, Import,
    ImportFrom, CallExpr, CastExpr, TypeVarExpr, TypeApplication, IndexExpr, UnaryExpr, OpExpr,
    ComparisonExpr, GeneratorExpr, DictionaryComprehension, StarExpr, PrintStmt, ForStmt, WithStmt,
    TupleExpr, OperatorAssignmentStmt, DelStmt, YieldFromExpr, Decorator, Block,
    TypeInfo, FuncBase, OverloadedFuncDef, RefExpr, SuperExpr, Var, NamedTupleExpr, TypedDictExpr,
    LDEF, MDEF, GDEF, TypeAliasExpr, NewTypeExpr, ImportAll, EnumCallExpr, AwaitExpr,
    AssertTypeExpr,
)
from mypy.operators import (
    op_methods, reverse_op_methods, ops_with_inplace_method, unary_op_methods
)
from mypy.traverser import TraverserVisitor
from mypy.types import (
    Type, Instance, AnyType, NoneType, TypeVisitor, CallableType, DeletedType, PartialType,
    TupleType, TypeType, TypeVarType, TypedDictType, UnboundType, UninhabitedType, UnionType,
    FunctionLike, Overloaded, TypeOfAny, LiteralType, ErasedType, get_proper_type, ProperType,
    TypeAliasType, ParamSpecType, Parameters, UnpackType, TypeVarTupleType,
)
from mypy.server.trigger import make_trigger, make_wildcard_trigger
from mypy.util import correct_relative_import
from mypy.scope import Scope
from mypy.typestate import TypeState
from mypy.options import Options


@others
</t>
<t tx="ekr.20220525082936.62">def visit_callable_argument(self, t: CallableArgument) -&gt; T:
    return t.typ.accept(self)

</t>
<t tx="ekr.20220525082936.620">def get_dependencies(target: MypyFile,
                     type_map: Dict[Expression, Type],
                     python_version: Tuple[int, int],
                     options: Options) -&gt; Dict[str, Set[str]]:
    """Get all dependencies of a node, recursively."""
    visitor = DependencyVisitor(type_map, python_version, target.alias_deps, options)
    target.accept(visitor)
    return visitor.map


</t>
<t tx="ekr.20220525082936.621">def get_dependencies_of_target(module_id: str,
                               module_tree: MypyFile,
                               target: Node,
                               type_map: Dict[Expression, Type],
                               python_version: Tuple[int, int]) -&gt; Dict[str, Set[str]]:
    """Get dependencies of a target -- don't recursive into nested targets."""
    # TODO: Add tests for this function.
    visitor = DependencyVisitor(type_map, python_version, module_tree.alias_deps)
    with visitor.scope.module_scope(module_id):
        if isinstance(target, MypyFile):
            # Only get dependencies of the top-level of the module. Don't recurse into
            # functions.
            for defn in target.defs:
                # TODO: Recurse into top-level statements and class bodies but skip functions.
                if not isinstance(defn, (ClassDef, Decorator, FuncDef, OverloadedFuncDef)):
                    defn.accept(visitor)
        elif isinstance(target, FuncBase) and target.info:
            # It's a method.
            # TODO: Methods in nested classes.
            with visitor.scope.class_scope(target.info):
                target.accept(visitor)
        else:
            target.accept(visitor)
    return visitor.map


</t>
<t tx="ekr.20220525082936.622">class DependencyVisitor(TraverserVisitor):
    @others
</t>
<t tx="ekr.20220525082936.623">def __init__(self,
             type_map: Dict[Expression, Type],
             python_version: Tuple[int, int],
             alias_deps: 'DefaultDict[str, Set[str]]',
             options: Optional[Options] = None) -&gt; None:
    self.scope = Scope()
    self.type_map = type_map
    self.python2 = python_version[0] == 2
    # This attribute holds a mapping from target to names of type aliases
    # it depends on. These need to be processed specially, since they are
    # only present in expanded form in symbol tables. For example, after:
    #    A = List[int]
    #    x: A
    # The module symbol table will just have a Var `x` with type `List[int]`,
    # and the dependency of `x` on `A` is lost. Therefore the alias dependencies
    # are preserved at alias expansion points in `semanal.py`, stored as an attribute
    # on MypyFile, and then passed here.
    self.alias_deps = alias_deps
    self.map: Dict[str, Set[str]] = {}
    self.is_class = False
    self.is_package_init_file = False
    self.options = options

</t>
<t tx="ekr.20220525082936.624">def visit_mypy_file(self, o: MypyFile) -&gt; None:
    with self.scope.module_scope(o.fullname):
        self.is_package_init_file = o.is_package_init_file()
        self.add_type_alias_deps(self.scope.current_target())
        for trigger, targets in o.plugin_deps.items():
            self.map.setdefault(trigger, set()).update(targets)
        super().visit_mypy_file(o)

</t>
<t tx="ekr.20220525082936.625">def visit_func_def(self, o: FuncDef) -&gt; None:
    with self.scope.function_scope(o):
        target = self.scope.current_target()
        if o.type:
            if self.is_class and isinstance(o.type, FunctionLike):
                signature: Type = bind_self(o.type)
            else:
                signature = o.type
            for trigger in self.get_type_triggers(signature):
                self.add_dependency(trigger)
                self.add_dependency(trigger, target=make_trigger(target))
        if o.info:
            for base in non_trivial_bases(o.info):
                # Base class __init__/__new__ doesn't generate a logical
                # dependency since the override can be incompatible.
                if not self.use_logical_deps() or o.name not in ('__init__', '__new__'):
                    self.add_dependency(make_trigger(base.fullname + '.' + o.name))
        self.add_type_alias_deps(self.scope.current_target())
        super().visit_func_def(o)
        variants = set(o.expanded) - {o}
        for ex in variants:
            if isinstance(ex, FuncDef):
                super().visit_func_def(ex)

</t>
<t tx="ekr.20220525082936.626">def visit_decorator(self, o: Decorator) -&gt; None:
    if not self.use_logical_deps():
        # We don't need to recheck outer scope for an overload, only overload itself.
        # Also if any decorator is nested, it is not externally visible, so we don't need to
        # generate dependency.
        if not o.func.is_overload and self.scope.current_function_name() is None:
            self.add_dependency(make_trigger(o.func.fullname))
    else:
        # Add logical dependencies from decorators to the function. For example,
        # if we have
        #     @dec
        #     def func(): ...
        # then if `dec` is unannotated, then it will "spoil" `func` and consequently
        # all call sites, making them all `Any`.
        for d in o.decorators:
            tname: Optional[str] = None
            if isinstance(d, RefExpr) and d.fullname is not None:
                tname = d.fullname
            if (isinstance(d, CallExpr) and isinstance(d.callee, RefExpr) and
                    d.callee.fullname is not None):
                tname = d.callee.fullname
            if tname is not None:
                self.add_dependency(make_trigger(tname), make_trigger(o.func.fullname))
    super().visit_decorator(o)

</t>
<t tx="ekr.20220525082936.627">def visit_class_def(self, o: ClassDef) -&gt; None:
    with self.scope.class_scope(o.info):
        target = self.scope.current_full_target()
        self.add_dependency(make_trigger(target), target)
        old_is_class = self.is_class
        self.is_class = True
        # Add dependencies to type variables of a generic class.
        for tv in o.type_vars:
            self.add_dependency(make_trigger(tv.fullname), target)
        self.process_type_info(o.info)
        super().visit_class_def(o)
        self.is_class = old_is_class

</t>
<t tx="ekr.20220525082936.628">def visit_newtype_expr(self, o: NewTypeExpr) -&gt; None:
    if o.info:
        with self.scope.class_scope(o.info):
            self.process_type_info(o.info)

</t>
<t tx="ekr.20220525082936.629">def process_type_info(self, info: TypeInfo) -&gt; None:
    target = self.scope.current_full_target()
    for base in info.bases:
        self.add_type_dependencies(base, target=target)
    if info.tuple_type:
        self.add_type_dependencies(info.tuple_type, target=make_trigger(target))
    if info.typeddict_type:
        self.add_type_dependencies(info.typeddict_type, target=make_trigger(target))
    if info.declared_metaclass:
        self.add_type_dependencies(info.declared_metaclass, target=make_trigger(target))
    if info.is_protocol:
        for base_info in info.mro[:-1]:
            # We add dependencies from whole MRO to cover explicit subprotocols.
            # For example:
            #
            #     class Super(Protocol):
            #         x: int
            #     class Sub(Super, Protocol):
            #         y: int
            #
            # In this example we add &lt;Super[wildcard]&gt; -&gt; &lt;Sub&gt;, to invalidate Sub if
            # a new member is added to Super.
            self.add_dependency(make_wildcard_trigger(base_info.fullname),
                                target=make_trigger(target))
            # More protocol dependencies are collected in TypeState._snapshot_protocol_deps
            # after a full run or update is finished.

    self.add_type_alias_deps(self.scope.current_target())
    for name, node in info.names.items():
        if isinstance(node.node, Var):
            # Recheck Liskov if needed, self definitions are checked in the defining method
            if node.node.is_initialized_in_class and has_user_bases(info):
                self.add_dependency(make_trigger(info.fullname + '.' + name))
            for base_info in non_trivial_bases(info):
                # If the type of an attribute changes in a base class, we make references
                # to the attribute in the subclass stale.
                self.add_dependency(make_trigger(base_info.fullname + '.' + name),
                                    target=make_trigger(info.fullname + '.' + name))
    for base_info in non_trivial_bases(info):
        for name, node in base_info.names.items():
            if self.use_logical_deps():
                # Skip logical dependency if an attribute is not overridden. For example,
                # in case of:
                #     class Base:
                #         x = 1
                #         y = 2
                #     class Sub(Base):
                #         x = 3
                # we skip &lt;Base.y&gt; -&gt; &lt;Child.y&gt;, because even if `y` is unannotated it
                # doesn't affect precision of Liskov checking.
                if name not in info.names:
                    continue
                # __init__ and __new__ can be overridden with different signatures, so no
                # logical dependency.
                if name in ('__init__', '__new__'):
                    continue
            self.add_dependency(make_trigger(base_info.fullname + '.' + name),
                                target=make_trigger(info.fullname + '.' + name))
        if not self.use_logical_deps():
            # These dependencies are only useful for propagating changes --
            # they aren't logical dependencies since __init__ and __new__ can be
            # overridden with a different signature.
            self.add_dependency(make_trigger(base_info.fullname + '.__init__'),
                                target=make_trigger(info.fullname + '.__init__'))
            self.add_dependency(make_trigger(base_info.fullname + '.__new__'),
                                target=make_trigger(info.fullname + '.__new__'))
            # If the set of abstract attributes change, this may invalidate class
            # instantiation, or change the generated error message, since Python checks
            # class abstract status when creating an instance.
            self.add_dependency(make_trigger(base_info.fullname + '.(abstract)'),
                                target=make_trigger(info.fullname + '.__init__'))
            # If the base class abstract attributes change, subclass abstract
            # attributes need to be recalculated.
            self.add_dependency(make_trigger(base_info.fullname + '.(abstract)'))

</t>
<t tx="ekr.20220525082936.63">def visit_any(self, t: AnyType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20220525082936.630">def visit_import(self, o: Import) -&gt; None:
    for id, as_id in o.ids:
        self.add_dependency(make_trigger(id), self.scope.current_target())

</t>
<t tx="ekr.20220525082936.631">def visit_import_from(self, o: ImportFrom) -&gt; None:
    if self.use_logical_deps():
        # Just importing a name doesn't create a logical dependency.
        return
    module_id, _ = correct_relative_import(self.scope.current_module_id(),
                                           o.relative,
                                           o.id,
                                           self.is_package_init_file)
    self.add_dependency(make_trigger(module_id))  # needed if module is added/removed
    for name, as_name in o.names:
        self.add_dependency(make_trigger(module_id + '.' + name))

</t>
<t tx="ekr.20220525082936.632">def visit_import_all(self, o: ImportAll) -&gt; None:
    module_id, _ = correct_relative_import(self.scope.current_module_id(),
                                           o.relative,
                                           o.id,
                                           self.is_package_init_file)
    # The current target needs to be rechecked if anything "significant" changes in the
    # target module namespace (as the imported definitions will need to be updated).
    self.add_dependency(make_wildcard_trigger(module_id))

</t>
<t tx="ekr.20220525082936.633">def visit_block(self, o: Block) -&gt; None:
    if not o.is_unreachable:
        super().visit_block(o)

</t>
<t tx="ekr.20220525082936.634">def visit_assignment_stmt(self, o: AssignmentStmt) -&gt; None:
    rvalue = o.rvalue
    if isinstance(rvalue, CallExpr) and isinstance(rvalue.analyzed, TypeVarExpr):
        analyzed = rvalue.analyzed
        self.add_type_dependencies(analyzed.upper_bound,
                                   target=make_trigger(analyzed.fullname))
        for val in analyzed.values:
            self.add_type_dependencies(val, target=make_trigger(analyzed.fullname))
        # We need to re-analyze the definition if bound or value is deleted.
        super().visit_call_expr(rvalue)
    elif isinstance(rvalue, CallExpr) and isinstance(rvalue.analyzed, NamedTupleExpr):
        # Depend on types of named tuple items.
        info = rvalue.analyzed.info
        prefix = f'{self.scope.current_full_target()}.{info.name}'
        for name, symnode in info.names.items():
            if not name.startswith('_') and isinstance(symnode.node, Var):
                typ = symnode.node.type
                if typ:
                    self.add_type_dependencies(typ)
                    self.add_type_dependencies(typ, target=make_trigger(prefix))
                    attr_target = make_trigger(f'{prefix}.{name}')
                    self.add_type_dependencies(typ, target=attr_target)
    elif isinstance(rvalue, CallExpr) and isinstance(rvalue.analyzed, TypedDictExpr):
        # Depend on the underlying typeddict type
        info = rvalue.analyzed.info
        assert info.typeddict_type is not None
        prefix = f'{self.scope.current_full_target()}.{info.name}'
        self.add_type_dependencies(info.typeddict_type, target=make_trigger(prefix))
    elif isinstance(rvalue, CallExpr) and isinstance(rvalue.analyzed, EnumCallExpr):
        # Enum values are currently not checked, but for future we add the deps on them
        for name, symnode in rvalue.analyzed.info.names.items():
            if isinstance(symnode.node, Var) and symnode.node.type:
                self.add_type_dependencies(symnode.node.type)
    elif o.is_alias_def:
        assert len(o.lvalues) == 1
        lvalue = o.lvalues[0]
        assert isinstance(lvalue, NameExpr)
        typ = get_proper_type(self.type_map.get(lvalue))
        if isinstance(typ, FunctionLike) and typ.is_type_obj():
            class_name = typ.type_object().fullname
            self.add_dependency(make_trigger(class_name + '.__init__'))
            self.add_dependency(make_trigger(class_name + '.__new__'))
        if isinstance(rvalue, IndexExpr) and isinstance(rvalue.analyzed, TypeAliasExpr):
            self.add_type_dependencies(rvalue.analyzed.type)
        elif typ:
            self.add_type_dependencies(typ)
    else:
        # Normal assignment
        super().visit_assignment_stmt(o)
        for lvalue in o.lvalues:
            self.process_lvalue(lvalue)
        items = o.lvalues + [rvalue]
        for i in range(len(items) - 1):
            lvalue = items[i]
            rvalue = items[i + 1]
            if isinstance(lvalue, TupleExpr):
                self.add_attribute_dependency_for_expr(rvalue, '__iter__')
        if o.type:
            self.add_type_dependencies(o.type)
    if self.use_logical_deps() and o.unanalyzed_type is None:
        # Special case: for definitions without an explicit type like this:
        #     x = func(...)
        # we add a logical dependency &lt;func&gt; -&gt; &lt;x&gt;, because if `func` is not annotated,
        # then it will make all points of use of `x` unchecked.
        if (isinstance(rvalue, CallExpr) and isinstance(rvalue.callee, RefExpr)
                and rvalue.callee.fullname is not None):
            fname: Optional[str] = None
            if isinstance(rvalue.callee.node, TypeInfo):
                # use actual __init__ as a dependency source
                init = rvalue.callee.node.get('__init__')
                if init and isinstance(init.node, FuncBase):
                    fname = init.node.fullname
            else:
                fname = rvalue.callee.fullname
            if fname is None:
                return
            for lv in o.lvalues:
                if isinstance(lv, RefExpr) and lv.fullname and lv.is_new_def:
                    if lv.kind == LDEF:
                        return  # local definitions don't generate logical deps
                    self.add_dependency(make_trigger(fname), make_trigger(lv.fullname))

</t>
<t tx="ekr.20220525082936.635">def process_lvalue(self, lvalue: Expression) -&gt; None:
    """Generate additional dependencies for an lvalue."""
    if isinstance(lvalue, IndexExpr):
        self.add_operator_method_dependency(lvalue.base, '__setitem__')
    elif isinstance(lvalue, NameExpr):
        if lvalue.kind in (MDEF, GDEF):
            # Assignment to an attribute in the class body, or direct assignment to a
            # global variable.
            lvalue_type = self.get_non_partial_lvalue_type(lvalue)
            type_triggers = self.get_type_triggers(lvalue_type)
            attr_trigger = make_trigger('{}.{}'.format(self.scope.current_full_target(),
                                                   lvalue.name))
            for type_trigger in type_triggers:
                self.add_dependency(type_trigger, attr_trigger)
    elif isinstance(lvalue, MemberExpr):
        if self.is_self_member_ref(lvalue) and lvalue.is_new_def:
            node = lvalue.node
            if isinstance(node, Var):
                info = node.info
                if info and has_user_bases(info):
                    # Recheck Liskov for self definitions
                    self.add_dependency(make_trigger(info.fullname + '.' + lvalue.name))
        if lvalue.kind is None:
            # Reference to a non-module attribute
            if lvalue.expr not in self.type_map:
                # Unreachable assignment -&gt; not checked so no dependencies to generate.
                return
            object_type = self.type_map[lvalue.expr]
            lvalue_type = self.get_non_partial_lvalue_type(lvalue)
            type_triggers = self.get_type_triggers(lvalue_type)
            for attr_trigger in self.attribute_triggers(object_type, lvalue.name):
                for type_trigger in type_triggers:
                    self.add_dependency(type_trigger, attr_trigger)
    elif isinstance(lvalue, TupleExpr):
        for item in lvalue.items:
            self.process_lvalue(item)
    elif isinstance(lvalue, StarExpr):
        self.process_lvalue(lvalue.expr)

</t>
<t tx="ekr.20220525082936.636">def is_self_member_ref(self, memberexpr: MemberExpr) -&gt; bool:
    """Does memberexpr to refer to an attribute of self?"""
    if not isinstance(memberexpr.expr, NameExpr):
        return False
    node = memberexpr.expr.node
    return isinstance(node, Var) and node.is_self

</t>
<t tx="ekr.20220525082936.637">def get_non_partial_lvalue_type(self, lvalue: RefExpr) -&gt; Type:
    if lvalue not in self.type_map:
        # Likely a block considered unreachable during type checking.
        return UninhabitedType()
    lvalue_type = get_proper_type(self.type_map[lvalue])
    if isinstance(lvalue_type, PartialType):
        if isinstance(lvalue.node, Var):
            if lvalue.node.type:
                lvalue_type = get_proper_type(lvalue.node.type)
            else:
                lvalue_type = UninhabitedType()
        else:
            # Probably a secondary, non-definition assignment that doesn't
            # result in a non-partial type. We won't be able to infer any
            # dependencies from this so just return something. (The first,
            # definition assignment with a partial type is handled
            # differently, in the semantic analyzer.)
            assert not lvalue.is_new_def
            return UninhabitedType()
    return lvalue_type

</t>
<t tx="ekr.20220525082936.638">def visit_operator_assignment_stmt(self, o: OperatorAssignmentStmt) -&gt; None:
    super().visit_operator_assignment_stmt(o)
    self.process_lvalue(o.lvalue)
    method = op_methods[o.op]
    self.add_attribute_dependency_for_expr(o.lvalue, method)
    if o.op in ops_with_inplace_method:
        inplace_method = '__i' + method[2:]
        self.add_attribute_dependency_for_expr(o.lvalue, inplace_method)

</t>
<t tx="ekr.20220525082936.639">def visit_for_stmt(self, o: ForStmt) -&gt; None:
    super().visit_for_stmt(o)
    if not o.is_async:
        # __getitem__ is only used if __iter__ is missing but for simplicity we
        # just always depend on both.
        self.add_attribute_dependency_for_expr(o.expr, '__iter__')
        self.add_attribute_dependency_for_expr(o.expr, '__getitem__')
        if o.inferred_iterator_type:
            if self.python2:
                method = 'next'
            else:
                method = '__next__'
            self.add_attribute_dependency(o.inferred_iterator_type, method)
    else:
        self.add_attribute_dependency_for_expr(o.expr, '__aiter__')
        if o.inferred_iterator_type:
            self.add_attribute_dependency(o.inferred_iterator_type, '__anext__')

    self.process_lvalue(o.index)
    if isinstance(o.index, TupleExpr):
        # Process multiple assignment to index variables.
        item_type = o.inferred_item_type
        if item_type:
            # This is similar to above.
            self.add_attribute_dependency(item_type, '__iter__')
            self.add_attribute_dependency(item_type, '__getitem__')
    if o.index_type:
        self.add_type_dependencies(o.index_type)

</t>
<t tx="ekr.20220525082936.64">def visit_uninhabited_type(self, t: UninhabitedType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20220525082936.640">def visit_with_stmt(self, o: WithStmt) -&gt; None:
    super().visit_with_stmt(o)
    for e in o.expr:
        if not o.is_async:
            self.add_attribute_dependency_for_expr(e, '__enter__')
            self.add_attribute_dependency_for_expr(e, '__exit__')
        else:
            self.add_attribute_dependency_for_expr(e, '__aenter__')
            self.add_attribute_dependency_for_expr(e, '__aexit__')
    for typ in o.analyzed_types:
        self.add_type_dependencies(typ)

</t>
<t tx="ekr.20220525082936.641">def visit_print_stmt(self, o: PrintStmt) -&gt; None:
    super().visit_print_stmt(o)
    if o.target:
        self.add_attribute_dependency_for_expr(o.target, 'write')

</t>
<t tx="ekr.20220525082936.642">def visit_del_stmt(self, o: DelStmt) -&gt; None:
    super().visit_del_stmt(o)
    if isinstance(o.expr, IndexExpr):
        self.add_attribute_dependency_for_expr(o.expr.base, '__delitem__')

</t>
<t tx="ekr.20220525082936.643"># Expressions

</t>
<t tx="ekr.20220525082936.644">def process_global_ref_expr(self, o: RefExpr) -&gt; None:
    if o.fullname is not None:
        self.add_dependency(make_trigger(o.fullname))

    # If this is a reference to a type, generate a dependency to its
    # constructor.
    # IDEA: Avoid generating spurious dependencies for except statements,
    #       class attribute references, etc., if performance is a problem.
    typ = get_proper_type(self.type_map.get(o))
    if isinstance(typ, FunctionLike) and typ.is_type_obj():
        class_name = typ.type_object().fullname
        self.add_dependency(make_trigger(class_name + '.__init__'))
        self.add_dependency(make_trigger(class_name + '.__new__'))

</t>
<t tx="ekr.20220525082936.645">def visit_name_expr(self, o: NameExpr) -&gt; None:
    if o.kind == LDEF:
        # We don't track dependencies to local variables, since they
        # aren't externally visible.
        return
    if o.kind == MDEF:
        # Direct reference to member is only possible in the scope that
        # defined the name, so no dependency is required.
        return
    self.process_global_ref_expr(o)

</t>
<t tx="ekr.20220525082936.646">def visit_member_expr(self, e: MemberExpr) -&gt; None:
    if isinstance(e.expr, RefExpr) and isinstance(e.expr.node, TypeInfo):
        # Special case class attribute so that we don't depend on "__init__".
        self.add_dependency(make_trigger(e.expr.node.fullname))
    else:
        super().visit_member_expr(e)
    if e.kind is not None:
        # Reference to a module attribute
        self.process_global_ref_expr(e)
    else:
        # Reference to a non-module (or missing) attribute
        if e.expr not in self.type_map:
            # No type available -- this happens for unreachable code. Since it's unreachable,
            # it wasn't type checked and we don't need to generate dependencies.
            return
        if isinstance(e.expr, RefExpr) and isinstance(e.expr.node, MypyFile):
            # Special case: reference to a missing module attribute.
            self.add_dependency(make_trigger(e.expr.node.fullname + '.' + e.name))
            return
        typ = get_proper_type(self.type_map[e.expr])
        self.add_attribute_dependency(typ, e.name)
        if self.use_logical_deps() and isinstance(typ, AnyType):
            name = self.get_unimported_fullname(e, typ)
            if name is not None:
                # Generate a logical dependency from an unimported
                # definition (which comes from a missing module).
                # Example:
                #     import missing  # "missing" not in build
                #
                #     def g() -&gt; None:
                #         missing.f()  # Generate dependency from "missing.f"
                self.add_dependency(make_trigger(name))

</t>
<t tx="ekr.20220525082936.647">def get_unimported_fullname(self, e: MemberExpr, typ: AnyType) -&gt; Optional[str]:
    """If e refers to an unimported definition, infer the fullname of this.

    Return None if e doesn't refer to an unimported definition or if we can't
    determine the name.
    """
    suffix = ''
    # Unwrap nested member expression to handle cases like "a.b.c.d" where
    # "a.b" is a known reference to an unimported module. Find the base
    # reference to an unimported module (such as "a.b") and the name suffix
    # (such as "c.d") needed to build a full name.
    while typ.type_of_any == TypeOfAny.from_another_any and isinstance(e.expr, MemberExpr):
        suffix = '.' + e.name + suffix
        e = e.expr
        if e.expr not in self.type_map:
            return None
        obj_type = get_proper_type(self.type_map[e.expr])
        if not isinstance(obj_type, AnyType):
            # Can't find the base reference to the unimported module.
            return None
        typ = obj_type
    if typ.type_of_any == TypeOfAny.from_unimported_type and typ.missing_import_name:
        # Infer the full name of the unimported definition.
        return typ.missing_import_name + '.' + e.name + suffix
    return None

</t>
<t tx="ekr.20220525082936.648">def visit_super_expr(self, e: SuperExpr) -&gt; None:
    # Arguments in "super(C, self)" won't generate useful logical deps.
    if not self.use_logical_deps():
        super().visit_super_expr(e)
    if e.info is not None:
        name = e.name
        for base in non_trivial_bases(e.info):
            self.add_dependency(make_trigger(base.fullname + '.' + name))
            if name in base.names:
                # No need to depend on further base classes, since we found
                # the target.  This is safe since if the target gets
                # deleted or modified, we'll trigger it.
                break

</t>
<t tx="ekr.20220525082936.649">def visit_call_expr(self, e: CallExpr) -&gt; None:
    if isinstance(e.callee, RefExpr) and e.callee.fullname == 'builtins.isinstance':
        self.process_isinstance_call(e)
    else:
        super().visit_call_expr(e)
        typ = self.type_map.get(e.callee)
        if typ is not None:
            typ = get_proper_type(typ)
            if not isinstance(typ, FunctionLike):
                self.add_attribute_dependency(typ, '__call__')

</t>
<t tx="ekr.20220525082936.65">def visit_none_type(self, t: NoneType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20220525082936.650">def process_isinstance_call(self, e: CallExpr) -&gt; None:
    """Process "isinstance(...)" in a way to avoid some extra dependencies."""
    if len(e.args) == 2:
        arg = e.args[1]
        if (isinstance(arg, RefExpr)
                and arg.kind == GDEF
                and isinstance(arg.node, TypeInfo)
                and arg.fullname):
            # Special case to avoid redundant dependencies from "__init__".
            self.add_dependency(make_trigger(arg.fullname))
            return
    # In uncommon cases generate normal dependencies. These will include
    # spurious dependencies, but the performance impact is small.
    super().visit_call_expr(e)

</t>
<t tx="ekr.20220525082936.651">def visit_cast_expr(self, e: CastExpr) -&gt; None:
    super().visit_cast_expr(e)
    self.add_type_dependencies(e.type)

</t>
<t tx="ekr.20220525082936.652">def visit_assert_type_expr(self, e: AssertTypeExpr) -&gt; None:
    super().visit_assert_type_expr(e)
    self.add_type_dependencies(e.type)

</t>
<t tx="ekr.20220525082936.653">def visit_type_application(self, e: TypeApplication) -&gt; None:
    super().visit_type_application(e)
    for typ in e.types:
        self.add_type_dependencies(typ)

</t>
<t tx="ekr.20220525082936.654">def visit_index_expr(self, e: IndexExpr) -&gt; None:
    super().visit_index_expr(e)
    self.add_operator_method_dependency(e.base, '__getitem__')

</t>
<t tx="ekr.20220525082936.655">def visit_unary_expr(self, e: UnaryExpr) -&gt; None:
    super().visit_unary_expr(e)
    if e.op not in unary_op_methods:
        return
    method = unary_op_methods[e.op]
    self.add_operator_method_dependency(e.expr, method)

</t>
<t tx="ekr.20220525082936.656">def visit_op_expr(self, e: OpExpr) -&gt; None:
    super().visit_op_expr(e)
    self.process_binary_op(e.op, e.left, e.right)

</t>
<t tx="ekr.20220525082936.657">def visit_comparison_expr(self, e: ComparisonExpr) -&gt; None:
    super().visit_comparison_expr(e)
    for i, op in enumerate(e.operators):
        left = e.operands[i]
        right = e.operands[i + 1]
        self.process_binary_op(op, left, right)
        if self.python2 and op in ('==', '!=', '&lt;', '&lt;=', '&gt;', '&gt;='):
            self.add_operator_method_dependency(left, '__cmp__')
            self.add_operator_method_dependency(right, '__cmp__')

</t>
<t tx="ekr.20220525082936.658">def process_binary_op(self, op: str, left: Expression, right: Expression) -&gt; None:
    method = op_methods.get(op)
    if method:
        if op == 'in':
            self.add_operator_method_dependency(right, method)
        else:
            self.add_operator_method_dependency(left, method)
            rev_method = reverse_op_methods.get(method)
            if rev_method:
                self.add_operator_method_dependency(right, rev_method)

</t>
<t tx="ekr.20220525082936.659">def add_operator_method_dependency(self, e: Expression, method: str) -&gt; None:
    typ = get_proper_type(self.type_map.get(e))
    if typ is not None:
        self.add_operator_method_dependency_for_type(typ, method)

</t>
<t tx="ekr.20220525082936.66">def visit_erased_type(self, t: ErasedType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20220525082936.660">def add_operator_method_dependency_for_type(self, typ: ProperType, method: str) -&gt; None:
    # Note that operator methods can't be (non-metaclass) methods of type objects
    # (that is, TypeType objects or Callables representing a type).
    if isinstance(typ, TypeVarType):
        typ = get_proper_type(typ.upper_bound)
    if isinstance(typ, TupleType):
        typ = typ.partial_fallback
    if isinstance(typ, Instance):
        trigger = make_trigger(typ.type.fullname + '.' + method)
        self.add_dependency(trigger)
    elif isinstance(typ, UnionType):
        for item in typ.items:
            self.add_operator_method_dependency_for_type(get_proper_type(item), method)
    elif isinstance(typ, FunctionLike) and typ.is_type_obj():
        self.add_operator_method_dependency_for_type(typ.fallback, method)
    elif isinstance(typ, TypeType):
        if isinstance(typ.item, Instance) and typ.item.type.metaclass_type is not None:
            self.add_operator_method_dependency_for_type(typ.item.type.metaclass_type, method)

</t>
<t tx="ekr.20220525082936.661">def visit_generator_expr(self, e: GeneratorExpr) -&gt; None:
    super().visit_generator_expr(e)
    for seq in e.sequences:
        self.add_iter_dependency(seq)

</t>
<t tx="ekr.20220525082936.662">def visit_dictionary_comprehension(self, e: DictionaryComprehension) -&gt; None:
    super().visit_dictionary_comprehension(e)
    for seq in e.sequences:
        self.add_iter_dependency(seq)

</t>
<t tx="ekr.20220525082936.663">def visit_star_expr(self, e: StarExpr) -&gt; None:
    super().visit_star_expr(e)
    self.add_iter_dependency(e.expr)

</t>
<t tx="ekr.20220525082936.664">def visit_yield_from_expr(self, e: YieldFromExpr) -&gt; None:
    super().visit_yield_from_expr(e)
    self.add_iter_dependency(e.expr)

</t>
<t tx="ekr.20220525082936.665">def visit_await_expr(self, e: AwaitExpr) -&gt; None:
    super().visit_await_expr(e)
    self.add_attribute_dependency_for_expr(e.expr, '__await__')

</t>
<t tx="ekr.20220525082936.666"># Helpers

</t>
<t tx="ekr.20220525082936.667">def add_type_alias_deps(self, target: str) -&gt; None:
    # Type aliases are special, because some of the dependencies are calculated
    # in semanal.py, before they are expanded.
    if target in self.alias_deps:
        for alias in self.alias_deps[target]:
            self.add_dependency(make_trigger(alias))

</t>
<t tx="ekr.20220525082936.668">def add_dependency(self, trigger: str, target: Optional[str] = None) -&gt; None:
    """Add dependency from trigger to a target.

    If the target is not given explicitly, use the current target.
    """
    if trigger.startswith(('&lt;builtins.', '&lt;typing.',
                           '&lt;mypy_extensions.', '&lt;typing_extensions.')):
        # Don't track dependencies to certain library modules to keep the size of
        # the dependencies manageable. These dependencies should only
        # change on mypy version updates, which will require a full rebuild
        # anyway.
        return
    if target is None:
        target = self.scope.current_target()
    self.map.setdefault(trigger, set()).add(target)

</t>
<t tx="ekr.20220525082936.669">def add_type_dependencies(self, typ: Type, target: Optional[str] = None) -&gt; None:
    """Add dependencies to all components of a type.

    Args:
        target: If not None, override the default (current) target of the
            generated dependency.
    """
    for trigger in self.get_type_triggers(typ):
        self.add_dependency(trigger, target)

</t>
<t tx="ekr.20220525082936.67">def visit_deleted_type(self, t: DeletedType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20220525082936.670">def add_attribute_dependency(self, typ: Type, name: str) -&gt; None:
    """Add dependencies for accessing a named attribute of a type."""
    targets = self.attribute_triggers(typ, name)
    for target in targets:
        self.add_dependency(target)

</t>
<t tx="ekr.20220525082936.671">def attribute_triggers(self, typ: Type, name: str) -&gt; List[str]:
    """Return all triggers associated with the attribute of a type."""
    typ = get_proper_type(typ)
    if isinstance(typ, TypeVarType):
        typ = get_proper_type(typ.upper_bound)
    if isinstance(typ, TupleType):
        typ = typ.partial_fallback
    if isinstance(typ, Instance):
        member = f'{typ.type.fullname}.{name}'
        return [make_trigger(member)]
    elif isinstance(typ, FunctionLike) and typ.is_type_obj():
        member = f'{typ.type_object().fullname}.{name}'
        triggers = [make_trigger(member)]
        triggers.extend(self.attribute_triggers(typ.fallback, name))
        return triggers
    elif isinstance(typ, UnionType):
        targets = []
        for item in typ.items:
            targets.extend(self.attribute_triggers(item, name))
        return targets
    elif isinstance(typ, TypeType):
        triggers = self.attribute_triggers(typ.item, name)
        if isinstance(typ.item, Instance) and typ.item.type.metaclass_type is not None:
            triggers.append(make_trigger('%s.%s' %
                                         (typ.item.type.metaclass_type.type.fullname,
                                          name)))
        return triggers
    else:
        return []

</t>
<t tx="ekr.20220525082936.672">def add_attribute_dependency_for_expr(self, e: Expression, name: str) -&gt; None:
    typ = self.type_map.get(e)
    if typ is not None:
        self.add_attribute_dependency(typ, name)

</t>
<t tx="ekr.20220525082936.673">def add_iter_dependency(self, node: Expression) -&gt; None:
    typ = self.type_map.get(node)
    if typ:
        self.add_attribute_dependency(typ, '__iter__')

</t>
<t tx="ekr.20220525082936.674">def use_logical_deps(self) -&gt; bool:
    return self.options is not None and self.options.logical_deps

</t>
<t tx="ekr.20220525082936.675">def get_type_triggers(self, typ: Type) -&gt; List[str]:
    return get_type_triggers(typ, self.use_logical_deps())


</t>
<t tx="ekr.20220525082936.676">def get_type_triggers(typ: Type, use_logical_deps: bool) -&gt; List[str]:
    """Return all triggers that correspond to a type becoming stale."""
    return typ.accept(TypeTriggersVisitor(use_logical_deps))


</t>
<t tx="ekr.20220525082936.677">class TypeTriggersVisitor(TypeVisitor[List[str]]):
    @others
</t>
<t tx="ekr.20220525082936.678">def __init__(self, use_logical_deps: bool) -&gt; None:
    self.deps: List[str] = []
    self.use_logical_deps = use_logical_deps

</t>
<t tx="ekr.20220525082936.679">def get_type_triggers(self, typ: Type) -&gt; List[str]:
    return get_type_triggers(typ, self.use_logical_deps)

</t>
<t tx="ekr.20220525082936.68">def visit_type_var(self, t: TypeVarType) -&gt; T:
    return self.query_types([t.upper_bound] + t.values)

</t>
<t tx="ekr.20220525082936.680">def visit_instance(self, typ: Instance) -&gt; List[str]:
    trigger = make_trigger(typ.type.fullname)
    triggers = [trigger]
    for arg in typ.args:
        triggers.extend(self.get_type_triggers(arg))
    if typ.last_known_value:
        triggers.extend(self.get_type_triggers(typ.last_known_value))
    return triggers

</t>
<t tx="ekr.20220525082936.681">def visit_type_alias_type(self, typ: TypeAliasType) -&gt; List[str]:
    assert typ.alias is not None
    trigger = make_trigger(typ.alias.fullname)
    triggers = [trigger]
    for arg in typ.args:
        triggers.extend(self.get_type_triggers(arg))
    # TODO: Add guard for infinite recursion here. Moreover, now that type aliases
    # are its own kind of types we can simplify the logic to rely on intermediate
    # dependencies (like for instance types).
    triggers.extend(self.get_type_triggers(typ.alias.target))
    return triggers

</t>
<t tx="ekr.20220525082936.682">def visit_any(self, typ: AnyType) -&gt; List[str]:
    if typ.missing_import_name is not None:
        return [make_trigger(typ.missing_import_name)]
    return []

</t>
<t tx="ekr.20220525082936.683">def visit_none_type(self, typ: NoneType) -&gt; List[str]:
    return []

</t>
<t tx="ekr.20220525082936.684">def visit_callable_type(self, typ: CallableType) -&gt; List[str]:
    triggers = []
    for arg in typ.arg_types:
        triggers.extend(self.get_type_triggers(arg))
    triggers.extend(self.get_type_triggers(typ.ret_type))
    # fallback is a metaclass type for class objects, and is
    # processed separately.
    return triggers

</t>
<t tx="ekr.20220525082936.685">def visit_overloaded(self, typ: Overloaded) -&gt; List[str]:
    triggers = []
    for item in typ.items:
        triggers.extend(self.get_type_triggers(item))
    return triggers

</t>
<t tx="ekr.20220525082936.686">def visit_erased_type(self, t: ErasedType) -&gt; List[str]:
    # This type should exist only temporarily during type inference
    assert False, "Should not see an erased type here"

</t>
<t tx="ekr.20220525082936.687">def visit_deleted_type(self, typ: DeletedType) -&gt; List[str]:
    return []

</t>
<t tx="ekr.20220525082936.688">def visit_partial_type(self, typ: PartialType) -&gt; List[str]:
    assert False, "Should not see a partial type here"

</t>
<t tx="ekr.20220525082936.689">def visit_tuple_type(self, typ: TupleType) -&gt; List[str]:
    triggers = []
    for item in typ.items:
        triggers.extend(self.get_type_triggers(item))
    triggers.extend(self.get_type_triggers(typ.partial_fallback))
    return triggers

</t>
<t tx="ekr.20220525082936.69">def visit_param_spec(self, t: ParamSpecType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20220525082936.690">def visit_type_type(self, typ: TypeType) -&gt; List[str]:
    triggers = self.get_type_triggers(typ.item)
    if not self.use_logical_deps:
        old_triggers = triggers[:]
        for trigger in old_triggers:
            triggers.append(trigger.rstrip('&gt;') + '.__init__&gt;')
            triggers.append(trigger.rstrip('&gt;') + '.__new__&gt;')
    return triggers

</t>
<t tx="ekr.20220525082936.691">def visit_type_var(self, typ: TypeVarType) -&gt; List[str]:
    triggers = []
    if typ.fullname:
        triggers.append(make_trigger(typ.fullname))
    if typ.upper_bound:
        triggers.extend(self.get_type_triggers(typ.upper_bound))
    for val in typ.values:
        triggers.extend(self.get_type_triggers(val))
    return triggers

</t>
<t tx="ekr.20220525082936.692">def visit_param_spec(self, typ: ParamSpecType) -&gt; List[str]:
    triggers = []
    if typ.fullname:
        triggers.append(make_trigger(typ.fullname))
    triggers.extend(self.get_type_triggers(typ.upper_bound))
    return triggers

</t>
<t tx="ekr.20220525082936.693">def visit_type_var_tuple(self, typ: TypeVarTupleType) -&gt; List[str]:
    triggers = []
    if typ.fullname:
        triggers.append(make_trigger(typ.fullname))
    triggers.extend(self.get_type_triggers(typ.upper_bound))
    return triggers

</t>
<t tx="ekr.20220525082936.694">def visit_unpack_type(self, typ: UnpackType) -&gt; List[str]:
    return typ.type.accept(self)

</t>
<t tx="ekr.20220525082936.695">def visit_parameters(self, typ: Parameters) -&gt; List[str]:
    triggers = []
    for arg in typ.arg_types:
        triggers.extend(self.get_type_triggers(arg))
    return triggers

</t>
<t tx="ekr.20220525082936.696">def visit_typeddict_type(self, typ: TypedDictType) -&gt; List[str]:
    triggers = []
    for item in typ.items.values():
        triggers.extend(self.get_type_triggers(item))
    triggers.extend(self.get_type_triggers(typ.fallback))
    return triggers

</t>
<t tx="ekr.20220525082936.697">def visit_literal_type(self, typ: LiteralType) -&gt; List[str]:
    return self.get_type_triggers(typ.fallback)

</t>
<t tx="ekr.20220525082936.698">def visit_unbound_type(self, typ: UnboundType) -&gt; List[str]:
    return []

</t>
<t tx="ekr.20220525082936.699">def visit_uninhabited_type(self, typ: UninhabitedType) -&gt; List[str]:
    return []

</t>
<t tx="ekr.20220525082936.7">@abstractmethod
def visit_any(self, t: AnyType) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.70">def visit_type_var_tuple(self, t: TypeVarTupleType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20220525082936.700">def visit_union_type(self, typ: UnionType) -&gt; List[str]:
    triggers = []
    for item in typ.items:
        triggers.extend(self.get_type_triggers(item))
    return triggers


</t>
<t tx="ekr.20220525082936.701">def merge_dependencies(new_deps: Dict[str, Set[str]],
                       deps: Dict[str, Set[str]]) -&gt; None:
    for trigger, targets in new_deps.items():
        deps.setdefault(trigger, set()).update(targets)


</t>
<t tx="ekr.20220525082936.702">def non_trivial_bases(info: TypeInfo) -&gt; List[TypeInfo]:
    return [base for base in info.mro[1:]
            if base.fullname != 'builtins.object']


</t>
<t tx="ekr.20220525082936.703">def has_user_bases(info: TypeInfo) -&gt; bool:
    return any(base.module_name not in ('builtins', 'typing', 'enum') for base in info.mro[1:])


</t>
<t tx="ekr.20220525082936.704">def dump_all_dependencies(modules: Dict[str, MypyFile],
                          type_map: Dict[Expression, Type],
                          python_version: Tuple[int, int],
                          options: Options) -&gt; None:
    """Generate dependencies for all interesting modules and print them to stdout."""
    all_deps: Dict[str, Set[str]] = {}
    for id, node in modules.items():
        # Uncomment for debugging:
        # print('processing', id)
        if id in ('builtins', 'typing') or '/typeshed/' in node.path:
            continue
        assert id == node.fullname
        deps = get_dependencies(node, type_map, python_version, options)
        for trigger, targets in deps.items():
            all_deps.setdefault(trigger, set()).update(targets)
    TypeState.add_all_protocol_deps(all_deps)

    for trigger, targets in sorted(all_deps.items(), key=lambda x: x[0]):
        print(trigger)
        for target in sorted(targets):
            print(f'    {target}')
</t>
<t tx="ekr.20220525082936.705">@path C:/Repos/mypy/mypy/server/
@language python
@tabwidth -4
"""Check for duplicate AST nodes after merge."""

from typing import Dict, List, Tuple
from typing_extensions import Final

from mypy.nodes import FakeInfo, SymbolNode, Var, Decorator, FuncDef
from mypy.server.objgraph import get_reachable_graph, get_path

# If True, print more verbose output on failure.
DUMP_MISMATCH_NODES: Final = False


@others
</t>
<t tx="ekr.20220525082936.706">def check_consistency(o: object) -&gt; None:
    """Fail if there are two AST nodes with the same fullname reachable from 'o'.

    Raise AssertionError on failure and print some debugging output.
    """
    seen, parents = get_reachable_graph(o)
    reachable = list(seen.values())
    syms = [x for x in reachable if isinstance(x, SymbolNode)]

    m: Dict[str, SymbolNode] = {}
    for sym in syms:
        if isinstance(sym, FakeInfo):
            continue

        fn = sym.fullname
        # Skip None names, since they are ambiguous.
        # TODO: Everything should have a proper full name?
        if fn is None:
            continue
        # Skip stuff that should be expected to have duplicate names
        if isinstance(sym, (Var, Decorator)):
            continue
        if isinstance(sym, FuncDef) and sym.is_overload:
            continue

        if fn not in m:
            m[sym.fullname] = sym
            continue

        # We have trouble and need to decide what to do about it.
        sym1, sym2 = sym, m[fn]

        # If the type changed, then it shouldn't have been merged.
        if type(sym1) is not type(sym2):
            continue

        path1 = get_path(sym1, seen, parents)
        path2 = get_path(sym2, seen, parents)

        if fn in m:
            print('\nDuplicate {!r} nodes with fullname {!r} found:'.format(
                type(sym).__name__, fn))
            print('[1] %d: %s' % (id(sym1), path_to_str(path1)))
            print('[2] %d: %s' % (id(sym2), path_to_str(path2)))

        if DUMP_MISMATCH_NODES and fn in m:
            # Add verbose output with full AST node contents.
            print('---')
            print(id(sym1), sym1)
            print('---')
            print(id(sym2), sym2)

        assert sym.fullname not in m


</t>
<t tx="ekr.20220525082936.707">def path_to_str(path: List[Tuple[object, object]]) -&gt; str:
    result = '&lt;root&gt;'
    for attr, obj in path:
        t = type(obj).__name__
        if t in ('dict', 'tuple', 'SymbolTable', 'list'):
            result += f'[{repr(attr)}]'
        else:
            if isinstance(obj, Var):
                result += f'.{attr}({t}:{obj.name})'
            elif t in ('BuildManager', 'FineGrainedBuildManager'):
                # Omit class name for some classes that aren't part of a class
                # hierarchy since there isn't much ambiguity.
                result += f'.{attr}'
            else:
                result += f'.{attr}({t})'
    return result
</t>
<t tx="ekr.20220525082936.708">@path C:/Repos/mypy/mypy/server/
@language python
@tabwidth -4
"""Find all objects reachable from a root object."""

from collections.abc import Iterable
import weakref
import types

from typing import List, Dict, Iterator, Tuple, Mapping
from typing_extensions import Final

method_descriptor_type: Final = type(object.__dir__)
method_wrapper_type: Final = type(object().__ne__)
wrapper_descriptor_type: Final = type(object.__ne__)

FUNCTION_TYPES: Final = (
    types.BuiltinFunctionType,
    types.FunctionType,
    types.MethodType,
    method_descriptor_type,
    wrapper_descriptor_type,
    method_wrapper_type,
)

ATTR_BLACKLIST: Final = {
    '__doc__',
    '__name__',
    '__class__',
    '__dict__',
}

# Instances of these types can't have references to other objects
ATOMIC_TYPE_BLACKLIST: Final = {
    bool,
    int,
    float,
    str,
    type(None),
    object,
}

# Don't look at most attributes of these types
COLLECTION_TYPE_BLACKLIST: Final = {
    list,
    set,
    dict,
    tuple,
}

# Don't return these objects
TYPE_BLACKLIST: Final = {
    weakref.ReferenceType,
}


@others
</t>
<t tx="ekr.20220525082936.709">def isproperty(o: object, attr: str) -&gt; bool:
    return isinstance(getattr(type(o), attr, None), property)


</t>
<t tx="ekr.20220525082936.71">def visit_unpack_type(self, t: UnpackType) -&gt; T:
    return self.query_types([t.type])

</t>
<t tx="ekr.20220525082936.710">def get_edge_candidates(o: object) -&gt; Iterator[Tuple[object, object]]:
    # use getattr because mypyc expects dict, not mappingproxy
    if '__getattribute__' in getattr(type(o), '__dict__'):  # noqa
        return
    if type(o) not in COLLECTION_TYPE_BLACKLIST:
        for attr in dir(o):
            try:
                if attr not in ATTR_BLACKLIST and hasattr(o, attr) and not isproperty(o, attr):
                    e = getattr(o, attr)
                    if not type(e) in ATOMIC_TYPE_BLACKLIST:
                        yield attr, e
            except AssertionError:
                pass
    if isinstance(o, Mapping):
        yield from o.items()
    elif isinstance(o, Iterable) and not isinstance(o, str):
        for i, e in enumerate(o):
            yield i, e


</t>
<t tx="ekr.20220525082936.711">def get_edges(o: object) -&gt; Iterator[Tuple[object, object]]:
    for s, e in get_edge_candidates(o):
        if (isinstance(e, FUNCTION_TYPES)):
            # We don't want to collect methods, but do want to collect values
            # in closures and self pointers to other objects

            if hasattr(e, '__closure__'):
                yield (s, '__closure__'), e.__closure__  # type: ignore
            if hasattr(e, '__self__'):
                se = e.__self__  # type: ignore
                if se is not o and se is not type(o) and hasattr(s, '__self__'):
                    yield s.__self__, se  # type: ignore
        else:
            if not type(e) in TYPE_BLACKLIST:
                yield s, e


</t>
<t tx="ekr.20220525082936.712">def get_reachable_graph(root: object) -&gt; Tuple[Dict[int, object],
                                               Dict[int, Tuple[int, object]]]:
    parents = {}
    seen = {id(root): root}
    worklist = [root]
    while worklist:
        o = worklist.pop()
        for s, e in get_edges(o):
            if id(e) in seen:
                continue
            parents[id(e)] = (id(o), s)
            seen[id(e)] = e
            worklist.append(e)

    return seen, parents


</t>
<t tx="ekr.20220525082936.713">def get_path(o: object,
             seen: Dict[int, object],
             parents: Dict[int, Tuple[int, object]]) -&gt; List[Tuple[object, object]]:
    path = []
    while id(o) in parents:
        pid, attr = parents[id(o)]
        o = seen[pid]
        path.append((attr, o))
    path.reverse()
    return path
</t>
<t tx="ekr.20220525082936.714">@path C:/Repos/mypy/mypy/server/
@language python
@tabwidth -4
"""Find all subexpressions of an AST node."""

from typing import List

from mypy.nodes import (
    Expression, Node, MemberExpr, YieldFromExpr, YieldExpr, CallExpr, OpExpr, ComparisonExpr,
    SliceExpr, CastExpr, RevealExpr, UnaryExpr, ListExpr, TupleExpr, DictExpr, SetExpr,
    IndexExpr, GeneratorExpr, ListComprehension, SetComprehension, DictionaryComprehension,
    ConditionalExpr, TypeApplication, LambdaExpr, StarExpr, BackquoteExpr, AwaitExpr,
    AssignmentExpr, AssertTypeExpr,
)
from mypy.traverser import TraverserVisitor


@others
</t>
<t tx="ekr.20220525082936.715">def get_subexpressions(node: Node) -&gt; List[Expression]:
    visitor = SubexpressionFinder()
    node.accept(visitor)
    return visitor.expressions


</t>
<t tx="ekr.20220525082936.716">class SubexpressionFinder(TraverserVisitor):
    @others
</t>
<t tx="ekr.20220525082936.717">def __init__(self) -&gt; None:
    self.expressions: List[Expression] = []

</t>
<t tx="ekr.20220525082936.718">def visit_int_expr(self, o: Expression) -&gt; None:
    self.add(o)

</t>
<t tx="ekr.20220525082936.719">def visit_name_expr(self, o: Expression) -&gt; None:
    self.add(o)

</t>
<t tx="ekr.20220525082936.72">def visit_parameters(self, t: Parameters) -&gt; T:
    return self.query_types(t.arg_types)

</t>
<t tx="ekr.20220525082936.720">def visit_float_expr(self, o: Expression) -&gt; None:
    self.add(o)

</t>
<t tx="ekr.20220525082936.721">def visit_str_expr(self, o: Expression) -&gt; None:
    self.add(o)

</t>
<t tx="ekr.20220525082936.722">def visit_bytes_expr(self, o: Expression) -&gt; None:
    self.add(o)

</t>
<t tx="ekr.20220525082936.723">def visit_unicode_expr(self, o: Expression) -&gt; None:
    self.add(o)

</t>
<t tx="ekr.20220525082936.724">def visit_complex_expr(self, o: Expression) -&gt; None:
    self.add(o)

</t>
<t tx="ekr.20220525082936.725">def visit_ellipsis(self, o: Expression) -&gt; None:
    self.add(o)

</t>
<t tx="ekr.20220525082936.726">def visit_super_expr(self, o: Expression) -&gt; None:
    self.add(o)

</t>
<t tx="ekr.20220525082936.727">def visit_type_var_expr(self, o: Expression) -&gt; None:
    self.add(o)

</t>
<t tx="ekr.20220525082936.728">def visit_type_alias_expr(self, o: Expression) -&gt; None:
    self.add(o)

</t>
<t tx="ekr.20220525082936.729">def visit_namedtuple_expr(self, o: Expression) -&gt; None:
    self.add(o)

</t>
<t tx="ekr.20220525082936.73">def visit_partial_type(self, t: PartialType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20220525082936.730">def visit_typeddict_expr(self, o: Expression) -&gt; None:
    self.add(o)

</t>
<t tx="ekr.20220525082936.731">def visit__promote_expr(self, o: Expression) -&gt; None:
    self.add(o)

</t>
<t tx="ekr.20220525082936.732">def visit_newtype_expr(self, o: Expression) -&gt; None:
    self.add(o)

</t>
<t tx="ekr.20220525082936.733">def visit_member_expr(self, e: MemberExpr) -&gt; None:
    self.add(e)
    super().visit_member_expr(e)

</t>
<t tx="ekr.20220525082936.734">def visit_yield_from_expr(self, e: YieldFromExpr) -&gt; None:
    self.add(e)
    super().visit_yield_from_expr(e)

</t>
<t tx="ekr.20220525082936.735">def visit_yield_expr(self, e: YieldExpr) -&gt; None:
    self.add(e)
    super().visit_yield_expr(e)

</t>
<t tx="ekr.20220525082936.736">def visit_call_expr(self, e: CallExpr) -&gt; None:
    self.add(e)
    super().visit_call_expr(e)

</t>
<t tx="ekr.20220525082936.737">def visit_op_expr(self, e: OpExpr) -&gt; None:
    self.add(e)
    super().visit_op_expr(e)

</t>
<t tx="ekr.20220525082936.738">def visit_comparison_expr(self, e: ComparisonExpr) -&gt; None:
    self.add(e)
    super().visit_comparison_expr(e)

</t>
<t tx="ekr.20220525082936.739">def visit_slice_expr(self, e: SliceExpr) -&gt; None:
    self.add(e)
    super().visit_slice_expr(e)

</t>
<t tx="ekr.20220525082936.74">def visit_instance(self, t: Instance) -&gt; T:
    return self.query_types(t.args)

</t>
<t tx="ekr.20220525082936.740">def visit_cast_expr(self, e: CastExpr) -&gt; None:
    self.add(e)
    super().visit_cast_expr(e)

</t>
<t tx="ekr.20220525082936.741">def visit_assert_type_expr(self, e: AssertTypeExpr) -&gt; None:
    self.add(e)
    super().visit_assert_type_expr(e)

</t>
<t tx="ekr.20220525082936.742">def visit_reveal_expr(self, e: RevealExpr) -&gt; None:
    self.add(e)
    super().visit_reveal_expr(e)

</t>
<t tx="ekr.20220525082936.743">def visit_assignment_expr(self, e: AssignmentExpr) -&gt; None:
    self.add(e)
    super().visit_assignment_expr(e)

</t>
<t tx="ekr.20220525082936.744">def visit_unary_expr(self, e: UnaryExpr) -&gt; None:
    self.add(e)
    super().visit_unary_expr(e)

</t>
<t tx="ekr.20220525082936.745">def visit_list_expr(self, e: ListExpr) -&gt; None:
    self.add(e)
    super().visit_list_expr(e)

</t>
<t tx="ekr.20220525082936.746">def visit_tuple_expr(self, e: TupleExpr) -&gt; None:
    self.add(e)
    super().visit_tuple_expr(e)

</t>
<t tx="ekr.20220525082936.747">def visit_dict_expr(self, e: DictExpr) -&gt; None:
    self.add(e)
    super().visit_dict_expr(e)

</t>
<t tx="ekr.20220525082936.748">def visit_set_expr(self, e: SetExpr) -&gt; None:
    self.add(e)
    super().visit_set_expr(e)

</t>
<t tx="ekr.20220525082936.749">def visit_index_expr(self, e: IndexExpr) -&gt; None:
    self.add(e)
    super().visit_index_expr(e)

</t>
<t tx="ekr.20220525082936.75">def visit_callable_type(self, t: CallableType) -&gt; T:
    # FIX generics
    return self.query_types(t.arg_types + [t.ret_type])

</t>
<t tx="ekr.20220525082936.750">def visit_generator_expr(self, e: GeneratorExpr) -&gt; None:
    self.add(e)
    super().visit_generator_expr(e)

</t>
<t tx="ekr.20220525082936.751">def visit_dictionary_comprehension(self, e: DictionaryComprehension) -&gt; None:
    self.add(e)
    super().visit_dictionary_comprehension(e)

</t>
<t tx="ekr.20220525082936.752">def visit_list_comprehension(self, e: ListComprehension) -&gt; None:
    self.add(e)
    super().visit_list_comprehension(e)

</t>
<t tx="ekr.20220525082936.753">def visit_set_comprehension(self, e: SetComprehension) -&gt; None:
    self.add(e)
    super().visit_set_comprehension(e)

</t>
<t tx="ekr.20220525082936.754">def visit_conditional_expr(self, e: ConditionalExpr) -&gt; None:
    self.add(e)
    super().visit_conditional_expr(e)

</t>
<t tx="ekr.20220525082936.755">def visit_type_application(self, e: TypeApplication) -&gt; None:
    self.add(e)
    super().visit_type_application(e)

</t>
<t tx="ekr.20220525082936.756">def visit_lambda_expr(self, e: LambdaExpr) -&gt; None:
    self.add(e)
    super().visit_lambda_expr(e)

</t>
<t tx="ekr.20220525082936.757">def visit_star_expr(self, e: StarExpr) -&gt; None:
    self.add(e)
    super().visit_star_expr(e)

</t>
<t tx="ekr.20220525082936.758">def visit_backquote_expr(self, e: BackquoteExpr) -&gt; None:
    self.add(e)
    super().visit_backquote_expr(e)

</t>
<t tx="ekr.20220525082936.759">def visit_await_expr(self, e: AwaitExpr) -&gt; None:
    self.add(e)
    super().visit_await_expr(e)

</t>
<t tx="ekr.20220525082936.76">def visit_tuple_type(self, t: TupleType) -&gt; T:
    return self.query_types(t.items)

</t>
<t tx="ekr.20220525082936.760">def add(self, e: Expression) -&gt; None:
    self.expressions.append(e)
</t>
<t tx="ekr.20220525082936.761">@path C:/Repos/mypy/mypy/server/
@language python
@tabwidth -4
def trigger_to_target(s: str) -&gt; str:
    assert s[0] == '&lt;'
    # Strip off the angle brackets
    s = s[1:-1]
    # If there is a [wildcard] or similar, strip that off too
    if s[-1] == ']':
        s = s.split('[')[0]
    return s
</t>
<t tx="ekr.20220525082936.762">@path C:/Repos/mypy/mypy/server/
@language python
@tabwidth -4
"""AST triggers that are used for fine-grained dependency handling."""

from typing_extensions import Final

# Used as a suffix for triggers to handle "from m import *" dependencies (see also
# make_wildcard_trigger)

WILDCARD_TAG: Final = "[wildcard]"


@others
</t>
<t tx="ekr.20220525082936.763">def make_trigger(name: str) -&gt; str:
    return f'&lt;{name}&gt;'


</t>
<t tx="ekr.20220525082936.764">def make_wildcard_trigger(module: str) -&gt; str:
    """Special trigger fired when any top-level name is changed in a module.

    Note that this is different from a module trigger, as module triggers are only
    fired if the module is created, deleted, or replaced with a non-module, whereas
    a wildcard trigger is triggered for namespace changes.

    This is used for "from m import *" dependencies.
    """
    return f'&lt;{module}{WILDCARD_TAG}&gt;'
</t>
<t tx="ekr.20220525082936.765">@path C:/Repos/mypy/mypy/server/
@language python
@tabwidth -4
"""Update build by processing changes using fine-grained dependencies.

Use fine-grained dependencies to update targets in other modules that
may be affected by externally-visible changes in the changed modules.

This forms the core of the fine-grained incremental daemon mode. This
module is not used at all by the 'classic' (non-daemon) incremental
mode.

Here is some motivation for this mode:

* By keeping program state in memory between incremental runs, we
  only have to process changed modules, not their dependencies. The
  classic incremental mode has to deserialize the symbol tables of
  all dependencies of changed modules, which can be slow for large
  programs.

* Fine-grained dependencies allow processing only the relevant parts
  of modules indirectly affected by a change. Say, if only one function
  in a large module is affected by a change in another module, only this
  function is processed. The classic incremental mode always processes
  an entire file as a unit, which is typically much slower.

* It's possible to independently process individual modules within an
  import cycle (SCC). Small incremental changes can be fast independent
  of the size of the related SCC. In classic incremental mode, any change
  within a SCC requires the entire SCC to be processed, which can slow
  things down considerably.

Some terms:

* A *target* is a function/method definition or the top level of a module.
  We refer to targets using their fully qualified name (e.g.
  'mod.Cls.method'). Targets are the smallest units of processing during
  fine-grained incremental checking.

* A *trigger* represents the properties of a part of a program, and it
  gets triggered/fired when these properties change. For example,
  '&lt;mod.func&gt;' refers to a module-level function. It gets triggered if
  the signature of the function changes, or if the function is removed,
  for example.

Some program state is maintained across multiple build increments in
memory:

* The full ASTs of all modules are stored in memory all the time (this
  includes the type map).

* A fine-grained dependency map is maintained, which maps triggers to
  affected program locations (these can be targets, triggers, or
  classes). The latter determine what other parts of a program need to
  be processed again due to a fired trigger.

Here's a summary of how a fine-grained incremental program update happens:

* Determine which modules have changes in their source code since the
  previous update.

* Process changed modules one at a time. Perform a separate full update
  for each changed module, but only report the errors after all modules
  have been processed, since the intermediate states can generate bogus
  errors due to only seeing a partial set of changes.

* Each changed module is processed in full. We parse the module, and
  run semantic analysis to create a new AST and symbol table for the
  module. Reuse the existing ASTs and symbol tables of modules that
  have no changes in their source code. At the end of this stage, we have
  two ASTs and symbol tables for the changed module (the old and the new
  versions). The latter AST has not yet been type checked.

* Take a snapshot of the old symbol table. This is used later to determine
  which properties of the module have changed and which triggers to fire.

* Merge the old AST with the new AST, preserving the identities of
  externally visible AST nodes for which we can find a corresponding node
  in the new AST. (Look at mypy.server.astmerge for the details.) This
  way all external references to AST nodes in the changed module will
  continue to point to the right nodes (assuming they still have a valid
  target).

* Type check the new module.

* Take another snapshot of the symbol table of the changed module.
  Look at the differences between the old and new snapshots to determine
  which parts of the changed modules have changed. The result is a set of
  fired triggers.

* Using the dependency map and the fired triggers, decide which other
  targets have become stale and need to be reprocessed.

* Create new fine-grained dependencies for the changed module. We don't
  garbage collect old dependencies, since extra dependencies are relatively
  harmless (they take some memory and can theoretically slow things down
  a bit by causing redundant work). This is implemented in
  mypy.server.deps.

* Strip the stale AST nodes that we found above. This returns them to a
  state resembling the end of semantic analysis pass 1. We'll run semantic
  analysis again on the existing AST nodes, and since semantic analysis
  is not idempotent, we need to revert some changes made during semantic
  analysis. This is implemented in mypy.server.aststrip.

* Run semantic analyzer passes 2 and 3 on the stale AST nodes, and type
  check them. We also need to do the symbol table snapshot comparison
  dance to find any changes, and we need to merge ASTs to preserve AST node
  identities.

* If some triggers haven been fired, continue processing and repeat the
  previous steps until no triggers are fired.

This is module is tested using end-to-end fine-grained incremental mode
test cases (test-data/unit/fine-grained*.test).
"""

import os
import sys
import time
from typing import (
    Dict, List, Set, Tuple, Union, Optional, NamedTuple, Sequence, Callable
)
from typing_extensions import Final

from mypy.build import (
    BuildManager, State, BuildResult, Graph, load_graph,
    process_fresh_modules, DEBUG_FINE_GRAINED,
    FAKE_ROOT_MODULE,
)
from mypy.modulefinder import BuildSource
from mypy.checker import FineGrainedDeferredNode
from mypy.errors import CompileError
from mypy.nodes import (
    MypyFile, FuncDef, TypeInfo, SymbolNode, Decorator,
    OverloadedFuncDef, SymbolTable, ImportFrom
)
from mypy.options import Options
from mypy.fscache import FileSystemCache
from mypy.server.astdiff import (
    snapshot_symbol_table, compare_symbol_table_snapshots, SnapshotItem
)
from mypy.semanal_main import (
    semantic_analysis_for_scc, semantic_analysis_for_targets, core_modules
)
from mypy.server.astmerge import merge_asts
from mypy.server.aststrip import strip_target, SavedAttributes
from mypy.server.deps import get_dependencies_of_target, merge_dependencies
from mypy.server.target import trigger_to_target
from mypy.server.trigger import make_trigger, WILDCARD_TAG
from mypy.util import module_prefix, split_target
from mypy.typestate import TypeState

MAX_ITER: Final = 1000

SENSITIVE_INTERNAL_MODULES = tuple(core_modules) + ("mypy_extensions", "typing_extensions")


@others
</t>
<t tx="ekr.20220525082936.766">class FineGrainedBuildManager:
    @others
</t>
<t tx="ekr.20220525082936.767">def __init__(self, result: BuildResult) -&gt; None:
    """Initialize fine-grained build based on a batch build.

    Args:
        result: Result from the initialized build.
                The manager and graph will be taken over by this class.
        manager: State of the build (mutated by this class)
        graph: Additional state of the build (mutated by this class)
    """
    manager = result.manager
    self.manager = manager
    self.graph = result.graph
    self.previous_modules = get_module_to_path_map(self.graph)
    self.deps = manager.fg_deps
    # Merge in any root dependencies that may not have been loaded
    merge_dependencies(manager.load_fine_grained_deps(FAKE_ROOT_MODULE), self.deps)
    self.previous_targets_with_errors = manager.errors.targets()
    self.previous_messages = result.errors[:]
    # Module, if any, that had blocking errors in the last run as (id, path) tuple.
    self.blocking_error: Optional[Tuple[str, str]] = None
    # Module that we haven't processed yet but that are known to be stale.
    self.stale: List[Tuple[str, str]] = []
    # Disable the cache so that load_graph doesn't try going back to disk
    # for the cache.
    self.manager.cache_enabled = False

    # Some hints to the test suite about what is going on:
    # Active triggers during the last update
    self.triggered: List[str] = []
    # Modules passed to update during the last update
    self.changed_modules: List[Tuple[str, str]] = []
    # Modules processed during the last update
    self.updated_modules: List[str] = []
    # Targets processed during last update (for testing only).
    self.processed_targets: List[str] = []

</t>
<t tx="ekr.20220525082936.768">def update(self,
           changed_modules: List[Tuple[str, str]],
           removed_modules: List[Tuple[str, str]]) -&gt; List[str]:
    """Update previous build result by processing changed modules.

    Also propagate changes to other modules as needed, but only process
    those parts of other modules that are affected by the changes. Retain
    the existing ASTs and symbol tables of unaffected modules.

    Reuses original BuildManager and Graph.

    Args:
        changed_modules: Modules changed since the previous update/build; each is
            a (module id, path) tuple. Includes modified and added modules.
            Assume this is correct; it's not validated here.
        removed_modules: Modules that have been deleted since the previous update
            or removed from the build.

    Returns:
        A list of errors.
    """
    self.processed_targets.clear()
    changed_modules = changed_modules + removed_modules
    removed_set = {module for module, _ in removed_modules}
    self.changed_modules = changed_modules

    if not changed_modules:
        return self.previous_messages

    # Reset find_module's caches for the new build.
    self.manager.find_module_cache.clear()

    self.triggered = []
    self.updated_modules = []
    changed_modules = dedupe_modules(changed_modules + self.stale)
    initial_set = {id for id, _ in changed_modules}
    self.manager.log_fine_grained('==== update %s ====' % ', '.join(
        repr(id) for id, _ in changed_modules))
    if self.previous_targets_with_errors and is_verbose(self.manager):
        self.manager.log_fine_grained('previous targets with errors: %s' %
                         sorted(self.previous_targets_with_errors))

    blocking_error = None
    if self.blocking_error:
        # Handle blocking errors first. We'll exit as soon as we find a
        # module that still has blocking errors.
        self.manager.log_fine_grained(f'existing blocker: {self.blocking_error[0]}')
        changed_modules = dedupe_modules([self.blocking_error] + changed_modules)
        blocking_error = self.blocking_error[0]
        self.blocking_error = None

    while True:
        result = self.update_one(changed_modules, initial_set, removed_set, blocking_error)
        changed_modules, (next_id, next_path), blocker_messages = result

        if blocker_messages is not None:
            self.blocking_error = (next_id, next_path)
            self.stale = changed_modules
            messages = blocker_messages
            break

        # It looks like we are done processing everything, so now
        # reprocess all targets with errors. We are careful to
        # support the possibility that reprocessing an errored module
        # might trigger loading of a module, but I am not sure
        # if this can really happen.
        if not changed_modules:
            # N.B: We just checked next_id, so manager.errors contains
            # the errors from it. Thus we consider next_id up to date
            # when propagating changes from the errored targets,
            # which prevents us from reprocessing errors in it.
            changed_modules = propagate_changes_using_dependencies(
                self.manager, self.graph, self.deps, set(), {next_id},
                self.previous_targets_with_errors, self.processed_targets)
            changed_modules = dedupe_modules(changed_modules)
            if not changed_modules:
                # Preserve state needed for the next update.
                self.previous_targets_with_errors = self.manager.errors.targets()
                messages = self.manager.errors.new_messages()
                break

    self.previous_messages = messages[:]
    return messages

</t>
<t tx="ekr.20220525082936.769">def trigger(self, target: str) -&gt; List[str]:
    """Trigger a specific target explicitly.

    This is intended for use by the suggestions engine.
    """
    self.manager.errors.reset()
    changed_modules = propagate_changes_using_dependencies(
        self.manager, self.graph, self.deps, set(), set(),
        self.previous_targets_with_errors | {target}, [])
    # Preserve state needed for the next update.
    self.previous_targets_with_errors = self.manager.errors.targets()
    self.previous_messages = self.manager.errors.new_messages()[:]
    return self.update(changed_modules, [])

</t>
<t tx="ekr.20220525082936.77">def visit_typeddict_type(self, t: TypedDictType) -&gt; T:
    return self.query_types(t.items.values())

</t>
<t tx="ekr.20220525082936.770">def flush_cache(self) -&gt; None:
    """Flush AST cache.

    This needs to be called after each increment, or file changes won't
    be detected reliably.
    """
    self.manager.ast_cache.clear()

</t>
<t tx="ekr.20220525082936.771">def update_one(self,
               changed_modules: List[Tuple[str, str]],
               initial_set: Set[str],
               removed_set: Set[str],
               blocking_error: Optional[str]) -&gt; Tuple[List[Tuple[str, str]],
                                                       Tuple[str, str],
                                                       Optional[List[str]]]:
    """Process a module from the list of changed modules.

    Returns:
        Tuple with these items:

        - Updated list of pending changed modules as (module id, path) tuples
        - Module which was actually processed as (id, path) tuple
        - If there was a blocking error, the error messages from it
    """
    t0 = time.time()
    next_id, next_path = changed_modules.pop(0)

    # If we have a module with a blocking error that is no longer
    # in the import graph, we must skip it as otherwise we'll be
    # stuck with the blocking error.
    if (next_id == blocking_error
            and next_id not in self.previous_modules
            and next_id not in initial_set):
        self.manager.log_fine_grained(
            f'skip {next_id!r} (module with blocking error not in import graph)')
        return changed_modules, (next_id, next_path), None

    result = self.update_module(next_id, next_path, next_id in removed_set)
    remaining, (next_id, next_path), blocker_messages = result
    changed_modules = [(id, path) for id, path in changed_modules
                       if id != next_id]
    changed_modules = dedupe_modules(remaining + changed_modules)
    t1 = time.time()

    self.manager.log_fine_grained(
        f"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left")

    return changed_modules, (next_id, next_path), blocker_messages

</t>
<t tx="ekr.20220525082936.772">def update_module(self,
                  module: str,
                  path: str,
                  force_removed: bool) -&gt; Tuple[List[Tuple[str, str]],
                                                Tuple[str, str],
                                                Optional[List[str]]]:
    """Update a single modified module.

    If the module contains imports of previously unseen modules, only process one of
    the new modules and return the remaining work to be done.

    Args:
        module: Id of the module
        path: File system path of the module
        force_removed: If True, consider module removed from the build even if path
            exists (used for removing an existing file from the build)

    Returns:
        Tuple with these items:

        - Remaining modules to process as (module id, path) tuples
        - Module which was actually processed as (id, path) tuple
        - If there was a blocking error, the error messages from it
    """
    self.manager.log_fine_grained(f'--- update single {module!r} ---')
    self.updated_modules.append(module)

    # builtins and friends could potentially get triggered because
    # of protocol stuff, but nothing good could possibly come from
    # actually updating them.
    if module in SENSITIVE_INTERNAL_MODULES:
        return [], (module, path), None

    manager = self.manager
    previous_modules = self.previous_modules
    graph = self.graph

    ensure_deps_loaded(module, self.deps, graph)

    # If this is an already existing module, make sure that we have
    # its tree loaded so that we can snapshot it for comparison.
    ensure_trees_loaded(manager, graph, [module])

    t0 = time.time()
    # Record symbol table snapshot of old version the changed module.
    old_snapshots: Dict[str, Dict[str, SnapshotItem]] = {}
    if module in manager.modules:
        snapshot = snapshot_symbol_table(module, manager.modules[module].names)
        old_snapshots[module] = snapshot

    manager.errors.reset()
    self.processed_targets.append(module)
    result = update_module_isolated(module, path, manager, previous_modules, graph,
                                    force_removed)
    if isinstance(result, BlockedUpdate):
        # Blocking error -- just give up
        module, path, remaining, errors = result
        self.previous_modules = get_module_to_path_map(graph)
        return remaining, (module, path), errors
    assert isinstance(result, NormalUpdate)  # Work around #4124
    module, path, remaining, tree = result

    # TODO: What to do with stale dependencies?
    t1 = time.time()
    triggered = calculate_active_triggers(manager, old_snapshots, {module: tree})
    if is_verbose(self.manager):
        filtered = [trigger for trigger in triggered
                    if not trigger.endswith('__&gt;')]
        self.manager.log_fine_grained(f'triggered: {sorted(filtered)!r}')
    self.triggered.extend(triggered | self.previous_targets_with_errors)
    if module in graph:
        graph[module].update_fine_grained_deps(self.deps)
        graph[module].free_state()
    remaining += propagate_changes_using_dependencies(
        manager, graph, self.deps, triggered,
        {module},
        targets_with_errors=set(), processed_targets=self.processed_targets)
    t2 = time.time()
    manager.add_stats(
        update_isolated_time=t1 - t0,
        propagate_time=t2 - t1)

    # Preserve state needed for the next update.
    self.previous_targets_with_errors.update(manager.errors.targets())
    self.previous_modules = get_module_to_path_map(graph)

    return remaining, (module, path), None


</t>
<t tx="ekr.20220525082936.773">def find_unloaded_deps(manager: BuildManager, graph: Dict[str, State],
                       initial: Sequence[str]) -&gt; List[str]:
    """Find all the deps of the nodes in initial that haven't had their tree loaded.

    The key invariant here is that if a module is loaded, so are all
    of their dependencies. This means that when we encounter a loaded
    module, we don't need to explore its dependencies.  (This
    invariant is slightly violated when dependencies are added, which
    can be handled by calling find_unloaded_deps directly on the new
    dependencies.)
    """
    worklist = list(initial)
    seen: Set[str] = set()
    unloaded = []
    while worklist:
        node = worklist.pop()
        if node in seen or node not in graph:
            continue
        seen.add(node)
        if node not in manager.modules:
            ancestors = graph[node].ancestors or []
            worklist.extend(graph[node].dependencies + ancestors)
            unloaded.append(node)

    return unloaded


</t>
<t tx="ekr.20220525082936.774">def ensure_deps_loaded(module: str,
                       deps: Dict[str, Set[str]], graph: Dict[str, State]) -&gt; None:
    """Ensure that the dependencies on a module are loaded.

    Dependencies are loaded into the 'deps' dictionary.

    This also requires loading dependencies from any parent modules,
    since dependencies will get stored with parent modules when a module
    doesn't exist.
    """
    if module in graph and graph[module].fine_grained_deps_loaded:
        return
    parts = module.split('.')
    for i in range(len(parts)):
        base = '.'.join(parts[:i + 1])
        if base in graph and not graph[base].fine_grained_deps_loaded:
            merge_dependencies(graph[base].load_fine_grained_deps(), deps)
            graph[base].fine_grained_deps_loaded = True


</t>
<t tx="ekr.20220525082936.775">def ensure_trees_loaded(manager: BuildManager, graph: Dict[str, State],
                        initial: Sequence[str]) -&gt; None:
    """Ensure that the modules in initial and their deps have loaded trees."""
    to_process = find_unloaded_deps(manager, graph, initial)
    if to_process:
        if is_verbose(manager):
            manager.log_fine_grained("Calling process_fresh_modules on set of size {} ({})".format(
                len(to_process), sorted(to_process)))
        process_fresh_modules(graph, to_process, manager)


</t>
<t tx="ekr.20220525082936.776"># The result of update_module_isolated when no blockers, with these items:
#
# - Id of the changed module (can be different from the module argument)
# - Path of the changed module
# - New AST for the changed module (None if module was deleted)
# - Remaining changed modules that are not processed yet as (module id, path)
#   tuples (non-empty if the original changed module imported other new
#   modules)
class NormalUpdate(NamedTuple):
    module: str
    path: str
    remaining: List[Tuple[str, str]]
    tree: Optional[MypyFile]


</t>
<t tx="ekr.20220525082936.777"># The result of update_module_isolated when there is a blocking error. Items
# are similar to NormalUpdate (but there are fewer).
class BlockedUpdate(NamedTuple):
    module: str
    path: str
    remaining: List[Tuple[str, str]]
    messages: List[str]


</t>
<t tx="ekr.20220525082936.778">UpdateResult = Union[NormalUpdate, BlockedUpdate]


</t>
<t tx="ekr.20220525082936.779">def update_module_isolated(module: str,
                           path: str,
                           manager: BuildManager,
                           previous_modules: Dict[str, str],
                           graph: Graph,
                           force_removed: bool) -&gt; UpdateResult:
    """Build a new version of one changed module only.

    Don't propagate changes to elsewhere in the program. Raise CompileError on
    encountering a blocking error.

    Args:
        module: Changed module (modified, created or deleted)
        path: Path of the changed module
        manager: Build manager
        graph: Build graph
        force_removed: If True, consider the module removed from the build even it the
            file exists

    Returns a named tuple describing the result (see above for details).
    """
    if module not in graph:
        manager.log_fine_grained(f'new module {module!r}')

    if not manager.fscache.isfile(path) or force_removed:
        delete_module(module, path, graph, manager)
        return NormalUpdate(module, path, [], None)

    sources = get_sources(manager.fscache, previous_modules, [(module, path)])

    if module in manager.missing_modules:
        manager.missing_modules.remove(module)

    orig_module = module
    orig_state = graph.get(module)
    orig_tree = manager.modules.get(module)

    @others
    new_modules: List[State] = []
    try:
        if module in graph:
            del graph[module]
        load_graph(sources, manager, graph, new_modules)
    except CompileError as err:
        # Parse error somewhere in the program -- a blocker
        assert err.module_with_blocker
        restore([module] + [st.id for st in new_modules])
        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)

    # Reparsing the file may have brought in dependencies that we
    # didn't have before. Make sure that they are loaded to restore
    # the invariant that a module having a loaded tree implies that
    # its dependencies do as well.
    ensure_trees_loaded(manager, graph, graph[module].dependencies)

    # Find any other modules brought in by imports.
    changed_modules = [(st.id, st.xpath) for st in new_modules]

    # If there are multiple modules to process, only process one of them and return
    # the remaining ones to the caller.
    if len(changed_modules) &gt; 1:
        # As an optimization, look for a module that imports no other changed modules.
        module, path = find_relative_leaf_module(changed_modules, graph)
        changed_modules.remove((module, path))
        remaining_modules = changed_modules
        # The remaining modules haven't been processed yet so drop them.
        restore([id for id, _ in remaining_modules])
        manager.log_fine_grained(f'--&gt; {module!r} (newly imported)')
    else:
        remaining_modules = []

    state = graph[module]

    # Process the changed file.
    state.parse_file()
    assert state.tree is not None, "file must be at least parsed"
    t0 = time.time()
    try:
        semantic_analysis_for_scc(graph, [state.id], manager.errors)
    except CompileError as err:
        # There was a blocking error, so module AST is incomplete. Restore old modules.
        restore([module])
        return BlockedUpdate(module, path, remaining_modules, err.messages)

    # Merge old and new ASTs.
    new_modules_dict: Dict[str, Optional[MypyFile]] = {module: state.tree}
    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)

    t1 = time.time()
    # Perform type checking.
    state.type_checker().reset()
    state.type_check_first_pass()
    state.type_check_second_pass()
    t2 = time.time()
    state.finish_passes()
    t3 = time.time()
    manager.add_stats(
        semanal_time=t1 - t0,
        typecheck_time=t2 - t1,
        finish_passes_time=t3 - t2)

    graph[module] = state

    return NormalUpdate(module, path, remaining_modules, state.tree)


</t>
<t tx="ekr.20220525082936.78">def visit_raw_expression_type(self, t: RawExpressionType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20220525082936.780">def restore(ids: List[str]) -&gt; None:
    # For each of the modules in ids, restore that id's old
    # manager.modules and graphs entries. (Except for the original
    # module, this means deleting them.)
    for id in ids:
        if id == orig_module and orig_tree:
            manager.modules[id] = orig_tree
        elif id in manager.modules:
            del manager.modules[id]
        if id == orig_module and orig_state:
            graph[id] = orig_state
        elif id in graph:
            del graph[id]

</t>
<t tx="ekr.20220525082936.781">def find_relative_leaf_module(modules: List[Tuple[str, str]], graph: Graph) -&gt; Tuple[str, str]:
    """Find a module in a list that directly imports no other module in the list.

    If no such module exists, return the lexicographically first module from the list.
    Always return one of the items in the modules list.

    NOTE: If both 'abc' and 'typing' have changed, an effect of the above rule is that
        we prefer 'abc', even if both are in the same SCC. This works around a false
        positive in 'typing', at least in tests.

    Args:
        modules: List of (module, path) tuples (non-empty)
        graph: Program import graph that contains all modules in the module list
    """
    assert modules
    # Sort for repeatable results.
    modules = sorted(modules)
    module_set = {module for module, _ in modules}
    for module, path in modules:
        state = graph[module]
        if len(set(state.dependencies) &amp; module_set) == 0:
            # Found it!
            return module, path
    # Could not find any. Just return the first module (by lexicographic order).
    return modules[0]


</t>
<t tx="ekr.20220525082936.782">def delete_module(module_id: str,
                  path: str,
                  graph: Graph,
                  manager: BuildManager) -&gt; None:
    manager.log_fine_grained(f'delete module {module_id!r}')
    # TODO: Remove deps for the module (this only affects memory use, not correctness)
    if module_id in graph:
        del graph[module_id]
    if module_id in manager.modules:
        del manager.modules[module_id]
    components = module_id.split('.')
    if len(components) &gt; 1:
        # Delete reference to module in parent module.
        parent_id = '.'.join(components[:-1])
        # If parent module is ignored, it won't be included in the modules dictionary.
        if parent_id in manager.modules:
            parent = manager.modules[parent_id]
            if components[-1] in parent.names:
                del parent.names[components[-1]]
    # If the module is removed from the build but still exists, then
    # we mark it as missing so that it will get picked up by import from still.
    if manager.fscache.isfile(path):
        manager.missing_modules.add(module_id)


</t>
<t tx="ekr.20220525082936.783">def dedupe_modules(modules: List[Tuple[str, str]]) -&gt; List[Tuple[str, str]]:
    seen: Set[str] = set()
    result = []
    for id, path in modules:
        if id not in seen:
            seen.add(id)
            result.append((id, path))
    return result


</t>
<t tx="ekr.20220525082936.784">def get_module_to_path_map(graph: Graph) -&gt; Dict[str, str]:
    return {module: node.xpath
            for module, node in graph.items()}


</t>
<t tx="ekr.20220525082936.785">def get_sources(fscache: FileSystemCache,
                modules: Dict[str, str],
                changed_modules: List[Tuple[str, str]]) -&gt; List[BuildSource]:
    sources = []
    for id, path in changed_modules:
        if fscache.isfile(path):
            sources.append(BuildSource(path, id, None))
    return sources


</t>
<t tx="ekr.20220525082936.786">def calculate_active_triggers(manager: BuildManager,
                              old_snapshots: Dict[str, Dict[str, SnapshotItem]],
                              new_modules: Dict[str, Optional[MypyFile]]) -&gt; Set[str]:
    """Determine activated triggers by comparing old and new symbol tables.

    For example, if only the signature of function m.f is different in the new
    symbol table, return {'&lt;m.f&gt;'}.
    """
    names: Set[str] = set()
    for id in new_modules:
        snapshot1 = old_snapshots.get(id)
        if snapshot1 is None:
            names.add(id)
            snapshot1 = {}
        new = new_modules[id]
        if new is None:
            snapshot2 = snapshot_symbol_table(id, SymbolTable())
            names.add(id)
        else:
            snapshot2 = snapshot_symbol_table(id, new.names)
        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)
        package_nesting_level = id.count('.')
        for item in diff.copy():
            if (item.count('.') &lt;= package_nesting_level + 1
                    and item.split('.')[-1] not in ('__builtins__',
                                                    '__file__',
                                                    '__name__',
                                                    '__package__',
                                                    '__doc__')):
                # Activate catch-all wildcard trigger for top-level module changes (used for
                # "from m import *"). This also gets triggered by changes to module-private
                # entries, but as these unneeded dependencies only result in extra processing,
                # it's a minor problem.
                #
                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of
                #     special casing them here.
                diff.add(id + WILDCARD_TAG)
            if item.count('.') &gt; package_nesting_level + 1:
                # These are for changes within classes, used by protocols.
                diff.add(item.rsplit('.', 1)[0] + WILDCARD_TAG)

        names |= diff
    return {make_trigger(name) for name in names}


</t>
<t tx="ekr.20220525082936.787">def replace_modules_with_new_variants(
        manager: BuildManager,
        graph: Dict[str, State],
        old_modules: Dict[str, Optional[MypyFile]],
        new_modules: Dict[str, Optional[MypyFile]]) -&gt; None:
    """Replace modules with newly builds versions.

    Retain the identities of externally visible AST nodes in the
    old ASTs so that references to the affected modules from other
    modules will still be valid (unless something was deleted or
    replaced with an incompatible definition, in which case there
    will be dangling references that will be handled by
    propagate_changes_using_dependencies).
    """
    for id in new_modules:
        preserved_module = old_modules.get(id)
        new_module = new_modules[id]
        if preserved_module and new_module is not None:
            merge_asts(preserved_module, preserved_module.names,
                       new_module, new_module.names)
            manager.modules[id] = preserved_module
            graph[id].tree = preserved_module


</t>
<t tx="ekr.20220525082936.788">def propagate_changes_using_dependencies(
        manager: BuildManager,
        graph: Dict[str, State],
        deps: Dict[str, Set[str]],
        triggered: Set[str],
        up_to_date_modules: Set[str],
        targets_with_errors: Set[str],
        processed_targets: List[str]) -&gt; List[Tuple[str, str]]:
    """Transitively rechecks targets based on triggers and the dependency map.

    Returns a list (module id, path) tuples representing modules that contain
    a target that needs to be reprocessed but that has not been parsed yet.

    Processed targets should be appended to processed_targets (used in tests only,
    to test the order of processing targets).
    """

    num_iter = 0
    remaining_modules: List[Tuple[str, str]] = []

    # Propagate changes until nothing visible has changed during the last
    # iteration.
    while triggered or targets_with_errors:
        num_iter += 1
        if num_iter &gt; MAX_ITER:
            raise RuntimeError('Max number of iterations (%d) reached (endless loop?)' % MAX_ITER)

        todo, unloaded, stale_protos = find_targets_recursive(manager, graph,
                                                              triggered, deps, up_to_date_modules)
        # TODO: we sort to make it deterministic, but this is *incredibly* ad hoc
        remaining_modules.extend((id, graph[id].xpath) for id in sorted(unloaded))
        # Also process targets that used to have errors, as otherwise some
        # errors might be lost.
        for target in targets_with_errors:
            id = module_prefix(graph, target)
            if id is not None and id not in up_to_date_modules:
                if id not in todo:
                    todo[id] = set()
                manager.log_fine_grained(f'process target with error: {target}')
                more_nodes, _ = lookup_target(manager, target)
                todo[id].update(more_nodes)
        triggered = set()
        # First invalidate subtype caches in all stale protocols.
        # We need to do this to avoid false negatives if the protocol itself is
        # unchanged, but was marked stale because its sub- (or super-) type changed.
        for info in stale_protos:
            TypeState.reset_subtype_caches_for(info)
        # Then fully reprocess all targets.
        # TODO: Preserve order (set is not optimal)
        for id, nodes in sorted(todo.items(), key=lambda x: x[0]):
            assert id not in up_to_date_modules
            triggered |= reprocess_nodes(manager, graph, id, nodes, deps, processed_targets)
        # Changes elsewhere may require us to reprocess modules that were
        # previously considered up to date. For example, there may be a
        # dependency loop that loops back to an originally processed module.
        up_to_date_modules = set()
        targets_with_errors = set()
        if is_verbose(manager):
            manager.log_fine_grained(f'triggered: {list(triggered)!r}')

    return remaining_modules


</t>
<t tx="ekr.20220525082936.789">def find_targets_recursive(
        manager: BuildManager,
        graph: Graph,
        triggers: Set[str],
        deps: Dict[str, Set[str]],
        up_to_date_modules: Set[str]) -&gt; Tuple[Dict[str, Set[FineGrainedDeferredNode]],
                                               Set[str], Set[TypeInfo]]:
    """Find names of all targets that need to reprocessed, given some triggers.

    Returns: A tuple containing a:
     * Dictionary from module id to a set of stale targets.
     * A set of module ids for unparsed modules with stale targets.
    """
    result: Dict[str, Set[FineGrainedDeferredNode]] = {}
    worklist = triggers
    processed: Set[str] = set()
    stale_protos: Set[TypeInfo] = set()
    unloaded_files: Set[str] = set()

    # Find AST nodes corresponding to each target.
    #
    # TODO: Don't rely on a set, since the items are in an unpredictable order.
    while worklist:
        processed |= worklist
        current = worklist
        worklist = set()
        for target in current:
            if target.startswith('&lt;'):
                module_id = module_prefix(graph, trigger_to_target(target))
                if module_id:
                    ensure_deps_loaded(module_id, deps, graph)

                worklist |= deps.get(target, set()) - processed
            else:
                module_id = module_prefix(graph, target)
                if module_id is None:
                    # Deleted module.
                    continue
                if module_id in up_to_date_modules:
                    # Already processed.
                    continue
                if (module_id not in manager.modules
                        or manager.modules[module_id].is_cache_skeleton):
                    # We haven't actually parsed and checked the module, so we don't have
                    # access to the actual nodes.
                    # Add it to the queue of files that need to be processed fully.
                    unloaded_files.add(module_id)
                    continue

                if module_id not in result:
                    result[module_id] = set()
                manager.log_fine_grained(f'process: {target}')
                deferred, stale_proto = lookup_target(manager, target)
                if stale_proto:
                    stale_protos.add(stale_proto)
                result[module_id].update(deferred)

    return result, unloaded_files, stale_protos


</t>
<t tx="ekr.20220525082936.79">def visit_literal_type(self, t: LiteralType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20220525082936.790">def reprocess_nodes(manager: BuildManager,
                    graph: Dict[str, State],
                    module_id: str,
                    nodeset: Set[FineGrainedDeferredNode],
                    deps: Dict[str, Set[str]],
                    processed_targets: List[str]) -&gt; Set[str]:
    """Reprocess a set of nodes within a single module.

    Return fired triggers.
    """
    if module_id not in graph:
        manager.log_fine_grained('%s not in graph (blocking errors or deleted?)' %
                    module_id)
        return set()

    file_node = manager.modules[module_id]
    old_symbols = find_symbol_tables_recursive(file_node.fullname, file_node.names)
    old_symbols = {name: names.copy() for name, names in old_symbols.items()}
    old_symbols_snapshot = snapshot_symbol_table(file_node.fullname, file_node.names)

    @others
    nodes = sorted(nodeset, key=key)

    options = graph[module_id].options
    manager.errors.set_file_ignored_lines(
        file_node.path, file_node.ignored_lines, options.ignore_errors)

    targets = set()
    for node in nodes:
        target = target_from_node(module_id, node.node)
        if target is not None:
            targets.add(target)
    manager.errors.clear_errors_in_targets(file_node.path, targets)

    # If one of the nodes is the module itself, emit any errors that
    # happened before semantic analysis.
    for target in targets:
        if target == module_id:
            for info in graph[module_id].early_errors:
                manager.errors.add_error_info(info)

    # Strip semantic analysis information.
    saved_attrs: SavedAttributes = {}
    for deferred in nodes:
        processed_targets.append(deferred.node.fullname)
        strip_target(deferred.node, saved_attrs)
    semantic_analysis_for_targets(graph[module_id], nodes, graph, saved_attrs)
    # Merge symbol tables to preserve identities of AST nodes. The file node will remain
    # the same, but other nodes may have been recreated with different identities, such as
    # NamedTuples defined using assignment statements.
    new_symbols = find_symbol_tables_recursive(file_node.fullname, file_node.names)
    for name in old_symbols:
        if name in new_symbols:
            merge_asts(file_node, old_symbols[name], file_node, new_symbols[name])

    # Type check.
    checker = graph[module_id].type_checker()
    checker.reset()
    # We seem to need additional passes in fine-grained incremental mode.
    checker.pass_num = 0
    checker.last_pass = 3
    more = checker.check_second_pass(nodes)
    while more:
        more = False
        if graph[module_id].type_checker().check_second_pass():
            more = True

    if manager.options.export_types:
        manager.all_types.update(graph[module_id].type_map())

    new_symbols_snapshot = snapshot_symbol_table(file_node.fullname, file_node.names)
    # Check if any attribute types were changed and need to be propagated further.
    changed = compare_symbol_table_snapshots(file_node.fullname,
                                             old_symbols_snapshot,
                                             new_symbols_snapshot)
    new_triggered = {make_trigger(name) for name in changed}

    # Dependencies may have changed.
    update_deps(module_id, nodes, graph, deps, options)

    # Report missing imports.
    graph[module_id].verify_dependencies()

    graph[module_id].free_state()

    return new_triggered


</t>
<t tx="ekr.20220525082936.791">def key(node: FineGrainedDeferredNode) -&gt; int:
    # Unlike modules which are sorted by name within SCC,
    # nodes within the same module are sorted by line number, because
    # this is how they are processed in normal mode.
    return node.node.line

</t>
<t tx="ekr.20220525082936.792">def find_symbol_tables_recursive(prefix: str, symbols: SymbolTable) -&gt; Dict[str, SymbolTable]:
    """Find all nested symbol tables.

    Args:
        prefix: Full name prefix (used for return value keys and to filter result so that
            cross references to other modules aren't included)
        symbols: Root symbol table

    Returns a dictionary from full name to corresponding symbol table.
    """
    result = {}
    result[prefix] = symbols
    for name, node in symbols.items():
        if isinstance(node.node, TypeInfo) and node.node.fullname.startswith(prefix + '.'):
            more = find_symbol_tables_recursive(prefix + '.' + name, node.node.names)
            result.update(more)
    return result


</t>
<t tx="ekr.20220525082936.793">def update_deps(module_id: str,
                nodes: List[FineGrainedDeferredNode],
                graph: Dict[str, State],
                deps: Dict[str, Set[str]],
                options: Options) -&gt; None:
    for deferred in nodes:
        node = deferred.node
        type_map = graph[module_id].type_map()
        tree = graph[module_id].tree
        assert tree is not None, "Tree must be processed at this stage"
        new_deps = get_dependencies_of_target(module_id, tree, node, type_map,
                                              options.python_version)
        for trigger, targets in new_deps.items():
            deps.setdefault(trigger, set()).update(targets)
    # Merge also the newly added protocol deps (if any).
    TypeState.update_protocol_deps(deps)


</t>
<t tx="ekr.20220525082936.794">def lookup_target(manager: BuildManager,
                  target: str) -&gt; Tuple[List[FineGrainedDeferredNode], Optional[TypeInfo]]:
    """Look up a target by fully-qualified name.

    The first item in the return tuple is a list of deferred nodes that
    needs to be reprocessed. If the target represents a TypeInfo corresponding
    to a protocol, return it as a second item in the return tuple, otherwise None.
    """
    @others
    modules = manager.modules
    items = split_target(modules, target)
    if items is None:
        not_found()  # Stale dependency
        return [], None
    module, rest = items
    if rest:
        components = rest.split('.')
    else:
        components = []
    node: Optional[SymbolNode] = modules[module]
    file: Optional[MypyFile] = None
    active_class = None
    for c in components:
        if isinstance(node, TypeInfo):
            active_class = node
        if isinstance(node, MypyFile):
            file = node
        if (not isinstance(node, (MypyFile, TypeInfo))
                or c not in node.names):
            not_found()  # Stale dependency
            return [], None
        # Don't reprocess plugin generated targets. They should get
        # stripped and regenerated when the containing target is
        # reprocessed.
        if node.names[c].plugin_generated:
            return [], None
        node = node.names[c].node
    if isinstance(node, TypeInfo):
        # A ClassDef target covers the body of the class and everything defined
        # within it.  To get the body we include the entire surrounding target,
        # typically a module top-level, since we don't support processing class
        # bodies as separate entities for simplicity.
        assert file is not None
        if node.fullname != target:
            # This is a reference to a different TypeInfo, likely due to a stale dependency.
            # Processing them would spell trouble -- for example, we could be refreshing
            # a deserialized TypeInfo with missing attributes.
            not_found()
            return [], None
        result = [FineGrainedDeferredNode(file, None)]
        stale_info: Optional[TypeInfo] = None
        if node.is_protocol:
            stale_info = node
        for name, symnode in node.names.items():
            node = symnode.node
            if isinstance(node, FuncDef):
                method, _ = lookup_target(manager, target + '.' + name)
                result.extend(method)
        return result, stale_info
    if isinstance(node, Decorator):
        # Decorator targets actually refer to the function definition only.
        node = node.func
    if not isinstance(node, (FuncDef,
                             MypyFile,
                             OverloadedFuncDef)):
        # The target can't be refreshed. It's possible that the target was
        # changed to another type and we have a stale dependency pointing to it.
        not_found()
        return [], None
    if node.fullname != target:
        # Stale reference points to something unexpected. We shouldn't process since the
        # context will be wrong and it could be a partially initialized deserialized node.
        not_found()
        return [], None
    return [FineGrainedDeferredNode(node, active_class)], None


</t>
<t tx="ekr.20220525082936.795">def not_found() -&gt; None:
    manager.log_fine_grained(
        f"Can't find matching target for {target} (stale dependency?)")

</t>
<t tx="ekr.20220525082936.796">def is_verbose(manager: BuildManager) -&gt; bool:
    return manager.options.verbosity &gt;= 1 or DEBUG_FINE_GRAINED


</t>
<t tx="ekr.20220525082936.797">def target_from_node(module: str,
                     node: Union[FuncDef, MypyFile, OverloadedFuncDef]
                     ) -&gt; Optional[str]:
    """Return the target name corresponding to a deferred node.

    Args:
        module: Must be module id of the module that defines 'node'

    Returns the target name, or None if the node is not a valid target in the given
    module (for example, if it's actually defined in another module).
    """
    if isinstance(node, MypyFile):
        if module != node.fullname:
            # Actually a reference to another module -- likely a stale dependency.
            return None
        return module
    else:  # OverloadedFuncDef or FuncDef
        if node.info:
            return f'{node.info.fullname}.{node.name}'
        else:
            return f'{module}.{node.name}'


</t>
<t tx="ekr.20220525082936.798">if sys.platform != "win32":
    INIT_SUFFIXES: Final = ("/__init__.py", "/__init__.pyi")
else:
    INIT_SUFFIXES: Final = (
        os.sep + '__init__.py',
        os.sep + '__init__.pyi',
        os.altsep + '__init__.py',
        os.altsep + '__init__.pyi',
    )


</t>
<t tx="ekr.20220525082936.799">def refresh_suppressed_submodules(
        module: str,
        path: Optional[str],
        deps: Dict[str, Set[str]],
        graph: Graph,
        fscache: FileSystemCache,
        refresh_file: Callable[[str, str], List[str]]) -&gt; Optional[List[str]]:
    """Look for submodules that are now suppressed in target package.

    If a submodule a.b gets added, we need to mark it as suppressed
    in modules that contain "from a import b". Previously we assumed
    that 'a.b' is not a module but a regular name.

    This is only relevant when following imports normally.

    Args:
        module: target package in which to look for submodules
        path: path of the module
        refresh_file: function that reads the AST of a module (returns error messages)

    Return a list of errors from refresh_file() if it was called. If the
    return value is None, we didn't call refresh_file().
    """
    messages = None
    if path is None or not path.endswith(INIT_SUFFIXES):
        # Only packages have submodules.
        return None
    # Find any submodules present in the directory.
    pkgdir = os.path.dirname(path)
    try:
        entries = fscache.listdir(pkgdir)
    except FileNotFoundError:
        entries = []
    for fnam in entries:
        if (not fnam.endswith(('.py', '.pyi'))
                or fnam.startswith("__init__.")
                or fnam.count('.') != 1):
            continue
        shortname = fnam.split('.')[0]
        submodule = module + '.' + shortname
        trigger = make_trigger(submodule)

        # We may be missing the required fine-grained deps.
        ensure_deps_loaded(module, deps, graph)

        if trigger in deps:
            for dep in deps[trigger]:
                # We can ignore &lt;...&gt; deps since a submodule can't trigger any.
                state = graph.get(dep)
                if not state:
                    # Maybe it's a non-top-level target. We only care about the module.
                    dep_module = module_prefix(graph, dep)
                    if dep_module is not None:
                        state = graph.get(dep_module)
                if state:
                    # Is the file may missing an AST in case it's read from cache?
                    if state.tree is None:
                        # Create AST for the file. This may produce some new errors
                        # that we need to propagate.
                        assert state.path is not None
                        messages = refresh_file(state.id, state.path)
                    tree = state.tree
                    assert tree  # Will be fine, due to refresh_file() above
                    for imp in tree.imports:
                        if isinstance(imp, ImportFrom):
                            if (imp.id == module
                                    and any(name == shortname for name, _ in imp.names)
                                    and submodule not in state.suppressed_set):
                                state.suppressed.append(submodule)
                                state.suppressed_set.add(submodule)
    return messages
</t>
<t tx="ekr.20220525082936.8">@abstractmethod
def visit_none_type(self, t: NoneType) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.80">def visit_star_type(self, t: StarType) -&gt; T:
    return t.type.accept(self)

</t>
<t tx="ekr.20220525082936.802"></t>
<t tx="ekr.20220525082936.803">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
import os.path

provided_prefix = os.getenv('MYPY_TEST_PREFIX', None)
if provided_prefix:
    PREFIX = provided_prefix
else:
    this_file_dir = os.path.dirname(os.path.realpath(__file__))
    PREFIX = os.path.dirname(os.path.dirname(this_file_dir))

# Location of test data files such as test case descriptions.
test_data_prefix = os.path.join(PREFIX, 'test-data', 'unit')
package_path = os.path.join(PREFIX, 'test-data', 'packages')

# Temp directory used for the temp files created when running test cases.
# This is *within* the tempfile.TemporaryDirectory that is chroot'ed per testcase.
# It is also hard-coded in numerous places, so don't change it.
test_temp_dir = 'tmp'

# The PEP 561 tests do a bunch of pip installs which, even though they operate
# on distinct temporary virtual environments, run into race conditions on shared
# file-system state. To make this work reliably in parallel mode, we'll use a
# FileLock courtesy of the tox-dev/py-filelock package.
# Ref. https://github.com/python/mypy/issues/12615
# Ref. mypy/test/testpep561.py
pip_lock = os.path.join(package_path, '.pip_lock')
pip_timeout = 60
</t>
<t tx="ekr.20220525082936.804">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Utilities for processing .test files containing test case descriptions."""

import os.path
import os
import tempfile
import posixpath
import re
import shutil
from abc import abstractmethod
import sys

import pytest
from typing import List, Tuple, Set, Optional, Iterator, Any, Dict, NamedTuple, Union, Pattern
from typing_extensions import Final

from mypy.test.config import test_data_prefix, test_temp_dir, PREFIX

root_dir = os.path.normpath(PREFIX)

# Debuggers that we support for debugging mypyc run tests
# implementation of using each of these debuggers is in test_run.py
# TODO: support more debuggers
SUPPORTED_DEBUGGERS: Final = ["gdb", "lldb"]


@others
</t>
<t tx="ekr.20220525082936.805"># File modify/create operation: copy module contents from source_path.
class UpdateFile(NamedTuple):
    module: str
    content: str
    target_path: str


</t>
<t tx="ekr.20220525082936.806"># File delete operation: delete module file.
class DeleteFile(NamedTuple):
    module: str
    path: str


</t>
<t tx="ekr.20220525082936.807">FileOperation = Union[UpdateFile, DeleteFile]


</t>
<t tx="ekr.20220525082936.808">def parse_test_case(case: 'DataDrivenTestCase') -&gt; None:
    """Parse and prepare a single case from suite with test case descriptions.

    This method is part of the setup phase, just before the test case is run.
    """
    test_items = parse_test_data(case.data, case.name)
    base_path = case.suite.base_path
    if case.suite.native_sep:
        join = os.path.join
    else:
        join = posixpath.join

    out_section_missing = case.suite.required_out_section
    normalize_output = True

    files: List[Tuple[str, str]] = []  # path and contents
    output_files: List[Tuple[str, Union[str, Pattern[str]]]] = []  # output path and contents
    output: List[str] = []  # Regular output errors
    output2: Dict[int, List[str]] = {}  # Output errors for incremental, runs 2+
    deleted_paths: Dict[int, Set[str]] = {}  # from run number of paths
    stale_modules: Dict[int, Set[str]] = {}  # from run number to module names
    rechecked_modules: Dict[int, Set[str]] = {}  # from run number module names
    triggered: List[str] = []  # Active triggers (one line per incremental step)
    targets: Dict[int, List[str]] = {}  # Fine-grained targets (per fine-grained update)

    # Process the parsed items. Each item has a header of form [id args],
    # optionally followed by lines of text.
    item = first_item = test_items[0]
    for item in test_items[1:]:
        if item.id in {'file', 'outfile', 'outfile-re'}:
            # Record an extra file needed for the test case.
            assert item.arg is not None
            contents = expand_variables('\n'.join(item.data))
            file_entry = (join(base_path, item.arg), contents)
            if item.id == 'file':
                files.append(file_entry)
            elif item.id == 'outfile-re':
                output_files.append((file_entry[0], re.compile(file_entry[1].rstrip(), re.S)))
            else:
                output_files.append(file_entry)
        elif item.id in ('builtins', 'builtins_py2'):
            # Use an alternative stub file for the builtins module.
            assert item.arg is not None
            mpath = join(os.path.dirname(case.file), item.arg)
            fnam = 'builtins.pyi' if item.id == 'builtins' else '__builtin__.pyi'
            with open(mpath, encoding='utf8') as f:
                files.append((join(base_path, fnam), f.read()))
        elif item.id == 'typing':
            # Use an alternative stub file for the typing module.
            assert item.arg is not None
            src_path = join(os.path.dirname(case.file), item.arg)
            with open(src_path, encoding='utf8') as f:
                files.append((join(base_path, 'typing.pyi'), f.read()))
        elif re.match(r'stale[0-9]*$', item.id):
            passnum = 1 if item.id == 'stale' else int(item.id[len('stale'):])
            assert passnum &gt; 0
            modules = (set() if item.arg is None else {t.strip() for t in item.arg.split(',')})
            stale_modules[passnum] = modules
        elif re.match(r'rechecked[0-9]*$', item.id):
            passnum = 1 if item.id == 'rechecked' else int(item.id[len('rechecked'):])
            assert passnum &gt; 0
            modules = (set() if item.arg is None else {t.strip() for t in item.arg.split(',')})
            rechecked_modules[passnum] = modules
        elif re.match(r'targets[0-9]*$', item.id):
            passnum = 1 if item.id == 'targets' else int(item.id[len('targets'):])
            assert passnum &gt; 0
            reprocessed = [] if item.arg is None else [t.strip() for t in item.arg.split(',')]
            targets[passnum] = reprocessed
        elif item.id == 'delete':
            # File/directory to delete during a multi-step test case
            assert item.arg is not None
            m = re.match(r'(.*)\.([0-9]+)$', item.arg)
            assert m, f'Invalid delete section: {item.arg}'
            num = int(m.group(2))
            assert num &gt;= 2, f"Can't delete during step {num}"
            full = join(base_path, m.group(1))
            deleted_paths.setdefault(num, set()).add(full)
        elif re.match(r'out[0-9]*$', item.id):
            if item.arg is None:
                args = []
            else:
                args = item.arg.split(",")

            version_check = True
            for arg in args:
                if arg == 'skip-path-normalization':
                    normalize_output = False
                if arg.startswith("version"):
                    compare_op = arg[7:9]
                    if compare_op not in {"&gt;=", "=="}:
                        raise ValueError(
                            "{}, line {}: Only &gt;= and == version checks are currently supported"
                            .format(
                                case.file, item.line
                            )
                        )
                    version_str = arg[9:]
                    try:
                        version = tuple(int(x) for x in version_str.split("."))
                    except ValueError:
                        raise ValueError(
                            '{}, line {}: "{}" is not a valid python version'.format(
                                case.file, item.line, version_str))
                    if compare_op == "&gt;=":
                        version_check = sys.version_info &gt;= version
                    elif compare_op == "==":
                        if not 1 &lt; len(version) &lt; 4:
                            raise ValueError(
                                '{}, line {}: Only minor or patch version checks '
                                'are currently supported with "==": "{}"'.format(
                                    case.file, item.line, version_str
                                )
                            )
                        version_check = sys.version_info[:len(version)] == version
            if version_check:
                tmp_output = [expand_variables(line) for line in item.data]
                if os.path.sep == '\\' and normalize_output:
                    tmp_output = [fix_win_path(line) for line in tmp_output]
                if item.id == 'out' or item.id == 'out1':
                    output = tmp_output
                else:
                    passnum = int(item.id[len('out'):])
                    assert passnum &gt; 1
                    output2[passnum] = tmp_output
                out_section_missing = False
        elif item.id == 'triggered' and item.arg is None:
            triggered = item.data
        else:
            raise ValueError(
                f'Invalid section header {item.id} in {case.file} at line {item.line}')

    if out_section_missing:
        raise ValueError(
            f'{case.file}, line {first_item.line}: Required output section not found')

    for passnum in stale_modules.keys():
        if passnum not in rechecked_modules:
            # If the set of rechecked modules isn't specified, make it the same as the set
            # of modules with a stale public interface.
            rechecked_modules[passnum] = stale_modules[passnum]
        if (passnum in stale_modules
                and passnum in rechecked_modules
                and not stale_modules[passnum].issubset(rechecked_modules[passnum])):
            raise ValueError(
                ('Stale modules after pass {} must be a subset of rechecked '
                 'modules ({}:{})').format(passnum, case.file, first_item.line))

    input = first_item.data
    expand_errors(input, output, 'main')
    for file_path, contents in files:
        expand_errors(contents.split('\n'), output, file_path)

    case.input = input
    case.output = output
    case.output2 = output2
    case.last_line = case.line + item.line + len(item.data) - 2
    case.files = files
    case.output_files = output_files
    case.expected_stale_modules = stale_modules
    case.expected_rechecked_modules = rechecked_modules
    case.deleted_paths = deleted_paths
    case.triggered = triggered or []
    case.normalize_output = normalize_output
    case.expected_fine_grained_targets = targets


</t>
<t tx="ekr.20220525082936.809">class DataDrivenTestCase(pytest.Item):
    """Holds parsed data-driven test cases, and handles directory setup and teardown."""

    # Override parent member type
    parent: "DataSuiteCollector"

    input: List[str]
    output: List[str]  # Output for the first pass
    output2: Dict[int, List[str]]  # Output for runs 2+, indexed by run number

    # full path of test suite
    file = ''
    line = 0

    # (file path, file content) tuples
    files: List[Tuple[str, str]]
    expected_stale_modules: Dict[int, Set[str]]
    expected_rechecked_modules: Dict[int, Set[str]]
    expected_fine_grained_targets: Dict[int, List[str]]

    # Whether or not we should normalize the output to standardize things like
    # forward vs backward slashes in file paths for Windows vs Linux.
    normalize_output = True

    # Extra attributes used by some tests.
    last_line: int
    output_files: List[Tuple[str, Union[str, Pattern[str]]]]  # Path and contents for output files
    deleted_paths: Dict[int, Set[str]]  # Mapping run number -&gt; paths
    triggered: List[str]  # Active triggers (one line per incremental step)

    @others
</t>
<t tx="ekr.20220525082936.81">def visit_union_type(self, t: UnionType) -&gt; T:
    return self.query_types(t.items)

</t>
<t tx="ekr.20220525082936.810">def __init__(self,
             parent: 'DataSuiteCollector',
             suite: 'DataSuite',
             file: str,
             name: str,
             writescache: bool,
             only_when: str,
             platform: Optional[str],
             skip: bool,
             xfail: bool,
             data: str,
             line: int) -&gt; None:
    super().__init__(name, parent)
    self.suite = suite
    self.file = file
    self.writescache = writescache
    self.only_when = only_when
    if ((platform == 'windows' and sys.platform != 'win32')
            or (platform == 'posix' and sys.platform == 'win32')):
        skip = True
    self.skip = skip
    self.xfail = xfail
    self.data = data
    self.line = line
    self.old_cwd: Optional[str] = None
    self.tmpdir: Optional[tempfile.TemporaryDirectory[str]] = None

</t>
<t tx="ekr.20220525082936.811">def runtest(self) -&gt; None:
    if self.skip:
        pytest.skip()
    # TODO: add a better error message for when someone uses skip and xfail at the same time
    elif self.xfail:
        self.add_marker(pytest.mark.xfail)
    parent = self.getparent(DataSuiteCollector)
    assert parent is not None, 'Should not happen'
    suite = parent.obj()
    suite.setup()
    try:
        suite.run_case(self)
    except Exception:
        # As a debugging aid, support copying the contents of the tmp directory somewhere
        save_dir: Optional[str] = self.config.getoption("--save-failures-to", None)
        if save_dir:
            assert self.tmpdir is not None
            target_dir = os.path.join(save_dir, os.path.basename(self.tmpdir.name))
            print(f"Copying data from test {self.name} to {target_dir}")
            if not os.path.isabs(target_dir):
                assert self.old_cwd
                target_dir = os.path.join(self.old_cwd, target_dir)
            shutil.copytree(self.tmpdir.name, target_dir)
        raise

</t>
<t tx="ekr.20220525082936.812">def setup(self) -&gt; None:
    parse_test_case(case=self)
    self.old_cwd = os.getcwd()
    self.tmpdir = tempfile.TemporaryDirectory(prefix='mypy-test-')
    os.chdir(self.tmpdir.name)
    os.mkdir(test_temp_dir)

    # Precalculate steps for find_steps()
    steps: Dict[int, List[FileOperation]] = {}

    for path, content in self.files:
        m = re.match(r'.*\.([0-9]+)$', path)
        if m:
            # Skip writing subsequent incremental steps - rather
            # store them as operations.
            num = int(m.group(1))
            assert num &gt;= 2
            target_path = re.sub(r'\.[0-9]+$', '', path)
            module = module_from_path(target_path)
            operation = UpdateFile(module, content, target_path)
            steps.setdefault(num, []).append(operation)
        else:
            # Write the first incremental steps
            dir = os.path.dirname(path)
            os.makedirs(dir, exist_ok=True)
            with open(path, 'w', encoding='utf8') as f:
                f.write(content)

    for num, paths in self.deleted_paths.items():
        assert num &gt;= 2
        for path in paths:
            module = module_from_path(path)
            steps.setdefault(num, []).append(DeleteFile(module, path))
    max_step = max(steps) if steps else 2
    self.steps = [steps.get(num, []) for num in range(2, max_step + 1)]

</t>
<t tx="ekr.20220525082936.813">def teardown(self) -&gt; None:
    assert self.old_cwd is not None and self.tmpdir is not None, \
        "test was not properly set up"
    os.chdir(self.old_cwd)
    try:
        self.tmpdir.cleanup()
    except OSError:
        pass
    self.old_cwd = None
    self.tmpdir = None

</t>
<t tx="ekr.20220525082936.814">def reportinfo(self) -&gt; Tuple[str, int, str]:
    return self.file, self.line, self.name

</t>
<t tx="ekr.20220525082936.815">def repr_failure(self, excinfo: Any, style: Optional[Any] = None) -&gt; str:
    if excinfo.errisinstance(SystemExit):
        # We assume that before doing exit() (which raises SystemExit) we've printed
        # enough context about what happened so that a stack trace is not useful.
        # In particular, uncaught exceptions during semantic analysis or type checking
        # call exit() and they already print out a stack trace.
        excrepr = excinfo.exconly()
    else:
        self.parent._prunetraceback(excinfo)
        excrepr = excinfo.getrepr(style='short')

    return f"data: {self.file}:{self.line}:\n{excrepr}"

</t>
<t tx="ekr.20220525082936.816">def find_steps(self) -&gt; List[List[FileOperation]]:
    """Return a list of descriptions of file operations for each incremental step.

    The first list item corresponds to the first incremental step, the second for the
    second step, etc. Each operation can either be a file modification/creation (UpdateFile)
    or deletion (DeleteFile).

    Defaults to having two steps if there aern't any operations.
    """
    return self.steps


</t>
<t tx="ekr.20220525082936.817">def module_from_path(path: str) -&gt; str:
    path = re.sub(r'\.pyi?$', '', path)
    # We can have a mix of Unix-style and Windows-style separators.
    parts = re.split(r'[/\\]', path)
    del parts[0]
    module = '.'.join(parts)
    module = re.sub(r'\.__init__$', '', module)
    return module


</t>
<t tx="ekr.20220525082936.818">class TestItem:
    """Parsed test caseitem.

    An item is of the form
      [id arg]
      .. data ..
    """

    id = ""
    arg: Optional[str] = ""

    # Text data, array of 8-bit strings
    data: List[str]

    file = ''
    line = 0  # Line number in file

    @others
</t>
<t tx="ekr.20220525082936.819">def __init__(self, id: str, arg: Optional[str], data: List[str],
             line: int) -&gt; None:
    self.id = id
    self.arg = arg
    self.data = data
    self.line = line


</t>
<t tx="ekr.20220525082936.82">def visit_overloaded(self, t: Overloaded) -&gt; T:
    return self.query_types(t.items)

</t>
<t tx="ekr.20220525082936.820">def parse_test_data(raw_data: str, name: str) -&gt; List[TestItem]:
    """Parse a list of lines that represent a sequence of test items."""

    lines = ['', '[case ' + name + ']'] + raw_data.split('\n')
    ret: List[TestItem] = []
    data: List[str] = []

    id: Optional[str] = None
    arg: Optional[str] = None

    i = 0
    i0 = 0
    while i &lt; len(lines):
        s = lines[i].strip()

        if lines[i].startswith('[') and s.endswith(']'):
            if id:
                data = collapse_line_continuation(data)
                data = strip_list(data)
                ret.append(TestItem(id, arg, strip_list(data), i0 + 1))

            i0 = i
            id = s[1:-1]
            arg = None
            if ' ' in id:
                arg = id[id.index(' ') + 1:]
                id = id[:id.index(' ')]
            data = []
        elif lines[i].startswith('\\['):
            data.append(lines[i][1:])
        elif not lines[i].startswith('--'):
            data.append(lines[i])
        elif lines[i].startswith('----'):
            data.append(lines[i][2:])
        i += 1

    # Process the last item.
    if id:
        data = collapse_line_continuation(data)
        data = strip_list(data)
        ret.append(TestItem(id, arg, data, i0 + 1))

    return ret


</t>
<t tx="ekr.20220525082936.821">def strip_list(l: List[str]) -&gt; List[str]:
    """Return a stripped copy of l.

    Strip whitespace at the end of all lines, and strip all empty
    lines from the end of the array.
    """

    r: List[str] = []
    for s in l:
        # Strip spaces at end of line
        r.append(re.sub(r'\s+$', '', s))

    while len(r) &gt; 0 and r[-1] == '':
        r.pop()

    return r


</t>
<t tx="ekr.20220525082936.822">def collapse_line_continuation(l: List[str]) -&gt; List[str]:
    r: List[str] = []
    cont = False
    for s in l:
        ss = re.sub(r'\\$', '', s)
        if cont:
            r[-1] += re.sub('^ +', '', ss)
        else:
            r.append(ss)
        cont = s.endswith('\\')
    return r


</t>
<t tx="ekr.20220525082936.823">def expand_variables(s: str) -&gt; str:
    return s.replace('&lt;ROOT&gt;', root_dir)


</t>
<t tx="ekr.20220525082936.824">def expand_errors(input: List[str], output: List[str], fnam: str) -&gt; None:
    """Transform comments such as '# E: message' or
    '# E:3: message' in input.

    The result is lines like 'fnam:line: error: message'.
    """

    for i in range(len(input)):
        # The first in the split things isn't a comment
        for possible_err_comment in input[i].split(' # ')[1:]:
            m = re.search(
                r'^([ENW]):((?P&lt;col&gt;\d+):)? (?P&lt;message&gt;.*)$',
                possible_err_comment.strip())
            if m:
                if m.group(1) == 'E':
                    severity = 'error'
                elif m.group(1) == 'N':
                    severity = 'note'
                elif m.group(1) == 'W':
                    severity = 'warning'
                col = m.group('col')
                message = m.group('message')
                message = message.replace('\\#', '#')  # adds back escaped # character
                if col is None:
                    output.append(
                        f'{fnam}:{i + 1}: {severity}: {message}')
                else:
                    output.append(f'{fnam}:{i + 1}:{col}: {severity}: {message}')


</t>
<t tx="ekr.20220525082936.825">def fix_win_path(line: str) -&gt; str:
    r"""Changes Windows paths to Linux paths in error messages.

    E.g. foo\bar.py -&gt; foo/bar.py.
    """
    line = line.replace(root_dir, root_dir.replace('\\', '/'))
    m = re.match(r'^([\S/]+):(\d+:)?(\s+.*)', line)
    if not m:
        return line
    else:
        filename, lineno, message = m.groups()
        return '{}:{}{}'.format(filename.replace('\\', '/'),
                                lineno or '', message)


</t>
<t tx="ekr.20220525082936.826">def fix_cobertura_filename(line: str) -&gt; str:
    r"""Changes filename paths to Linux paths in Cobertura output files.

    E.g. filename="pkg\subpkg\a.py" -&gt; filename="pkg/subpkg/a.py".
    """
    m = re.search(r'&lt;class .* filename="(?P&lt;filename&gt;.*?)"', line)
    if not m:
        return line
    return '{}{}{}'.format(line[:m.start(1)],
                           m.group('filename').replace('\\', '/'),
                           line[m.end(1):])


</t>
<t tx="ekr.20220525082936.827">##
#
# pytest setup
#
##


</t>
<t tx="ekr.20220525082936.828"># This function name is special to pytest.  See
# https://docs.pytest.org/en/latest/reference.html#initialization-hooks
def pytest_addoption(parser: Any) -&gt; None:
    group = parser.getgroup('mypy')
    group.addoption('--update-data', action='store_true', default=False,
                    help='Update test data to reflect actual output'
                         ' (supported only for certain tests)')
    group.addoption('--save-failures-to', default=None,
                    help='Copy the temp directories from failing tests to a target directory')
    group.addoption('--mypy-verbose', action='count',
                    help='Set the verbose flag when creating mypy Options')
    group.addoption('--mypyc-showc', action='store_true', default=False,
                    help='Display C code on mypyc test failures')
    group.addoption(
        "--mypyc-debug",
        default=None,
        dest="debugger",
        choices=SUPPORTED_DEBUGGERS,
        help="Run the first mypyc run test with the specified debugger",
    )


</t>
<t tx="ekr.20220525082936.829"># This function name is special to pytest.  See
# http://doc.pytest.org/en/latest/writing_plugins.html#collection-hooks
def pytest_pycollect_makeitem(collector: Any, name: str,
                              obj: object) -&gt; 'Optional[Any]':
    """Called by pytest on each object in modules configured in conftest.py files.

    collector is pytest.Collector, returns Optional[pytest.Class]
    """
    if isinstance(obj, type):
        # Only classes derived from DataSuite contain test cases, not the DataSuite class itself
        if issubclass(obj, DataSuite) and obj is not DataSuite:
            # Non-None result means this obj is a test case.
            # The collect method of the returned DataSuiteCollector instance will be called later,
            # with self.obj being obj.
            return DataSuiteCollector.from_parent(  # type: ignore[no-untyped-call]
                parent=collector, name=name,
            )
    return None


</t>
<t tx="ekr.20220525082936.83">def visit_type_type(self, t: TypeType) -&gt; T:
    return t.item.accept(self)

</t>
<t tx="ekr.20220525082936.830">def split_test_cases(parent: 'DataFileCollector', suite: 'DataSuite',
                     file: str) -&gt; Iterator['DataDrivenTestCase']:
    """Iterate over raw test cases in file, at collection time, ignoring sub items.

    The collection phase is slow, so any heavy processing should be deferred to after
    uninteresting tests are filtered (when using -k PATTERN switch).
    """
    with open(file, encoding='utf-8') as f:
        data = f.read()
    # number of groups in the below regex
    NUM_GROUPS = 7
    cases = re.split(r'^\[case ([a-zA-Z_0-9]+)'
                     r'(-writescache)?'
                     r'(-only_when_cache|-only_when_nocache)?'
                     r'(-posix|-windows)?'
                     r'(-skip)?'
                     r'(-xfail)?'
                     r'\][ \t]*$\n',
                     data,
                     flags=re.DOTALL | re.MULTILINE)
    line_no = cases[0].count('\n') + 1
    test_names = set()
    for i in range(1, len(cases), NUM_GROUPS):
        name, writescache, only_when, platform_flag, skip, xfail, data = cases[i:i + NUM_GROUPS]
        if name in test_names:
            raise RuntimeError('Found a duplicate test name "{}" in {} on line {}'.format(
                name, parent.name, line_no,
            ))
        platform = platform_flag[1:] if platform_flag else None
        yield DataDrivenTestCase.from_parent(
            parent=parent,
            suite=suite,
            file=file,
            name=add_test_name_suffix(name, suite.test_name_suffix),
            writescache=bool(writescache),
            only_when=only_when,
            platform=platform,
            skip=bool(skip),
            xfail=bool(xfail),
            data=data,
            line=line_no,
        )
        line_no += data.count('\n') + 1

        # Record existing tests to prevent duplicates:
        test_names.update({name})


</t>
<t tx="ekr.20220525082936.831">class DataSuiteCollector(pytest.Class):
    @others
</t>
<t tx="ekr.20220525082936.832">def collect(self) -&gt; Iterator['DataFileCollector']:
    """Called by pytest on each of the object returned from pytest_pycollect_makeitem"""

    # obj is the object for which pytest_pycollect_makeitem returned self.
    suite: DataSuite = self.obj

    assert os.path.isdir(suite.data_prefix), \
        f'Test data prefix ({suite.data_prefix}) not set correctly'

    for data_file in suite.files:
        yield DataFileCollector.from_parent(parent=self, name=data_file)


</t>
<t tx="ekr.20220525082936.833">class DataFileCollector(pytest.Collector):
    """Represents a single `.test` data driven test file.

    More context: https://github.com/python/mypy/issues/11662
    """
    parent: DataSuiteCollector

    @others
</t>
<t tx="ekr.20220525082936.834">@classmethod  # We have to fight with pytest here:
def from_parent(  # type: ignore[override]
    cls,
    parent: DataSuiteCollector,
    *,
    name: str,
) -&gt; 'DataFileCollector':
    return super().from_parent(parent, name=name)

</t>
<t tx="ekr.20220525082936.835">def collect(self) -&gt; Iterator['DataDrivenTestCase']:
    yield from split_test_cases(
        parent=self,
        suite=self.parent.obj,
        file=os.path.join(self.parent.obj.data_prefix, self.name),
    )


</t>
<t tx="ekr.20220525082936.836">def add_test_name_suffix(name: str, suffix: str) -&gt; str:
    # Find magic suffix of form "-foobar" (used for things like "-skip").
    m = re.search(r'-[-A-Za-z0-9]+$', name)
    if m:
        # Insert suite-specific test name suffix before the magic suffix
        # which must be the last thing in the test case name since we
        # are using endswith() checks.
        magic_suffix = m.group(0)
        return name[:-len(magic_suffix)] + suffix + magic_suffix
    else:
        return name + suffix


</t>
<t tx="ekr.20220525082936.837">def is_incremental(testcase: DataDrivenTestCase) -&gt; bool:
    return 'incremental' in testcase.name.lower() or 'incremental' in testcase.file


</t>
<t tx="ekr.20220525082936.838">def has_stable_flags(testcase: DataDrivenTestCase) -&gt; bool:
    if any(re.match(r'# flags[2-9]:', line) for line in testcase.input):
        return False
    for filename, contents in testcase.files:
        if os.path.basename(filename).startswith('mypy.ini.'):
            return False
    return True


</t>
<t tx="ekr.20220525082936.839">class DataSuite:
    # option fields - class variables
    files: List[str]

    base_path = test_temp_dir

    # Allow external users of the test code to override the data prefix
    data_prefix = test_data_prefix

    required_out_section = False

    native_sep = False

    # Name suffix automatically added to each test case in the suite (can be
    # used to distinguish test cases in suites that share data files)
    test_name_suffix = ''

    @others
</t>
<t tx="ekr.20220525082936.84">def visit_ellipsis_type(self, t: EllipsisType) -&gt; T:
    return self.strategy([])

</t>
<t tx="ekr.20220525082936.840">def setup(self) -&gt; None:
    """Setup fixtures (ad-hoc)"""
    pass

</t>
<t tx="ekr.20220525082936.841">@abstractmethod
def run_case(self, testcase: DataDrivenTestCase) -&gt; None:
    raise NotImplementedError
</t>
<t tx="ekr.20220525082936.842">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
import os
import re
import sys
import time
import shutil
import contextlib

from typing import List, Iterable, Dict, Tuple, Callable, Any, Iterator, Union, Pattern

from mypy import defaults
import mypy.api as api

import pytest

# Exporting Suite as alias to TestCase for backwards compatibility
# TODO: avoid aliasing - import and subclass TestCase directly
from unittest import TestCase as Suite  # noqa: F401 (re-exporting)

from mypy.main import process_options
from mypy.options import Options
from mypy.test.data import (
    DataDrivenTestCase, fix_cobertura_filename, UpdateFile, DeleteFile
)
from mypy.test.config import test_temp_dir
import mypy.version

skip = pytest.mark.skip

# AssertStringArraysEqual displays special line alignment helper messages if
# the first different line has at least this many characters,
MIN_LINE_LENGTH_FOR_ALIGNMENT = 5


@others
</t>
<t tx="ekr.20220525082936.843">def run_mypy(args: List[str]) -&gt; None:
    __tracebackhide__ = True
    # We must enable site packages even though they could cause problems,
    # since stubs for typing_extensions live there.
    outval, errval, status = api.run(args + ['--show-traceback',
                                             '--no-silence-site-packages'])
    if status != 0:
        sys.stdout.write(outval)
        sys.stderr.write(errval)
        pytest.fail(msg="Sample check failed", pytrace=False)


</t>
<t tx="ekr.20220525082936.844">def assert_string_arrays_equal(expected: List[str], actual: List[str],
                               msg: str) -&gt; None:
    """Assert that two string arrays are equal.

    We consider "can't" and "cannot" equivalent, by replacing the
    former with the latter before comparing.

    Display any differences in a human-readable form.
    """
    __tracebackhide__ = True

    actual = clean_up(actual)
    actual = [line.replace("can't", "cannot") for line in actual]
    expected = [line.replace("can't", "cannot") for line in expected]

    if actual != expected:
        num_skip_start = num_skipped_prefix_lines(expected, actual)
        num_skip_end = num_skipped_suffix_lines(expected, actual)

        sys.stderr.write('Expected:\n')

        # If omit some lines at the beginning, indicate it by displaying a line
        # with '...'.
        if num_skip_start &gt; 0:
            sys.stderr.write('  ...\n')

        # Keep track of the first different line.
        first_diff = -1

        # Display only this many first characters of identical lines.
        width = 75

        for i in range(num_skip_start, len(expected) - num_skip_end):
            if i &gt;= len(actual) or expected[i] != actual[i]:
                if first_diff &lt; 0:
                    first_diff = i
                sys.stderr.write(f'  {expected[i]:&lt;45} (diff)')
            else:
                e = expected[i]
                sys.stderr.write('  ' + e[:width])
                if len(e) &gt; width:
                    sys.stderr.write('...')
            sys.stderr.write('\n')
        if num_skip_end &gt; 0:
            sys.stderr.write('  ...\n')

        sys.stderr.write('Actual:\n')

        if num_skip_start &gt; 0:
            sys.stderr.write('  ...\n')

        for j in range(num_skip_start, len(actual) - num_skip_end):
            if j &gt;= len(expected) or expected[j] != actual[j]:
                sys.stderr.write(f'  {actual[j]:&lt;45} (diff)')
            else:
                a = actual[j]
                sys.stderr.write('  ' + a[:width])
                if len(a) &gt; width:
                    sys.stderr.write('...')
            sys.stderr.write('\n')
        if not actual:
            sys.stderr.write('  (empty)\n')
        if num_skip_end &gt; 0:
            sys.stderr.write('  ...\n')

        sys.stderr.write('\n')

        if 0 &lt;= first_diff &lt; len(actual) and (
                len(expected[first_diff]) &gt;= MIN_LINE_LENGTH_FOR_ALIGNMENT
                or len(actual[first_diff]) &gt;= MIN_LINE_LENGTH_FOR_ALIGNMENT):
            # Display message that helps visualize the differences between two
            # long lines.
            show_align_message(expected[first_diff], actual[first_diff])

        raise AssertionError(msg)


</t>
<t tx="ekr.20220525082936.845">def assert_module_equivalence(name: str,
                              expected: Iterable[str], actual: Iterable[str]) -&gt; None:
    expected_normalized = sorted(expected)
    actual_normalized = sorted(set(actual).difference({"__main__"}))
    assert_string_arrays_equal(
        expected_normalized,
        actual_normalized,
        ('Actual modules ({}) do not match expected modules ({}) '
         'for "[{} ...]"').format(
             ', '.join(actual_normalized),
             ', '.join(expected_normalized),
             name))


</t>
<t tx="ekr.20220525082936.846">def assert_target_equivalence(name: str,
                              expected: List[str], actual: List[str]) -&gt; None:
    """Compare actual and expected targets (order sensitive)."""
    assert_string_arrays_equal(
        expected,
        actual,
        ('Actual targets ({}) do not match expected targets ({}) '
         'for "[{} ...]"').format(
             ', '.join(actual),
             ', '.join(expected),
             name))


</t>
<t tx="ekr.20220525082936.847">def update_testcase_output(testcase: DataDrivenTestCase, output: List[str]) -&gt; None:
    assert testcase.old_cwd is not None, "test was not properly set up"
    testcase_path = os.path.join(testcase.old_cwd, testcase.file)
    with open(testcase_path, encoding='utf8') as f:
        data_lines = f.read().splitlines()
    test = '\n'.join(data_lines[testcase.line:testcase.last_line])

    mapping: Dict[str, List[str]] = {}
    for old, new in zip(testcase.output, output):
        PREFIX = 'error:'
        ind = old.find(PREFIX)
        if ind != -1 and old[:ind] == new[:ind]:
            old, new = old[ind + len(PREFIX):], new[ind + len(PREFIX):]
        mapping.setdefault(old, []).append(new)

    for old in mapping:
        if test.count(old) == len(mapping[old]):
            betweens = test.split(old)

            # Interleave betweens and mapping[old]
            from itertools import chain
            interleaved = [betweens[0]] + \
                list(chain.from_iterable(zip(mapping[old], betweens[1:])))
            test = ''.join(interleaved)

    data_lines[testcase.line:testcase.last_line] = [test]
    data = '\n'.join(data_lines)
    with open(testcase_path, 'w', encoding='utf8') as f:
        print(data, file=f)


</t>
<t tx="ekr.20220525082936.848">def show_align_message(s1: str, s2: str) -&gt; None:
    """Align s1 and s2 so that the their first difference is highlighted.

    For example, if s1 is 'foobar' and s2 is 'fobar', display the
    following lines:

      E: foobar
      A: fobar
           ^

    If s1 and s2 are long, only display a fragment of the strings around the
    first difference. If s1 is very short, do nothing.
    """

    # Seeing what went wrong is trivial even without alignment if the expected
    # string is very short. In this case do nothing to simplify output.
    if len(s1) &lt; 4:
        return

    maxw = 72  # Maximum number of characters shown

    sys.stderr.write('Alignment of first line difference:\n')

    trunc = False
    while s1[:30] == s2[:30]:
        s1 = s1[10:]
        s2 = s2[10:]
        trunc = True

    if trunc:
        s1 = '...' + s1
        s2 = '...' + s2

    max_len = max(len(s1), len(s2))
    extra = ''
    if max_len &gt; maxw:
        extra = '...'

    # Write a chunk of both lines, aligned.
    sys.stderr.write(f'  E: {s1[:maxw]}{extra}\n')
    sys.stderr.write(f'  A: {s2[:maxw]}{extra}\n')
    # Write an indicator character under the different columns.
    sys.stderr.write('     ')
    for j in range(min(maxw, max(len(s1), len(s2)))):
        if s1[j:j + 1] != s2[j:j + 1]:
            sys.stderr.write('^')  # Difference
            break
        else:
            sys.stderr.write(' ')  # Equal
    sys.stderr.write('\n')


</t>
<t tx="ekr.20220525082936.849">def clean_up(a: List[str]) -&gt; List[str]:
    """Remove common directory prefix from all strings in a.

    This uses a naive string replace; it seems to work well enough. Also
    remove trailing carriage returns.
    """
    res = []
    pwd = os.getcwd()
    driver = pwd + '/driver.py'
    for s in a:
        prefix = os.sep
        ss = s
        for p in prefix, prefix.replace(os.sep, '/'):
            if p != '/' and p != '//' and p != '\\' and p != '\\\\':
                ss = ss.replace(p, '')
        # Ignore spaces at end of line.
        ss = re.sub(' +$', '', ss)
        # Remove pwd from driver.py's path
        ss = ss.replace(driver, 'driver.py')
        res.append(re.sub('\\r$', '', ss))
    return res


</t>
<t tx="ekr.20220525082936.85">def visit_placeholder_type(self, t: PlaceholderType) -&gt; T:
    return self.query_types(t.args)

</t>
<t tx="ekr.20220525082936.850">@contextlib.contextmanager
def local_sys_path_set() -&gt; Iterator[None]:
    """Temporary insert current directory into sys.path.

    This can be used by test cases that do runtime imports, for example
    by the stubgen tests.
    """
    old_sys_path = sys.path[:]
    if not ('' in sys.path or '.' in sys.path):
        sys.path.insert(0, '')
    try:
        yield
    finally:
        sys.path = old_sys_path


</t>
<t tx="ekr.20220525082936.851">def num_skipped_prefix_lines(a1: List[str], a2: List[str]) -&gt; int:
    num_eq = 0
    while num_eq &lt; min(len(a1), len(a2)) and a1[num_eq] == a2[num_eq]:
        num_eq += 1
    return max(0, num_eq - 4)


</t>
<t tx="ekr.20220525082936.852">def num_skipped_suffix_lines(a1: List[str], a2: List[str]) -&gt; int:
    num_eq = 0
    while (num_eq &lt; min(len(a1), len(a2))
           and a1[-num_eq - 1] == a2[-num_eq - 1]):
        num_eq += 1
    return max(0, num_eq - 4)


</t>
<t tx="ekr.20220525082936.853">def testfile_pyversion(path: str) -&gt; Tuple[int, int]:
    if path.endswith('python2.test'):
        return defaults.PYTHON2_VERSION
    elif path.endswith('python310.test'):
        return 3, 10
    else:
        return defaults.PYTHON3_VERSION


</t>
<t tx="ekr.20220525082936.854">def testcase_pyversion(path: str, testcase_name: str) -&gt; Tuple[int, int]:
    if testcase_name.endswith('python2'):
        return defaults.PYTHON2_VERSION
    else:
        return testfile_pyversion(path)


</t>
<t tx="ekr.20220525082936.855">def normalize_error_messages(messages: List[str]) -&gt; List[str]:
    """Translate an array of error messages to use / as path separator."""

    a = []
    for m in messages:
        a.append(m.replace(os.sep, '/'))
    return a


</t>
<t tx="ekr.20220525082936.856">def retry_on_error(func: Callable[[], Any], max_wait: float = 1.0) -&gt; None:
    """Retry callback with exponential backoff when it raises OSError.

    If the function still generates an error after max_wait seconds, propagate
    the exception.

    This can be effective against random file system operation failures on
    Windows.
    """
    t0 = time.time()
    wait_time = 0.01
    while True:
        try:
            func()
            return
        except OSError:
            wait_time = min(wait_time * 2, t0 + max_wait - time.time())
            if wait_time &lt;= 0.01:
                # Done enough waiting, the error seems persistent.
                raise
            time.sleep(wait_time)


</t>
<t tx="ekr.20220525082936.857">def good_repr(obj: object) -&gt; str:
    if isinstance(obj, str):
        if obj.count('\n') &gt; 1:
            bits = ["'''\\"]
            for line in obj.split('\n'):
                # force repr to use ' not ", then cut it off
                bits.append(repr('"' + line)[2:-1])
            bits[-1] += "'''"
            return '\n'.join(bits)
    return repr(obj)


</t>
<t tx="ekr.20220525082936.858">def assert_equal(a: object, b: object, fmt: str = '{} != {}') -&gt; None:
    __tracebackhide__ = True
    if a != b:
        raise AssertionError(fmt.format(good_repr(a), good_repr(b)))


</t>
<t tx="ekr.20220525082936.859">def typename(t: type) -&gt; str:
    if '.' in str(t):
        return str(t).split('.')[-1].rstrip("'&gt;")
    else:
        return str(t)[8:-2]


</t>
<t tx="ekr.20220525082936.86">def visit_type_alias_type(self, t: TypeAliasType) -&gt; T:
    return get_proper_type(t).accept(self)

</t>
<t tx="ekr.20220525082936.860">def assert_type(typ: type, value: object) -&gt; None:
    __tracebackhide__ = True
    if type(value) != typ:
        raise AssertionError('Invalid type {}, expected {}'.format(
            typename(type(value)), typename(typ)))


</t>
<t tx="ekr.20220525082936.861">def parse_options(program_text: str, testcase: DataDrivenTestCase,
                  incremental_step: int) -&gt; Options:
    """Parse comments like '# flags: --foo' in a test case."""
    options = Options()
    flags = re.search('# flags: (.*)$', program_text, flags=re.MULTILINE)
    if incremental_step &gt; 1:
        flags2 = re.search(f'# flags{incremental_step}: (.*)$', program_text,
                           flags=re.MULTILINE)
        if flags2:
            flags = flags2

    if flags:
        flag_list = flags.group(1).split()
        flag_list.append('--no-site-packages')  # the tests shouldn't need an installed Python
        targets, options = process_options(flag_list, require_targets=False)
        if targets:
            # TODO: support specifying targets via the flags pragma
            raise RuntimeError('Specifying targets via the flags pragma is not supported.')
    else:
        flag_list = []
        options = Options()
        # TODO: Enable strict optional in test cases by default (requires *many* test case changes)
        options.strict_optional = False
        options.error_summary = False

    # Allow custom python version to override testcase_pyversion.
    if all(flag.split('=')[0] not in ['--python-version', '-2', '--py2'] for flag in flag_list):
        options.python_version = testcase_pyversion(testcase.file, testcase.name)

    if testcase.config.getoption('--mypy-verbose'):
        options.verbosity = testcase.config.getoption('--mypy-verbose')

    return options


</t>
<t tx="ekr.20220525082936.862">def split_lines(*streams: bytes) -&gt; List[str]:
    """Returns a single list of string lines from the byte streams in args."""
    return [
        s
        for stream in streams
        for s in stream.decode('utf8').splitlines()
    ]


</t>
<t tx="ekr.20220525082936.863">def write_and_fudge_mtime(content: str, target_path: str) -&gt; None:
    # In some systems, mtime has a resolution of 1 second which can
    # cause annoying-to-debug issues when a file has the same size
    # after a change. We manually set the mtime to circumvent this.
    # Note that we increment the old file's mtime, which guarantees a
    # different value, rather than incrementing the mtime after the
    # copy, which could leave the mtime unchanged if the old file had
    # a similarly fudged mtime.
    new_time = None
    if os.path.isfile(target_path):
        new_time = os.stat(target_path).st_mtime + 1

    dir = os.path.dirname(target_path)
    os.makedirs(dir, exist_ok=True)
    with open(target_path, "w", encoding="utf-8") as target:
        target.write(content)

    if new_time:
        os.utime(target_path, times=(new_time, new_time))


</t>
<t tx="ekr.20220525082936.864">def perform_file_operations(
        operations: List[Union[UpdateFile, DeleteFile]]) -&gt; None:
    for op in operations:
        if isinstance(op, UpdateFile):
            # Modify/create file
            write_and_fudge_mtime(op.content, op.target_path)
        else:
            # Delete file/directory
            if os.path.isdir(op.path):
                # Sanity check to avoid unexpected deletions
                assert op.path.startswith('tmp')
                shutil.rmtree(op.path)
            else:
                # Use retries to work around potential flakiness on Windows (AppVeyor).
                path = op.path
                retry_on_error(lambda: os.remove(path))


</t>
<t tx="ekr.20220525082936.865">def check_test_output_files(testcase: DataDrivenTestCase,
                            step: int,
                            strip_prefix: str = '') -&gt; None:
    for path, expected_content in testcase.output_files:
        if path.startswith(strip_prefix):
            path = path[len(strip_prefix):]
        if not os.path.exists(path):
            raise AssertionError(
                'Expected file {} was not produced by test case{}'.format(
                    path, ' on step %d' % step if testcase.output2 else ''))
        with open(path, encoding='utf8') as output_file:
            actual_output_content = output_file.read()

        if isinstance(expected_content, Pattern):
            if expected_content.fullmatch(actual_output_content) is not None:
                continue
            raise AssertionError(
                'Output file {} did not match its expected output pattern\n---\n{}\n---'.format(
                    path, actual_output_content)
            )

        normalized_output = normalize_file_output(actual_output_content.splitlines(),
                                                  os.path.abspath(test_temp_dir))
        # We always normalize things like timestamp, but only handle operating-system
        # specific things if requested.
        if testcase.normalize_output:
            if testcase.suite.native_sep and os.path.sep == '\\':
                normalized_output = [fix_cobertura_filename(line)
                                     for line in normalized_output]
            normalized_output = normalize_error_messages(normalized_output)
        assert_string_arrays_equal(expected_content.splitlines(), normalized_output,
                                   'Output file {} did not match its expected output{}'.format(
                                       path, ' on step %d' % step if testcase.output2 else ''))


</t>
<t tx="ekr.20220525082936.866">def normalize_file_output(content: List[str], current_abs_path: str) -&gt; List[str]:
    """Normalize file output for comparison."""
    timestamp_regex = re.compile(r'\d{10}')
    result = [x.replace(current_abs_path, '$PWD') for x in content]
    version = mypy.version.__version__
    result = [re.sub(r'\b' + re.escape(version) + r'\b', '$VERSION', x) for x in result]
    # We generate a new mypy.version when building mypy wheels that
    # lacks base_version, so handle that case.
    base_version = getattr(mypy.version, 'base_version', version)
    result = [re.sub(r'\b' + re.escape(base_version) + r'\b', '$VERSION', x) for x in result]
    result = [timestamp_regex.sub('$TIMESTAMP', x) for x in result]
    return result
</t>
<t tx="ekr.20220525082936.867">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
from io import StringIO
import sys

import mypy.api

from mypy.test.helpers import Suite


@others
</t>
<t tx="ekr.20220525082936.868">class APISuite(Suite):

    @others
</t>
<t tx="ekr.20220525082936.869">def setUp(self) -&gt; None:
    self.sys_stdout = sys.stdout
    self.sys_stderr = sys.stderr
    sys.stdout = self.stdout = StringIO()
    sys.stderr = self.stderr = StringIO()

</t>
<t tx="ekr.20220525082936.87">def query_types(self, types: Iterable[Type]) -&gt; T:
    """Perform a query for a list of types.

    Use the strategy to combine the results.
    Skip type aliases already visited types to avoid infinite recursion.
    """
    res: List[T] = []
    for t in types:
        if isinstance(t, TypeAliasType):
            # Avoid infinite recursion for recursive type aliases.
            # TODO: Ideally we should fire subvisitors here (or use caching) if we care
            #       about duplicates.
            if t in self.seen_aliases:
                continue
            self.seen_aliases.add(t)
        res.append(t.accept(self))
    return self.strategy(res)
</t>
<t tx="ekr.20220525082936.870">def tearDown(self) -&gt; None:
    sys.stdout = self.sys_stdout
    sys.stderr = self.sys_stderr
    assert self.stdout.getvalue() == ''
    assert self.stderr.getvalue() == ''

</t>
<t tx="ekr.20220525082936.871">def test_capture_bad_opt(self) -&gt; None:
    """stderr should be captured when a bad option is passed."""
    _, stderr, _ = mypy.api.run(['--some-bad-option'])
    assert isinstance(stderr, str)
    assert stderr != ''

</t>
<t tx="ekr.20220525082936.872">def test_capture_empty(self) -&gt; None:
    """stderr should be captured when a bad option is passed."""
    _, stderr, _ = mypy.api.run([])
    assert isinstance(stderr, str)
    assert stderr != ''

</t>
<t tx="ekr.20220525082936.873">def test_capture_help(self) -&gt; None:
    """stdout should be captured when --help is passed."""
    stdout, _, _ = mypy.api.run(['--help'])
    assert isinstance(stdout, str)
    assert stdout != ''

</t>
<t tx="ekr.20220525082936.874">def test_capture_version(self) -&gt; None:
    """stdout should be captured when --version is passed."""
    stdout, _, _ = mypy.api.run(['--version'])
    assert isinstance(stdout, str)
    assert stdout != ''
</t>
<t tx="ekr.20220525082936.875">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Ensure the argparse parser and Options class are in sync.

In particular, verify that the argparse defaults are the same as the Options
defaults, and that argparse doesn't assign any new members to the Options
object it creates.
"""
import argparse
import sys

from mypy.test.helpers import Suite, assert_equal
from mypy.options import Options
from mypy.main import process_options, infer_python_executable


@others
</t>
<t tx="ekr.20220525082936.876">class ArgSuite(Suite):
    @others
</t>
<t tx="ekr.20220525082936.877">def test_coherence(self) -&gt; None:
    options = Options()
    _, parsed_options = process_options([], require_targets=False)
    # FIX: test this too. Requires changing working dir to avoid finding 'setup.cfg'
    options.config_file = parsed_options.config_file
    assert_equal(options.snapshot(), parsed_options.snapshot())

</t>
<t tx="ekr.20220525082936.878">def test_executable_inference(self) -&gt; None:
    """Test the --python-executable flag with --python-version"""
    sys_ver_str = '{ver.major}.{ver.minor}'.format(ver=sys.version_info)

    base = ['file.py']  # dummy file

    # test inference given one (infer the other)
    matching_version = base + [f'--python-version={sys_ver_str}']
    _, options = process_options(matching_version)
    assert options.python_version == sys.version_info[:2]
    assert options.python_executable == sys.executable

    matching_version = base + [f'--python-executable={sys.executable}']
    _, options = process_options(matching_version)
    assert options.python_version == sys.version_info[:2]
    assert options.python_executable == sys.executable

    # test inference given both
    matching_version = base + [f'--python-version={sys_ver_str}',
                               f'--python-executable={sys.executable}']
    _, options = process_options(matching_version)
    assert options.python_version == sys.version_info[:2]
    assert options.python_executable == sys.executable

    # test that --no-site-packages will disable executable inference
    matching_version = base + [f'--python-version={sys_ver_str}',
                               '--no-site-packages']
    _, options = process_options(matching_version)
    assert options.python_version == sys.version_info[:2]
    assert options.python_executable is None

    # Test setting python_version/executable from config file
    special_opts = argparse.Namespace()
    special_opts.python_executable = None
    special_opts.python_version = None
    special_opts.no_executable = None

    # first test inferring executable from version
    options = Options()
    options.python_executable = None
    options.python_version = sys.version_info[:2]
    infer_python_executable(options, special_opts)
    assert options.python_version == sys.version_info[:2]
    assert options.python_executable == sys.executable

    # then test inferring version from executable
    options = Options()
    options.python_executable = sys.executable
    infer_python_executable(options, special_opts)
    assert options.python_version == sys.version_info[:2]
    assert options.python_executable == sys.executable
</t>
<t tx="ekr.20220525082936.879">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Type checker test cases"""

import os
import re
import sys

from typing import Dict, List, Set, Tuple

from mypy import build
from mypy.build import Graph
from mypy.modulefinder import BuildSource, SearchPaths, FindModuleCache
from mypy.test.config import test_temp_dir, test_data_prefix
from mypy.test.data import (
    DataDrivenTestCase, DataSuite, FileOperation, module_from_path
)
from mypy.test.helpers import (
    assert_string_arrays_equal, normalize_error_messages, assert_module_equivalence,
    update_testcase_output, parse_options,
    assert_target_equivalence, check_test_output_files, perform_file_operations,
)
from mypy.errors import CompileError
from mypy.semanal_main import core_modules

try:
    import lxml  # type: ignore
except ImportError:
    lxml = None

import pytest

# List of files that contain test case descriptions.
typecheck_files = [
    'check-basic.test',
    'check-union-or-syntax.test',
    'check-callable.test',
    'check-classes.test',
    'check-statements.test',
    'check-generics.test',
    'check-dynamic-typing.test',
    'check-inference.test',
    'check-inference-context.test',
    'check-kwargs.test',
    'check-overloading.test',
    'check-type-checks.test',
    'check-abstract.test',
    'check-multiple-inheritance.test',
    'check-super.test',
    'check-modules.test',
    'check-modules-fast.test',
    'check-typevar-values.test',
    'check-unsupported.test',
    'check-unreachable-code.test',
    'check-unions.test',
    'check-isinstance.test',
    'check-lists.test',
    'check-namedtuple.test',
    'check-narrowing.test',
    'check-typeddict.test',
    'check-type-aliases.test',
    'check-ignore.test',
    'check-type-promotion.test',
    'check-semanal-error.test',
    'check-flags.test',
    'check-incremental.test',
    'check-serialize.test',
    'check-bound.test',
    'check-optional.test',
    'check-fastparse.test',
    'check-warnings.test',
    'check-async-await.test',
    'check-newtype.test',
    'check-class-namedtuple.test',
    'check-selftype.test',
    'check-python2.test',
    'check-columns.test',
    'check-functions.test',
    'check-tuples.test',
    'check-expressions.test',
    'check-generic-subtyping.test',
    'check-varargs.test',
    'check-newsyntax.test',
    'check-protocols.test',
    'check-underscores.test',
    'check-classvar.test',
    'check-enum.test',
    'check-incomplete-fixture.test',
    'check-custom-plugin.test',
    'check-default-plugin.test',
    'check-attr.test',
    'check-ctypes.test',
    'check-dataclasses.test',
    'check-final.test',
    'check-redefine.test',
    'check-literal.test',
    'check-newsemanal.test',
    'check-inline-config.test',
    'check-reports.test',
    'check-errorcodes.test',
    'check-annotated.test',
    'check-parameter-specification.test',
    'check-typevar-tuple.test',
    'check-generic-alias.test',
    'check-typeguard.test',
    'check-functools.test',
    'check-singledispatch.test',
    'check-slots.test',
    'check-formatting.test',
]

# Tests that use Python 3.8-only AST features (like expression-scoped ignores):
if sys.version_info &gt;= (3, 8):
    typecheck_files.append('check-python38.test')
if sys.version_info &gt;= (3, 9):
    typecheck_files.append('check-python39.test')
if sys.version_info &gt;= (3, 10):
    typecheck_files.append('check-python310.test')

# Special tests for platforms with case-insensitive filesystems.
if sys.platform in ('darwin', 'win32'):
    typecheck_files.extend(['check-modules-case.test'])


@others
</t>
<t tx="ekr.20220525082936.88">@path C:/Repos/mypy/mypy/
@language python
@tabwidth -4
"""Utility functions with no non-trivial dependencies."""

import os
import pathlib
import re
import subprocess
import sys
import hashlib
import io
import shutil
import time

from typing import (
    TypeVar, List, Tuple, Optional, Dict, Sequence, Iterable, Container, IO, Callable, Union, Sized
)
from typing_extensions import Final, Type, Literal

try:
    import curses
    import _curses  # noqa
    CURSES_ENABLED = True
except ImportError:
    CURSES_ENABLED = False

T = TypeVar('T')

ENCODING_RE: Final = re.compile(br"([ \t\v]*#.*(\r\n?|\n))??[ \t\v]*#.*coding[:=][ \t]*([-\w.]+)")

DEFAULT_SOURCE_OFFSET: Final = 4
DEFAULT_COLUMNS: Final = 80

# At least this number of columns will be shown on each side of
# error location when printing source code snippet.
MINIMUM_WIDTH: Final = 20

# VT100 color code processing was added in Windows 10, but only the second major update,
# Threshold 2. Fortunately, everyone (even on LTSB, Long Term Support Branch) should
# have a version of Windows 10 newer than this. Note that Windows 8 and below are not
# supported, but are either going out of support, or make up only a few % of the market.
MINIMUM_WINDOWS_MAJOR_VT100: Final = 10
MINIMUM_WINDOWS_BUILD_VT100: Final = 10586

default_python2_interpreter: Final = [
    "python2",
    "python",
    "/usr/bin/python",
    "C:\\Python27\\python.exe",
]

SPECIAL_DUNDERS: Final = frozenset((
    "__init__", "__new__", "__call__", "__init_subclass__", "__class_getitem__",
))


@others
</t>
<t tx="ekr.20220525082936.880">class TypeCheckSuite(DataSuite):
    files = typecheck_files

    @others
</t>
<t tx="ekr.20220525082936.881">def run_case(self, testcase: DataDrivenTestCase) -&gt; None:
    if lxml is None and os.path.basename(testcase.file) == 'check-reports.test':
        pytest.skip("Cannot import lxml. Is it installed?")
    incremental = ('incremental' in testcase.name.lower()
                   or 'incremental' in testcase.file
                   or 'serialize' in testcase.file)
    if incremental:
        # Incremental tests are run once with a cold cache, once with a warm cache.
        # Expect success on first run, errors from testcase.output (if any) on second run.
        num_steps = max([2] + list(testcase.output2.keys()))
        # Check that there are no file changes beyond the last run (they would be ignored).
        for dn, dirs, files in os.walk(os.curdir):
            for file in files:
                m = re.search(r'\.([2-9])$', file)
                if m and int(m.group(1)) &gt; num_steps:
                    raise ValueError(
                        'Output file {} exists though test case only has {} runs'.format(
                            file, num_steps))
        steps = testcase.find_steps()
        for step in range(1, num_steps + 1):
            idx = step - 2
            ops = steps[idx] if idx &lt; len(steps) and idx &gt;= 0 else []
            self.run_case_once(testcase, ops, step)
    else:
        self.run_case_once(testcase)

</t>
<t tx="ekr.20220525082936.882">def run_case_once(self, testcase: DataDrivenTestCase,
                  operations: List[FileOperation] = [],
                  incremental_step: int = 0) -&gt; None:
    original_program_text = '\n'.join(testcase.input)
    module_data = self.parse_module(original_program_text, incremental_step)

    # Unload already loaded plugins, they may be updated.
    for file, _ in testcase.files:
        module = module_from_path(file)
        if module.endswith('_plugin') and module in sys.modules:
            del sys.modules[module]
    if incremental_step == 0 or incremental_step == 1:
        # In run 1, copy program text to program file.
        for module_name, program_path, program_text in module_data:
            if module_name == '__main__':
                with open(program_path, 'w', encoding='utf8') as f:
                    f.write(program_text)
                break
    elif incremental_step &gt; 1:
        # In runs 2+, copy *.[num] files to * files.
        perform_file_operations(operations)

    # Parse options after moving files (in case mypy.ini is being moved).
    options = parse_options(original_program_text, testcase, incremental_step)
    options.use_builtins_fixtures = True
    options.enable_incomplete_features = True
    options.show_traceback = True

    # Enable some options automatically based on test file name.
    if 'optional' in testcase.file:
        options.strict_optional = True
    if 'columns' in testcase.file:
        options.show_column_numbers = True
    if 'errorcodes' in testcase.file:
        options.show_error_codes = True

    if incremental_step and options.incremental:
        # Don't overwrite # flags: --no-incremental in incremental test cases
        options.incremental = True
    else:
        options.incremental = False
        # Don't waste time writing cache unless we are specifically looking for it
        if not testcase.writescache:
            options.cache_dir = os.devnull

    sources = []
    for module_name, program_path, program_text in module_data:
        # Always set to none so we're forced to reread the module in incremental mode
        sources.append(BuildSource(program_path, module_name,
                                   None if incremental_step else program_text))

    plugin_dir = os.path.join(test_data_prefix, 'plugins')
    sys.path.insert(0, plugin_dir)

    res = None
    try:
        res = build.build(sources=sources,
                          options=options,
                          alt_lib_path=test_temp_dir)
        a = res.errors
    except CompileError as e:
        a = e.messages
    finally:
        assert sys.path[0] == plugin_dir
        del sys.path[0]

    if testcase.normalize_output:
        a = normalize_error_messages(a)

    # Make sure error messages match
    if incremental_step == 0:
        # Not incremental
        msg = 'Unexpected type checker output ({}, line {})'
        output = testcase.output
    elif incremental_step == 1:
        msg = 'Unexpected type checker output in incremental, run 1 ({}, line {})'
        output = testcase.output
    elif incremental_step &gt; 1:
        msg = ('Unexpected type checker output in incremental, run {}'.format(
            incremental_step) + ' ({}, line {})')
        output = testcase.output2.get(incremental_step, [])
    else:
        raise AssertionError()

    if output != a and testcase.config.getoption('--update-data', False):
        update_testcase_output(testcase, a)
    assert_string_arrays_equal(output, a, msg.format(testcase.file, testcase.line))

    if res:
        if options.cache_dir != os.devnull:
            self.verify_cache(module_data, res.errors, res.manager, res.graph)

        name = 'targets'
        if incremental_step:
            name += str(incremental_step + 1)
        expected = testcase.expected_fine_grained_targets.get(incremental_step + 1)
        actual = res.manager.processed_targets
        # Skip the initial builtin cycle.
        actual = [t for t in actual
                  if not any(t.startswith(mod)
                             for mod in core_modules + ['mypy_extensions'])]
        if expected is not None:
            assert_target_equivalence(name, expected, actual)
        if incremental_step &gt; 1:
            suffix = '' if incremental_step == 2 else str(incremental_step - 1)
            expected_rechecked = testcase.expected_rechecked_modules.get(incremental_step - 1)
            if expected_rechecked is not None:
                assert_module_equivalence(
                    'rechecked' + suffix,
                    expected_rechecked, res.manager.rechecked_modules)
            expected_stale = testcase.expected_stale_modules.get(incremental_step - 1)
            if expected_stale is not None:
                assert_module_equivalence(
                    'stale' + suffix,
                    expected_stale, res.manager.stale_modules)

    if testcase.output_files:
        check_test_output_files(testcase, incremental_step, strip_prefix='tmp/')

</t>
<t tx="ekr.20220525082936.883">def verify_cache(self, module_data: List[Tuple[str, str, str]], a: List[str],
                 manager: build.BuildManager, graph: Graph) -&gt; None:
    # There should be valid cache metadata for each module except
    # for those that had an error in themselves or one of their
    # dependencies.
    error_paths = self.find_error_message_paths(a)
    busted_paths = {m.path for id, m in manager.modules.items()
                    if graph[id].transitive_error}
    modules = self.find_module_files(manager)
    modules.update({module_name: path for module_name, path, text in module_data})
    missing_paths = self.find_missing_cache_files(modules, manager)
    # We would like to assert error_paths.issubset(busted_paths)
    # but this runs into trouble because while some 'notes' are
    # really errors that cause an error to be marked, many are
    # just notes attached to other errors.
    assert error_paths or not busted_paths, "Some modules reported error despite no errors"
    if not missing_paths == busted_paths:
        raise AssertionError("cache data discrepancy %s != %s" %
                             (missing_paths, busted_paths))
    assert os.path.isfile(os.path.join(manager.options.cache_dir, ".gitignore"))
    cachedir_tag = os.path.join(manager.options.cache_dir, "CACHEDIR.TAG")
    assert os.path.isfile(cachedir_tag)
    with open(cachedir_tag) as f:
        assert f.read().startswith("Signature: 8a477f597d28d172789f06886806bc55")

</t>
<t tx="ekr.20220525082936.884">def find_error_message_paths(self, a: List[str]) -&gt; Set[str]:
    hits = set()
    for line in a:
        m = re.match(r'([^\s:]+):(\d+:)?(\d+:)? (error|warning|note):', line)
        if m:
            p = m.group(1)
            hits.add(p)
    return hits

</t>
<t tx="ekr.20220525082936.885">def find_module_files(self, manager: build.BuildManager) -&gt; Dict[str, str]:
    modules = {}
    for id, module in manager.modules.items():
        modules[id] = module.path
    return modules

</t>
<t tx="ekr.20220525082936.886">def find_missing_cache_files(self, modules: Dict[str, str],
                             manager: build.BuildManager) -&gt; Set[str]:
    ignore_errors = True
    missing = {}
    for id, path in modules.items():
        meta = build.find_cache_meta(id, path, manager)
        if not build.validate_meta(meta, id, path, ignore_errors, manager):
            missing[id] = path
    return set(missing.values())

</t>
<t tx="ekr.20220525082936.887">def parse_module(self,
                 program_text: str,
                 incremental_step: int = 0) -&gt; List[Tuple[str, str, str]]:
    """Return the module and program names for a test case.

    Normally, the unit tests will parse the default ('__main__')
    module and follow all the imports listed there. You can override
    this behavior and instruct the tests to check multiple modules
    by using a comment like this in the test case input:

      # cmd: mypy -m foo.bar foo.baz

    You can also use `# cmdN:` to have a different cmd for incremental
    step N (2, 3, ...).

    Return a list of tuples (module name, file name, program text).
    """
    m = re.search('# cmd: mypy -m ([a-zA-Z0-9_. ]+)$', program_text, flags=re.MULTILINE)
    if incremental_step &gt; 1:
        alt_regex = f'# cmd{incremental_step}: mypy -m ([a-zA-Z0-9_. ]+)$'
        alt_m = re.search(alt_regex, program_text, flags=re.MULTILINE)
        if alt_m is not None:
            # Optionally return a different command if in a later step
            # of incremental mode, otherwise default to reusing the
            # original cmd.
            m = alt_m

    if m:
        # The test case wants to use a non-default main
        # module. Look up the module and give it as the thing to
        # analyze.
        module_names = m.group(1)
        out = []
        search_paths = SearchPaths((test_temp_dir,), (), (), ())
        cache = FindModuleCache(search_paths, fscache=None, options=None)
        for module_name in module_names.split(' '):
            path = cache.find_module(module_name)
            assert isinstance(path, str), f"Can't find ad hoc case file: {module_name}"
            with open(path, encoding='utf8') as f:
                program_text = f.read()
            out.append((module_name, path, program_text))
        return out
    else:
        return [('__main__', 'main', program_text)]
</t>
<t tx="ekr.20220525082936.888">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Test cases for the command line.

To begin we test that "mypy &lt;directory&gt;[/]" always recurses down the
whole tree.
"""

import os
import re
import subprocess
import sys

from typing import List
from typing import Optional

from mypy.test.config import test_temp_dir, PREFIX
from mypy.test.data import DataDrivenTestCase, DataSuite
from mypy.test.helpers import (
    assert_string_arrays_equal, normalize_error_messages, check_test_output_files
)

try:
    import lxml  # type: ignore
except ImportError:
    lxml = None

import pytest

# Path to Python 3 interpreter
python3_path = sys.executable

# Files containing test case descriptions.
cmdline_files = [
    'cmdline.test',
    'cmdline.pyproject.test',
    'reports.test',
    'envvars.test',
]


@others
</t>
<t tx="ekr.20220525082936.889">class PythonCmdlineSuite(DataSuite):
    files = cmdline_files
    native_sep = True

    @others
</t>
<t tx="ekr.20220525082936.89">def is_dunder(name: str, exclude_special: bool = False) -&gt; bool:
    """Returns whether name is a dunder name.

    Args:
        exclude_special: Whether to return False for a couple special dunder methods.

    """
    if exclude_special and name in SPECIAL_DUNDERS:
        return False
    return name.startswith("__") and name.endswith("__")


</t>
<t tx="ekr.20220525082936.890">def run_case(self, testcase: DataDrivenTestCase) -&gt; None:
    if lxml is None and os.path.basename(testcase.file) == 'reports.test':
        pytest.skip("Cannot import lxml. Is it installed?")
    for step in [1] + sorted(testcase.output2):
        test_python_cmdline(testcase, step)


</t>
<t tx="ekr.20220525082936.891">def test_python_cmdline(testcase: DataDrivenTestCase, step: int) -&gt; None:
    assert testcase.old_cwd is not None, "test was not properly set up"
    # Write the program to a file.
    program = '_program.py'
    program_path = os.path.join(test_temp_dir, program)
    with open(program_path, 'w', encoding='utf8') as file:
        for s in testcase.input:
            file.write(f'{s}\n')
    args = parse_args(testcase.input[0])
    custom_cwd = parse_cwd(testcase.input[1]) if len(testcase.input) &gt; 1 else None
    args.append('--show-traceback')
    if '--error-summary' not in args:
        args.append('--no-error-summary')
    # Type check the program.
    fixed = [python3_path, '-m', 'mypy']
    env = os.environ.copy()
    env.pop('COLUMNS', None)
    env['PYTHONPATH'] = PREFIX
    process = subprocess.Popen(fixed + args,
                               stdout=subprocess.PIPE,
                               stderr=subprocess.PIPE,
                               cwd=os.path.join(
                                   test_temp_dir,
                                   custom_cwd or ""
                                   ),
                               env=env)
    outb, errb = process.communicate()
    result = process.returncode
    # Split output into lines.
    out = [s.rstrip('\n\r') for s in str(outb, 'utf8').splitlines()]
    err = [s.rstrip('\n\r') for s in str(errb, 'utf8').splitlines()]

    if "PYCHARM_HOSTED" in os.environ:
        for pos, line in enumerate(err):
            if line.startswith('pydev debugger: '):
                # Delete the attaching debugger message itself, plus the extra newline added.
                del err[pos:pos + 2]
                break

    # Remove temp file.
    os.remove(program_path)
    # Compare actual output to expected.
    if testcase.output_files:
        # Ignore stdout, but we insist on empty stderr and zero status.
        if err or result:
            raise AssertionError(
                'Expected zero status and empty stderr%s, got %d and\n%s' %
                (' on step %d' % step if testcase.output2 else '',
                 result, '\n'.join(err + out)))
        check_test_output_files(testcase, step)
    else:
        if testcase.normalize_output:
            out = normalize_error_messages(err + out)
        obvious_result = 1 if out else 0
        if obvious_result != result:
            out.append(f'== Return code: {result}')
        expected_out = testcase.output if step == 1 else testcase.output2[step]
        # Strip "tmp/" out of the test so that # E: works...
        expected_out = [s.replace("tmp" + os.sep, "") for s in expected_out]
        assert_string_arrays_equal(expected_out, out,
                                   'Invalid output ({}, line {}){}'.format(
                                       testcase.file, testcase.line,
                                       ' on step %d' % step if testcase.output2 else ''))


</t>
<t tx="ekr.20220525082936.892">def parse_args(line: str) -&gt; List[str]:
    """Parse the first line of the program for the command line.

    This should have the form

      # cmd: mypy &lt;options&gt;

    For example:

      # cmd: mypy pkg/
    """
    m = re.match('# cmd: mypy (.*)$', line)
    if not m:
        return []  # No args; mypy will spit out an error.
    return m.group(1).split()


</t>
<t tx="ekr.20220525082936.893">def parse_cwd(line: str) -&gt; Optional[str]:
    """Parse the second line of the program for the command line.

    This should have the form

      # cwd: &lt;directory&gt;

    For example:

      # cwd: main/subdir
    """
    m = re.match('# cwd: (.*)$', line)
    return m.group(1) if m else None
</t>
<t tx="ekr.20220525082936.894">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""End-to-end test cases for the daemon (dmypy).

These are special because they run multiple shell commands.

This also includes some unit tests.
"""

import os
import subprocess
import sys
import tempfile
import unittest
from typing import List, Tuple

from mypy.modulefinder import SearchPaths
from mypy.fscache import FileSystemCache
from mypy.dmypy_server import filter_out_missing_top_level_packages

from mypy.test.config import test_temp_dir, PREFIX
from mypy.test.data import DataDrivenTestCase, DataSuite
from mypy.test.helpers import assert_string_arrays_equal, normalize_error_messages

# Files containing test cases descriptions.
daemon_files = [
    'daemon.test',
]


@others
</t>
<t tx="ekr.20220525082936.895">class DaemonSuite(DataSuite):
    files = daemon_files

    @others
</t>
<t tx="ekr.20220525082936.896">def run_case(self, testcase: DataDrivenTestCase) -&gt; None:
    try:
        test_daemon(testcase)
    finally:
        # Kill the daemon if it's still running.
        run_cmd('dmypy kill')


</t>
<t tx="ekr.20220525082936.897">def test_daemon(testcase: DataDrivenTestCase) -&gt; None:
    assert testcase.old_cwd is not None, "test was not properly set up"
    for i, step in enumerate(parse_script(testcase.input)):
        cmd = step[0]
        expected_lines = step[1:]
        assert cmd.startswith('$')
        cmd = cmd[1:].strip()
        cmd = cmd.replace('{python}', sys.executable)
        sts, output = run_cmd(cmd)
        output_lines = output.splitlines()
        output_lines = normalize_error_messages(output_lines)
        if sts:
            output_lines.append('== Return code: %d' % sts)
        assert_string_arrays_equal(expected_lines,
                                   output_lines,
                                   "Command %d (%s) did not give expected output" %
                                   (i + 1, cmd))


</t>
<t tx="ekr.20220525082936.898">def parse_script(input: List[str]) -&gt; List[List[str]]:
    """Parse testcase.input into steps.

    Each command starts with a line starting with '$'.
    The first line (less '$') is sent to the shell.
    The remaining lines are expected output.
    """
    steps = []
    step: List[str] = []
    for line in input:
        if line.startswith('$'):
            if step:
                assert step[0].startswith('$')
                steps.append(step)
                step = []
        step.append(line)
    if step:
        steps.append(step)
    return steps


</t>
<t tx="ekr.20220525082936.899">def run_cmd(input: str) -&gt; Tuple[int, str]:
    if input.startswith('dmypy '):
        input = sys.executable + ' -m mypy.' + input
    if input.startswith('mypy '):
        input = sys.executable + ' -m' + input
    env = os.environ.copy()
    env['PYTHONPATH'] = PREFIX
    try:
        output = subprocess.check_output(input,
                                         shell=True,
                                         stderr=subprocess.STDOUT,
                                         universal_newlines=True,
                                         cwd=test_temp_dir,
                                         env=env)
        return 0, output
    except subprocess.CalledProcessError as err:
        return err.returncode, err.output


</t>
<t tx="ekr.20220525082936.9">@abstractmethod
def visit_uninhabited_type(self, t: UninhabitedType) -&gt; T:
    pass

</t>
<t tx="ekr.20220525082936.90">def is_sunder(name: str) -&gt; bool:
    return not is_dunder(name) and name.startswith('_') and name.endswith('_')


</t>
<t tx="ekr.20220525082936.900">class DaemonUtilitySuite(unittest.TestCase):
    """Unit tests for helpers"""

    @others
</t>
<t tx="ekr.20220525082936.901">def test_filter_out_missing_top_level_packages(self) -&gt; None:
    with tempfile.TemporaryDirectory() as td:
        self.make_file(td, 'base/a/')
        self.make_file(td, 'base/b.py')
        self.make_file(td, 'base/c.pyi')
        self.make_file(td, 'base/missing.txt')
        self.make_file(td, 'typeshed/d.pyi')
        self.make_file(td, 'typeshed/@python2/e')
        self.make_file(td, 'pkg1/f-stubs')
        self.make_file(td, 'pkg2/g-python2-stubs')
        self.make_file(td, 'mpath/sub/long_name/')

        @others
        search = SearchPaths(python_path=(makepath('base'),),
                             mypy_path=(makepath('mpath/sub'),),
                             package_path=(makepath('pkg1'), makepath('pkg2')),
                             typeshed_path=(makepath('typeshed'),))
        fscache = FileSystemCache()
        res = filter_out_missing_top_level_packages(
            {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'long_name', 'ff', 'missing'},
            search,
            fscache)
        assert res == {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'long_name'}

</t>
<t tx="ekr.20220525082936.902">def makepath(p: str) -&gt; str:
    return os.path.join(td, p)

</t>
<t tx="ekr.20220525082936.903">def make_file(self, base: str, path: str) -&gt; None:
    fullpath = os.path.join(base, path)
    os.makedirs(os.path.dirname(fullpath), exist_ok=True)
    if not path.endswith('/'):
        with open(fullpath, 'w') as f:
            f.write('# test file')
</t>
<t tx="ekr.20220525082936.904">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Test cases for generating node-level dependencies (for fine-grained incremental checking)"""

import os
from collections import defaultdict

from typing import List, Tuple, Dict, Optional, Set
from typing_extensions import DefaultDict

from mypy import build, defaults
from mypy.modulefinder import BuildSource
from mypy.errors import CompileError
from mypy.nodes import MypyFile, Expression
from mypy.options import Options
from mypy.server.deps import get_dependencies
from mypy.test.config import test_temp_dir
from mypy.test.data import DataDrivenTestCase, DataSuite
from mypy.test.helpers import assert_string_arrays_equal, parse_options
from mypy.types import Type
from mypy.typestate import TypeState

# Only dependencies in these modules are dumped
dumped_modules = ['__main__', 'pkg', 'pkg.mod']


@others
</t>
<t tx="ekr.20220525082936.905">class GetDependenciesSuite(DataSuite):
    files = [
        'deps.test',
        'deps-types.test',
        'deps-generics.test',
        'deps-expressions.test',
        'deps-statements.test',
        'deps-classes.test',
    ]

    @others
</t>
<t tx="ekr.20220525082936.906">def run_case(self, testcase: DataDrivenTestCase) -&gt; None:
    src = '\n'.join(testcase.input)
    dump_all = '# __dump_all__' in src
    options = parse_options(src, testcase, incremental_step=1)
    if testcase.name.endswith('python2'):
        options.python_version = defaults.PYTHON2_VERSION
    options.use_builtins_fixtures = True
    options.show_traceback = True
    options.cache_dir = os.devnull
    options.export_types = True
    options.preserve_asts = True
    messages, files, type_map = self.build(src, options)
    a = messages
    if files is None or type_map is None:
        if not a:
            a = ['Unknown compile error (likely syntax error in test case or fixture)']
    else:
        deps: DefaultDict[str, Set[str]] = defaultdict(set)
        for module in files:
            if module in dumped_modules or dump_all and module not in ('abc',
                                                                       'typing',
                                                                       'mypy_extensions',
                                                                       'typing_extensions',
                                                                       'enum'):
                new_deps = get_dependencies(files[module], type_map, options.python_version,
                                            options)
                for source in new_deps:
                    deps[source].update(new_deps[source])

        TypeState.add_all_protocol_deps(deps)

        for source, targets in sorted(deps.items()):
            if source.startswith(('&lt;enum', '&lt;typing', '&lt;mypy')):
                # Remove noise.
                continue
            line = f"{source} -&gt; {', '.join(sorted(targets))}"
            # Clean up output a bit
            line = line.replace('__main__', 'm')
            a.append(line)

    assert_string_arrays_equal(
        testcase.output, a,
        f'Invalid output ({testcase.file}, line {testcase.line})')

</t>
<t tx="ekr.20220525082936.907">def build(self,
          source: str,
          options: Options) -&gt; Tuple[List[str],
                                     Optional[Dict[str, MypyFile]],
                                     Optional[Dict[Expression, Type]]]:
    try:
        result = build.build(sources=[BuildSource('main', None, source)],
                             options=options,
                             alt_lib_path=test_temp_dir)
    except CompileError as e:
        # TODO: Should perhaps not return None here.
        return e.messages, None, None
    return result.errors, result.files, result.types
</t>
<t tx="ekr.20220525082936.908">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Test cases for AST diff (used for fine-grained incremental checking)"""

import os
from typing import List, Tuple, Dict, Optional

from mypy import build
from mypy.modulefinder import BuildSource
from mypy.defaults import PYTHON3_VERSION
from mypy.errors import CompileError
from mypy.nodes import MypyFile
from mypy.options import Options
from mypy.server.astdiff import snapshot_symbol_table, compare_symbol_table_snapshots
from mypy.test.config import test_temp_dir
from mypy.test.data import DataDrivenTestCase, DataSuite
from mypy.test.helpers import assert_string_arrays_equal, parse_options


@others
</t>
<t tx="ekr.20220525082936.909">class ASTDiffSuite(DataSuite):
    files = [
        'diff.test',
    ]

    @others
</t>
<t tx="ekr.20220525082936.91">def split_module_names(mod_name: str) -&gt; List[str]:
    """Return the module and all parent module names.

    So, if `mod_name` is 'a.b.c', this function will return
    ['a.b.c', 'a.b', and 'a'].
    """
    out = [mod_name]
    while '.' in mod_name:
        mod_name = mod_name.rsplit('.', 1)[0]
        out.append(mod_name)
    return out


</t>
<t tx="ekr.20220525082936.910">def run_case(self, testcase: DataDrivenTestCase) -&gt; None:
    first_src = '\n'.join(testcase.input)
    files_dict = dict(testcase.files)
    second_src = files_dict['tmp/next.py']
    options = parse_options(first_src, testcase, 1)

    messages1, files1 = self.build(first_src, options)
    messages2, files2 = self.build(second_src, options)

    a = []
    if messages1:
        a.extend(messages1)
    if messages2:
        a.append('== next ==')
        a.extend(messages2)

    assert files1 is not None and files2 is not None, ('cases where CompileError'
                                                       ' occurred should not be run')
    prefix = '__main__'
    snapshot1 = snapshot_symbol_table(prefix, files1['__main__'].names)
    snapshot2 = snapshot_symbol_table(prefix, files2['__main__'].names)
    diff = compare_symbol_table_snapshots(prefix, snapshot1, snapshot2)
    for trigger in sorted(diff):
        a.append(trigger)

    assert_string_arrays_equal(
        testcase.output, a,
        f'Invalid output ({testcase.file}, line {testcase.line})')

</t>
<t tx="ekr.20220525082936.911">def build(self, source: str,
          options: Options) -&gt; Tuple[List[str], Optional[Dict[str, MypyFile]]]:
    options.use_builtins_fixtures = True
    options.show_traceback = True
    options.cache_dir = os.devnull
    options.python_version = PYTHON3_VERSION
    try:
        result = build.build(sources=[BuildSource('main', None, source)],
                             options=options,
                             alt_lib_path=test_temp_dir)
    except CompileError as e:
        # TODO: Is it okay to return None?
        return e.messages, None
    return result.errors, result.files
</t>
<t tx="ekr.20220525082936.912">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Tests for mypy incremental error output."""
from typing import List

from mypy import build
from mypy.test.helpers import assert_string_arrays_equal
from mypy.test.data import DataDrivenTestCase, DataSuite
from mypy.modulefinder import BuildSource
from mypy.errors import CompileError
from mypy.options import Options


@others
</t>
<t tx="ekr.20220525082936.913">class ErrorStreamSuite(DataSuite):
    required_out_section = True
    base_path = '.'
    files = ['errorstream.test']

    def run_case(self, testcase: DataDrivenTestCase) -&gt; None:
        test_error_stream(testcase)


</t>
<t tx="ekr.20220525082936.914">def test_error_stream(testcase: DataDrivenTestCase) -&gt; None:
    """Perform a single error streaming test case.

    The argument contains the description of the test case.
    """
    options = Options()
    options.show_traceback = True

    logged_messages: List[str] = []

    @others
    sources = [BuildSource('main', '__main__', '\n'.join(testcase.input))]
    try:
        build.build(sources=sources,
                    options=options,
                    flush_errors=flush_errors)
    except CompileError as e:
        assert e.messages == []

    assert_string_arrays_equal(testcase.output, logged_messages,
                               'Invalid output ({}, line {})'.format(
                                   testcase.file, testcase.line))
</t>
<t tx="ekr.20220525082936.915">def flush_errors(msgs: List[str], serious: bool) -&gt; None:
    if msgs:
        logged_messages.append('==== Errors flushed ====')
        logged_messages.extend(msgs)

</t>
<t tx="ekr.20220525082936.916">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Test cases for fine-grained incremental checking.

Each test cases runs a batch build followed by one or more fine-grained
incremental steps. We verify that each step produces the expected output.

See the comment at the top of test-data/unit/fine-grained.test for more
information.

N.B.: Unlike most of the other test suites, testfinegrained does not
rely on an alt_lib_path for finding source files. This means that they
can test interactions with the lib_path that is built implicitly based
on specified sources.
"""

import os
import re

from typing import List, Dict, Any, Tuple, Union, cast

from mypy import build
from mypy.modulefinder import BuildSource
from mypy.errors import CompileError
from mypy.options import Options
from mypy.test.config import test_temp_dir
from mypy.test.data import (
    DataDrivenTestCase, DataSuite, UpdateFile, DeleteFile
)
from mypy.test.helpers import (
    assert_string_arrays_equal, parse_options, assert_module_equivalence,
    assert_target_equivalence, perform_file_operations,
)
from mypy.server.mergecheck import check_consistency
from mypy.dmypy_util import DEFAULT_STATUS_FILE
from mypy.dmypy_server import Server
from mypy.config_parser import parse_config_file
from mypy.find_sources import create_source_list

import pytest

# Set to True to perform (somewhat expensive) checks for duplicate AST nodes after merge
CHECK_CONSISTENCY = False


@others
</t>
<t tx="ekr.20220525082936.917">class FineGrainedSuite(DataSuite):
    files = [
        'fine-grained.test',
        'fine-grained-cycles.test',
        'fine-grained-blockers.test',
        'fine-grained-modules.test',
        'fine-grained-follow-imports.test',
        'fine-grained-suggest.test',
        'fine-grained-attr.test',
    ]

    # Whether to use the fine-grained cache in the testing. This is overridden
    # by a trivial subclass to produce a suite that uses the cache.
    use_cache = False

    @others
</t>
<t tx="ekr.20220525082936.918">def should_skip(self, testcase: DataDrivenTestCase) -&gt; bool:
    # Decide whether to skip the test. This could have been structured
    # as a filter() classmethod also, but we want the tests reported
    # as skipped, not just elided.
    if self.use_cache:
        if testcase.only_when == '-only_when_nocache':
            return True
        # TODO: In caching mode we currently don't well support
        # starting from cached states with errors in them.
        if testcase.output and testcase.output[0] != '==':
            return True
    else:
        if testcase.only_when == '-only_when_cache':
            return True

    return False

</t>
<t tx="ekr.20220525082936.919">def run_case(self, testcase: DataDrivenTestCase) -&gt; None:
    if self.should_skip(testcase):
        pytest.skip()
        return

    main_src = '\n'.join(testcase.input)
    main_path = os.path.join(test_temp_dir, 'main')
    with open(main_path, 'w', encoding='utf8') as f:
        f.write(main_src)

    options = self.get_options(main_src, testcase, build_cache=False)
    build_options = self.get_options(main_src, testcase, build_cache=True)
    server = Server(options, DEFAULT_STATUS_FILE)

    num_regular_incremental_steps = self.get_build_steps(main_src)
    step = 1
    sources = self.parse_sources(main_src, step, options)
    if step &lt;= num_regular_incremental_steps:
        messages = self.build(build_options, sources)
    else:
        messages = self.run_check(server, sources)

    a = []
    if messages:
        a.extend(normalize_messages(messages))

    assert testcase.tmpdir
    a.extend(self.maybe_suggest(step, server, main_src, testcase.tmpdir.name))

    if server.fine_grained_manager:
        if CHECK_CONSISTENCY:
            check_consistency(server.fine_grained_manager)

    steps = testcase.find_steps()
    all_triggered = []

    for operations in steps:
        step += 1
        output, triggered = self.perform_step(
            operations,
            server,
            options,
            build_options,
            testcase,
            main_src,
            step,
            num_regular_incremental_steps,
        )
        a.append('==')
        a.extend(output)
        all_triggered.extend(triggered)

    # Normalize paths in test output (for Windows).
    a = [line.replace('\\', '/') for line in a]

    assert_string_arrays_equal(
        testcase.output, a,
        f'Invalid output ({testcase.file}, line {testcase.line})')

    if testcase.triggered:
        assert_string_arrays_equal(
            testcase.triggered,
            self.format_triggered(all_triggered),
            f'Invalid active triggers ({testcase.file}, line {testcase.line})')

</t>
<t tx="ekr.20220525082936.92">def module_prefix(modules: Iterable[str], target: str) -&gt; Optional[str]:
    result = split_target(modules, target)
    if result is None:
        return None
    return result[0]


</t>
<t tx="ekr.20220525082936.920">def get_options(self,
                source: str,
                testcase: DataDrivenTestCase,
                build_cache: bool,) -&gt; Options:
    # This handles things like '# flags: --foo'.
    options = parse_options(source, testcase, incremental_step=1)
    options.incremental = True
    options.use_builtins_fixtures = True
    options.show_traceback = True
    options.error_summary = False
    options.fine_grained_incremental = not build_cache
    options.use_fine_grained_cache = self.use_cache and not build_cache
    options.cache_fine_grained = self.use_cache
    options.local_partial_types = True
    if re.search('flags:.*--follow-imports', source) is None:
        # Override the default for follow_imports
        options.follow_imports = 'error'

    for name, _ in testcase.files:
        if 'mypy.ini' in name or 'pyproject.toml' in name:
            parse_config_file(options, lambda: None, name)
            break

    return options

</t>
<t tx="ekr.20220525082936.921">def run_check(self, server: Server, sources: List[BuildSource]) -&gt; List[str]:
    response = server.check(sources, is_tty=False, terminal_width=-1)
    out = cast(str, response['out'] or response['err'])
    return out.splitlines()

</t>
<t tx="ekr.20220525082936.922">def build(self,
          options: Options,
          sources: List[BuildSource]) -&gt; List[str]:
    try:
        result = build.build(sources=sources,
                             options=options)
    except CompileError as e:
        return e.messages
    return result.errors

</t>
<t tx="ekr.20220525082936.923">def format_triggered(self, triggered: List[List[str]]) -&gt; List[str]:
    result = []
    for n, triggers in enumerate(triggered):
        filtered = [trigger for trigger in triggers
                    if not trigger.endswith('__&gt;')]
        filtered = sorted(filtered)
        result.append(('%d: %s' % (n + 2, ', '.join(filtered))).strip())
    return result

</t>
<t tx="ekr.20220525082936.924">def get_build_steps(self, program_text: str) -&gt; int:
    """Get the number of regular incremental steps to run, from the test source"""
    if not self.use_cache:
        return 0
    m = re.search('# num_build_steps: ([0-9]+)$', program_text, flags=re.MULTILINE)
    if m is not None:
        return int(m.group(1))
    return 1

</t>
<t tx="ekr.20220525082936.925">def perform_step(self,
                 operations: List[Union[UpdateFile, DeleteFile]],
                 server: Server,
                 options: Options,
                 build_options: Options,
                 testcase: DataDrivenTestCase,
                 main_src: str,
                 step: int,
                 num_regular_incremental_steps: int) -&gt; Tuple[List[str], List[List[str]]]:
    """Perform one fine-grained incremental build step (after some file updates/deletions).

    Return (mypy output, triggered targets).
    """
    perform_file_operations(operations)
    sources = self.parse_sources(main_src, step, options)

    if step &lt;= num_regular_incremental_steps:
        new_messages = self.build(build_options, sources)
    else:
        new_messages = self.run_check(server, sources)

    updated: List[str] = []
    changed: List[str] = []
    targets: List[str] = []
    triggered = []
    if server.fine_grained_manager:
        if CHECK_CONSISTENCY:
            check_consistency(server.fine_grained_manager)
        triggered.append(server.fine_grained_manager.triggered)

        updated = server.fine_grained_manager.updated_modules
        changed = [mod for mod, file in server.fine_grained_manager.changed_modules]
        targets = server.fine_grained_manager.processed_targets

    expected_stale = testcase.expected_stale_modules.get(step - 1)
    if expected_stale is not None:
        assert_module_equivalence(
            'stale' + str(step - 1),
            expected_stale, changed)

    expected_rechecked = testcase.expected_rechecked_modules.get(step - 1)
    if expected_rechecked is not None:
        assert_module_equivalence(
            'rechecked' + str(step - 1),
            expected_rechecked, updated)

    expected = testcase.expected_fine_grained_targets.get(step)
    if expected:
        assert_target_equivalence(
            'targets' + str(step),
            expected, targets)

    new_messages = normalize_messages(new_messages)

    a = new_messages
    assert testcase.tmpdir
    a.extend(self.maybe_suggest(step, server, main_src, testcase.tmpdir.name))

    return a, triggered

</t>
<t tx="ekr.20220525082936.926">def parse_sources(self, program_text: str,
                  incremental_step: int,
                  options: Options) -&gt; List[BuildSource]:
    """Return target BuildSources for a test case.

    Normally, the unit tests will check all files included in the test
    case. This differs from how testcheck works by default, as dmypy
    doesn't currently support following imports.

    You can override this behavior and instruct the tests to check
    multiple modules by using a comment like this in the test case
    input:

      # cmd: main a.py

    You can also use `# cmdN:` to have a different cmd for incremental
    step N (2, 3, ...).

    """
    m = re.search('# cmd: mypy ([a-zA-Z0-9_./ ]+)$', program_text, flags=re.MULTILINE)
    regex = f'# cmd{incremental_step}: mypy ([a-zA-Z0-9_./ ]+)$'
    alt_m = re.search(regex, program_text, flags=re.MULTILINE)
    if alt_m is not None:
        # Optionally return a different command if in a later step
        # of incremental mode, otherwise default to reusing the
        # original cmd.
        m = alt_m

    if m:
        # The test case wants to use a non-default set of files.
        paths = [os.path.join(test_temp_dir, path) for path in m.group(1).strip().split()]
        return create_source_list(paths, options)
    else:
        base = BuildSource(os.path.join(test_temp_dir, 'main'), '__main__', None)
        # Use expand_dir instead of create_source_list to avoid complaints
        # when there aren't any .py files in an increment
        return [base] + create_source_list([test_temp_dir], options,
                                           allow_empty_dir=True)

</t>
<t tx="ekr.20220525082936.927">def maybe_suggest(self, step: int, server: Server, src: str, tmp_dir: str) -&gt; List[str]:
    output: List[str] = []
    targets = self.get_suggest(src, step)
    for flags, target in targets:
        json = '--json' in flags
        callsites = '--callsites' in flags
        no_any = '--no-any' in flags
        no_errors = '--no-errors' in flags
        try_text = '--try-text' in flags
        m = re.match('--flex-any=([0-9.]+)', flags)
        flex_any = float(m.group(1)) if m else None
        m = re.match(r'--use-fixme=(\w+)', flags)
        use_fixme = m.group(1) if m else None
        m = re.match('--max-guesses=([0-9]+)', flags)
        max_guesses = int(m.group(1)) if m else None
        res = cast(Dict[str, Any],
                   server.cmd_suggest(
                       target.strip(), json=json, no_any=no_any, no_errors=no_errors,
                       try_text=try_text, flex_any=flex_any, use_fixme=use_fixme,
                       callsites=callsites, max_guesses=max_guesses))
        val = res['error'] if 'error' in res else res['out'] + res['err']
        if json:
            # JSON contains already escaped \ on Windows, so requires a bit of care.
            val = val.replace('\\\\', '\\')
            val = val.replace(os.path.realpath(tmp_dir) + os.path.sep, '')
        output.extend(val.strip().split('\n'))
    return normalize_messages(output)

</t>
<t tx="ekr.20220525082936.928">def get_suggest(self, program_text: str,
                incremental_step: int) -&gt; List[Tuple[str, str]]:
    step_bit = '1?' if incremental_step == 1 else str(incremental_step)
    regex = f'# suggest{step_bit}: (--[a-zA-Z0-9_\\-./=?^ ]+ )*([a-zA-Z0-9_.:/?^ ]+)$'
    m = re.findall(regex, program_text, flags=re.MULTILINE)
    return m


</t>
<t tx="ekr.20220525082936.929">def normalize_messages(messages: List[str]) -&gt; List[str]:
    return [re.sub('^tmp' + re.escape(os.sep), '', message)
            for message in messages]
</t>
<t tx="ekr.20220525082936.93">def split_target(modules: Iterable[str], target: str) -&gt; Optional[Tuple[str, str]]:
    remaining: List[str] = []
    while True:
        if target in modules:
            return target, '.'.join(remaining)
        components = target.rsplit('.', 1)
        if len(components) == 1:
            return None
        target = components[0]
        remaining.insert(0, components[1])


</t>
<t tx="ekr.20220525082936.930">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Tests for fine-grained incremental checking using the cache.

All of the real code for this lives in testfinegrained.py.
"""

# We can't "import FineGrainedSuite from ..." because that will cause pytest
# to collect the non-caching tests when running this file.
import mypy.test.testfinegrained


@others
</t>
<t tx="ekr.20220525082936.931">class FineGrainedCacheSuite(mypy.test.testfinegrained.FineGrainedSuite):
    use_cache = True
    test_name_suffix = '_cached'
    files = (
        mypy.test.testfinegrained.FineGrainedSuite.files + ['fine-grained-cache-incremental.test'])
</t>
<t tx="ekr.20220525082936.932">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
from unittest import TestCase, main

from mypy.util import trim_source_line, split_words


@others
if __name__ == '__main__':
    main()
</t>
<t tx="ekr.20220525082936.933">class FancyErrorFormattingTestCases(TestCase):
    @others
</t>
<t tx="ekr.20220525082936.934">def test_trim_source(self) -&gt; None:
    assert trim_source_line('0123456789abcdef',
                            max_len=16, col=5, min_width=2) == ('0123456789abcdef', 0)

    # Locations near start.
    assert trim_source_line('0123456789abcdef',
                            max_len=7, col=0, min_width=2) == ('0123456...', 0)
    assert trim_source_line('0123456789abcdef',
                            max_len=7, col=4, min_width=2) == ('0123456...', 0)

    # Middle locations.
    assert trim_source_line('0123456789abcdef',
                            max_len=7, col=5, min_width=2) == ('...1234567...', -2)
    assert trim_source_line('0123456789abcdef',
                            max_len=7, col=6, min_width=2) == ('...2345678...', -1)
    assert trim_source_line('0123456789abcdef',
                            max_len=7, col=8, min_width=2) == ('...456789a...', 1)

    # Locations near the end.
    assert trim_source_line('0123456789abcdef',
                            max_len=7, col=11, min_width=2) == ('...789abcd...', 4)
    assert trim_source_line('0123456789abcdef',
                            max_len=7, col=13, min_width=2) == ('...9abcdef', 6)
    assert trim_source_line('0123456789abcdef',
                            max_len=7, col=15, min_width=2) == ('...9abcdef', 6)

</t>
<t tx="ekr.20220525082936.935">def test_split_words(self) -&gt; None:
    assert split_words('Simple message') == ['Simple', 'message']
    assert split_words('Message with "Some[Long, Types]"'
                       ' in it') == ['Message', 'with',
                                     '"Some[Long, Types]"', 'in', 'it']
    assert split_words('Message with "Some[Long, Types]"'
                       ' and [error-code]') == ['Message', 'with', '"Some[Long, Types]"',
                                                'and', '[error-code]']
    assert split_words('"Type[Stands, First]" then words') == ['"Type[Stands, First]"',
                                                               'then', 'words']
    assert split_words('First words "Then[Stands, Type]"') == ['First', 'words',
                                                               '"Then[Stands, Type]"']
    assert split_words('"Type[Only, Here]"') == ['"Type[Only, Here]"']
    assert split_words('OneWord') == ['OneWord']
    assert split_words(' ') == ['', '']


</t>
<t tx="ekr.20220525082936.936">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Unit tests for file system cache."""

import os
import shutil
import sys
import tempfile
import unittest
from typing import Optional

from mypy.fscache import FileSystemCache


@others
</t>
<t tx="ekr.20220525082936.937">class TestFileSystemCache(unittest.TestCase):
    @others
</t>
<t tx="ekr.20220525082936.938">def setUp(self) -&gt; None:
    self.tempdir = tempfile.mkdtemp()
    self.oldcwd = os.getcwd()
    os.chdir(self.tempdir)
    self.fscache = FileSystemCache()

</t>
<t tx="ekr.20220525082936.939">def tearDown(self) -&gt; None:
    os.chdir(self.oldcwd)
    shutil.rmtree(self.tempdir)

</t>
<t tx="ekr.20220525082936.94">def short_type(obj: object) -&gt; str:
    """Return the last component of the type name of an object.

    If obj is None, return 'nil'. For example, if obj is 1, return 'int'.
    """
    if obj is None:
        return 'nil'
    t = str(type(obj))
    return t.split('.')[-1].rstrip("'&gt;")


</t>
<t tx="ekr.20220525082936.940">def test_isfile_case_1(self) -&gt; None:
    self.make_file('bar.py')
    self.make_file('pkg/sub_package/__init__.py')
    self.make_file('pkg/sub_package/foo.py')
    # Run twice to test both cached and non-cached code paths.
    for i in range(2):
        assert self.isfile_case('bar.py')
        assert self.isfile_case('pkg/sub_package/__init__.py')
        assert self.isfile_case('pkg/sub_package/foo.py')
        assert not self.isfile_case('non_existent.py')
        assert not self.isfile_case('pkg/non_existent.py')
        assert not self.isfile_case('pkg/')
        assert not self.isfile_case('bar.py/')
    for i in range(2):
        assert not self.isfile_case('Bar.py')
        assert not self.isfile_case('pkg/sub_package/__init__.PY')
        assert not self.isfile_case('pkg/Sub_Package/foo.py')
        assert not self.isfile_case('Pkg/sub_package/foo.py')

</t>
<t tx="ekr.20220525082936.941">def test_isfile_case_2(self) -&gt; None:
    self.make_file('bar.py')
    self.make_file('pkg/sub_package/__init__.py')
    self.make_file('pkg/sub_package/foo.py')
    # Run twice to test both cached and non-cached code paths.
    # This reverses the order of checks from test_isfile_case_1.
    for i in range(2):
        assert not self.isfile_case('Bar.py')
        assert not self.isfile_case('pkg/sub_package/__init__.PY')
        assert not self.isfile_case('pkg/Sub_Package/foo.py')
        assert not self.isfile_case('Pkg/sub_package/foo.py')
    for i in range(2):
        assert self.isfile_case('bar.py')
        assert self.isfile_case('pkg/sub_package/__init__.py')
        assert self.isfile_case('pkg/sub_package/foo.py')
        assert not self.isfile_case('non_existent.py')
        assert not self.isfile_case('pkg/non_existent.py')

</t>
<t tx="ekr.20220525082936.942">def test_isfile_case_3(self) -&gt; None:
    self.make_file('bar.py')
    self.make_file('pkg/sub_package/__init__.py')
    self.make_file('pkg/sub_package/foo.py')
    # Run twice to test both cached and non-cached code paths.
    for i in range(2):
        assert self.isfile_case('bar.py')
        assert not self.isfile_case('non_existent.py')
        assert not self.isfile_case('pkg/non_existent.py')
        assert not self.isfile_case('Bar.py')
        assert not self.isfile_case('pkg/sub_package/__init__.PY')
        assert not self.isfile_case('pkg/Sub_Package/foo.py')
        assert not self.isfile_case('Pkg/sub_package/foo.py')
        assert self.isfile_case('pkg/sub_package/__init__.py')
        assert self.isfile_case('pkg/sub_package/foo.py')

</t>
<t tx="ekr.20220525082936.943">def test_isfile_case_other_directory(self) -&gt; None:
    self.make_file('bar.py')
    with tempfile.TemporaryDirectory() as other:
        self.make_file('other_dir.py', base=other)
        self.make_file('pkg/other_dir.py', base=other)
        assert self.isfile_case(os.path.join(other, 'other_dir.py'))
        assert not self.isfile_case(os.path.join(other, 'Other_Dir.py'))
        assert not self.isfile_case(os.path.join(other, 'bar.py'))
        if sys.platform in ('win32', 'darwin'):
            # We only check case for directories under our prefix, and since
            # this path is not under the prefix, case difference is fine.
            assert self.isfile_case(os.path.join(other, 'PKG/other_dir.py'))

</t>
<t tx="ekr.20220525082936.944">def make_file(self, path: str, base: Optional[str] = None) -&gt; None:
    if base is None:
        base = self.tempdir
    fullpath = os.path.join(base, path)
    os.makedirs(os.path.dirname(fullpath), exist_ok=True)
    if not path.endswith('/'):
        with open(fullpath, 'w') as f:
            f.write('# test file')

</t>
<t tx="ekr.20220525082936.945">def isfile_case(self, path: str) -&gt; bool:
    return self.fscache.isfile_case(os.path.join(self.tempdir, path), self.tempdir)
</t>
<t tx="ekr.20220525082936.946">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Test cases for graph processing code in build.py."""

import sys
from typing import AbstractSet, Dict, Set, List

from mypy.test.helpers import assert_equal, Suite
from mypy.build import BuildManager, State, BuildSourceSet
from mypy.modulefinder import SearchPaths
from mypy.build import topsort, strongly_connected_components, sorted_components, order_ascc
from mypy.version import __version__
from mypy.options import Options
from mypy.report import Reports
from mypy.plugin import Plugin
from mypy.errors import Errors
from mypy.fscache import FileSystemCache


@others
</t>
<t tx="ekr.20220525082936.947">class GraphSuite(Suite):

    @others
</t>
<t tx="ekr.20220525082936.948">def test_topsort(self) -&gt; None:
    a = frozenset({'A'})
    b = frozenset({'B'})
    c = frozenset({'C'})
    d = frozenset({'D'})
    data: Dict[AbstractSet[str], Set[AbstractSet[str]]] = {a: {b, c}, b: {d}, c: {d}}
    res = list(topsort(data))
    assert_equal(res, [{d}, {b, c}, {a}])

</t>
<t tx="ekr.20220525082936.949">def test_scc(self) -&gt; None:
    vertices = {"A", "B", "C", "D"}
    edges: Dict[str, List[str]] = {"A": ["B", "C"], "B": ["C"], "C": ["B", "D"], "D": []}
    sccs = {frozenset(x) for x in strongly_connected_components(vertices, edges)}
    assert_equal(sccs,
                 {frozenset({'A'}),
                  frozenset({'B', 'C'}),
                  frozenset({'D'})})

</t>
<t tx="ekr.20220525082936.95">def find_python_encoding(text: bytes, pyversion: Tuple[int, int]) -&gt; Tuple[str, int]:
    """PEP-263 for detecting Python file encoding"""
    result = ENCODING_RE.match(text)
    if result:
        line = 2 if result.group(1) else 1
        encoding = result.group(3).decode('ascii')
        # Handle some aliases that Python is happy to accept and that are used in the wild.
        if encoding.startswith(('iso-latin-1-', 'latin-1-')) or encoding == 'iso-latin-1':
            encoding = 'latin-1'
        return encoding, line
    else:
        default_encoding = 'utf8' if pyversion[0] &gt;= 3 else 'ascii'
        return default_encoding, -1


</t>
<t tx="ekr.20220525082936.950">def _make_manager(self) -&gt; BuildManager:
    errors = Errors()
    options = Options()
    fscache = FileSystemCache()
    search_paths = SearchPaths((), (), (), ())
    manager = BuildManager(
        data_dir='',
        search_paths=search_paths,
        ignore_prefix='',
        source_set=BuildSourceSet([]),
        reports=Reports('', {}),
        options=options,
        version_id=__version__,
        plugin=Plugin(options),
        plugins_snapshot={},
        errors=errors,
        flush_errors=lambda msgs, serious: None,
        fscache=fscache,
        stdout=sys.stdout,
        stderr=sys.stderr,
    )
    return manager

</t>
<t tx="ekr.20220525082936.951">def test_sorted_components(self) -&gt; None:
    manager = self._make_manager()
    graph = {'a': State('a', None, 'import b, c', manager),
             'd': State('d', None, 'pass', manager),
             'b': State('b', None, 'import c', manager),
             'c': State('c', None, 'import b, d', manager),
             }
    res = sorted_components(graph)
    assert_equal(res, [frozenset({'d'}), frozenset({'c', 'b'}), frozenset({'a'})])

</t>
<t tx="ekr.20220525082936.952">def test_order_ascc(self) -&gt; None:
    manager = self._make_manager()
    graph = {'a': State('a', None, 'import b, c', manager),
             'd': State('d', None, 'def f(): import a', manager),
             'b': State('b', None, 'import c', manager),
             'c': State('c', None, 'import b, d', manager),
             }
    res = sorted_components(graph)
    assert_equal(res, [frozenset({'a', 'd', 'c', 'b'})])
    ascc = res[0]
    scc = order_ascc(graph, ascc)
    assert_equal(scc, ['d', 'c', 'b', 'a'])
</t>
<t tx="ekr.20220525082936.953">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Test cases for type inference helper functions."""

from typing import List, Optional, Tuple, Union, Dict, Set

from mypy.test.helpers import Suite, assert_equal
from mypy.argmap import map_actuals_to_formals
from mypy.checker import group_comparison_operands, DisjointDict
from mypy.literals import Key
from mypy.nodes import ArgKind, ARG_POS, ARG_OPT, ARG_STAR, ARG_STAR2, ARG_NAMED, NameExpr
from mypy.types import AnyType, TupleType, Type, TypeOfAny
from mypy.test.typefixture import TypeFixture


@others
</t>
<t tx="ekr.20220525082936.954">class MapActualsToFormalsSuite(Suite):
    """Test cases for argmap.map_actuals_to_formals."""

    @others
</t>
<t tx="ekr.20220525082936.955">def test_basic(self) -&gt; None:
    self.assert_map([], [], [])

</t>
<t tx="ekr.20220525082936.956">def test_positional_only(self) -&gt; None:
    self.assert_map([ARG_POS],
                    [ARG_POS],
                    [[0]])
    self.assert_map([ARG_POS, ARG_POS],
                    [ARG_POS, ARG_POS],
                    [[0], [1]])

</t>
<t tx="ekr.20220525082936.957">def test_optional(self) -&gt; None:
    self.assert_map([],
                    [ARG_OPT],
                    [[]])
    self.assert_map([ARG_POS],
                    [ARG_OPT],
                    [[0]])
    self.assert_map([ARG_POS],
                    [ARG_OPT, ARG_OPT],
                    [[0], []])

</t>
<t tx="ekr.20220525082936.958">def test_callee_star(self) -&gt; None:
    self.assert_map([],
                    [ARG_STAR],
                    [[]])
    self.assert_map([ARG_POS],
                    [ARG_STAR],
                    [[0]])
    self.assert_map([ARG_POS, ARG_POS],
                    [ARG_STAR],
                    [[0, 1]])

</t>
<t tx="ekr.20220525082936.959">def test_caller_star(self) -&gt; None:
    self.assert_map([ARG_STAR],
                    [ARG_STAR],
                    [[0]])
    self.assert_map([ARG_POS, ARG_STAR],
                    [ARG_STAR],
                    [[0, 1]])
    self.assert_map([ARG_STAR],
                    [ARG_POS, ARG_STAR],
                    [[0], [0]])
    self.assert_map([ARG_STAR],
                    [ARG_OPT, ARG_STAR],
                    [[0], [0]])

</t>
<t tx="ekr.20220525082936.96">def bytes_to_human_readable_repr(b: bytes) -&gt; str:
    """Converts bytes into some human-readable representation. Unprintable
    bytes such as the nul byte are escaped. For example:

        &gt;&gt;&gt; b = bytes([102, 111, 111, 10, 0])
        &gt;&gt;&gt; s = bytes_to_human_readable_repr(b)
        &gt;&gt;&gt; print(s)
        foo\n\x00
        &gt;&gt;&gt; print(repr(s))
        'foo\\n\\x00'
    """
    return repr(b)[2:-1]


</t>
<t tx="ekr.20220525082936.960">def test_too_many_caller_args(self) -&gt; None:
    self.assert_map([ARG_POS],
                    [],
                    [])
    self.assert_map([ARG_STAR],
                    [],
                    [])
    self.assert_map([ARG_STAR],
                    [ARG_POS],
                    [[0]])

</t>
<t tx="ekr.20220525082936.961">def test_tuple_star(self) -&gt; None:
    any_type = AnyType(TypeOfAny.special_form)
    self.assert_vararg_map(
        [ARG_STAR],
        [ARG_POS],
        [[0]],
        self.tuple(any_type))
    self.assert_vararg_map(
        [ARG_STAR],
        [ARG_POS, ARG_POS],
        [[0], [0]],
        self.tuple(any_type, any_type))
    self.assert_vararg_map(
        [ARG_STAR],
        [ARG_POS, ARG_OPT, ARG_OPT],
        [[0], [0], []],
        self.tuple(any_type, any_type))

</t>
<t tx="ekr.20220525082936.962">def tuple(self, *args: Type) -&gt; TupleType:
    return TupleType(list(args), TypeFixture().std_tuple)

</t>
<t tx="ekr.20220525082936.963">def test_named_args(self) -&gt; None:
    self.assert_map(
        ['x'],
        [(ARG_POS, 'x')],
        [[0]])
    self.assert_map(
        ['y', 'x'],
        [(ARG_POS, 'x'), (ARG_POS, 'y')],
        [[1], [0]])

</t>
<t tx="ekr.20220525082936.964">def test_some_named_args(self) -&gt; None:
    self.assert_map(
        ['y'],
        [(ARG_OPT, 'x'), (ARG_OPT, 'y'), (ARG_OPT, 'z')],
        [[], [0], []])

</t>
<t tx="ekr.20220525082936.965">def test_missing_named_arg(self) -&gt; None:
    self.assert_map(
        ['y'],
        [(ARG_OPT, 'x')],
        [[]])

</t>
<t tx="ekr.20220525082936.966">def test_duplicate_named_arg(self) -&gt; None:
    self.assert_map(
        ['x', 'x'],
        [(ARG_OPT, 'x')],
        [[0, 1]])

</t>
<t tx="ekr.20220525082936.967">def test_varargs_and_bare_asterisk(self) -&gt; None:
    self.assert_map(
        [ARG_STAR],
        [ARG_STAR, (ARG_NAMED, 'x')],
        [[0], []])
    self.assert_map(
        [ARG_STAR, 'x'],
        [ARG_STAR, (ARG_NAMED, 'x')],
        [[0], [1]])

</t>
<t tx="ekr.20220525082936.968">def test_keyword_varargs(self) -&gt; None:
    self.assert_map(
        ['x'],
        [ARG_STAR2],
        [[0]])
    self.assert_map(
        ['x', ARG_STAR2],
        [ARG_STAR2],
        [[0, 1]])
    self.assert_map(
        ['x', ARG_STAR2],
        [(ARG_POS, 'x'), ARG_STAR2],
        [[0], [1]])
    self.assert_map(
        [ARG_POS, ARG_STAR2],
        [(ARG_POS, 'x'), ARG_STAR2],
        [[0], [1]])

</t>
<t tx="ekr.20220525082936.969">def test_both_kinds_of_varargs(self) -&gt; None:
    self.assert_map(
        [ARG_STAR, ARG_STAR2],
        [(ARG_POS, 'x'), (ARG_POS, 'y')],
        [[0, 1], [0, 1]])

</t>
<t tx="ekr.20220525082936.97">class DecodeError(Exception):
    """Exception raised when a file cannot be decoded due to an unknown encoding type.

    Essentially a wrapper for the LookupError raised by `bytearray.decode`
    """


</t>
<t tx="ekr.20220525082936.970">def test_special_cases(self) -&gt; None:
    self.assert_map([ARG_STAR],
                    [ARG_STAR, ARG_STAR2],
                    [[0], []])
    self.assert_map([ARG_STAR, ARG_STAR2],
                    [ARG_STAR, ARG_STAR2],
                    [[0], [1]])
    self.assert_map([ARG_STAR2],
                    [(ARG_POS, 'x'), ARG_STAR2],
                    [[0], [0]])
    self.assert_map([ARG_STAR2],
                    [ARG_STAR2],
                    [[0]])

</t>
<t tx="ekr.20220525082936.971">def assert_map(self,
               caller_kinds_: List[Union[ArgKind, str]],
               callee_kinds_: List[Union[ArgKind, Tuple[ArgKind, str]]],
               expected: List[List[int]],
               ) -&gt; None:
    caller_kinds, caller_names = expand_caller_kinds(caller_kinds_)
    callee_kinds, callee_names = expand_callee_kinds(callee_kinds_)
    result = map_actuals_to_formals(
        caller_kinds,
        caller_names,
        callee_kinds,
        callee_names,
        lambda i: AnyType(TypeOfAny.special_form))
    assert_equal(result, expected)

</t>
<t tx="ekr.20220525082936.972">def assert_vararg_map(self,
                      caller_kinds: List[ArgKind],
                      callee_kinds: List[ArgKind],
                      expected: List[List[int]],
                      vararg_type: Type,
                      ) -&gt; None:
    result = map_actuals_to_formals(
        caller_kinds,
        [],
        callee_kinds,
        [],
        lambda i: vararg_type)
    assert_equal(result, expected)


</t>
<t tx="ekr.20220525082936.973">def expand_caller_kinds(kinds_or_names: List[Union[ArgKind, str]]
                        ) -&gt; Tuple[List[ArgKind], List[Optional[str]]]:
    kinds = []
    names: List[Optional[str]] = []
    for k in kinds_or_names:
        if isinstance(k, str):
            kinds.append(ARG_NAMED)
            names.append(k)
        else:
            kinds.append(k)
            names.append(None)
    return kinds, names


</t>
<t tx="ekr.20220525082936.974">def expand_callee_kinds(kinds_and_names: List[Union[ArgKind, Tuple[ArgKind, str]]]
                        ) -&gt; Tuple[List[ArgKind], List[Optional[str]]]:
    kinds = []
    names: List[Optional[str]] = []
    for v in kinds_and_names:
        if isinstance(v, tuple):
            kinds.append(v[0])
            names.append(v[1])
        else:
            kinds.append(v)
            names.append(None)
    return kinds, names


</t>
<t tx="ekr.20220525082936.975">class OperandDisjointDictSuite(Suite):
    """Test cases for checker.DisjointDict, which is used for type inference with operands."""
    @others
</t>
<t tx="ekr.20220525082936.976">def new(self) -&gt; DisjointDict[int, str]:
    return DisjointDict()

</t>
<t tx="ekr.20220525082936.977">def test_independent_maps(self) -&gt; None:
    d = self.new()
    d.add_mapping({0, 1}, {"group1"})
    d.add_mapping({2, 3, 4}, {"group2"})
    d.add_mapping({5, 6, 7}, {"group3"})

    self.assertEqual(d.items(), [
        ({0, 1}, {"group1"}),
        ({2, 3, 4}, {"group2"}),
        ({5, 6, 7}, {"group3"}),
    ])

</t>
<t tx="ekr.20220525082936.978">def test_partial_merging(self) -&gt; None:
    d = self.new()
    d.add_mapping({0, 1}, {"group1"})
    d.add_mapping({1, 2}, {"group2"})
    d.add_mapping({3, 4}, {"group3"})
    d.add_mapping({5, 0}, {"group4"})
    d.add_mapping({5, 6}, {"group5"})
    d.add_mapping({4, 7}, {"group6"})

    self.assertEqual(d.items(), [
        ({0, 1, 2, 5, 6}, {"group1", "group2", "group4", "group5"}),
        ({3, 4, 7}, {"group3", "group6"}),
    ])

</t>
<t tx="ekr.20220525082936.979">def test_full_merging(self) -&gt; None:
    d = self.new()
    d.add_mapping({0, 1, 2}, {"a"})
    d.add_mapping({3, 4, 2}, {"b"})
    d.add_mapping({10, 11, 12}, {"c"})
    d.add_mapping({13, 14, 15}, {"d"})
    d.add_mapping({14, 10, 16}, {"e"})
    d.add_mapping({0, 10}, {"f"})

    self.assertEqual(d.items(), [
        ({0, 1, 2, 3, 4, 10, 11, 12, 13, 14, 15, 16}, {"a", "b", "c", "d", "e", "f"}),
    ])

</t>
<t tx="ekr.20220525082936.98">def decode_python_encoding(source: bytes, pyversion: Tuple[int, int]) -&gt; str:
    """Read the Python file with while obeying PEP-263 encoding detection.

    Returns the source as a string.
    """
    # check for BOM UTF-8 encoding and strip it out if present
    if source.startswith(b'\xef\xbb\xbf'):
        encoding = 'utf8'
        source = source[3:]
    else:
        # look at first two lines and check if PEP-263 coding is present
        encoding, _ = find_python_encoding(source, pyversion)

    try:
        source_text = source.decode(encoding)
    except LookupError as lookuperr:
        raise DecodeError(str(lookuperr)) from lookuperr
    return source_text


</t>
<t tx="ekr.20220525082936.980">def test_merge_with_multiple_overlaps(self) -&gt; None:
    d = self.new()
    d.add_mapping({0, 1, 2}, {"a"})
    d.add_mapping({3, 4, 5}, {"b"})
    d.add_mapping({1, 2, 4, 5}, {"c"})
    d.add_mapping({6, 1, 2, 4, 5}, {"d"})
    d.add_mapping({6, 1, 2, 4, 5}, {"e"})

    self.assertEqual(d.items(), [
        ({0, 1, 2, 3, 4, 5, 6}, {"a", "b", "c", "d", "e"}),
    ])


</t>
<t tx="ekr.20220525082936.981">class OperandComparisonGroupingSuite(Suite):
    """Test cases for checker.group_comparison_operands."""
    @others
</t>
<t tx="ekr.20220525082936.982">def literal_keymap(self, assignable_operands: Dict[int, NameExpr]) -&gt; Dict[int, Key]:
    output: Dict[int, Key] = {}
    for index, expr in assignable_operands.items():
        output[index] = ('FakeExpr', expr.name)
    return output

</t>
<t tx="ekr.20220525082936.983">def test_basic_cases(self) -&gt; None:
    # Note: the grouping function doesn't actually inspect the input exprs, so we
    # just default to using NameExprs for simplicity.
    x0 = NameExpr('x0')
    x1 = NameExpr('x1')
    x2 = NameExpr('x2')
    x3 = NameExpr('x3')
    x4 = NameExpr('x4')

    basic_input = [('==', x0, x1), ('==', x1, x2), ('&lt;', x2, x3), ('==', x3, x4)]

    none_assignable = self.literal_keymap({})
    all_assignable = self.literal_keymap({0: x0, 1: x1, 2: x2, 3: x3, 4: x4})

    for assignable in [none_assignable, all_assignable]:
        self.assertEqual(
            group_comparison_operands(basic_input, assignable, set()),
            [('==', [0, 1]), ('==', [1, 2]), ('&lt;', [2, 3]), ('==', [3, 4])],
        )
        self.assertEqual(
            group_comparison_operands(basic_input, assignable, {'=='}),
            [('==', [0, 1, 2]), ('&lt;', [2, 3]), ('==', [3, 4])],
        )
        self.assertEqual(
            group_comparison_operands(basic_input, assignable, {'&lt;'}),
            [('==', [0, 1]), ('==', [1, 2]), ('&lt;', [2, 3]), ('==', [3, 4])],
        )
        self.assertEqual(
            group_comparison_operands(basic_input, assignable, {'==', '&lt;'}),
            [('==', [0, 1, 2]), ('&lt;', [2, 3]), ('==', [3, 4])],
        )

</t>
<t tx="ekr.20220525082936.984">def test_multiple_groups(self) -&gt; None:
    x0 = NameExpr('x0')
    x1 = NameExpr('x1')
    x2 = NameExpr('x2')
    x3 = NameExpr('x3')
    x4 = NameExpr('x4')
    x5 = NameExpr('x5')

    self.assertEqual(
        group_comparison_operands(
            [('==', x0, x1), ('==', x1, x2), ('is', x2, x3), ('is', x3, x4)],
            self.literal_keymap({}),
            {'==', 'is'},
        ),
        [('==', [0, 1, 2]), ('is', [2, 3, 4])],
    )
    self.assertEqual(
        group_comparison_operands(
            [('==', x0, x1), ('==', x1, x2), ('==', x2, x3), ('==', x3, x4)],
            self.literal_keymap({}),
            {'==', 'is'},
        ),
        [('==', [0, 1, 2, 3, 4])],
    )
    self.assertEqual(
        group_comparison_operands(
            [('is', x0, x1), ('==', x1, x2), ('==', x2, x3), ('==', x3, x4)],
            self.literal_keymap({}),
            {'==', 'is'},
        ),
        [('is', [0, 1]), ('==', [1, 2, 3, 4])],
    )
    self.assertEqual(
        group_comparison_operands(
            [('is', x0, x1), ('is', x1, x2), ('&lt;', x2, x3), ('==', x3, x4), ('==', x4, x5)],
            self.literal_keymap({}),
            {'==', 'is'},
        ),
        [('is', [0, 1, 2]), ('&lt;', [2, 3]), ('==', [3, 4, 5])],
    )

</t>
<t tx="ekr.20220525082936.985">def test_multiple_groups_coalescing(self) -&gt; None:
    x0 = NameExpr('x0')
    x1 = NameExpr('x1')
    x2 = NameExpr('x2')
    x3 = NameExpr('x3')
    x4 = NameExpr('x4')

    nothing_combined = [('==', [0, 1, 2]), ('&lt;', [2, 3]), ('==', [3, 4, 5])]
    everything_combined = [('==', [0, 1, 2, 3, 4, 5]), ('&lt;', [2, 3])]

    # Note: We do 'x4 == x0' at the very end!
    two_groups = [
        ('==', x0, x1), ('==', x1, x2), ('&lt;', x2, x3), ('==', x3, x4), ('==', x4, x0),
    ]
    self.assertEqual(
        group_comparison_operands(
            two_groups,
            self.literal_keymap({0: x0, 1: x1, 2: x2, 3: x3, 4: x4, 5: x0}),
            {'=='},
        ),
        everything_combined,
        "All vars are assignable, everything is combined"
    )
    self.assertEqual(
        group_comparison_operands(
            two_groups,
            self.literal_keymap({1: x1, 2: x2, 3: x3, 4: x4}),
            {'=='},
        ),
        nothing_combined,
        "x0 is unassignable, so no combining"
    )
    self.assertEqual(
        group_comparison_operands(
            two_groups,
            self.literal_keymap({0: x0, 1: x1, 3: x3, 5: x0}),
            {'=='},
        ),
        everything_combined,
        "Some vars are unassignable but x0 is, so we combine"
    )
    self.assertEqual(
        group_comparison_operands(
            two_groups,
            self.literal_keymap({0: x0, 5: x0}),
            {'=='},
        ),
        everything_combined,
        "All vars are unassignable but x0 is, so we combine"
    )

</t>
<t tx="ekr.20220525082936.986">def test_multiple_groups_different_operators(self) -&gt; None:
    x0 = NameExpr('x0')
    x1 = NameExpr('x1')
    x2 = NameExpr('x2')
    x3 = NameExpr('x3')

    groups = [('==', x0, x1), ('==', x1, x2), ('is', x2, x3), ('is', x3, x0)]
    keymap = self.literal_keymap({0: x0, 1: x1, 2: x2, 3: x3, 4: x0})
    self.assertEqual(
        group_comparison_operands(groups, keymap, {'==', 'is'}),
        [('==', [0, 1, 2]), ('is', [2, 3, 4])],
        "Different operators can never be combined"
    )

</t>
<t tx="ekr.20220525082936.987">def test_single_pair(self) -&gt; None:
    x0 = NameExpr('x0')
    x1 = NameExpr('x1')

    single_comparison = [('==', x0, x1)]
    expected_output = [('==', [0, 1])]

    assignable_combinations: List[Dict[int, NameExpr]] = [
        {}, {0: x0}, {1: x1}, {0: x0, 1: x1},
    ]
    to_group_by: List[Set[str]] = [set(), {"=="}, {"is"}]

    for combo in assignable_combinations:
        for operators in to_group_by:
            keymap = self.literal_keymap(combo)
            self.assertEqual(
                group_comparison_operands(single_comparison, keymap, operators),
                expected_output,
            )

</t>
<t tx="ekr.20220525082936.988">def test_empty_pair_list(self) -&gt; None:
    # This case should never occur in practice -- ComparisionExprs
    # always contain at least one comparison. But in case it does...

    self.assertEqual(group_comparison_operands([], {}, set()), [])
    self.assertEqual(group_comparison_operands([], {}, {'=='}), [])
</t>
<t tx="ekr.20220525082936.989">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
from unittest import TestCase, main
from multiprocessing import Process, Queue

from mypy.ipc import IPCClient, IPCServer

import pytest
import sys
import time

CONNECTION_NAME = 'dmypy-test-ipc'


@others
if __name__ == '__main__':
    main()
</t>
<t tx="ekr.20220525082936.99">def read_py_file(path: str, read: Callable[[str], bytes],
                 pyversion: Tuple[int, int]) -&gt; Optional[List[str]]:
    """Try reading a Python file as list of source lines.

    Return None if something goes wrong.
    """
    try:
        source = read(path)
    except OSError:
        return None
    else:
        try:
            source_lines = decode_python_encoding(source, pyversion).splitlines()
        except DecodeError:
            return None
        return source_lines


</t>
<t tx="ekr.20220525082936.990">def server(msg: str, q: 'Queue[str]') -&gt; None:
    server = IPCServer(CONNECTION_NAME)
    q.put(server.connection_name)
    data = b''
    while not data:
        with server:
            server.write(msg.encode())
            data = server.read()
    server.cleanup()


</t>
<t tx="ekr.20220525082936.991">class IPCTests(TestCase):
    @others
</t>
<t tx="ekr.20220525082936.992">def test_transaction_large(self) -&gt; None:
    queue: Queue[str] = Queue()
    msg = 't' * 200000  # longer than the max read size of 100_000
    p = Process(target=server, args=(msg, queue), daemon=True)
    p.start()
    connection_name = queue.get()
    with IPCClient(connection_name, timeout=1) as client:
        assert client.read() == msg.encode()
        client.write(b'test')
    queue.close()
    queue.join_thread()
    p.join()

</t>
<t tx="ekr.20220525082936.993">def test_connect_twice(self) -&gt; None:
    queue: Queue[str] = Queue()
    msg = 'this is a test message'
    p = Process(target=server, args=(msg, queue), daemon=True)
    p.start()
    connection_name = queue.get()
    with IPCClient(connection_name, timeout=1) as client:
        assert client.read() == msg.encode()
        client.write(b'')  # don't let the server hang up yet, we want to connect again.

    with IPCClient(connection_name, timeout=1) as client:
        assert client.read() == msg.encode()
        client.write(b'test')
    queue.close()
    queue.join_thread()
    p.join()
    assert p.exitcode == 0

</t>
<t tx="ekr.20220525082936.994"># Run test_connect_twice a lot, in the hopes of finding issues.
# This is really slow, so it is skipped, but can be enabled if
# needed to debug IPC issues.
@pytest.mark.skip
def test_connect_alot(self) -&gt; None:
    t0 = time.time()
    for i in range(1000):
        try:
            print(i, 'start')
            self.test_connect_twice()
        finally:
            t1 = time.time()
            print(i, t1 - t0)
            sys.stdout.flush()
            t0 = t1


</t>
<t tx="ekr.20220525082936.995">@path C:/Repos/mypy/mypy/test/
@language python
@tabwidth -4
"""Test cases for AST merge (used for fine-grained incremental checking)"""

import os
import shutil
from typing import List, Tuple, Dict, Optional

from mypy import build
from mypy.build import BuildResult
from mypy.modulefinder import BuildSource
from mypy.defaults import PYTHON3_VERSION
from mypy.errors import CompileError
from mypy.nodes import (
    Node, MypyFile, SymbolTable, SymbolTableNode, TypeInfo, Expression, Var, TypeVarExpr,
    UNBOUND_IMPORTED
)
from mypy.server.subexpr import get_subexpressions
from mypy.server.update import FineGrainedBuildManager
from mypy.strconv import StrConv
from mypy.test.config import test_temp_dir
from mypy.test.data import DataDrivenTestCase, DataSuite
from mypy.test.helpers import assert_string_arrays_equal, normalize_error_messages, parse_options
from mypy.types import TypeStrVisitor, Type
from mypy.util import short_type, IdMapper


# Which data structures to dump in a test case?
SYMTABLE = 'SYMTABLE'
TYPEINFO = ' TYPEINFO'
TYPES = 'TYPES'
AST = 'AST'


NOT_DUMPED_MODULES = (
    'builtins',
    'typing',
    'abc',
    'contextlib',
    'sys',
    'mypy_extensions',
    'typing_extensions',
    'enum',
)


@others
</t>
<t tx="ekr.20220525082936.996">class ASTMergeSuite(DataSuite):
    files = ['merge.test']

    @others
</t>
<t tx="ekr.20220525082936.997">def setup(self) -&gt; None:
    super().setup()
    self.str_conv = StrConv(show_ids=True)
    assert self.str_conv.id_mapper is not None
    self.id_mapper: IdMapper = self.str_conv.id_mapper
    self.type_str_conv = TypeStrVisitor(self.id_mapper)

</t>
<t tx="ekr.20220525082936.998">def run_case(self, testcase: DataDrivenTestCase) -&gt; None:
    name = testcase.name
    # We use the test case name to decide which data structures to dump.
    # Dumping everything would result in very verbose test cases.
    if name.endswith('_symtable'):
        kind = SYMTABLE
    elif name.endswith('_typeinfo'):
        kind = TYPEINFO
    elif name.endswith('_types'):
        kind = TYPES
    else:
        kind = AST

    main_src = '\n'.join(testcase.input)
    result = self.build(main_src, testcase)
    assert result is not None, 'cases where CompileError occurred should not be run'
    result.manager.fscache.flush()
    fine_grained_manager = FineGrainedBuildManager(result)

    a = []
    if result.errors:
        a.extend(result.errors)

    target_path = os.path.join(test_temp_dir, 'target.py')
    shutil.copy(os.path.join(test_temp_dir, 'target.py.next'), target_path)

    a.extend(self.dump(fine_grained_manager, kind))
    old_subexpr = get_subexpressions(result.manager.modules['target'])

    a.append('==&gt;')

    new_file, new_types = self.build_increment(fine_grained_manager, 'target', target_path)
    a.extend(self.dump(fine_grained_manager, kind))

    for expr in old_subexpr:
        if isinstance(expr, TypeVarExpr):
            # These are merged so we can't perform the check.
            continue
        # Verify that old AST nodes are removed from the expression type map.
        assert expr not in new_types

    if testcase.normalize_output:
        a = normalize_error_messages(a)

    assert_string_arrays_equal(
        testcase.output, a,
        f'Invalid output ({testcase.file}, line {testcase.line})')

</t>
<t tx="ekr.20220525082936.999">def build(self, source: str, testcase: DataDrivenTestCase) -&gt; Optional[BuildResult]:
    options = parse_options(source, testcase, incremental_step=1)
    options.incremental = True
    options.fine_grained_incremental = True
    options.use_builtins_fixtures = True
    options.export_types = True
    options.show_traceback = True
    options.python_version = PYTHON3_VERSION
    main_path = os.path.join(test_temp_dir, 'main')
    with open(main_path, 'w', encoding='utf8') as f:
        f.write(source)
    try:
        result = build.build(sources=[BuildSource(main_path, None, None)],
                             options=options,
                             alt_lib_path=test_temp_dir)
    except CompileError:
        # TODO: Is it okay to return None?
        return None
    return result

</t>
<t tx="ekr.20220526075331.1"></t>
<t tx="ekr.20220526075427.1">@language rest
@wrap

**Background**

- message_registry.py defines ARGUMENT_TYPE_EXPECTED as follows:
 
cffs reveal:

ARGUMENT_TYPE_EXPECTED: Final = ErrorMessage(
    "Function is missing a type annotation for one or more arguments", codes.NO_UNTYPED_DEF
)

- message_registry.ARGUMENT_TYPE_EXPECTED appears in only: TypeChecker.check_for_missing_annotations.
  Therefore, only this method needs to changed.
  
  *Note*: only TypeChecker.check_func_def calls check_for_missing_annotations.
  
- check_for_missing_annotations uses the following local helper function for all check:

    def is_unannotated_any(t: Type) -&gt; bool:
        if not isinstance(t, ProperType):
            return False
        return isinstance(t, AnyType) and t.type_of_any == TypeOfAny.unannotated
        
**The fix**

The error message comes from these lines in check_for_missing_annotations.

    if any(is_unannotated_any(t) for t in fdef.type.arg_types):
        self.fail(message_registry.ARGUMENT_TYPE_EXPECTED, fdef)
        
I'll replace this with something like:

    if any(arg_is_unannotated_and_is_not_initialized(t) for t in fdef.type.arg_types):
        self.fail(message_registry.ARGUMENT_TYPE_EXPECTED, fdef)

where arg_is_unannotated_and_is_not_initialized will be similar to is_unannotated_any
but will allow an untyped arg provided the arg has a constant initializer of known type.

@language python
</t>
<t tx="ekr.20220526075516.1"></t>
<t tx="ekr.20220526075541.1"></t>
<t tx="ekr.20220526080725.1">git clone https://github.com/python/mypy.git

cd mypy
python3 -m venv venv  # works.

activate  # c:\scripts\activate.cmd  # works.

python3 -m pip install -r test-requirements.txt  # works.
python3 -m pip install -e .  # works.

python3 -m pytest</t>
</tnodes>
</leo_file>
